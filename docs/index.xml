<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes – Documentation</title>
    <link>https://lostsquirrel.github.io/k8sDocs/docs/</link>
    <description>Recent content in Documentation on Kubernetes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 09 Oct 2020 00:00:00 +0000</lastBuildDate>
    
	  <atom:link href="https://lostsquirrel.github.io/k8sDocs/docs/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Pod 生命周期</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/</guid>
      <description>
        
        
        &lt;!--
---
title: Pod Lifecycle
content_type: concept
weight: 30
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This page describes the lifecycle of a Pod. Pods follow a defined lifecycle, starting
in the `Pending` [phase](#pod-phase), moving through `Running` if at least one
of its primary containers starts OK, and then through either the `Succeeded` or
`Failed` phases depending on whether any container in the Pod terminated in failure.

Whilst a Pod is running, the kubelet is able to restart containers to handle some
kind of faults. Within a Pod, Kubernetes tracks different container
[states](#container-states) and handles

In the Kubernetes API, Pods have both a specification and an actual status. The
status for a Pod object consists of a set of [Pod conditions](#pod-conditions).
You can also inject [custom readiness information](#pod-readiness-gate) into the
condition data for a Pod, if that is useful to your application.

Pods are only [scheduled](/k8sDocs/concepts/scheduling-eviction/) once in their lifetime.
Once a Pod is scheduled (assigned) to a Node, the Pod runs on that Node until it stops
or is [terminated](#pod-termination).


 --&gt;
&lt;p&gt;本文介经 Pod 的生命周期。 Pod 有一个既定的生命周期，开启后进行 &lt;code&gt;Pending&lt;/code&gt; &lt;a href=&#34;#pod-phase&#34;&gt;阶段&lt;/a&gt;，
当其中至少有一个主要容器正常启动后变更为 &lt;code&gt;Running&lt;/code&gt; 阶段， 如果所有容器全部正常启动则进入 &lt;code&gt;Succeeded&lt;/code&gt; 阶段，
如果有任意容器启动失败则进行 &lt;code&gt;Failed&lt;/code&gt; 阶段。&lt;/p&gt;
&lt;p&gt;当一个 Pod 在运行中， kubelet 可以在某些情况下容器挂掉后将其重启。 在 Pod 中， k8s 会跟踪和处理容器的 &lt;a href=&#34;#container-states&#34;&gt;状态&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在 k8s 的 API 对象中， Pod 对象拥有定义明细和实时状态。 Pod 对象的状态上包含一系列 &lt;a href=&#34;#pod-conditions&#34;&gt;Pod 条件&lt;/a&gt;
如果应用有需要，可以向 Pod 中加入 &lt;a href=&#34;#pod-readiness-gate&#34;&gt;自定义就绪信息&lt;/a&gt; 到条件子对象。&lt;/p&gt;
&lt;p&gt;在 Pod 的整个生命周期中只会被&lt;a href=&#34;../../../scheduling-eviction/&#34;&gt;调度&lt;/a&gt;一次，
当一个 Pod 被调度(分配)到一个节点后，就会一直运行在这个节点上，直接被停止或被&lt;a href=&#34;#pod-termination&#34;&gt;终止&lt;/a&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Pod lifetime

Like individual application containers, Pods are considered to be relatively
ephemeral (rather than durable) entities. Pods are created, assigned a unique
ID ([UID](/k8sDocs/concepts/overview/working-with-objects/names/#uids)), and scheduled
to nodes where they remain until termination (according to restart policy) or
deletion.  
If a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;节点&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; dies, the Pods scheduled to that node
are [scheduled for deletion](#pod-garbage-collection) after a timeout period.

Pods do not, by themselves, self-heal. If a Pod is scheduled to a
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;node&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; that then fails,
or if the scheduling operation itself fails, the Pod is deleted; likewise, a Pod won&#39;t
survive an eviction due to a lack of resources or Node maintenance. Kubernetes uses a
higher-level abstraction, called a
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;controller&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt;, that handles the work of
managing the relatively disposable Pod instances.

A given Pod (as defined by a UID) is never &#34;rescheduled&#34; to a different node; instead,
that Pod can be replaced by a new, near-identical Pod, with even the same name i
desired, but with a different UID.

When something is said to have the same lifetime as a Pod, such as a
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;volume&lt;span class=&#39;tooltip-text&#39;&gt;A directory containing data, accessible to the containers in a pod.&lt;/span&gt;
&lt;/a&gt;,
that means that the thing exists as long as that specific Pod (with that exact UID)
exists. If that Pod is deleted for any reason, and even if an identical replacement
is created, the related thing (a volume, in this example) is also destroyed and
created anew.

&lt;figure&gt;
    &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/images/docs/pod.svg&#34; width=&#34;50%&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Pod diagram&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


*A multi-container Pod that contains a file puller and a
web server that uses a persistent volume for shared storage between the containers.*

 --&gt;
&lt;h2 id=&#34;pod-的一生&#34;&gt;Pod 的一生&lt;/h2&gt;
&lt;p&gt;与单独使用应用容器一样, Pod 可以被认为是一个相对临时(而不是长期存在)的实体. Pod 在创建时会被分配
一个唯一的 ID(&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/names/#uids&#34;&gt;UID&lt;/a&gt;),
然后被到一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;node&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; 上,直到被终止(依照重启策略)或者被删除.
如果一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;node&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; 挂了, 这个节点上的 Pod 会在超时后
&lt;a href=&#34;#pod-garbage-collection&#34;&gt;因删除被调度&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Pod 并不能独自实现自愈. 如果 Pod 被调度的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;node&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; 挂了,
或者是调度操作本身失败, Pod 就被删除了, Pod 也不会在因为资源不足或 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;node&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; 节点被驱逐中幸存.
k8s 通过一个叫 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; 的更高层级的抽象,来处理这些相对来说是一次的的 Pod 实例的管理工作.&lt;/p&gt;
&lt;p&gt;某个 Pod(拥有特定 UID) 是永远不会被重新调度到另一个节点上; 而是被一个基本相同, 甚至可以名称也相同,但 UID 不同的 Pod 所取代.&lt;/p&gt;
&lt;p&gt;当某些对象(如 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;volume&lt;span class=&#39;tooltip-text&#39;&gt;A directory containing data, accessible to the containers in a pod.&lt;/span&gt;
&lt;/a&gt;) 被描述为与 Pod 拥有一致的生命期,
表示这些对象会与指定的 (拥有那个 UID 的) &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 同时存在,
如果那个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 因为某些原为被删除, 即便同样的代替 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 已经被创建,
相关的对象(如 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;Volume&lt;span class=&#39;tooltip-text&#39;&gt;A directory containing data, accessible to the containers in a pod.&lt;/span&gt;
&lt;/a&gt; 也会被随同 Pod 一起被销毁重建).&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
    &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/docs/pod.svg&#34;
         alt=&#34;Pod diagram&#34; width=&#34;50%&#34;/&gt; 
&lt;/figure&gt;

&lt;em&gt;有一个容器作为 web 服务，为共享数据卷的文件提供访问服务，另一个独立的容器作为 边车，负责从远程的源更新这些文件&lt;/em&gt;&lt;/p&gt;
&lt;!--  
## Pod phase

A Pod&#39;s `status` field is a
[PodStatus](/k8sDocs/reference/generated/kubernetes-api/v1.19/#podstatus-v1-core)
object, which has a `phase` field.

The phase of a Pod is a simple, high-level summary of where the Pod is in its
lifecycle. The phase is not intended to be a comprehensive rollup of observations
of container or Pod state, nor is it intended to be a comprehensive state machine.

The number and meanings of Pod phase values are tightly guarded.
Other than what is documented here, nothing should be assumed about Pods that
have a given `phase` value.

Here are the possible values for `phase`:

Value | Description
:-----|:-----------
`Pending` | The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to bescheduled as well as the time spent downloading container images over the network.
`Running` | The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.
`Succeeded` | All containers in the Pod have terminated in success, and will not be restarted.
`Failed` | All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system.
`Unknown` | For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.

If a node dies or is disconnected from the rest of the cluster, Kubernetes
applies a policy for setting the `phase` of all Pods on the lost node to Failed.
--&gt;
&lt;h2 id=&#34;pod-的人生阶段&#34;&gt;Pod 的人生阶段&lt;/h2&gt;
&lt;p&gt;在 Pod 的 &lt;code&gt;status&lt;/code&gt; 字段是一个 &lt;a href=&#34;https://kubernetes.io/k8sDocs/reference/generated/kubernetes-api/v1.19/#podstatus-v1-core&#34;&gt;PodStatus&lt;/a&gt;
对象, 上面有一个 &lt;code&gt;phase&lt;/code&gt; 字段.&lt;/p&gt;
&lt;p&gt;Pod 的人生阶段是对 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 生命周期的调度总结.
Pod 的人生阶段并不是对容器或 Pod 状态的容易理解的总结, 也不是一个容易理解的状态机.&lt;/p&gt;
&lt;p&gt;Pod 的人生阶段的数量与意义与其值都是很有限的. 除了以下对各阶段的说明, Pod 不会有其它的阶段.
以下为 &lt;code&gt;phase&lt;/code&gt; 字段的可能值:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;字段值&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Pending&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 已经被集群确立， 但是其中的一个或多个容器还没有完成配置并准备就绪。 包括 Pod 等调度的时间和从网上下载容器镜像的时间&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Running&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 已经在节点上，并且其中的所有容器已经完成创建，至少有一个容器正在运行，或在启动或重启的过程中&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Succeeded&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 中所有的容器都已经成功运行完成并终止，并且不会再被重启&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Failed&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 中所有的容器被终止，且至少有一个容器是因为失败而被终止的。 容器失败的原因可能是因返回非零而退出或被系统终止&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Unknown&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;因为某些原因导至无法获取 Pod 的状态。 这个阶段一般是因为与 Pod 所在的节点无法通信。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;如果一个节点挂了或者与集群失联， k8s 会执行一个策略，让节点上所有的 Pod 的 &lt;code&gt;phase&lt;/code&gt; 字段设置为 &lt;code&gt;Failed&lt;/code&gt;。&lt;/p&gt;
&lt;!--
## Container states

As well as the [phase](#pod-phase) of the Pod overall, Kubernetes tracks the state of
each container inside a Pod. You can use
[container lifecycle hooks](/k8sDocs/concepts/containers/container-lifecycle-hooks/) to
trigger events to run at certain points in a container&#39;s lifecycle.

Once the &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-scheduler/&#39; target=&#39;_blank&#39;&gt;scheduler&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.&lt;/span&gt;
&lt;/a&gt;
assigns a Pod to a Node, the kubelet starts creating containers for that Pod
using a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;container runtime&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt;.
There are three possible container states: `Waiting`, `Running`, and `Terminated`.

To the check state of a Pod&#39;s containers, you can use
`kubectl describe pod &lt;name-of-pod&gt;`. The output shows the state for each container
within that Pod.

Each state has a specific meaning:

 --&gt;
&lt;h2 id=&#34;容器的状态&#34;&gt;容器的状态&lt;/h2&gt;
&lt;p&gt;与 Pod 存在几个&lt;a href=&#34;#pod-phase&#34;&gt;阶段&lt;/a&gt;一个样，k8s 也会跟踪 Pod 内的容器的状态。 用户可以通过
&lt;a href=&#34;../../../containers/container-lifecycle-hooks/&#34;&gt;容器的生命周期钩子&lt;/a&gt;
来以容器生命周期事件来触发一些需要工作的运行&lt;/p&gt;
&lt;p&gt;当一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-scheduler/&#39; target=&#39;_blank&#39;&gt;kube-scheduler&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.&lt;/span&gt;
&lt;/a&gt;
将一个 Pod 调度到一个节点时， kubelet 就会通过 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;容器运行环境&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt;
为该 Pod 创建对应的容器。 容器可能存在三种状态 &lt;code&gt;Waiting&lt;/code&gt;, &lt;code&gt;Running&lt;/code&gt;, &lt;code&gt;Terminated&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;用户可以通过命令 &lt;code&gt;kubectl describe pod &amp;lt;name-of-pod&amp;gt;&lt;/code&gt; 查看 Pod 中容器的状态。
命令输出结果会包含其中所有容器的状态。
接下来介绍每一种状的具体含义&lt;/p&gt;
&lt;h3 id=&#34;container-state-waiting&#34;&gt;&lt;code&gt;Waiting&lt;/code&gt;&lt;/h3&gt;
&lt;!--
If a container is not in either the `Running` or `Terminated` state, it `Waiting`.
A container in the `Waiting` state is still running the operations it requires in
order to complete start up: for example, pulling the container image from a container
image registry, or applying &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.&lt;/span&gt;
&lt;/a&gt;
data.
When you use `kubectl` to query a Pod with a container that is `Waiting`, you also see
a Reason field to summarize why the container is in that state.
 --&gt;
&lt;p&gt;当一个容器的状不是 &lt;code&gt;Running&lt;/code&gt; 或 &lt;code&gt;Terminated&lt;/code&gt; 就是 &lt;code&gt;Waiting&lt;/code&gt;。当一个容器状态为 &lt;code&gt;Waiting&lt;/code&gt;
表示容器正在进行启动需要的前置操作: 如， 从镜像仓库拉取容器所需要的镜像， 或配置
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.&lt;/span&gt;
&lt;/a&gt; 数据。&lt;/p&gt;
&lt;p&gt;当 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;Kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt; 查询到Pod中的状是 &lt;code&gt;Waiting&lt;/code&gt;， 同时也会看到
一个 &lt;code&gt;Reason&lt;/code&gt; 字段，其值是对容器保持在该状态原因的总结&lt;/p&gt;
&lt;h3 id=&#34;container-state-running&#34;&gt;&lt;code&gt;Running&lt;/code&gt;&lt;/h3&gt;
&lt;!--
The `Running` status indicates that a container is executing without issues. If there
was a `postStart` hook configured, it has already executed and executed. When you use
`kubectl` to query a Pod with a container that is `Running`, you also see information
about when the container entered the `Running` state.

 --&gt;
&lt;p&gt;&lt;code&gt;Running&lt;/code&gt; 状态表示容器正在欢快地运行，没啥毛病。 如果配置了钩子 &lt;code&gt;postStart&lt;/code&gt;， 这个钩子的处理器也
已经执行而且成功完成。 当使用 &lt;code&gt;kubectl&lt;/code&gt; 查询容器为 &lt;code&gt;Running&lt;/code&gt; 的 Pod 时，同时可以看到容器进入
&lt;code&gt;Running&lt;/code&gt; 状态的时长。&lt;/p&gt;
&lt;h3 id=&#34;container-state-terminated&#34;&gt;&lt;code&gt;Terminated&lt;/code&gt;&lt;/h3&gt;
&lt;!--
A container in the `Terminated` state has begin execution and has then either run to
completion or has failed for some reason. When you use `kubectl` to query a Pod with
a container that is `Terminated`, you see a reason, and exit code, and the start and
finish time for that container&#39;s period of execution.

If a container has a `preStop` hook configured, that runs before the container enters
the `Terminated` state.
 --&gt;
&lt;p&gt;当一个容器状态为 &lt;code&gt;Terminated&lt;/code&gt; 时，表示任务已经执行了，要么执行完成，要么因为某些原因失败了。
当使用 &lt;code&gt;kubectl&lt;/code&gt; 查看容器为 &lt;code&gt;Terminated&lt;/code&gt; 状态的 Pod 时， 可以看到原因和退出码， 还有
容器内任务执行的开始和结束时间。&lt;/p&gt;
&lt;p&gt;如果一个容器配置了钩子 &lt;code&gt;preStop&lt;/code&gt;， 那么钩子对应的处理器会在容器进行&lt;code&gt;Terminated&lt;/code&gt; 状态之前执行。&lt;/p&gt;
&lt;!--
## Container restart policy {#restart-policy}

The `spec` of a Pod has a `restartPolicy` field with possible values Always, OnFailure,
and Never. The default value is Always.

The `restartPolicy` applies to all containers in the Pod. `restartPolicy` only
refers to restarts of the containers by the kubelet on the same node. After containers
in a Pod exit, the kubelet restarts them with an exponential back-off delay (10s, 20s,
40s, …), that is capped at five minutes. Once a container has executed with no problems
for 10 minutes without any problems, the kubelet resets the restart backoff timer for
that container.
 --&gt;
&lt;h2 id=&#34;restart-policy&#34;&gt;容器的重启策略&lt;/h2&gt;
&lt;p&gt;在 Pod 的 &lt;code&gt;spec&lt;/code&gt; 子对象上有一个 &lt;code&gt;restartPolicy&lt;/code&gt; 字段，可能的值有 &lt;code&gt;Always&lt;/code&gt;, &lt;code&gt;OnFailure&lt;/code&gt;, &lt;code&gt;Never&lt;/code&gt;
默认为 &lt;code&gt;Always&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;restartPolicy&lt;/code&gt; 适用于 Pod 中的所有容器。 &lt;code&gt;restartPolicy&lt;/code&gt; 只能让一个节点上的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;Kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt;
来重启其上的容器。 当 Pod 中的容器退出后， &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;Kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt;
会以指数延迟(10s, 20s, 40s, …)补偿机制来重启容器，延迟时间最长为 5 分钟。 当一个容器正常运行 10 分钟
后，  &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;Kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt; 才会重置该容器的补偿时钟。&lt;/p&gt;
&lt;!--
## Pod conditions

A Pod has a PodStatus, which has an array of
[PodConditions](https://kubernetes.io/k8sDocs/reference/generated/kubernetes-api/v1.19/#podcondition-v1-core)
through which the Pod has or has not passed:

* `PodScheduled`: the Pod has been scheduled to a node.
* `ContainersReady`: all containers in the Pod are ready.
* `Initialized`: all [init containers](/k8sDocs/concepts/workloads/pods/init-containers/)
  have started successfully.
* `Ready`: the Pod is able to serve requests and should be added to the load
  balancing pools of all matching Services.

Field name           | Description
:--------------------|:-----------
`type`               | Name of this Pod condition.
`status`             | Indicates whether that condition is applicable, with possible values &#34;`True`&#34;, &#34;`False`&#34;, or &#34;`Unknown`&#34;.
`lastProbeTime`      | Timestamp of when the Pod condition was last probed.
`lastTransitionTime` | Timestamp for when the Pod last transitioned from one status to another.
`reason`             | Machine-readable, UpperCamelCase text indicating the reason for the condition&#39;s last transition.
`message`            | Human-readable message indicating details about the last status transition.
 --&gt;
&lt;h2 id=&#34;pod-的就绪条件&#34;&gt;Pod 的就绪条件&lt;/h2&gt;
&lt;p&gt;在 Pod 上面有一个 &lt;code&gt;PodStatus&lt;/code&gt; 子对象，其中包含一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/generated/kubernetes-api/v1.19/#podcondition-v1-core&#34;&gt;PodConditions&lt;/a&gt;
的数组。表示这个 Pod 有没有通过这些条件， 具体如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PodScheduled&lt;/code&gt;: Pod 已经被调度到节点上.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ContainersReady&lt;/code&gt;: Pod 中所有的容器都已经就绪.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Initialized&lt;/code&gt;: 所有 &lt;a href=&#34;../init-containers/&#34;&gt;初始化容器&lt;/a&gt;
都启动成功.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Ready&lt;/code&gt;: Pod 已经能够处理请求，应该被加入对应 Service 的负载均衡池中。&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;字段名称&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;type&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;这个 Pod 条件的名称&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;status&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;表示这个条件的达成情况， 可能的值有 &amp;ldquo;&lt;code&gt;True&lt;/code&gt;&amp;rdquo;, &amp;ldquo;&lt;code&gt;False&lt;/code&gt;&amp;rdquo;, 或 &amp;ldquo;&lt;code&gt;Unknown&lt;/code&gt;&amp;rdquo;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;lastProbeTime&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;该条件上次探测的时间戳&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;lastTransitionTime&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;该条件的值最近发生变更的时间戳&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;reason&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;机器可读, 大写驼峰的文本，说明最近一次值变化的原因&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;message&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;人类可读的消息，详细说明最近一次值变化的原因&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
### Pod readiness {#pod-readiness-gate}






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [stable]&lt;/code&gt;
&lt;/div&gt;



Your application can inject extra feedback or signals into PodStatus:
_Pod readiness_. To use this, set `readinessGates` in the Pod&#39;s `spec` to
specify a list of additional conditions that the kubelet evaluates for Pod readiness.

Readiness gates are determined by the current state of `status.condition`
fields for the Pod. If Kubernetes cannot find such a condition in the
`status.conditions` field of a Pod, the status of the condition
is defaulted to &#34;`False`&#34;.

Here is an example:

```yaml
kind: Pod
...
spec:
  readinessGates:
    - conditionType: &#34;www.example.com/feature-1&#34;
status:
  conditions:
    - type: Ready                              # a built in PodCondition
      status: &#34;False&#34;
      lastProbeTime: null
      lastTransitionTime: 2018-01-01T00:00:00Z
    - type: &#34;www.example.com/feature-1&#34;        # an extra PodCondition
      status: &#34;False&#34;
      lastProbeTime: null
      lastTransitionTime: 2018-01-01T00:00:00Z
  containerStatuses:
    - containerID: docker://abcd...
      ready: true
...
```

The Pod conditions you add must have names that meet the Kubernetes [label key format](/k8sDocs/concepts/overview/working-with-objects/labels/#syntax-and-character-set).
  --&gt;
&lt;h3 id=&#34;pod-readiness-gate&#34;&gt;Pod 就绪阀&lt;/h3&gt;
&lt;p&gt;用户可以向应用中注入额外的反馈或信号到 &lt;code&gt;PodStatus&lt;/code&gt;: Pod readiness. 要使用该特性，需要在 Pod 的 &lt;code&gt;spec&lt;/code&gt; 子对象上设置 &lt;code&gt;readinessGates&lt;/code&gt;，定义追加额外的就绪条件到 k8s 检测 Pod 就绪条件列表中。&lt;/p&gt;
&lt;p&gt;就绪阀由 Pod 当前 &lt;code&gt;status.condition&lt;/code&gt; 的状态决定。 如果 k8s 不能在 Pod 的 &lt;code&gt;status.condition&lt;/code&gt; 中找到该条件， 条件的默认值为 &lt;code&gt;False&lt;/code&gt;
以下为示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
...
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;readinessGates&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;conditionType&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;www.example.com/feature-1&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;status&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;conditions&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ready                              # 内置的 PodCondition&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;status&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;False&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;lastProbeTime&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;null&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;lastTransitionTime&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;2018-01-01T00:00:00Z&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;www.example.com/feature-1&amp;#34;&lt;/span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# 外挂的 PodCondition&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;status&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;False&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;lastProbeTime&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;null&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;lastTransitionTime&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;2018-01-01T00:00:00Z&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containerStatuses&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;containerID&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;docker://abcd...&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;ready&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;用户添加的条件在命名是需要符合 k8s 的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/#syntax-and-character-set&#34;&gt;标签命名格式&lt;/a&gt;&lt;/p&gt;
&lt;!--
### Status for Pod readiness {#pod-readiness-status}

The `kubectl patch` command does not support patching object status.
To set these `status.conditions` for the pod, applications and
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/extend-kubernetes/operator/&#39; target=&#39;_blank&#39;&gt;operators&lt;span class=&#39;tooltip-text&#39;&gt;A specialized controller used to manage a custom resource&lt;/span&gt;
&lt;/a&gt; should use
the `PATCH` action.
You can use a [Kubernetes client library](/k8sDocs/reference/using-api/client-libraries/) to
write code that sets custom Pod conditions for Pod readiness.

For a Pod that uses custom conditions, that Pod is evaluated to be ready **only**
when both the following statements apply:

* All containers in the Pod are ready.
* All conditions specified in `readinessGates` are `True`.

When a Pod&#39;s containers are Ready but at least one custom condition is missing or
`False`, the kubelet sets the Pod&#39;s [condition](#pod-condition) to `ContainersReady`.
 --&gt;
&lt;h3 id=&#34;pod-readiness-status&#34;&gt;Pod 就绪条件的状态&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl patch&lt;/code&gt; 命令不支持对对象状态的修改。 想要对 Pod 的 &lt;code&gt;status.conditions&lt;/code&gt;， 应用，或其它进行 &lt;code&gt;PATCH&lt;/code&gt; 的操作
可以通过 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/using-api/client-libraries/&#34;&gt;k8s 客户端库&lt;/a&gt;
写代码的方式来自定义 Pod 就绪条件。&lt;/p&gt;
&lt;p&gt;对于使用自定义就绪条件的 Pod， 只有在达成以下条件时才能进入就绪状态:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod 中所有的容器都已经就绪&lt;/li&gt;
&lt;li&gt;所有有容器配置的 &lt;code&gt;readinessGates&lt;/code&gt; 的值都为 &lt;code&gt;True&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当一个 Pod 中所有容器已经就绪，但至少有一个自定义条件不存在或值为 &lt;code&gt;False&lt;/code&gt;，
kubelet 会将 Pod 的&lt;a href=&#34;#pod-condition&#34;&gt;就绪条件&lt;/a&gt;值设置为 &lt;code&gt;ContainersReady&lt;/code&gt;&lt;/p&gt;
&lt;!--
## Container probes

A [Probe](/k8sDocs/reference/generated/kubernetes-api/v1.19/#probe-v1-core) is a diagnostic
performed periodically by the [kubelet](/k8sDocs/admin/kubelet/)
on a Container. To perform a diagnostic,
the kubelet calls a
[Handler](/k8sDocs/reference/generated/kubernetes-api/v1.19/#handler-v1-core) implemented by
the container. There are three types of handlers:

* [ExecAction](/k8sDocs/reference/generated/kubernetes-api/v1.19/#execaction-v1-core):
  Executes a specified command inside the container. The diagnostic
  is considered successful if the command exits with a status code of 0.

* [TCPSocketAction](/k8sDocs/reference/generated/kubernetes-api/v1.19/#tcpsocketaction-v1-core):
  Performs a TCP check against the Pod&#39;s IP address on
  a specified port. The diagnostic is considered successful if the port is open.

* [HTTPGetAction](/k8sDocs/reference/generated/kubernetes-api/v1.19/#httpgetaction-v1-core):
  Performs an HTTP `GET` request against the Pod&#39;s IP
  address on a specified port and path. The diagnostic is considered successful
  if the response has a status code greater than or equal to 200 and less than 400.

Each probe has one of three results:

* `Success`: The container passed the diagnostic.
* `Failure`: The container failed the diagnostic.
* `Unknown`: The diagnostic failed, so no action should be taken.

The kubelet can optionally perform and react to three kinds of probes on running
containers:

* `livenessProbe`: Indicates whether the container is running. If
   the liveness probe fails, the kubelet kills the container, and the container
   is subjected to its [restart policy](#restart-policy). If a Container does not
   provide a liveness probe, the default state is `Success`.

* `readinessProbe`: Indicates whether the container is ready to respond to requests.
   If the readiness probe fails, the endpoints controller removes the Pod&#39;s IP
   address from the endpoints of all Services that match the Pod. The default
   state of readiness before the initial delay is `Failure`. If a Container does
   not provide a readiness probe, the default state is `Success`.

* `startupProbe`: Indicates whether the application within the container is started.
   All other probes are disabled if a startup probe is provided, until it succeeds.
   If the startup probe fails, the kubelet kills the container, and the container
   is subjected to its [restart policy](#restart-policy). If a Container does not
   provide a startup probe, the default state is `Success`.

For more information about how to set up a liveness, readiness, or startup probe,
see [Configure Liveness, Readiness and Startup Probes](/k8sDocs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).
 --&gt;
&lt;h2 id=&#34;容器探针&#34;&gt;容器探针&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#probe-v1-core&#34;&gt;探针&lt;/a&gt;
就是由 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/admin/kubelet/&#34;&gt;kubelet&lt;/a&gt; 定时对容器进行诊断操作，
诊断操作则是由 kubelet 调用一个由容器实现的
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#handler-v1-core&#34;&gt;处理器&lt;/a&gt;。
有以下三种类型处理器:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#execaction-v1-core&#34;&gt;ExecAction&lt;/a&gt;:
在容器内执行一个指定命令. 如果命令执行结束代码为 0 则表示诊断结果为成功&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#tcpsocketaction-v1-core&#34;&gt;TCPSocketAction&lt;/a&gt;:
向指定 IP 地址和端口发起 TCP 请求。 如果成功打开端口，表示诊断结果为成功&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#httpgetaction-v1-core&#34;&gt;HTTPGetAction&lt;/a&gt;:
向指定IP 地址，端口和路径发起 HTTP &lt;code&gt;GET&lt;/code&gt; 请求。如果响应码在 200 &amp;lt;= code &amp;lt; 400
表示诊断结果为成功&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上探针的结果的值可能为以下任意一个:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Success&lt;/code&gt;: 容器通过了诊断.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Failure&lt;/code&gt;: 容器没有通过诊断.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Unknown&lt;/code&gt;: 诊断过程失败，不执行任何操作.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;kubelet 可以选择是否对容器中以下探针的结果作出相应的操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;livenessProbe&lt;/code&gt;(存活探针): 指示容器是否正在运行.&lt;/p&gt;
&lt;p&gt;如果存活探针的诊断结果为未通过， 则 kubelet 会杀掉这个容器，而后容器操作则由其 &lt;a href=&#34;#restart-policy&#34;&gt;重启策略&lt;/a&gt;决定。
如果容器没有配置存活探针，则默认状态为成功。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;readinessProbe&lt;/code&gt;(就绪探针): 指示容器是否可以响应请求
如果就绪探针的诊断结果为未通过， 则 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-endpoint&#39; target=&#39;_blank&#39;&gt;Endpoints&lt;span class=&#39;tooltip-text&#39;&gt;Endpoint 跟踪匹配 Service 选择器的 Pod 的 IP 地址&lt;/span&gt;
&lt;/a&gt; 控制器
就会把该 Pod 从所有配置该 Pod 的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 的
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-endpoint&#39; target=&#39;_blank&#39;&gt;Endpoints&lt;span class=&#39;tooltip-text&#39;&gt;Endpoint 跟踪匹配 Service 选择器的 Pod 的 IP 地址&lt;/span&gt;
&lt;/a&gt; 中移出。
如果容器没有配置就绪探针，则默认状态为成功。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;startupProbe&lt;/code&gt;(启动探针): 指示容器内的应用是否启动
如果配置了启动探针，除非启动探针诊断结果为通过，否则所有其它探针都不会工作。 如果启动探针的诊断
结果为未通过，则 kubelet 会杀掉是这个容器， 而后容器操作则由其 &lt;a href=&#34;#restart-policy&#34;&gt;重启策略&lt;/a&gt;决定。
如果容器没有配置启动探针，则默认状态为成功。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;了角更多关于如何配置 存活探针，就绪探针，启动探针，见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/&#34;&gt;存活探针，就绪探针，启动探针配置&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### When should you use a liveness probe?






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.0 [stable]&lt;/code&gt;
&lt;/div&gt;



If the process in your container is able to crash on its own whenever it
encounters an issue or becomes unhealthy, you do not necessarily need a liveness
probe; the kubelet will automatically perform the correct action in accordance
with the Pod&#39;s `restartPolicy`.

If you&#39;d like your container to be killed and restarted if a probe fails, then
specify a liveness probe, and specify a `restartPolicy` of Always or OnFailure.
  --&gt;
&lt;h3 id=&#34;为啥需要用就绪readiness探针&#34;&gt;为啥需要用就绪(readiness)探针?&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.0 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;如果应用内的进程在出毛病或变得不健康是就能够自己挂掉，那么就是需要配置生存探针， kubelet 会
自动根据 Pod 的 &lt;code&gt;restartPolicy&lt;/code&gt; 正确处理这些问题。&lt;/p&gt;
&lt;p&gt;如果用户需要在探针诊断结果为未通过时杀掉容器并重启，就可以配置一个存活探针，并将 &lt;code&gt;restartPolicy&lt;/code&gt;
的值设置为 &lt;code&gt;Always&lt;/code&gt; 或 &lt;code&gt;OnFailure&lt;/code&gt;.&lt;/p&gt;
&lt;!--
### When should you use a readiness probe?






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.0 [stable]&lt;/code&gt;
&lt;/div&gt;



If you&#39;d like to start sending traffic to a Pod only when a probe succeeds,
specify a readiness probe. In this case, the readiness probe might be the same
as the liveness probe, but the existence of the readiness probe in the spec means
that the Pod will start without receiving any traffic and only start receiving
traffic after the probe starts succeeding.
If your container needs to work on loading large data, configuration files, or
migrations during startup, specify a readiness probe.

If you want your container to be able to take itself down for maintenance, you
can specify a readiness probe that checks an endpoint specific to readiness that
is different from the liveness probe.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If you just want to be able to drain requests when the Pod is deleted, you do not
necessarily need a readiness probe; on deletion, the Pod automatically puts itself
into an unready state regardless of whether the readiness probe exists.
The Pod remains in the unready state while it waits for the containers in the Pod
to stop.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;啥时候应该用就绪readiness探针&#34;&gt;啥时候应该用就绪(readiness)探针?&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.0 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;如果用户期望仅在探针诊断状态为通过时才向 Pod 调度流量，此时应该配置就绪探针。
在这种情况下，就绪探针可能与存活探针区别不大， 但就绪探针存在的意义在于 Pod 不会在就绪探针通过之前
接收到任何流量，只有在通过之后才会开始接收流量。
如果用户容器在启动时需要加载大量数据，配置文件，或迁移数据，这时就需要配置就绪探针。&lt;/p&gt;
&lt;p&gt;如果用户期望在需要维护容器可以自挂东南枝， 就可以设置一个与存活探针不同的探测接口作为就绪探针。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果用户期望仅在 Pod 被删除是不接收流量， 则不需要配置就绪探针。 在 Pod 被删除时，无论有没有就绪探针都自动将其状态
设置为未就绪状态。 Pod 在等待其中容器正常停止的过程中状态一直也都是未就绪。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### When should you use a startup probe?






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [alpha]&lt;/code&gt;
&lt;/div&gt;



Startup probes are useful for Pods that have containers that take a long time to
come into service. Rather than set a long liveness interval, you can configure
a separate configuration for probing the container as it starts up, allowing
a time longer than the liveness interval would allow.

If your container usually starts in more than
`initialDelaySeconds + failureThreshold × periodSeconds`, you should specify a
startup probe that checks the same endpoint as the liveness probe. The default for
`periodSeconds` is 30s. You should then set its `failureThreshold` high enough to
allow the container to start, without changing the default values of the liveness
probe. This helps to protect against deadlocks.
 --&gt;
&lt;h3 id=&#34;啥时候应该用启动探针&#34;&gt;啥时候应该用启动探针？&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;启动探针对于那些其中容器需要花费很长时间才能提供服务的 Pod 是相当有用的。并不需要配置一个长时间间隔的存活探针，
只需要配置一个独立的配置的探测容器的启动。而这样可以允许比存活探针时间间隔更长的时间来等待容器启动。&lt;/p&gt;
&lt;p&gt;如果用户容器通过启动时间大于 &lt;code&gt;initialDelaySeconds + failureThreshold × periodSeconds&lt;/code&gt;，
就应该配置一个与存活探针检查点相同的启动探针。 &lt;code&gt;periodSeconds&lt;/code&gt; 默认为 30秒。 所以需要设置一个
足够大的 &lt;code&gt;failureThreshold&lt;/code&gt; 值，以保证容器能够有足够的时间启动， 而不需要修改存活探针的默认配置。
这也能避免出现死锁。&lt;/p&gt;
&lt;!--
## Termination of Pods {#pod-termination}

Because Pods represent processes running on nodes in the cluster, it is important to
allow those processes to gracefully terminate when they are no longer needed (rather
than being abruptly stopped with a `KILL` signal and having no chance to clean up).

The design aim is for you to be able to request deletion and know when processes
terminate, but also be able to ensure that deletes eventually complete.
When you request deletion of a Pod, the cluster records and tracks the intended grace period
before the Pod is allowed to be forcefully killed. With that forceful shutdown tracking in
place, the &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt; attempts graceful
shutdown.

Typically, the container runtime sends a a TERM signal is sent to the main process in each
container. Once the grace period has expired, the KILL signal is sent to any remainig
processes, and the Pod is then deleted from the
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API Server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt;. If the kubelet or the
container runtime&#39;s management service is restarted while waiting for processes to terminate, the
cluster retries from the start including the full original grace period.

An example flow:

1. You use the `kubectl` tool to manually delete a specific Pod, with the default grace period
   (30 seconds).
1. The Pod in the API server is updated with the time beyond which the Pod is considered &#34;dead&#34;
   along with the grace period.  
   If you use `kubectl describe` to check on the Pod you&#39;re deleting, that Pod shows up as
   &#34;Terminating&#34;.  
   On the node where the Pod is running: as soon as the kubelet sees that a Pod has been marked
   as terminating (a graceful shutdown duration has been set), the kubelet begins the local Pod
   shutdown process.
   1. If one of the Pod&#39;s containers has defined a `preStop`
      [hook](/k8sDocs/concepts/containers/container-lifecycle-hooks/#hook-details), the kubelet
      runs that hook inside of the container. If the `preStop` hook is still running after the
      grace period expires, the kubelet requests a small, one-off grace period extension of 2
      seconds.
      &lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If the &lt;code&gt;preStop&lt;/code&gt; hook needs longer to complete than the default grace period allows,
you must modify &lt;code&gt;terminationGracePeriodSeconds&lt;/code&gt; to suit this.&lt;/div&gt;
&lt;/blockquote&gt;

   1. The kubelet triggers the container runtime to send a TERM signal to process 1 inside each
      container.
      &lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The containers in the Pod receive the TERM signal at different times and in an arbitrary
order. If the order of shutdowns matters, consider using a &lt;code&gt;preStop&lt;/code&gt; hook to synchronize.&lt;/div&gt;
&lt;/blockquote&gt;

1. At the same time as the kubelet is starting graceful shutdown, the control plane removes that
   shutting-down Pod from Endpoints (and, if enabled, EndpointSlice) objects where these represent
   a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; with a configured
   &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/&#39; target=&#39;_blank&#39;&gt;selector&lt;span class=&#39;tooltip-text&#39;&gt;允许用户基于标签过滤资源列表&lt;/span&gt;
&lt;/a&gt;.
   &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#39; target=&#39;_blank&#39;&gt;ReplicaSets&lt;span class=&#39;tooltip-text&#39;&gt;ReplicaSet 保证在任意时间点上某个 Pod 有指定数量副本在运行&lt;/span&gt;
&lt;/a&gt; and other workload resources
   no longer treat the shutting-down Pod as a valid, in-service replica. Pods that shut down slowly
   cannot continue to serve traffic as load balancers (like the service proxy) remove the Pod from
   the list of endpoints as soon as the termination grace period _begins_.
1. When the grace period expires, the kubelet triggers forcible shutdown. The container runtime sends
   `SIGKILL` to any processes still running in any container in the Pod.
   The kubelet also cleans up a hidden `pause` container if that container runtime uses one.
1. The kubelet triggers forcible removal of Pod object from the API server, by setting grace period
   to 0 (immediate deletion).  
1. The API server deletes the Pod&#39;s API object, which is then no longer visible from any client.
 --&gt;
&lt;h2 id=&#34;pod-termination&#34;&gt;Pod 的终结过程&lt;/h2&gt;
&lt;p&gt;因为 Pod 代表运行在集群节点上的一系列进程， 而要让这些进程在不需要时能够死得瞑目(而不是通过 KILL 信号突然被停止，连收尾的机会都没得)。&lt;/p&gt;
&lt;p&gt;在设计上旨在用户能够在发起删除请求并能够知晓啥时候进程终止， 但最终还要保证删除操作最终需要完成。
当用户发起删除一个 Pod 的请求， 集群会在预期的时间内跟踪和记录，如果超过这个时间则会强制终止 Pod 的进程。
在强制终止之前， kubelet 都会尝试平滑关闭。&lt;/p&gt;
&lt;p&gt;通常情况下，&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;容器运行环境&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt; 会向每个容器的主进程
发送一个 &lt;code&gt;TERM&lt;/code&gt; 信号。如果超过预期时间，再和仍然存在的进程发送 &lt;code&gt;KILL&lt;/code&gt; 信号， 然后从
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 中删除该 Pod 对象。 如果在这个等待过程中
kubelet 或 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;容器运行环境&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt; 发生重启， 集群会尝试对该删除操作
重启开启计时。&lt;/p&gt;
&lt;p&gt;以下为一个示例：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用户使用 kubectl 命令手动删除一个 Pod，使用默认的预期时间(30s).&lt;/li&gt;
&lt;li&gt;从命令执行开始到预期时间内 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 中的
Pod 对象就会更新，并标记为已经挂了。 如果使用 &lt;code&gt;kubectl describe&lt;/code&gt; 查看正在删除的 Pod，
看到它的状态应该是 &lt;code&gt;Terminating&lt;/code&gt;。 在 Pod 所在的节点上： 当 kubelet 看到 Pod 被标记为终止时(添加一个平滑关闭标记)
kubelet 就开始并本地的 Pod 的进程。
&lt;ol&gt;
&lt;li&gt;如果 Pod 中有任意容器配置了&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/containers/container-lifecycle-hooks/#hook-details&#34;&gt;钩子&lt;/a&gt; &lt;code&gt;preStop&lt;/code&gt;,
kubelet 会在对应容器中执行这个钩子。 如果在预期时间到达时 &lt;code&gt;preStop&lt;/code&gt; 钩子仍在运行，则 kubelet 一次性多给 2 秒。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果 &lt;code&gt;preStop&lt;/code&gt; 需要比默认的预期时间更长的时间，则需要设置一个合适的 &lt;code&gt;terminationGracePeriodSeconds&lt;/code&gt; 值&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;kubelet 触发 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;容器运行环境&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt; 向 Pod 中的每个容器的 1 号进程
发送 TERM 信号
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Pod 中的容器可能会以不同的顺序和时间接收到 TERM 信号， 如果需要进行有序关闭，考虑使用 &lt;code&gt;preStop&lt;/code&gt; 钩子来实现同步锁&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;在 kubelet 开始平滑关闭 Pod 的进程的同时， 控制中心将正在删除的 Pod 从 对应配置选择的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;
所代表的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-endpoint&#39; target=&#39;_blank&#39;&gt;Endpoints&lt;span class=&#39;tooltip-text&#39;&gt;Endpoint 跟踪匹配 Service 选择器的 Pod 的 IP 地址&lt;/span&gt;
&lt;/a&gt; (如果开启也可能是 EndpointSlice)中移出。
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#39; target=&#39;_blank&#39;&gt;ReplicaSet&lt;span class=&#39;tooltip-text&#39;&gt;ReplicaSet 保证在任意时间点上某个 Pod 有指定数量副本在运行&lt;/span&gt;
&lt;/a&gt; 和其它的工作负载资源都会将该Pod认作是失效的，对于那些半天关不掉又不能提供服务的Pod
负载均衡(如 service proxy)会在删除预期时间开始时就从 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-endpoint&#39; target=&#39;_blank&#39;&gt;Endpoints&lt;span class=&#39;tooltip-text&#39;&gt;Endpoint 跟踪匹配 Service 选择器的 Pod 的 IP 地址&lt;/span&gt;
&lt;/a&gt; 列表中移出。&lt;/li&gt;
&lt;li&gt;当预期时间用完后，就会触发 kubelet 强制删除。 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;容器运行环境&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt;
会向所有剩余的进程发送 &lt;code&gt;SIGKILL&lt;/code&gt; 信号。 如果容器用到了隐藏的 &lt;code&gt;pause&lt;/code&gt; 容器 kubelet 也会一起清理&lt;/li&gt;
&lt;li&gt;kubectl 通过将预期时间设置为0(立马删除)触发从 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt;
中强制删除 Pod 对象。&lt;/li&gt;
&lt;li&gt;&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 会在 Pod 对象对所有客户端不可见时，将其删除&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
### Forced Pod termination {#pod-termination-forced}

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; Forced deletions can be potentially disruptiove for some workloads and their Pods.&lt;/div&gt;
&lt;/blockquote&gt;


By default, all deletes are graceful within 30 seconds. The `kubectl delete` command supports
the `--grace-period=&lt;seconds&gt;` option which allows you to override the default and specify your
own value.

Setting the grace period to `0` forcibly and immediately deletes the Pod from the API
server. If the pod was still running on a node, that forcible deletion triggers the kubelet to
begin immediate cleanup.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You must specify an additional flag &lt;code&gt;--force&lt;/code&gt; along with &lt;code&gt;--grace-period=0&lt;/code&gt; in order to perform force deletions.&lt;/div&gt;
&lt;/blockquote&gt;


When a force deletion is performed, the API server does not wait for confirmation
from the kubelet that the Pod has been terminated on the node it was running on. It
removes the Pod in the API immediately so a new Pod can be created with the same
name. On the node, Pods that are set to terminate immediately will still be given
a small grace period before being force killed.

If you need to force-delete Pods that are part of a StatefulSet, refer to the task
documentation for
[deleting Pods from a StatefulSet](/k8sDocs/tasks/run-application/force-delete-stateful-set-pod/).
 --&gt;
&lt;h3 id=&#34;pod-termination-forced&#34;&gt;Pod 的强制删除&lt;/h3&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 强制删除存在破坏一些工作负载或其 Pod的潜在风险。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;在默认情况下，所有的删除操作的预期时间都是 30 秒，&lt;code&gt;kubectl delete&lt;/code&gt; 命令支持通过
&lt;code&gt;--grace-period=&amp;lt;seconds&amp;gt;&lt;/code&gt; 选择来自定义预期时间。&lt;/p&gt;
&lt;p&gt;将预期时间设置为 0， 会强制立马从 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 中删除 Pod 对象。
如果 Pod 仍然运行在某个节点上， 这种强制删除会触发 kubelet 开始立即清理。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 需要同时使用 &lt;code&gt;--force&lt;/code&gt; 和 &lt;code&gt;--grace-period=0&lt;/code&gt; 在能实现强制删除。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;当一个强制删除被执行时， &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 不会等待
Pod 所在节点的的 kubelet 确认 Pod 已经被终止。 只是立马删除 Pod 对象，这时可以马上创建一个同名的新 Pod
而在节点上，被设置为立马终止的 Pod 在被强制杀死前也会给予一小会时间，以期可能平滑关闭。&lt;/p&gt;
&lt;p&gt;如果用户需要强制删除一个 StatefulSet 的 Pod，
请见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/force-delete-stateful-set-pod/&#34;&gt;删除一个属于StatefulSet的Pod&lt;/a&gt;.&lt;/p&gt;
&lt;!--  
### Garbage collection of failed Pods {#pod-garbage-collection}

For failed Pods, the API objects remain in the cluster&#39;s API until a human or
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;controller&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; process
explicitly removes them.

The control plane cleans up terminated Pods (with a phase of `Succeeded` or
`Failed`), when the number of Pods exceeds the configured threshold
(determined by `terminated-pod-gc-threshold` in the kube-controller-manager).
This avoids a resource leak as Pods are created and terminated over time.
--&gt;
&lt;h3 id=&#34;pod-garbage-collection&#34;&gt;对失效 Pod 的垃圾回收&lt;/h3&gt;
&lt;p&gt;对于失效的 Pod, 其对应会存在于集群 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 中，
直至人工或 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;controller&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; 明确的删除它们&lt;/p&gt;
&lt;p&gt;控制中心清理终止的Pod (阶段的 &lt;code&gt;Succeeded&lt;/code&gt; 或 &lt;code&gt;Failed&lt;/code&gt;)， 如 Pod 的数量超过配置的阈值(由 kube-controller-manager 中的&lt;code&gt;terminated-pod-gc-threshold&lt;/code&gt;配置决定)
这会在长时间 Pod 创建和终止的过程中避免出现资源泄漏。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;实践
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/attach-handler-lifecycle-event/&#34;&gt;attaching handlers to Container lifecycle events&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这溃
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/&#34;&gt;configuring Liveness, Readiness and Startup Probes&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;了解&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/containers/container-lifecycle-hooks/&#34;&gt;container lifecycle hooks&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;更多关于 Pod / Container 状态的 API, 见 &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#podstatus-v1-core&#34;&gt;PodStatus&lt;/a&gt;,
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#containerstatus-v1-core&#34;&gt;ContainerStatus&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Service</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service/</link>
      <pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- bprashanth
title: Service
feature:
  title: Service discovery and load balancing
  description: &gt;
    No need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them.

content_type: concept
weight: 10
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
&lt;p&gt;以网络服务的方式让一个由一组
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
组成的应用能够对外提供服务的一种抽象方式&lt;/p&gt;

With Kubernetes you don&#39;t need to modify your application to use an unfamiliar service discovery mechanism.
Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods,
and can load-balance across them.
 --&gt;
&lt;p&gt;以网络服务的方式让一个由一组
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
组成的应用能够对外提供服务的一种抽象方式&lt;/p&gt;
&lt;p&gt;在使用 k8s 时并不需要修改应用来使用不熟悉的服务发现机制。 k8s 为 Pod 提供了自己的 IP 地址和
也为 Pod 集合提供单个 DNS 名称，并为其提供负载均衡。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Motivation

Kubernetes &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; are mortal.
They are born and when they die, they are not resurrected.
If you use a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#39; target=&#39;_blank&#39;&gt;Deployment&lt;span class=&#39;tooltip-text&#39;&gt;管理集群中的一个多副本应用&lt;/span&gt;
&lt;/a&gt; to run your app,
it can create and destroy Pods dynamically.

Each Pod gets its own IP address, however in a Deployment, the set of Pods
running in one moment in time could be different from
the set of Pods running that application a moment later.

This leads to a problem: if some set of Pods (call them &#34;backends&#34;) provides
functionality to other Pods (call them &#34;frontends&#34;) inside your cluster,
how do the frontends find out and keep track of which IP address to connect
to, so that the frontend can use the backend part of the workload?

Enter _Services_.
 --&gt;
&lt;h2 id=&#34;动机&#34;&gt;动机&lt;/h2&gt;
&lt;p&gt;k8s 的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 是会挂掉的。它们出生然后挂掉，
它们挂了以后就不能再重生了。 如果使用
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#39; target=&#39;_blank&#39;&gt;Deployment&lt;span class=&#39;tooltip-text&#39;&gt;管理集群中的一个多副本应用&lt;/span&gt;
&lt;/a&gt;&lt;br&gt;
来运行应用，则它会动态地创建和销毁 Pod。&lt;/p&gt;
&lt;p&gt;每个 Pod 都会有一个自己的 IP 地址， 但是在 Deployment 中，它所管理的 Pod 在这一个时间点和
另一个时间点可能是不一样的。&lt;/p&gt;
&lt;p&gt;这就会导致一个问题: 如果在集群中有一组 Pod (称作 &amp;ldquo;后端&amp;rdquo;)为另一组 Pod (称作 &amp;ldquo;前端&amp;rdquo;)提供功能，
那么前端的 Pod 怎么能一直找到后端的连接 IP 地址，然后使用后端作为工作负载呢。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Services&lt;/em&gt; 就闪亮登场了.&lt;/p&gt;
&lt;!--
## Service resources {#service-resource}

In Kubernetes, a Service is an abstraction which defines a logical set of Pods
and a policy by which to access them (sometimes this pattern is called
a micro-service). The set of Pods targeted by a Service is usually determined
by a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/&#39; target=&#39;_blank&#39;&gt;selector&lt;span class=&#39;tooltip-text&#39;&gt;允许用户基于标签过滤资源列表&lt;/span&gt;
&lt;/a&gt;
(see [below](#services-without-selectors) for why you might want a Service
_without_ a selector).

For example, consider a stateless image-processing backend which is running with
3 replicas.  Those replicas are fungible&amp;mdash;frontends do not care which backend
they use.  While the actual Pods that compose the backend set may change, the
frontend clients should not need to be aware of that, nor should they need to keep
track of the set of backends themselves.

The Service abstraction enables this decoupling.

### Cloud-native service discovery

If you&#39;re able to use Kubernetes APIs for service discovery in your application,
you can query the &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt;
for Endpoints, that get updated whenever the set of Pods in a Service changes.

For non-native applications, Kubernetes offers ways to place a network port or load
balancer in between your application and the backend Pods.

 --&gt;
&lt;h2 id=&#34;service-resource&#34;&gt;Service 资源&lt;/h2&gt;
&lt;p&gt;在 k8s 中， Service 是一个抽象概念，它定义的是逻辑组上的一组 Pod 与访问它们的策略(有时候这种模式也被称为 微服务)。
Service 所指向的是哪些 Pod 通常是由&lt;br&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/&#39; target=&#39;_blank&#39;&gt;选择器&lt;span class=&#39;tooltip-text&#39;&gt;允许用户基于标签过滤资源列表&lt;/span&gt;
&lt;/a&gt;
决定的(&lt;a href=&#34;#services-without-selectors&#34;&gt;下面&lt;/a&gt;还介绍了可能 &lt;em&gt;不需要&lt;/em&gt; 选择器的 Service).&lt;/p&gt;
&lt;p&gt;例如，假如有一个无状的图片处理后端，有3个副本在运行。 这些副本是可替代的 — 前端不关心它们
用的是哪个后台。 当组成后端的 Pod 可能发生变化， 但前端的客户端应该不能感知到，它们也不需要自己
来跟踪后端的具体成员。&lt;/p&gt;
&lt;p&gt;Service 的抽象实现了这样的解耦。&lt;/p&gt;
&lt;h3 id=&#34;云原生服务发现&#34;&gt;云原生服务发现&lt;/h3&gt;
&lt;p&gt;如果能够在应用中使用 k8s API 来实现服务发现， 可以通过
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt;
查询 Endpoint, 通过这种方式可以实时更新到 Service 的 Pod 变更。&lt;/p&gt;
&lt;p&gt;对于非原生应用， k8s 为应用与后端 Pod 之间通信提供了网络端口或负载均衡等方式。&lt;/p&gt;
&lt;!--
## Defining a Service

A Service in Kubernetes is a REST object, similar to a Pod.  Like all of the
REST objects, you can `POST` a Service definition to the API server to create
a new instance.
The name of a Service object must be a valid
[DNS label name](/docs/concepts/overview/working-with-objects/names#dns-label-names).

For example, suppose you have a set of Pods that each listen on TCP port 9376
and carry a label `app=MyApp`:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
```

This specification creates a new Service object named &#34;my-service&#34;, which
targets TCP port 9376 on any Pod with the `app=MyApp` label.

Kubernetes assigns this Service an IP address (sometimes called the &#34;cluster IP&#34;),
which is used by the Service proxies
(see [Virtual IPs and service proxies](#virtual-ips-and-service-proxies) below).

The controller for the Service selector continuously scans for Pods that
match its selector, and then POSTs any updates to an Endpoint object
also named &#34;my-service&#34;.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A Service can map &lt;em&gt;any&lt;/em&gt; incoming &lt;code&gt;port&lt;/code&gt; to a &lt;code&gt;targetPort&lt;/code&gt;. By default and
for convenience, the &lt;code&gt;targetPort&lt;/code&gt; is set to the same value as the &lt;code&gt;port&lt;/code&gt;
field.&lt;/div&gt;
&lt;/blockquote&gt;


Port definitions in Pods have names, and you can reference these names in the
`targetPort` attribute of a Service. This works even if there is a mixture
of Pods in the Service using a single configured name, with the same network
protocol available via different port numbers.
This offers a lot of flexibility for deploying and evolving your Services.
For example, you can change the port numbers that Pods expose in the next
version of your backend software, without breaking clients.

The default protocol for Services is TCP; you can also use any other
[supported protocol](#protocol-support).

As many Services need to expose more than one port, Kubernetes supports multiple
port definitions on a Service object.
Each port definition can have the same `protocol`, or a different one.
 --&gt;
&lt;h2 id=&#34;service-定义&#34;&gt;Service 定义&lt;/h2&gt;
&lt;p&gt;Service 在 k8s 中是一个 &lt;code&gt;REST&lt;/code&gt; 对象， 与 Pod 类似。 与其它所有 &lt;code&gt;REST&lt;/code&gt; 对象一样，
可以通过 &lt;code&gt;POST&lt;/code&gt; 请求将 Service 定义发送到 api-server 来创建一个新的实例。
Service 的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/names#dns-label-names&#34;&gt;DNS 标签名称&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;例如， 假如有一组 Pod， 每个 Pod 监听的端口都是 &lt;code&gt;9376&lt;/code&gt;， 都打着一个标签为 &lt;code&gt;app=MyApp&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上面的配置定义了一个 Service 对象，名字叫 &amp;ldquo;my-service&amp;rdquo;， 指向所有 TCP 端口为 &lt;code&gt;9376&lt;/code&gt;， 带有
&lt;code&gt;app=MyApp&lt;/code&gt; 标签的 Pod。&lt;/p&gt;
&lt;p&gt;k8s 会为 Service 分配一个 IP 地址(有时称为 &amp;ldquo;集群IP (cluster IP)&amp;quot;), 这个 IP 地址会被
Service 代理使用。
(见下面的 &lt;a href=&#34;#virtual-ips-and-service-proxies&#34;&gt;虚拟IP 和 service 代理&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Service 选择器的控制器会持续扫描匹配其选择器的 Pod，然后把这些变更以 POST 请求方式发送到一个
叫 &amp;ldquo;my-service&amp;rdquo; 的 Endpoint 对象。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Service 可以映射 &lt;em&gt;任意&lt;/em&gt; 输入 &lt;code&gt;port&lt;/code&gt; 到 &lt;code&gt;targetPort&lt;/code&gt;。 默认情况和为了方便， &lt;code&gt;targetPort&lt;/code&gt;
会设置与 &lt;code&gt;port&lt;/code&gt; 字段相同的值。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;Pod 中的 Port 定义是有名字的， 这个名字可以在 Service &lt;code&gt;targetPort&lt;/code&gt; 属性上引用。
这种方式甚至可以用在当 Service 中使用同一个配置名称的不同 Pod，使用相同的网络协议，不同的端口
这个特性为 Service 的部署和演进提供了很高的灵活性。
例如， 用户可以修改用于下一版后端软件 Pod 暴露的端口，而不影响客户端。&lt;/p&gt;
&lt;p&gt;Services 默认协议为 TCP; 也可以使用其它&lt;a href=&#34;#protocol-support&#34;&gt;支持的协议&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;当许多的服务需要显露不止一个端口， k8s 支持在一个 Service 对象上定义多个端口。 每个端口定义
可以使用同样的 协议(&lt;code&gt;protocol&lt;/code&gt;), 也可以使用不同的.&lt;/p&gt;
&lt;h3 id=&#34;无标签选择器-service&#34;&gt;无标签选择器 Service&lt;/h3&gt;
&lt;p&gt;Service 最常见的用户就是作为 k8s Pod 的入口， 但它也可以作为其它类型的后端的抽象入口。
例如:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在生产环境使用的集群外的数库，但是在测试环境用的是内部的数据。&lt;/li&gt;
&lt;li&gt;想要将 Service 集群中另一个命名空间中的 Service 或另一个集群的服务。&lt;/li&gt;
&lt;li&gt;在迁移工作负载到 k8s 时，为了评估是否可行，先使用一部分后端服务在 k8s 中。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In any of these scenarios you can define a Service &lt;em&gt;without&lt;/em&gt; a Pod selector.
For example:
在以上的任意一种场景中都需要定义 &lt;em&gt;没有&lt;/em&gt; Pod 选择器的 Service。
例如:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;因为这些 Service 没有选择器，所以对象的 Endpoint 也 &lt;em&gt;不会&lt;/em&gt; 自动创建。 可以通过手动创建 Endpoint
对象的方式将 Service 映射到实际运行的网络地址和端口。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Endpoints&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;subsets&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;addresses&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;ip&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;192.0.2.42&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Endpoint 对象的名称必以是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;Endpoint 对象所用的 IP &lt;em&gt;必须不能&lt;/em&gt; 是: 回环地址 (127.0.0.0/8 IPv4, ::1/128 IPv6)，或
链路本地(link-local) (169.254.0.0/16 和 224.0.0.0/24  IPv4, fe80::/64 IPv6).&lt;/p&gt;
&lt;p&gt;Endpoint IP 地址也不能是其它 k8s Service 的集群 IP， 因为
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/kube-proxy/&#39; target=&#39;_blank&#39;&gt;kube-proxy&lt;span class=&#39;tooltip-text&#39;&gt;kube-proxy is a network proxy that runs on each node in the cluster.&lt;/span&gt;
&lt;/a&gt;
不支持将虚拟IP作为目的地址。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;访问无选择器的 Service 与有选择器的 Service 是一样的。在上面的例子中， 流量会路由到YAML定义中
唯一的 Endpoint &lt;code&gt;192.0.2.42:9376&lt;/code&gt; (TCP)&lt;/p&gt;
&lt;p&gt;一个 ExternalName Service 是 Service 中的一种特殊情景， 它没有选择器而是 DNS 名称。
更多信息见本文下面的 &lt;a href=&#34;#externalname&#34;&gt;ExternalName&lt;/a&gt;。&lt;/p&gt;
&lt;!--
### EndpointSlices





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;



EndpointSlices are an API resource that can provide a more scalable alternative
to Endpoints. Although conceptually quite similar to Endpoints, EndpointSlices
allow for distributing network endpoints across multiple resources. By default,
an EndpointSlice is considered &#34;full&#34; once it reaches 100 endpoints, at which
point additional EndpointSlices will be created to store any additional
endpoints.

EndpointSlices provide additional attributes and functionality which is
described in detail in [EndpointSlices](/docs/concepts/services-networking/endpoint-slices/).
 --&gt;
&lt;h3 id=&#34;endpointslices&#34;&gt;EndpointSlices&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;EndpointSlice 是一种可比 Endpoint 提供更新好伸缩性替代方案的 API 资源。尽管在概念与 Endpoint
很相近， EndpointSlice 允许对铆中资源的网络末端进行分发. 默认情况下当一个 EndpointSlice
的网络末端数量达到 100 时就认为是 &amp;ldquo;满了&amp;rdquo;, 这时候就会创建新的 EndpointSlice 来存储更多的网络末端。&lt;/p&gt;
&lt;p&gt;更多 EndpointSlice 提供的属性和功能请见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/endpoint-slices/&#34;&gt;EndpointSlices&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### Application protocol






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;



The AppProtocol field provides a way to specify an application protocol to be
used for each Service port. The value of this field is mirrored by corresponding
Endpoints and EndpointSlice resources.
--&gt;
&lt;h3 id=&#34;应用协议&#34;&gt;应用协议&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;code&gt;AppProtocol&lt;/code&gt; 字段提供了指定每个 Service 端口对应的应用协议的一种方式。
这个字段是对 Endpoint 和 EndpointSlice 对应字段的镜像。&lt;/p&gt;
&lt;!--
## Virtual IPs and service proxies

Every node in a Kubernetes cluster runs a `kube-proxy`. `kube-proxy` is
responsible for implementing a form of virtual IP for `Services` of type other
than [`ExternalName`](#externalname).
--&gt;
&lt;h2 id=&#34;虚拟-ip-和-service-代理&#34;&gt;虚拟 IP 和 Service 代理&lt;/h2&gt;
&lt;p&gt;Every node in a Kubernetes cluster runs a &lt;code&gt;kube-proxy&lt;/code&gt;. &lt;code&gt;kube-proxy&lt;/code&gt; is
responsible for implementing a form of virtual IP for &lt;code&gt;Services&lt;/code&gt; of type other
than &lt;a href=&#34;#externalname&#34;&gt;&lt;code&gt;ExternalName&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;k8s 集群中的每一个节点上都运行了 &lt;code&gt;kube-proxy&lt;/code&gt;， &lt;code&gt;kube-proxy&lt;/code&gt; 负责实现除了
&lt;a href=&#34;#externalname&#34;&gt;&lt;code&gt;ExternalName&lt;/code&gt;&lt;/a&gt;
外其它类型的 &lt;code&gt;Services&lt;/code&gt; 虚拟 IP 的实现方式&lt;/p&gt;
&lt;!--
### Why not use round-robin DNS?

A question that pops up every now and then is why Kubernetes relies on
proxying to forward inbound traffic to backends. What about other
approaches? For example, would it be possible to configure DNS records that
have multiple A values (or AAAA for IPv6), and rely on round-robin name
resolution?

There are a few reasons for using proxying for Services:

 * There is a long history of DNS implementations not respecting record TTLs,
   and caching the results of name lookups after they should have expired.
 * Some apps do DNS lookups only once and cache the results indefinitely.
 * Even if apps and libraries did proper re-resolution, the low or zero TTLs
   on the DNS records could impose a high load on DNS that then becomes
   difficult to manage.
 --&gt;
&lt;h3 id=&#34;为嘛不用-round-robin-dns-&#34;&gt;为嘛不用 round-robin DNS ?&lt;/h3&gt;
&lt;p&gt;一个时不时被提起的问题就是为啥 k8s 信赖于代理来转发入站流量到后端。 为啥不用其它的方式？
例如，有没有可能通过配置包含多个 A 值的 DNS 记录(IPv6 用 AAAA)， 然后通过轮询域名解析结果。
以下为 Service 使用代理的几个原因:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DNS 实现不遵循记录的 TTL有长久的历史， 并且在结果过期后继续使用缓存查询结果&lt;/li&gt;
&lt;li&gt;有些应用一次查询 DNS 后永远使用缓存的查询结果&lt;/li&gt;
&lt;li&gt;即便每个应用规范地来查 DNS， 但是 DNS 记录上的 TTL 的值很低或为0 会导致 DNS 的负载很高，并且变得难于管理&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### User space proxy mode {#proxy-mode-userspace}

In this mode, kube-proxy watches the Kubernetes master for the addition and
removal of Service and Endpoint objects. For each Service it opens a
port (randomly chosen) on the local node.  Any connections to this &#34;proxy port&#34;
are
proxied to one of the Service&#39;s backend Pods (as reported via
Endpoints). kube-proxy takes the `SessionAffinity` setting of the Service into
account when deciding which backend Pod to use.

Lastly, the user-space proxy installs iptables rules which capture traffic to
the Service&#39;s `clusterIP` (which is virtual) and `port`. The rules
redirect that traffic to the proxy port which proxies the backend Pod.

By default, kube-proxy in userspace mode chooses a backend via a round-robin algorithm.

![Services overview diagram for userspace proxy](/images/docs/services-userspace-overview.svg)
 --&gt;
&lt;h3 id=&#34;proxy-mode-userspace&#34;&gt;user-space 代理模式&lt;/h3&gt;
&lt;p&gt;在这种模式下， kube-proxy 监听 k8s 主控节点上 Service 和 Endpoint 对象的添加和删除。
对每一个 Service 它会在本地节点打开一个端口(随机选择)。任意一个连接到该 &amp;ldquo;代理端口&amp;quot;的流量都会代理
到 Service 后端 Pod(由 Endpoint 报告) 中的一个上。 kube-proxy 使用 Service 的 &lt;code&gt;SessionAffinity&lt;/code&gt;
设置为决定使用哪个后端 Pod。&lt;/p&gt;
&lt;p&gt;最后 user-space 代理会添加相应的 iptables 规则将捕获到 Service &lt;code&gt;clusterIP&lt;/code&gt;(是一个虚拟IP) 和 &lt;code&gt;port&lt;/code&gt; 的流量
然后这些规则将这些流量重定向到刚提供的会代理到后端 Pod 的代理端口上。&lt;/p&gt;
&lt;p&gt;默认情况下， kube-proxy 在使用 user-space 代理模式是使用轮询算法选择后端的 Pod。
&lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/docs/services-userspace-overview.svg&#34; alt=&#34;Services overview diagram for userspace proxy&#34;&gt;&lt;/p&gt;
&lt;!--
### `iptables` proxy mode {#proxy-mode-iptables}

In this mode, kube-proxy watches the Kubernetes control plane for the addition and
removal of Service and Endpoint objects. For each Service, it installs
iptables rules, which capture traffic to the Service&#39;s `clusterIP` and `port`,
and redirect that traffic to one of the Service&#39;s
backend sets.  For each Endpoint object, it installs iptables rules which
select a backend Pod.

By default, kube-proxy in iptables mode chooses a backend at random.

Using iptables to handle traffic has a lower system overhead, because traffic
is handled by Linux netfilter without the need to switch between userspace and the
kernel space. This approach is also likely to be more reliable.

If kube-proxy is running in iptables mode and the first Pod that&#39;s selected
does not respond, the connection fails. This is different from userspace
mode: in that scenario, kube-proxy would detect that the connection to the first
Pod had failed and would automatically retry with a different backend Pod.

You can use Pod [readiness probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes)
to verify that backend Pods are working OK, so that kube-proxy in iptables mode
only sees backends that test out as healthy. Doing this means you avoid
having traffic sent via kube-proxy to a Pod that&#39;s known to have failed.

![Services overview diagram for iptables proxy](/k8sDocs/images/docs/services-iptables-overview.svg)
--&gt;
&lt;h3 id=&#34;proxy-mode-iptables&#34;&gt;&lt;code&gt;iptables&lt;/code&gt; 代理模式&lt;/h3&gt;
&lt;p&gt;在这种模式下， kube-proxy 监听 k8s 主控节点上 Service 和 Endpoint 对象的添加和删除。
对于每个 Service， 会添加一个 iptables 规则， 这个规则会捕获所有目标是 Service 的 &lt;code&gt;clusterIP&lt;/code&gt; 和 &lt;code&gt;port&lt;/code&gt;
的流量并将其重定向到 Service 后端 Pod 中的一个上。 对于每个 Endpoint 也会添加一个 iptables
规则，用来选择后端的 Pod。&lt;/p&gt;
&lt;p&gt;默认情况下，kube-proxy 在 &lt;code&gt;iptables&lt;/code&gt; 代理模式下，随机选择后端 Pod。&lt;/p&gt;
&lt;p&gt;使用 iptables 来处理流量系统开销更低， 因为流量由 Linux netfilter处理，而不需要在 userspace 和
kernelspace 之间来回切换。 这种方式也可能更加可靠。&lt;/p&gt;
&lt;p&gt;如果 kube-proxy 使用的 iptables 模式， 如果选择的第一个 Pod 没有响应，这个连接就失败了。
这与 userspace 模式不同： 在这种场景下， kube-proxy 会检测到第一个 Pod 的连接失败了，会自动
地尝试其它的后端 Pod。&lt;/p&gt;
&lt;p&gt;可以使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/pod-lifecycle/#container-probes&#34;&gt;就绪探针&lt;/a&gt;
来验证后端的 Pod 是在正常工作的， 因此 kube-proxy 在 iptables 模式下只会看到检测结果为健康的
后端 Pod。 这么做的意义在于避免了通过 kube-proxy 将流量发送到已经知道挂了的 Pod 上。
&lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/docs/services-iptables-overview.svg&#34; alt=&#34;Services overview diagram for iptables proxy&#34;&gt;&lt;/p&gt;
&lt;!--
### IPVS proxy mode {#proxy-mode-ipvs}






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [stable]&lt;/code&gt;
&lt;/div&gt;



In `ipvs` mode, kube-proxy watches Kubernetes Services and Endpoints,
calls `netlink` interface to create IPVS rules accordingly and synchronizes
IPVS rules with Kubernetes Services and Endpoints periodically.
This control loop ensures that IPVS status matches the desired
state.
When accessing a Service, IPVS directs traffic to one of the backend Pods.

The IPVS proxy mode is based on netfilter hook function that is similar to
iptables mode, but uses a hash table as the underlying data structure and works
in the kernel space.
That means kube-proxy in IPVS mode redirects traffic with lower latency than
kube-proxy in iptables mode, with much better performance when synchronising
proxy rules. Compared to the other proxy modes, IPVS mode also supports a
higher throughput of network traffic.

IPVS provides more options for balancing traffic to backend Pods;
these are:

- `rr`: round-robin
- `lc`: least connection (smallest number of open connections)
- `dh`: destination hashing
- `sh`: source hashing
- `sed`: shortest expected delay
- `nq`: never queue

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;To run kube-proxy in IPVS mode, you must make IPVS available on
the node before starting kube-proxy.&lt;/p&gt;
&lt;p&gt;When kube-proxy starts in IPVS proxy mode, it verifies whether IPVS
kernel modules are available. If the IPVS kernel modules are not detected, then kube-proxy
falls back to running in iptables proxy mode.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;


![Services overview diagram for IPVS proxy](/k8sDocs/images/docs/services-ipvs-overview.svg)

In these proxy models, the traffic bound for the Service&#39;s IP:Port is
proxied to an appropriate backend without the clients knowing anything
about Kubernetes or Services or Pods.

If you want to make sure that connections from a particular client
are passed to the same Pod each time, you can select the session affinity based
on the client&#39;s IP addresses by setting `service.spec.sessionAffinity` to &#34;ClientIP&#34;
(the default is &#34;None&#34;).
You can also set the maximum session sticky time by setting
`service.spec.sessionAffinityConfig.clientIP.timeoutSeconds` appropriately.
(the default value is 10800, which works out to be 3 hours).
 --&gt;
&lt;h3 id=&#34;proxy-mode-ipvs&#34;&gt;IPVS 代理&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;在 &lt;code&gt;ipvs&lt;/code&gt; 模式下， kube-proxy 监听 k8s Services 和 Endpoint,
调用 &lt;code&gt;netlink&lt;/code&gt; 接口来创建 IPVS 规则， 并定时根据 k8s Services 和 Endpoint 更新 IPVS 规则。
这个控制回环确保 IPVS 的状态与期望状态一至。 当访问一个 Service 时， IPVS 重定向流量到
后端 Pod 中的一个上。&lt;/p&gt;
&lt;p&gt;IPVS 代理模式基于 netfilter 钩子功能，与 iptables 类似， 但底层使用的数据结构是一个哈希表
并且是在内核空间中工作的。
也就是 kube-proxy 在 IPVS 模式下， 重定向流量会比 iptables 模式有更低的延迟，在同步代理
规则时也会有更好的恨不能。 与其它的代理模式相比， IPVS 模式也支持更高吞吐量的网络流量。&lt;/p&gt;
&lt;p&gt;IPVS 还提供了更多到后端 Pod 的负载均衡选择；
具体如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;rr&lt;/code&gt;: 轮询&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lc&lt;/code&gt;: 最少连接 (打开连接数最小的)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dh&lt;/code&gt;: 目标哈希&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sh&lt;/code&gt;: 源哈希&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sed&lt;/code&gt;: 最短期望延迟 (shortest expected delay)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nq&lt;/code&gt;: 无须队列等待 (never queue)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;要让 kube-proxy 以 IPVS 运行，必须要在 kube-proxy 启动之前让 IPVS 在节点上是可用的。&lt;/p&gt;
&lt;p&gt;当 kube-proxy 以 IPVS 代理模式启动时， 会检测 IPVS 内核模块是否可用。 如 IPVS 内核模块没有检测到，
则 kube-proxy 会回退以 iptables 模式运行。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/docs/services-ipvs-overview.svg&#34; alt=&#34;Services overview diagram for IPVS proxy&#34;&gt;&lt;/p&gt;
&lt;p&gt;In these proxy models, the traffic bound for the Service&amp;rsquo;s IP:Port is
proxied to an appropriate backend without the clients knowing anything
about Kubernetes or Services or Pods.
在使用这种代理模式时， 访问 Service IP:Port 的流量被代理到适当的后端时，客户端不会感知到
k8s 或 Service 或 Pod 这些的存在。&lt;/p&gt;
&lt;p&gt;如果需要保证一个特定客户端的连接每次都要转发到同一个 Pod 上面， 可能设置
&lt;code&gt;service.spec.sessionAffinity&lt;/code&gt; 为 &amp;ldquo;ClientIP&amp;rdquo; (默认为 &amp;ldquo;None&amp;rdquo;) 来选择基于客户端IP的会话亲和性(session affinity).
也可以设置基于时间的会话黏性，为 &lt;code&gt;service.spec.sessionAffinityConfig.clientIP.timeoutSeconds&lt;/code&gt;
设置一个合适的值。 (默认值为 10800， 也就是 3 个小时)&lt;/p&gt;
&lt;!--
## Multi-Port Services

For some Services, you need to expose more than one port.
Kubernetes lets you configure multiple port definitions on a Service object.
When using multiple ports for a Service, you must give all of your ports names
so that these are unambiguous.
For example:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 9376
    - name: https
      protocol: TCP
      port: 443
      targetPort: 9377
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;As with Kubernetes &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/names&#39; target=&#39;_blank&#39;&gt;names&lt;span class=&#39;tooltip-text&#39;&gt;A client-provided string that refers to an object in a resource URL, such as /api/v1/pods/some-name.&lt;/span&gt;
&lt;/a&gt; in general, names for ports
must only contain lowercase alphanumeric characters and &lt;code&gt;-&lt;/code&gt;. Port names must
also start and end with an alphanumeric character.&lt;/p&gt;
&lt;p&gt;For example, the names &lt;code&gt;123-abc&lt;/code&gt; and &lt;code&gt;web&lt;/code&gt; are valid, but &lt;code&gt;123_abc&lt;/code&gt; and &lt;code&gt;-web&lt;/code&gt; are not.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;多端口的-service&#34;&gt;多端口的 Service&lt;/h2&gt;
&lt;p&gt;对于有些 Service 需要显露多于一个端口， k8s 允许用户在 Service 对象上定义多个端口。
当在 Service 上使用多个端口时，必须要给所有的端口配置名字，这样才便于区分。
例如:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;443&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9377&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;依照 k8s 通用 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/names&#39; target=&#39;_blank&#39;&gt;Name&lt;span class=&#39;tooltip-text&#39;&gt;A client-provided string that refers to an object in a resource URL, such as /api/v1/pods/some-name.&lt;/span&gt;
&lt;/a&gt;， 端口的名称只能包含小写字母，数字，和 中划线(&lt;code&gt;-&lt;/code&gt;)
且端口名只能以字母数字开始和结尾。&lt;/p&gt;
&lt;p&gt;例如， &lt;code&gt;123-abc&lt;/code&gt; 和 &lt;code&gt;web&lt;/code&gt; 是有效的，&lt;code&gt;123_abc&lt;/code&gt; 和 &lt;code&gt;-web&lt;/code&gt; 是无效的&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Choosing your own IP address

You can specify your own cluster IP address as part of a `Service` creation
request.  To do this, set the `.spec.clusterIP` field. For example, if you
already have an existing DNS entry that you wish to reuse, or legacy systems
that are configured for a specific IP address and difficult to re-configure.

The IP address that you choose must be a valid IPv4 or IPv6 address from within the
`service-cluster-ip-range` CIDR range that is configured for the API server.
If you try to create a Service with an invalid clusterIP address value, the API
server will return a 422 HTTP status code to indicate that there&#39;s a problem.

--&gt;
&lt;h2 id=&#34;选择自己的-ip-地址&#34;&gt;选择自己的 IP 地址&lt;/h2&gt;
&lt;p&gt;在创建 &lt;code&gt;Service&lt;/code&gt; 的时候可以通过设置 &lt;code&gt;.spec.clusterIP&lt;/code&gt; 字段指定自己的集群IP地址。
例如， 希望复用已经存在的 DNS 记录，或者一个已经设置了IP 然后很难重新配置的旧系统。&lt;/p&gt;
&lt;p&gt;选择设置的IP 必须要是 api-server 上配置的 &lt;code&gt;service-cluster-ip-range&lt;/code&gt; CIDR 范围内有效的
IPv4 或 IPv6 地址。 如果尝试使用一个无效的集群IP地址，api-server 会返回一个 422 的 HTTP
状态码，表示配置有问题&lt;/p&gt;
&lt;!--
## Discovering services

Kubernetes supports 2 primary modes of finding a Service - environment
variables and DNS.
 --&gt;
&lt;h2 id=&#34;service-查找&#34;&gt;Service 查找&lt;/h2&gt;
&lt;p&gt;k8s 主要支持 2 种查找一个 Service的方式: 环境变量 和 DNS&lt;/p&gt;
&lt;!--
### Environment variables

When a Pod is run on a Node, the kubelet adds a set of environment variables
for each active Service.  It supports both [Docker links
compatible](https://docs.docker.com/userguide/dockerlinks/) variables (see
[makeLinkVariables](https://releases.k8s.io/master/pkg/kubelet/envvars/envvars.go#L49))
and simpler `{SVCNAME}_SERVICE_HOST` and `{SVCNAME}_SERVICE_PORT` variables,
where the Service name is upper-cased and dashes are converted to underscores.

For example, the Service `&#34;redis-master&#34;` which exposes TCP port 6379 and has been
allocated cluster IP address 10.0.0.11, produces the following environment
variables:

```shell
REDIS_MASTER_SERVICE_HOST=10.0.0.11
REDIS_MASTER_SERVICE_PORT=6379
REDIS_MASTER_PORT=tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP_PROTO=tcp
REDIS_MASTER_PORT_6379_TCP_PORT=6379
REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;When you have a Pod that needs to access a Service, and you are using
the environment variable method to publish the port and cluster IP to the client
Pods, you must create the Service &lt;em&gt;before&lt;/em&gt; the client Pods come into existence.
Otherwise, those client Pods won&amp;rsquo;t have their environment variables populated.&lt;/p&gt;
&lt;p&gt;If you only use DNS to discover the cluster IP for a Service, you don&amp;rsquo;t need to
worry about this ordering issue.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

--&gt;
&lt;h3 id=&#34;环境变量&#34;&gt;环境变量&lt;/h3&gt;
&lt;p&gt;当一个 Pod 运行到一个节点时， kubelet 会把每个活跃的 Service  作为环境变量添加到 Pod 中。
它支持
&lt;a href=&#34;https://docs.docker.com/userguide/dockerlinks/&#34;&gt;Docker 连接兼容&lt;/a&gt;
的变量
(见 &lt;a href=&#34;https://releases.k8s.io/master/pkg/kubelet/envvars/envvars.go#L49&#34;&gt;makeLinkVariables&lt;/a&gt;)
和简单些的 &lt;code&gt;{SVCNAME}_SERVICE_HOST&lt;/code&gt; 和 &lt;code&gt;{SVCNAME}_SERVICE_PORT&lt;/code&gt; 变量，其中 Service
的名称为大写，中划线会被转化为下划线。&lt;/p&gt;
&lt;p&gt;例如， 一个叫 &lt;code&gt;&amp;quot;redis-master&amp;quot;&lt;/code&gt; 的 Service， 暴露的端口是 TCP &lt;code&gt;6379&lt;/code&gt;， 分配的集群IP地址为
&lt;code&gt;10.0.0.11&lt;/code&gt;， 就会产生如下环境变量:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;REDIS_MASTER_SERVICE_HOST&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;10.0.0.11
REDIS_MASTER_SERVICE_PORT&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6379&lt;/span&gt;
REDIS_MASTER_PORT&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP_PROTO&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tcp
REDIS_MASTER_PORT_6379_TCP_PORT&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6379&lt;/span&gt;
REDIS_MASTER_PORT_6379_TCP_ADDR&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;10.0.0.11
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;当有一个 Pod 需要要访问一个 Service， 并且是使用环境变量的方式将端口和集群IP传递给客户端 Pod 的，
那么 Service 必须要在客户端 Pod 创建 &lt;em&gt;之前&lt;/em&gt; 就要存在。 否则客户端 Pod 中就不会加入它对应的环境变量。&lt;/p&gt;
&lt;p&gt;如果只使用 DNS 为查找 Service 的集群IP，则不需要担心这个顺序问题&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### DNS

You can (and almost always should) set up a DNS service for your Kubernetes
cluster using an [add-on](/docs/concepts/cluster-administration/addons/).

A cluster-aware DNS server, such as CoreDNS, watches the Kubernetes API for new
Services and creates a set of DNS records for each one.  If DNS has been enabled
throughout your cluster then all Pods should automatically be able to resolve
Services by their DNS name.

For example, if you have a Service called `&#34;my-service&#34;` in a Kubernetes
Namespace `&#34;my-ns&#34;`, the control plane and the DNS Service acting together
create a DNS record for `&#34;my-service.my-ns&#34;`. Pods in the `&#34;my-ns&#34;` Namespace
should be able to find it by simply doing a name lookup for `my-service`
(`&#34;my-service.my-ns&#34;` would also work).

Pods in other Namespaces must qualify the name as `my-service.my-ns`. These names
will resolve to the cluster IP assigned for the Service.

Kubernetes also supports DNS SRV (Service) records for named ports.  If the
`&#34;my-service.my-ns&#34;` Service has a port named `&#34;http&#34;` with the protocol set to
`TCP`, you can do a DNS SRV query for `_http._tcp.my-service.my-ns` to discover
the port number for `&#34;http&#34;`, as well as the IP address.

The Kubernetes DNS server is the only way to access `ExternalName` Services.
You can find more information about `ExternalName` resolution in
[DNS Pods and Services](/docs/concepts/services-networking/dns-pod-service/).
--&gt;
&lt;h3 id=&#34;dns&#34;&gt;DNS&lt;/h3&gt;
&lt;p&gt;用户可以(并且几乎绝大多时候应该)通过使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/cluster-administration/addons/&#34;&gt;插件&lt;/a&gt;.
为你的集群设置 DNS 服务。&lt;/p&gt;
&lt;p&gt;一个可感知集群的 DNS 服务， 例如 CoreDNS， 会监听 k8s API 创建的 Service 并创建对应的 DNS 记录。
如果集群启用的 DNS 服务，则所以的 Pod 都应该会自动地通过 DNS 名称解析 Service。&lt;/p&gt;
&lt;p&gt;例如，如果有一个名叫 &lt;code&gt;&amp;quot;my-service&amp;quot;&lt;/code&gt; Service 于 &lt;code&gt;&amp;quot;my-ns&amp;quot;&lt;/code&gt; 命名空间，控制中心和 DNS 服务会
协作创建一条 DNS 记录为 &lt;code&gt;&amp;quot;my-service.my-ns&amp;quot;&lt;/code&gt;。 在 &lt;code&gt;&amp;quot;my-ns&amp;quot;&lt;/code&gt; 命名空间的 Pod 可以只需要简单地
使用 &lt;code&gt;my-service&lt;/code&gt; 就能查到(&lt;code&gt;&amp;quot;my-service.my-ns&amp;quot;&lt;/code&gt; 也是可以的)&lt;/p&gt;
&lt;p&gt;在其它命名空间的 Pod 必须使用 &lt;code&gt;my-service.my-ns&lt;/code&gt; 这样的限定名。 这些名称会解析为 Service
分配的集群IP。&lt;/p&gt;
&lt;p&gt;k8s 还支持命名端口的 DNS SRV (Service) 记录。 如果叫 &lt;code&gt;&amp;quot;my-service.my-ns&amp;quot;&lt;/code&gt; 的 Service
有一个叫 &lt;code&gt;&amp;quot;http&amp;quot;&lt;/code&gt; 的 &lt;code&gt;TCP&lt;/code&gt; 端口， 就可以使用 &lt;code&gt;DNS SRV&lt;/code&gt; 查询 &lt;code&gt;_http._tcp.my-service.my-ns&lt;/code&gt;
得到 &lt;code&gt;&amp;quot;http&amp;quot;&lt;/code&gt; 对应的端口号和 IP 地址。&lt;/p&gt;
&lt;p&gt;k8s DNS 服务是访问 &lt;code&gt;ExternalName&lt;/code&gt; Service 的唯一方式。更多关于 &lt;code&gt;ExternalName&lt;/code&gt; 的信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/dns-pod-service/&#34;&gt;DNS Pod 和 Service&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Headless Services

Sometimes you don&#39;t need load-balancing and a single Service IP.  In
this case, you can create what are termed &#34;headless&#34; Services, by explicitly
specifying `&#34;None&#34;` for the cluster IP (`.spec.clusterIP`).

You can use a headless Service to interface with other service discovery mechanisms,
without being tied to Kubernetes&#39; implementation.

For headless `Services`, a cluster IP is not allocated, kube-proxy does not handle
these Services, and there is no load balancing or proxying done by the platform
for them. How DNS is automatically configured depends on whether the Service has
selectors defined:
 --&gt;
&lt;h2 id=&#34;headless-services&#34;&gt;Headless Services&lt;/h2&gt;
&lt;p&gt;有时候并不需要负载均衡和一个 Service 的 IP。 在这种情况下就可以创建一个被称为 &amp;ldquo;无头&amp;rdquo; 的 Service，
更确切的说就是将集群 IP (&lt;code&gt;.spec.clusterIP&lt;/code&gt;) 设置为 &lt;code&gt;&amp;quot;None&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;用户可以使用 无头 Service 作为其它服务发现机制的接口，而不需要与 k8s 实现耦合在一起。&lt;/p&gt;
&lt;p&gt;对于 无头的 &lt;code&gt;Services&lt;/code&gt; 是不会分配集群IP的， &lt;code&gt;kube-proxy&lt;/code&gt; 不会处理这些 Service，
平台也不会对它们提供负载均衡或代理。 DNS 是如何自动配置的基于 Service 是否定义了选择器:&lt;/p&gt;
&lt;!--
### With selectors

For headless Services that define selectors, the endpoints controller creates
`Endpoints` records in the API, and modifies the DNS configuration to return
records (addresses) that point directly to the `Pods` backing the `Service`.

### Without selectors

For headless Services that do not define selectors, the endpoints controller does
not create `Endpoints` records. However, the DNS system looks for and configures
either:

  * CNAME records for [`ExternalName`](#externalname)-type Services.
  * A records for any `Endpoints` that share a name with the Service, for all
    other types.
    --&gt;
&lt;h3 id=&#34;有选择器的&#34;&gt;有选择器的&lt;/h3&gt;
&lt;p&gt;对于有选择器的无头 Service， Endpoint 选择器会创建 &lt;code&gt;Endpoints&lt;/code&gt; 记录， 并修改 DNS 配置
直接返回记录为 &lt;code&gt;Service&lt;/code&gt; 后端 &lt;code&gt;Pod&lt;/code&gt; 的地址。&lt;/p&gt;
&lt;h3 id=&#34;没有选择器的&#34;&gt;没有选择器的&lt;/h3&gt;
&lt;p&gt;For headless Services that do not define selectors, the endpoints controller does
not create &lt;code&gt;Endpoints&lt;/code&gt; records. However, the DNS system looks for and configures
either:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CNAME records for &lt;a href=&#34;#externalname&#34;&gt;&lt;code&gt;ExternalName&lt;/code&gt;&lt;/a&gt;-type Services.&lt;/li&gt;
&lt;li&gt;A records for any &lt;code&gt;Endpoints&lt;/code&gt; that share a name with the Service, for all
other types.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于没有选择器的无头 Service， Endpoint 选择器不会创建 &lt;code&gt;Endpoints&lt;/code&gt; 记录，但是 DNS 系统会根据以
下情况来配置:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#externalname&#34;&gt;&lt;code&gt;ExternalName&lt;/code&gt;&lt;/a&gt; 类型的 Service 创建 CNAME&lt;/li&gt;
&lt;li&gt;所有其它类型，为其它任意与该 Service 同名的 &lt;code&gt;Endpoints&lt;/code&gt; 创建 A 记录&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Publishing Services (ServiceTypes) {#publishing-services-service-types}

For some parts of your application (for example, frontends) you may want to expose a
Service onto an external IP address, that&#39;s outside of your cluster.

Kubernetes `ServiceTypes` allow you to specify what kind of Service you want.
The default is `ClusterIP`.

`Type` values and their behaviors are:

   * `ClusterIP`: Exposes the Service on a cluster-internal IP. Choosing this value
     makes the Service only reachable from within the cluster. This is the
     default `ServiceType`.
   * [`NodePort`](#nodeport): Exposes the Service on each Node&#39;s IP at a static port
     (the `NodePort`). A `ClusterIP` Service, to which the `NodePort` Service
     routes, is automatically created.  You&#39;ll be able to contact the `NodePort` Service,
     from outside the cluster,
     by requesting `&lt;NodeIP&gt;:&lt;NodePort&gt;`.
   * [`LoadBalancer`](#loadbalancer): Exposes the Service externally using a cloud
     provider&#39;s load balancer. `NodePort` and `ClusterIP` Services, to which the external
     load balancer routes, are automatically created.
   * [`ExternalName`](#externalname): Maps the Service to the contents of the
     `externalName` field (e.g. `foo.bar.example.com`), by returning a `CNAME` record

     with its value. No proxying of any kind is set up.
     &lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You need either kube-dns version 1.7 or CoreDNS version 0.0.8 or higher to use the &lt;code&gt;ExternalName&lt;/code&gt; type.&lt;/div&gt;
&lt;/blockquote&gt;


You can also use [Ingress](/docs/concepts/services-networking/ingress/) to expose your Service. Ingress is not a Service type, but it acts as the entry point for your cluster. It lets you consolidate your routing rules into a single resource as it can expose multiple services under the same IP address.
--&gt;
&lt;h2 id=&#34;publishing-services-service-types&#34;&gt;发布 Service (ServiceTypes)&lt;/h2&gt;
&lt;p&gt;对于应用的一些部分(例如，前端)，需要将 Service 暴露到集群外部 IP 地址。&lt;/p&gt;
&lt;p&gt;k8s 可以通过 &lt;code&gt;ServiceTypes&lt;/code&gt; 来指定想要创建的 Service 类型， 默认为 &lt;code&gt;ClusterIP&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Type&lt;/code&gt; 的值各行为如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;ClusterIP&lt;/code&gt;: 以集群内部 IP 的形式暴露 Service， 使用这种方式 Service 只能在集群内部访问。
这是默认的 &lt;code&gt;ServiceType&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;#nodeport&#34;&gt;&lt;code&gt;NodePort&lt;/code&gt;&lt;/a&gt;: 将 Service 暴露到每个节点 IP和一个静态端口上(可以通过 &lt;code&gt;NodePort&lt;/code&gt;指定)
(&lt;code&gt;ClusterIP&lt;/code&gt; Service 到 &lt;code&gt;NodePort&lt;/code&gt; Service 的路由会自动创建)。
用户可以通过在集群外请求 &lt;code&gt;&amp;lt;NodeIP&amp;gt;:&amp;lt;NodePort&amp;gt;&lt;/code&gt; 方式访问 &lt;code&gt;NodePort&lt;/code&gt; Service。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;#loadbalancer&#34;&gt;&lt;code&gt;LoadBalancer&lt;/code&gt;&lt;/a&gt;: 使用云提供商的负载均衡器对外暴露 Service。
由 &lt;code&gt;NodePort&lt;/code&gt; 和 &lt;code&gt;ClusterIP&lt;/code&gt; Service 到外部负载均衡的路由会自动创建.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;#externalname&#34;&gt;&lt;code&gt;ExternalName&lt;/code&gt;&lt;/a&gt;:
通过返回 &lt;code&gt;CNAME&lt;/code&gt; 的方式 将 Service 映射到 &lt;code&gt;externalName&lt;/code&gt; 字段 (e.g. &lt;code&gt;foo.bar.example.com&lt;/code&gt;)的服务
没有设置任何类型的代理
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果使用 &lt;code&gt;ExternalName&lt;/code&gt; 类型，需要 kube-dns &lt;code&gt;v1.7+&lt;/code&gt; 或 CoreDNS &lt;code&gt;v0.0.8+&lt;/code&gt;&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用户也可以使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/ingress/&#34;&gt;Ingress&lt;/a&gt; 来暴露 Service。
Ingress 不是 Service 的一个类型， 但它扮演的是集群切入点的角色。 它让路由规则可以统一为一个资源。
并可以在同一个IP地址上暴露多个 Service&lt;/p&gt;
&lt;!--
### Type NodePort {#nodeport}

If you set the `type` field to `NodePort`, the Kubernetes control plane
allocates a port from a range specified by `--service-node-port-range` flag (default: 30000-32767).
Each node proxies that port (the same port number on every Node) into your Service.
Your Service reports the allocated port in its `.spec.ports[*].nodePort` field.


If you want to specify particular IP(s) to proxy the port, you can set the `--nodeport-addresses` flag in kube-proxy to particular IP block(s); this is supported since Kubernetes v1.10.
This flag takes a comma-delimited list of IP blocks (e.g. 10.0.0.0/8, 192.0.2.0/25) to specify IP address ranges that kube-proxy should consider as local to this node.

For example, if you start kube-proxy with the `--nodeport-addresses=127.0.0.0/8` flag, kube-proxy only selects the loopback interface for NodePort Services. The default for `--nodeport-addresses` is an empty list. This means that kube-proxy should consider all available network interfaces for NodePort. (That&#39;s also compatible with earlier Kubernetes releases).

If you want a specific port number, you can specify a value in the `nodePort`
field. The control plane will either allocate you that port or report that
the API transaction failed.
This means that you need to take care of possible port collisions yourself.
You also have to use a valid port number, one that&#39;s inside the range configured
for NodePort use.

Using a NodePort gives you the freedom to set up your own load balancing solution,
to configure environments that are not fully supported by Kubernetes, or even
to just expose one or more nodes&#39; IPs directly.

Note that this Service is visible as `&lt;NodeIP&gt;:spec.ports[*].nodePort`
and `.spec.clusterIP:spec.ports[*].port`. (If the `--nodeport-addresses` flag in kube-proxy is set, &lt;NodeIP&gt; would be filtered NodeIP(s).)

For example:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: NodePort
  selector:
    app: MyApp
  ports:
      # By default and for convenience, the `targetPort` is set to the same value as the `port` field.
    - port: 80
      targetPort: 80
      # Optional field
      # By default and for convenience, the Kubernetes control plane will allocate a port from a range (default: 30000-32767)
      nodePort: 30007
```
 --&gt;
&lt;h3 id=&#34;nodeport&#34;&gt;NodePort&lt;/h3&gt;
&lt;p&gt;如果将 &lt;code&gt;type&lt;/code&gt; 字段设置为 &lt;code&gt;NodePort&lt;/code&gt;， k8s 控制中心会从
&lt;code&gt;--service-node-port-range&lt;/code&gt; 选择配置的范围(默认 30000-32767)中分配一个端口。
集群中的每个节点都会将那个端口(每个节点使用相同的端口)代理到 Service 上。
Service 会将分配的的端口存放在它的 &lt;code&gt;.spec.ports[*].nodePort&lt;/code&gt; 字段。&lt;/p&gt;
&lt;p&gt;如果想要指定某些IP来代理这个端口，可以通过 kube-proxy 中的 &lt;code&gt;--nodeport-addresses&lt;/code&gt; 选择来
指定 IP 或 IP 段；这个特性自 k8s &lt;code&gt;v1.10&lt;/code&gt; 开始支持。
这个选择支持逗号分隔的 IP 段列表(e.g. 10.0.0.0/8, 192.0.2.0/25) 来指定 kube-proxy 是不是应该认为是应该代理的 IP 地址范围。&lt;/p&gt;
&lt;p&gt;例如，如果将 kube-proxy 设置 &lt;code&gt;--nodeport-addresses=127.0.0.0/8&lt;/code&gt;， 则 kube-proxy 只会
选择本地回环接口用作 Service 的 NodePort。 默认的  &lt;code&gt;--nodeport-addresses&lt;/code&gt; 是一个空列表。
其含义是 kube-proxy 会将所以可用的网络接口应用到 NodePort (这也与早期的 k8s 版本兼容)&lt;/p&gt;
&lt;p&gt;如果用户想要指定端口，可以通过设置 &lt;code&gt;nodePort&lt;/code&gt; 的值实现。 控制中心会分配那个端口或报告业务失败。
也就是说在设置该字段时需要用户自己解决端口冲突的问题。
并且首先设置的端口是一个有效的端口，其次端口是在之前提到所配置的 NodePort 的可用范围内。&lt;/p&gt;
&lt;p&gt;使用 NodePort 时就将负载均衡的解决方案选择交到用户手上, 可以用于配置那些 k8s 不完全支持的环境，
甚至直接暴露一个或多个节点的IP&lt;/p&gt;
&lt;p&gt;要注意 Service 可以通过 &lt;code&gt;&amp;lt;NodeIP&amp;gt;:spec.ports[*].nodePort&lt;/code&gt;:&lt;code&gt;.spec.clusterIP:spec.ports[*].port&lt;/code&gt; 访问。
(如果 kube-proxy 设置了 &lt;code&gt;--nodeport-addresses&lt;/code&gt;， 则 &lt;NodeIP&gt; 只能是配置的范围内的IP)&lt;/p&gt;
&lt;p&gt;例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NodePort&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
      &lt;span style=&#34;color:#75715e&#34;&gt;# 默认情况下和为了方便， `targetPort` 会与 `port` 字段使用相同的值&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# 可选字段&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# 默认情况下和为了方便，k8s 控制中心会在配置的端口范围(默认30000-32767) 内分配一个端口&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;nodePort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;30007&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Type LoadBalancer {#loadbalancer}

On cloud providers which support external load balancers, setting the `type`
field to `LoadBalancer` provisions a load balancer for your Service.
The actual creation of the load balancer happens asynchronously, and
information about the provisioned balancer is published in the Service&#39;s
`.status.loadBalancer` field.
For example:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  clusterIP: 10.0.171.239
  type: LoadBalancer
status:
  loadBalancer:
    ingress:
    - ip: 192.0.2.127
```

Traffic from the external load balancer is directed at the backend Pods. The cloud provider decides how it is load balanced.

For LoadBalancer type of Services, when there is more than one port defined, all
ports must have the same protocol and the protocol must be one of `TCP`, `UDP`,
and `SCTP`.

Some cloud providers allow you to specify the `loadBalancerIP`. In those cases, the load-balancer is created
with the user-specified `loadBalancerIP`. If the `loadBalancerIP` field is not specified,
the loadBalancer is set up with an ephemeral IP address. If you specify a `loadBalancerIP`
but your cloud provider does not support the feature, the `loadbalancerIP` field that you
set is ignored.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If you&amp;rsquo;re using SCTP, see the &lt;a href=&#34;#caveat-sctp-loadbalancer-service-type&#34;&gt;caveat&lt;/a&gt; below about the
&lt;code&gt;LoadBalancer&lt;/code&gt; Service type.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;On &lt;strong&gt;Azure&lt;/strong&gt;, if you want to use a user-specified public type &lt;code&gt;loadBalancerIP&lt;/code&gt;, you first need
to create a static type public IP address resource. This public IP address resource should
be in the same resource group of the other automatically created resources of the cluster.
For example, &lt;code&gt;MC_myResourceGroup_myAKSCluster_eastus&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Specify the assigned IP address as loadBalancerIP. Ensure that you have updated the securityGroupName in the cloud provider configuration file. For information about troubleshooting &lt;code&gt;CreatingLoadBalancerFailed&lt;/code&gt; permission issues see, &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/aks/static-ip&#34;&gt;Use a static IP address with the Azure Kubernetes Service (AKS) load balancer&lt;/a&gt; or &lt;a href=&#34;https://github.com/Azure/AKS/issues/357&#34;&gt;CreatingLoadBalancerFailed on AKS cluster with advanced networking&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;loadbalancer&#34;&gt;LoadBalancer&lt;/h3&gt;
&lt;p&gt;云提供商还支持外部的负载均衡器， 通过设置 &lt;code&gt;type&lt;/code&gt; 的值为 &lt;code&gt;LoadBalancer&lt;/code&gt; 为 Service 提供一个负载均衡器
实际上对负载均衡器的创建是异步的，关于添加的负载均衡器的信息会添加到 Service 的 &lt;code&gt;.status.loadBalancer&lt;/code&gt; 字段。&lt;/p&gt;
&lt;p&gt;例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;clusterIP&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10.0.171.239&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;LoadBalancer&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;status&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;loadBalancer&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;ingress&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;ip&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;192.0.2.127&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从外部负载均衡器进来的流量会直接转发到后端的 Pod上。 云提供商会决定负载均衡方式。&lt;/p&gt;
&lt;p&gt;对于 LoadBalancer 类型的 Service， 当一个 Service 上有不止一个端口时， 所以的端口必须要使用
相同的协议，并且协议只能是 &lt;code&gt;TCP&lt;/code&gt;, &lt;code&gt;UDP&lt;/code&gt;, &lt;code&gt;SCTP&lt;/code&gt; 中的一种.&lt;/p&gt;
&lt;p&gt;有些云提供商支持指定 &lt;code&gt;loadBalancerIP&lt;/code&gt;， 在这个情况下， 负载均衡是使用指定的 &lt;code&gt;loadBalancerIP&lt;/code&gt; 创建。
如果 &lt;code&gt;loadBalancerIP&lt;/code&gt; 没有指定则会使用一个临时 IP 地址。 如果指定了 &lt;code&gt;loadBalancerIP&lt;/code&gt;
但云提供商不支持该特性，则指定的 &lt;code&gt;loadbalancerIP&lt;/code&gt; 字段会被忽略。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果使用的是 SCTP 协议，见下面的 &lt;a href=&#34;#caveat-sctp-loadbalancer-service-type&#34;&gt;caveat&lt;/a&gt; 关于 &lt;code&gt;LoadBalancer&lt;/code&gt;
Service 类型的相关信息&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;在 &lt;strong&gt;Azure&lt;/strong&gt; 上， 如果想要使用用户指定的 &lt;code&gt;loadBalancerIP&lt;/code&gt;， 首先需要创建一个静态类型的
公网 IP 地址资源。 这个公网 IP 地址资源应该与集群其它自动创建的资源在同一个资源组。
例如，&lt;code&gt;MC_myResourceGroup_myAKSCluster_eastus&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;通过 loadBalancerIP 指定分配 IP， 需要确保已经更新云提供商配置文件中的 securityGroupName。
更多 &lt;code&gt;CreatingLoadBalancerFailed&lt;/code&gt; 权限问题的调度信息见
&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/aks/static-ip&#34;&gt;Use a static IP address with the Azure Kubernetes Service (AKS) load balancer&lt;/a&gt;
或
&lt;a href=&#34;https://github.com/Azure/AKS/issues/357&#34;&gt;CreatingLoadBalancerFailed on AKS cluster with advanced networking&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
#### Internal load balancer
In a mixed environment it is sometimes necessary to route traffic from Services inside the same
(virtual) network address block.

In a split-horizon DNS environment you would need two Services to be able to route both external and internal traffic to your endpoints.

You can achieve this by adding one the following annotations to a Service.
The annotation to add depends on the cloud Service provider you&#39;re using.

&lt;div id=&#34;service_tabs&#34;&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-0&#34;&gt;Default&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-1&#34;&gt;GCP&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-2&#34;&gt;AWS&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-3&#34;&gt;Azure&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-4&#34;&gt;IBM Cloud&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-5&#34;&gt;OpenStack&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-6&#34;&gt;Baidu Cloud&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-7&#34;&gt;Tencent Cloud&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-8&#34;&gt;Alibaba Cloud&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div id=&#34;service_tabs-0&#34;&gt;&lt;p&gt;Select one of the tabs.&lt;/p&gt;
&lt;/div&gt;&lt;div id=&#34;service_tabs-1&#34;&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;cloud.google.com/load-balancer-type&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Internal&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&#34;service_tabs-2&#34;&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-internal&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&#34;service_tabs-3&#34;&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/azure-load-balancer-internal&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&#34;service_tabs-4&#34;&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;private&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&#34;service_tabs-5&#34;&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/openstack-internal-load-balancer&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&#34;service_tabs-6&#34;&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/cce-load-balancer-internal-vpc&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&#34;service_tabs-7&#34;&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/qcloud-loadbalancer-internal-subnetid&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;subnet-xxxxx&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&#34;service_tabs-8&#34;&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;intranet&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;script&gt;$(function(){$(&#34;#service_tabs&#34;).tabs();});&lt;/script&gt;
 --&gt;
&lt;h4 id=&#34;内部负载均衡器&#34;&gt;内部负载均衡器&lt;/h4&gt;
&lt;p&gt;在一个混搭的环境中，有时候需要在同一个(虚拟)网络地址段从 Service 间路由流量。&lt;/p&gt;
&lt;p&gt;在一个水平分割的 DNS 环境，需要有两个 Service 才能够分别路由外部和内部的流量到 Endpoint.&lt;/p&gt;
&lt;p&gt;可以通过在 Service 上添加以下注解中的一个来达成这个目的。 添加哪个注解基于你用的云提供商。&lt;/p&gt;
&lt;div id=&#34;service_tabs&#34;&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-0&#34;&gt;Default&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-1&#34;&gt;GCP&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-2&#34;&gt;AWS&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-3&#34;&gt;Azure&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-4&#34;&gt;IBM Cloud&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-5&#34;&gt;OpenStack&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-6&#34;&gt;Baidu Cloud&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-7&#34;&gt;Tencent Cloud&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#service_tabs-8&#34;&gt;Alibaba Cloud&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div id=&#34;service_tabs-0&#34;&gt;&lt;p&gt;选择其中一个标签&lt;/p&gt;
&lt;/div&gt;&lt;div id=&#34;service_tabs-1&#34;&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;cloud.google.com/load-balancer-type&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Internal&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&#34;service_tabs-2&#34;&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-internal&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&#34;service_tabs-3&#34;&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/azure-load-balancer-internal&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&#34;service_tabs-4&#34;&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;private&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&#34;service_tabs-5&#34;&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/openstack-internal-load-balancer&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&#34;service_tabs-6&#34;&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/cce-load-balancer-internal-vpc&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&#34;service_tabs-7&#34;&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/qcloud-loadbalancer-internal-subnetid&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;subnet-xxxxx&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&#34;service_tabs-8&#34;&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;intranet&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;script&gt;$(function(){$(&#34;#service_tabs&#34;).tabs();});&lt;/script&gt;
&lt;!--
#### TLS support on AWS {#ssl-support-on-aws}

For partial TLS / SSL support on clusters running on AWS, you can add three
annotations to a `LoadBalancer` service:

```yaml
metadata:
  name: my-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012
```

The first specifies the ARN of the certificate to use. It can be either a
certificate from a third party issuer that was uploaded to IAM or one created
within AWS Certificate Manager.

```yaml
metadata:
  name: my-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: (https|http|ssl|tcp)
```

The second annotation specifies which protocol a Pod speaks. For HTTPS and
SSL, the ELB expects the Pod to authenticate itself over the encrypted
connection, using a certificate.

HTTP and HTTPS selects layer 7 proxying: the ELB terminates
the connection with the user, parses headers, and injects the `X-Forwarded-For`
header with the user&#39;s IP address (Pods only see the IP address of the
ELB at the other end of its connection) when forwarding requests.

TCP and SSL selects layer 4 proxying: the ELB forwards traffic without
modifying the headers.

In a mixed-use environment where some ports are secured and others are left unencrypted,
you can use the following annotations:

```yaml
    metadata:
      name: my-service
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
        service.beta.kubernetes.io/aws-load-balancer-ssl-ports: &#34;443,8443&#34;
```

In the above example, if the Service contained three ports, `80`, `443`, and
`8443`, then `443` and `8443` would use the SSL certificate, but `80` would just
be proxied HTTP.

From Kubernetes v1.9 onwards you can use [predefined AWS SSL policies](https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-policy-table.html) with HTTPS or SSL listeners for your Services.
To see which policies are available for use, you can use the `aws` command line tool:

```bash
aws elb describe-load-balancer-policies --query &#39;PolicyDescriptions[].PolicyName&#39;
```

You can then specify any one of those policies using the
&#34;`service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy`&#34;
annotation; for example:

```yaml
    metadata:
      name: my-service
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: &#34;ELBSecurityPolicy-TLS-1-2-2017-01&#34;
```
 --&gt;
&lt;h4 id=&#34;ssl-support-on-aws&#34;&gt;AWS 对 TLS 的支持&lt;/h4&gt;
&lt;p&gt;运行在 AWS 上的集群部分支持 TLS / SSL，可以添加以下三个注解到一个 &lt;code&gt;LoadBalancer&lt;/code&gt; 的 Service:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-ssl-cert&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;第一个指定 证书使用的 ARN。 可以是上传到  IAM 的第三方发行者的证书 或者是在 AWS 证书管理器创建的证书。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-backend-protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;(https|http|ssl|tcp)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;connection, using a certificate.
第二个注解指定 Pod 使用的是哪个协议。 对于 HTTPS 和 SSL，负载均衡期望的是 Pod 使用证书的安全连接来认证它自己。&lt;/p&gt;
&lt;p&gt;HTTP 和 HTTPS 使用第 7 层代理: 转发请求是由负载均衡器来终止用户的连接，解析头，插入包含用户 IP 地址
的 &lt;code&gt;X-Forwarded-For&lt;/code&gt; 头(Pod 只能看到连接另一头的负载均衡的IP地址)&lt;/p&gt;
&lt;p&gt;TCP 和 SSL 使用 4 层代理: 负载均衡器转发流量是不会修改头信息&lt;/p&gt;
&lt;p&gt;在混搭环境中，有些端口是安全的有是又是未加密的，可以使用如下注解:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-backend-protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-ssl-ports&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;443,8443&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在上面的例子中， 如果 Service 包含三个端口， &lt;code&gt;80&lt;/code&gt;, &lt;code&gt;443&lt;/code&gt;, &lt;code&gt;8443&lt;/code&gt;, 其中 &lt;code&gt;443&lt;/code&gt; 和 &lt;code&gt;8443&lt;/code&gt;
是使用 SSL 证书的，但 &lt;code&gt;80&lt;/code&gt; 只通过 HTTP 代理&lt;/p&gt;
&lt;p&gt;从 k8s v1.9 开始，可以使用
&lt;a href=&#34;https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-policy-table.html&#34;&gt;predefined AWS SSL policies&lt;/a&gt;
来配置 Service 的 HTTPS 或 SSL 监控器。
可以通过以下 aws 命令行工具来查看哪些可用的策略:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;aws elb describe-load-balancer-policies --query &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;PolicyDescriptions[].PolicyName&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;然后可以使
&amp;ldquo;&lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy&lt;/code&gt;&amp;rdquo;
注解来使用其中的某个策略，例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ELBSecurityPolicy-TLS-1-2-2017-01&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
#### PROXY protocol support on AWS

To enable [PROXY protocol](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt)
support for clusters running on AWS, you can use the following service
annotation:

```yaml
    metadata:
      name: my-service
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: &#34;*&#34;
```

Since version 1.3.0, the use of this annotation applies to all ports proxied by the ELB
and cannot be configured otherwise. --&gt;
&lt;h4 id=&#34;aws-支持的-proxy-协议&#34;&gt;AWS 支持的 PROXY 协议&lt;/h4&gt;
&lt;p&gt;要在 AWS 运行的集群中启用 &lt;a href=&#34;https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt&#34;&gt;PROXY protocol&lt;/a&gt;
可以在 Service 上添加以下注解:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-proxy-protocol&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从 v1.3.0 开始，就只能通过这个注解来让所以的端口通过 ELB 代理，不能通过其它方式配置了。&lt;/p&gt;
&lt;!--
#### ELB Access Logs on AWS

There are several annotations to manage access logs for ELB Services on AWS.

The annotation `service.beta.kubernetes.io/aws-load-balancer-access-log-enabled`
controls whether access logs are enabled.

The annotation `service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval`
controls the interval in minutes for publishing the access logs. You can specify
an interval of either 5 or 60 minutes.

The annotation `service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name`
controls the name of the Amazon S3 bucket where load balancer access logs are
stored.

The annotation `service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix`
specifies the logical hierarchy you created for your Amazon S3 bucket.

```yaml
    metadata:
      name: my-service
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-access-log-enabled: &#34;true&#34;
        # Specifies whether access logs are enabled for the load balancer
        service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval: &#34;60&#34;
        # The interval for publishing the access logs. You can specify an interval of either 5 or 60 (minutes).
        service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name: &#34;my-bucket&#34;
        # The name of the Amazon S3 bucket where the access logs are stored
        service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix: &#34;my-bucket-prefix/prod&#34;
        # The logical hierarchy you created for your Amazon S3 bucket, for example `my-bucket-prefix/prod`
```
 --&gt;
&lt;h4 id=&#34;aws-上-elb-的访问日志&#34;&gt;AWS 上 ELB 的访问日志&lt;/h4&gt;
&lt;p&gt;AWS 上 ELB 的访问日志 可以通过几个注解来管理。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-access-log-enabled&lt;/code&gt; 注解控制是否开启访问日志&lt;/p&gt;
&lt;p&gt;&lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval&lt;/code&gt; 注解控制发布
日志的时间间隔(单位为分钟)。 时间间隔可以是 5 分钟 或 60 分钟&lt;/p&gt;
&lt;p&gt;&lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name&lt;/code&gt; 注解控制访问
日志存储的 Amazon S3 bucket 名称
&lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix&lt;/code&gt; 注解
它指定创建的 Amazon S3 bucket 的逻辑层次&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-access-log-enabled&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 否开启访问日志&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;60&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 发布 日志的时间间隔(单位为分钟)。 时间间隔可以是 5 分钟 或 60 分钟&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;my-bucket&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 日志存储的 Amazon S3 bucket 名称&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;my-bucket-prefix/prod&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 创建的 Amazon S3 bucket 的逻辑层次, 例如 `my-bucket-prefix/prod`&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
#### Connection Draining on AWS

Connection draining for Classic ELBs can be managed with the annotation
`service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled` set
to the value of `&#34;true&#34;`. The annotation
`service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout` can
also be used to set maximum time, in seconds, to keep the existing connections open before deregistering the instances.


```yaml
    metadata:
      name: my-service
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled: &#34;true&#34;
        service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout: &#34;60&#34;
``` --&gt;
&lt;h4 id=&#34;aws-连接控制&#34;&gt;AWS 连接控制&lt;/h4&gt;
&lt;p&gt;经典ELB 的连接使用可以通过将注解
&lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled&lt;/code&gt;
的值设置为 &lt;code&gt;&amp;quot;true&amp;quot;&lt;/code&gt; 来管理。
还可以通过注解
&lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout&lt;/code&gt;
也可以用来设置保证已有连接的最大时间(单位秒)，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;60&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
#### Other ELB annotations

There are other annotations to manage Classic Elastic Load Balancers that are described below.

```yaml
    metadata:
      name: my-service
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: &#34;60&#34;
        # The time, in seconds, that the connection is allowed to be idle (no data has been sent over the connection) before it is closed by the load balancer

        service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: &#34;true&#34;
        # Specifies whether cross-zone load balancing is enabled for the load balancer

        service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: &#34;environment=prod,owner=devops&#34;
        # A comma-separated list of key-value pairs which will be recorded as
        # additional tags in the ELB.

        service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: &#34;&#34;
        # The number of successive successful health checks required for a backend to
        # be considered healthy for traffic. Defaults to 2, must be between 2 and 10

        service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: &#34;3&#34;
        # The number of unsuccessful health checks required for a backend to be
        # considered unhealthy for traffic. Defaults to 6, must be between 2 and 10

        service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: &#34;20&#34;
        # The approximate interval, in seconds, between health checks of an
        # individual instance. Defaults to 10, must be between 5 and 300

        service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: &#34;5&#34;
        # The amount of time, in seconds, during which no response means a failed
        # health check. This value must be less than the service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval
        # value. Defaults to 5, must be between 2 and 60

        service.beta.kubernetes.io/aws-load-balancer-extra-security-groups: &#34;sg-53fae93f,sg-42efd82e&#34;
        # A list of additional security groups to be added to the ELB

        service.beta.kubernetes.io/aws-load-balancer-target-node-labels: &#34;ingress-gw,gw-name=public-api&#34;
        # A comma separated list of key-value pairs which are used
        # to select the target nodes for the load balancer
```
 --&gt;
&lt;h4 id=&#34;elb-其它注解&#34;&gt;ELB 其它注解&lt;/h4&gt;
&lt;p&gt;以下介绍管理经典弹性负载均衡器的其它注解.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;60&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 连接在被负载均衡关闭前允许空闲(没有从连接发送数据)的时间，单位秒&lt;/span&gt;

        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 指定负载均衡器是否开启跨区负载均衡&lt;/span&gt;

        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;environment=prod,owner=devops&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 一个以逗号分隔的键值对列表，用于记在 ELB 中的额外标签&lt;/span&gt;

        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 一个后端服务实例被认为是健康可以处理流量所需要的成功健康检查次数，默认为 2， 只能在 2 到 10 之间&lt;/span&gt;

        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 一个后端服务实例被认为是不健康不能处理流量需要的失败的健康检查次数， 默认为 6， 必须在 2 到 10 之间&lt;/span&gt;

        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;20&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 针对每个独立实例进行健康检查的时间间隔，单位秒，默认为 10， 必须在 5 到 300 之间&lt;/span&gt;

        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;5&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 如果在这个时间(单位秒)内健康检查没有响应，则认为检测结果为失败。这个值必须小于&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval 的值&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 默认为 5， 必须在 2 到 10 之间。&lt;/span&gt;

        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-extra-security-groups&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sg-53fae93f,sg-42efd82e&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 被添加到 ELB 的额外的安全组列表&lt;/span&gt;

        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-target-node-labels&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ingress-gw,gw-name=public-api&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 一个以逗号分隔的键值对列表，用于选择负载均衡的节点&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
#### Network Load Balancer support on AWS {#aws-nlb-support}






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.15 [beta]&lt;/code&gt;
&lt;/div&gt;



To use a Network Load Balancer on AWS, use the annotation `service.beta.kubernetes.io/aws-load-balancer-type` with the value set to `nlb`.

```yaml
    metadata:
      name: my-service
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-type: &#34;nlb&#34;
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; NLB only works with certain instance classes; see the &lt;a href=&#34;https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-register-targets.html#register-deregister-targets&#34;&gt;AWS documentation&lt;/a&gt;
on Elastic Load Balancing for a list of supported instance types.&lt;/div&gt;
&lt;/blockquote&gt;


Unlike Classic Elastic Load Balancers, Network Load Balancers (NLBs) forward the
client&#39;s IP address through to the node. If a Service&#39;s `.spec.externalTrafficPolicy`
is set to `Cluster`, the client&#39;s IP address is not propagated to the end
Pods.

By setting `.spec.externalTrafficPolicy` to `Local`, the client IP addresses is
propagated to the end Pods, but this could result in uneven distribution of
traffic. Nodes without any Pods for a particular LoadBalancer Service will fail
the NLB Target Group&#39;s health check on the auto-assigned
`.spec.healthCheckNodePort` and not receive any traffic.

In order to achieve even traffic, either use a DaemonSet or specify a
[pod anti-affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)
to not locate on the same node.

You can also use NLB Services with the [internal load balancer](/docs/concepts/services-networking/service/#internal-load-balancer)
annotation.

In order for client traffic to reach instances behind an NLB, the Node security
groups are modified with the following IP rules:

| Rule | Protocol | Port(s) | IpRange(s) | IpRange Description |
|------|----------|---------|------------|---------------------|
| Health Check | TCP | NodePort(s) (`.spec.healthCheckNodePort` for `.spec.externalTrafficPolicy = Local`) | VPC CIDR | kubernetes.io/rule/nlb/health=\&lt;loadBalancerName\&gt; |
| Client Traffic | TCP | NodePort(s) | `.spec.loadBalancerSourceRanges` (defaults to `0.0.0.0/0`) | kubernetes.io/rule/nlb/client=\&lt;loadBalancerName\&gt; |
| MTU Discovery | ICMP | 3,4 | `.spec.loadBalancerSourceRanges` (defaults to `0.0.0.0/0`) | kubernetes.io/rule/nlb/mtu=\&lt;loadBalancerName\&gt; |

In order to limit which client IP&#39;s can access the Network Load Balancer,
specify `loadBalancerSourceRanges`.

```yaml
spec:
  loadBalancerSourceRanges:
    - &#34;143.231.0.0/16&#34;
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If &lt;code&gt;.spec.loadBalancerSourceRanges&lt;/code&gt; is not set, Kubernetes
allows traffic from &lt;code&gt;0.0.0.0/0&lt;/code&gt; to the Node Security Group(s). If nodes have
public IP addresses, be aware that non-NLB traffic can also reach all instances
in those modified security groups.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h4 id=&#34;aws-nlb-support&#34;&gt;AWS 上支持的网络负载均衡(NLB)&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.15 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;要使用 AWS 上的网络负载均衡器，使用注解 &lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-type&lt;/code&gt;
设置值为 &lt;code&gt;nlb&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-type&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nlb&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; NLB 只适用的特定类型的实例，ELB 支持的实例类型见
&lt;a href=&#34;https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-register-targets.html#register-deregister-targets&#34;&gt;AWS 文档&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;与经典 ELB 不同， NLB 转发的客户端 IP 地址穿透节点。 如果 Service 的
&lt;code&gt;.spec.externalTrafficPolicy&lt;/code&gt; 设置为 &lt;code&gt;Cluster&lt;/code&gt;， 客户端 IP 地址不会传递到最终的 Pod。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;.spec.healthCheckNodePort&lt;/code&gt; and not receive any traffic.
通过设置 &lt;code&gt;.spec.externalTrafficPolicy&lt;/code&gt; 为 &lt;code&gt;Local&lt;/code&gt;， 客户端 IP 地址会传递到最终的 Pod。
但这会导向流量分发不均衡。 没有包含该负载均衡器对应 Service 的 Pod 的节点，会在 NLB 目标组
健康检查时分配的 &lt;code&gt;.spec.healthCheckNodePort&lt;/code&gt; 检查失败。 并不会收到任何流量&lt;/p&gt;
&lt;p&gt;为了达成平衡的负载，要么使用 DaemonSet，要么设置
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity&#34;&gt;pod anti-affinity&lt;/a&gt;
让 Pod 不要调度到同一个节点上&lt;/p&gt;
&lt;p&gt;也可以在使用 NLB Service 时在其中包含
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service/#internal-load-balancer&#34;&gt;内部负载均衡器&lt;/a&gt;
注解&lt;/p&gt;
&lt;p&gt;为了能让客户端流量到达 NLB 后面的实例。 节点安全组需要以以下 IP 规则进行修改:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Rule&lt;/th&gt;
&lt;th&gt;Protocol&lt;/th&gt;
&lt;th&gt;Port(s)&lt;/th&gt;
&lt;th&gt;IpRange(s)&lt;/th&gt;
&lt;th&gt;IpRange Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Health Check&lt;/td&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;NodePort(s) (&lt;code&gt;.spec.healthCheckNodePort&lt;/code&gt; for &lt;code&gt;.spec.externalTrafficPolicy = Local&lt;/code&gt;)&lt;/td&gt;
&lt;td&gt;VPC CIDR&lt;/td&gt;
&lt;td&gt;kubernetes.io/rule/nlb/health=&amp;lt;loadBalancerName&amp;gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Client Traffic&lt;/td&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;NodePort(s)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;.spec.loadBalancerSourceRanges&lt;/code&gt; (defaults to &lt;code&gt;0.0.0.0/0&lt;/code&gt;)&lt;/td&gt;
&lt;td&gt;kubernetes.io/rule/nlb/client=&amp;lt;loadBalancerName&amp;gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MTU Discovery&lt;/td&gt;
&lt;td&gt;ICMP&lt;/td&gt;
&lt;td&gt;3,4&lt;/td&gt;
&lt;td&gt;&lt;code&gt;.spec.loadBalancerSourceRanges&lt;/code&gt; (defaults to &lt;code&gt;0.0.0.0/0&lt;/code&gt;)&lt;/td&gt;
&lt;td&gt;kubernetes.io/rule/nlb/mtu=&amp;lt;loadBalancerName&amp;gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;为限所哪个客户端 IP 可以访问 NLB， 指定 &lt;code&gt;loadBalancerSourceRanges&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;loadBalancerSourceRanges&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;143.231.0.0/16&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果 &lt;code&gt;.spec.loadBalancerSourceRanges&lt;/code&gt; 没有设置，k8s 允许来自 &lt;code&gt;0.0.0.0/0&lt;/code&gt; 流量到节点安全组。
如果节点有公网 IP 地址，要注意那些非 NLB 流量也会到达所有这些修改过的安全组。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;对 AWS 使用不熟悉，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
#### Other CLB annotations on Tencent Kubernetes Engine (TKE)

There are other annotations for managing Cloud Load Balancers on TKE as shown below.

```yaml
    metadata:
      name: my-service
      annotations:
        # Bind Loadbalancers with specified nodes
        service.kubernetes.io/qcloud-loadbalancer-backends-label: key in (value1, value2)

        # ID of an existing load balancer
        service.kubernetes.io/tke-existed-lbid：lb-6swtxxxx

        # Custom parameters for the load balancer (LB), does not support modification of LB type yet
        service.kubernetes.io/service.extensiveParameters: &#34;&#34;

        # Custom parameters for the LB listener
        service.kubernetes.io/service.listenerParameters: &#34;&#34;

        # Specifies the type of Load balancer;
        # valid values: classic (Classic Cloud Load Balancer) or application (Application Cloud Load Balancer)
        service.kubernetes.io/loadbalance-type: xxxxx

        # Specifies the public network bandwidth billing method;
        # valid values: TRAFFIC_POSTPAID_BY_HOUR(bill-by-traffic) and BANDWIDTH_POSTPAID_BY_HOUR (bill-by-bandwidth).
        service.kubernetes.io/qcloud-loadbalancer-internet-charge-type: xxxxxx

        # Specifies the bandwidth value (value range: [1,2000] Mbps).
        service.kubernetes.io/qcloud-loadbalancer-internet-max-bandwidth-out: &#34;10&#34;

        # When this annotation is set，the loadbalancers will only register nodes
        # with pod running on it, otherwise all nodes will be registered.
        service.kubernetes.io/local-svc-only-bind-node-with-pod: true
```
 --&gt;
&lt;h4 id=&#34;tencent-kubernetes-engine-tke-上其它-clb-注解&#34;&gt;Tencent Kubernetes Engine (TKE) 上其它 CLB 注解&lt;/h4&gt;
&lt;p&gt;以下为管理 TKE 上 CLB 的其它注解说明。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#75715e&#34;&gt;# 将 负载均衡器与指定节点绑定&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/qcloud-loadbalancer-backends-label&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;key in (value1, value2)&lt;/span&gt;

        &lt;span style=&#34;color:#75715e&#34;&gt;# 已经存在的负载均衡器的 ID&lt;/span&gt;
        &lt;span style=&#34;color:#ae81ff&#34;&gt;service.kubernetes.io/tke-existed-lbid：lb-6swtxxxx&lt;/span&gt;

        &lt;span style=&#34;color:#75715e&#34;&gt;# Custom parameters for the load balancer (LB), does not support modification of LB type yet&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 负载均衡器的自定义参数， 还不支持对负载均衡类型的修改&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/service.extensiveParameters&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;

        &lt;span style=&#34;color:#75715e&#34;&gt;# Custom parameters for the LB listener&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 负载均衡监听器的自定义参数&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/service.listenerParameters&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;

        &lt;span style=&#34;color:#75715e&#34;&gt;# 设置负载均衡器的类型&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 有效值为: classic (Classic Cloud Load Balancer) 或  application (Application Cloud Load Balancer)&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/loadbalance-type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;xxxxx&lt;/span&gt;

        &lt;span style=&#34;color:#75715e&#34;&gt;# 设置公网带宽收费方式&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 有效值为: TRAFFIC_POSTPAID_BY_HOUR(bill-by-traffic) 或 BANDWIDTH_POSTPAID_BY_HOUR (bill-by-bandwidth).&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/qcloud-loadbalancer-internet-charge-type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;xxxxxx&lt;/span&gt;

        &lt;span style=&#34;color:#75715e&#34;&gt;# 设置带宽的值(范围: [1,2000] Mbps)&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/qcloud-loadbalancer-internet-max-bandwidth-out&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10&amp;#34;&lt;/span&gt;

        &lt;span style=&#34;color:#75715e&#34;&gt;# 当这个注解被设置，负载均衡器只会注册有 Pod 在上面运行的节点，否则所有节点都会注册&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/local-svc-only-bind-node-with-pod&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Type ExternalName {#externalname}

Services of type ExternalName map a Service to a DNS name, not to a typical selector such as
`my-service` or `cassandra`. You specify these Services with the `spec.externalName` parameter.

This Service definition, for example, maps
the `my-service` Service in the `prod` namespace to `my.database.example.com`:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.database.example.com
```
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; ExternalName accepts an IPv4 address string, but as a DNS names comprised of digits, not as an IP address. ExternalNames that resemble IPv4 addresses are not resolved by CoreDNS or ingress-nginx because ExternalName
is intended to specify a canonical DNS name. To hardcode an IP address, consider using
&lt;a href=&#34;#headless-services&#34;&gt;headless Services&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;


When looking up the host `my-service.prod.svc.cluster.local`, the cluster DNS Service
returns a `CNAME` record with the value `my.database.example.com`. Accessing
`my-service` works in the same way as other Services but with the crucial
difference that redirection happens at the DNS level rather than via proxying or
forwarding. Should you later decide to move your database into your cluster, you
can start its Pods, add appropriate selectors or endpoints, and change the
Service&#39;s `type`.

&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; &lt;p&gt;You may have trouble using ExternalName for some common protocols, including HTTP and HTTPS. If you use ExternalName then the hostname used by clients inside your cluster is different from the name that the ExternalName references.&lt;/p&gt;
&lt;p&gt;For protocols that use hostnames this difference may lead to errors or unexpected responses. HTTP requests will have a &lt;code&gt;Host:&lt;/code&gt; header that the origin server does not recognize; TLS servers will not be able to provide a certificate matching the hostname that the client connected to.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; This section is indebted to the &lt;a href=&#34;https://akomljen.com/kubernetes-tips-part-1/&#34;&gt;Kubernetes Tips - Part
1&lt;/a&gt; blog post from &lt;a href=&#34;https://akomljen.com/&#34;&gt;Alen Komljen&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;externalname&#34;&gt;ExternalName&lt;/h3&gt;
&lt;p&gt;ExternalName 类型的 Service 将 Service 映射到一个 DNS 名称， 而不是一个常见的选择器，如
&lt;code&gt;my-service&lt;/code&gt; 或 &lt;code&gt;cassandra&lt;/code&gt;. 通过 Service &lt;code&gt;spec.externalName&lt;/code&gt; 字段设置 DNS 名称。&lt;/p&gt;
&lt;p&gt;以面示例配置中将在 &lt;code&gt;prod&lt;/code&gt; 命名空间中一个名叫 &lt;code&gt;my-service&lt;/code&gt; 的 Service 映射到 &lt;code&gt;my.database.example.com&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ExternalName&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;externalName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my.database.example.com&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; ExternalName 可以使用一个 IPv4 地址的字符串， 但是会被当作一个由数字组成的 DNS 名称，而不是
一个 IP 地址。 ExternalName 值与 IPv4 地址相似的不会被 CoreDNS 或 ingress-nginx 解析，
因为 ExternalName 在设计上就是用作一个标准的 DNS 名称的。 如果要硬编码一个 IP 地址，
考虑使用 &lt;a href=&#34;#headless-services&#34;&gt;无头 Services&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;当查询主机名 &lt;code&gt;my-service.prod.svc.cluster.local&lt;/code&gt; 时， 集群 DNS 服务会返回一个 &lt;code&gt;CNAME&lt;/code&gt;
记录，值为 &lt;code&gt;my.database.example.com&lt;/code&gt;. 访问 &lt;code&gt;my-service&lt;/code&gt; 的效果与访问其它的 Service 一样，
关键不同点在于重定义发生在 DNS 层， 而不是通过代理或转发。 如果用户决定以后会将数据库移到集群内，
就可以先启动 Pod， 再添加恰当 的 选择器 或 Endpoint, 再修改 Service 的类型就完成迁移了，而客户端不会有感知。&lt;/p&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; 那么集群内客户端使用的主机名与 ExternalName 所引用的名称必然是不一样的。
对于使用主机名的协议，这个不同可能会导致错误或意外的响应。 HTTP 请求会有一个 &lt;code&gt;Host:&lt;/code&gt; 头，原始
的服务会认不得。 TLS 服务也不能提供客户端连接主机名匹配的证书。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 这节感谢
&lt;a href=&#34;https://akomljen.com/&#34;&gt;Alen Komljen&lt;/a&gt;
的博客
&lt;a href=&#34;https://akomljen.com/kubernetes-tips-part-1/&#34;&gt;Kubernetes Tips - Part 1&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### External IPs

If there are external IPs that route to one or more cluster nodes, Kubernetes Services can be exposed on those
`externalIPs`. Traffic that ingresses into the cluster with the external IP (as destination IP), on the Service port,
will be routed to one of the Service endpoints. `externalIPs` are not managed by Kubernetes and are the responsibility
of the cluster administrator.

In the Service spec, `externalIPs` can be specified along with any of the `ServiceTypes`.
In the example below, &#34;`my-service`&#34; can be accessed by clients on &#34;`80.11.12.10:80`&#34; (`externalIP:port`)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 9376
  externalIPs:
    - 80.11.12.10
```
 --&gt;
&lt;h3 id=&#34;外部-ip&#34;&gt;外部 IP&lt;/h3&gt;
&lt;p&gt;如果有外部IP 能够路由到集群的一个或多个节点， k8s Service 可以通过这些 &lt;code&gt;externalIPs&lt;/code&gt; 来对外暴露。
通过外部IP(作为目标IP)的流量进入集群，到 Service 的端口，会被路由到一个 Service 的 Endpoint
上。 &lt;code&gt;externalIPs&lt;/code&gt; 不是由 k8s 管理的，这是集群管理员负责的。&lt;/p&gt;
&lt;p&gt;在 Service 中，&lt;code&gt;externalIPs&lt;/code&gt; 可以用在任意  &lt;code&gt;ServiceTypes&lt;/code&gt; 的 Service 上。 在下面的示例中
客户端可以通过 &lt;code&gt;80.11.12.10:80&lt;/code&gt; (&lt;code&gt;externalIP:port&lt;/code&gt;) 访问 &lt;code&gt;my-service&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;externalIPs&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;80.11.12.10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Shortcomings

Using the userspace proxy for VIPs, work at small to medium scale, but will
not scale to very large clusters with thousands of Services.  The
[original design proposal for portals](https://github.com/kubernetes/kubernetes/issues/1107)
has more details on this.

Using the userspace proxy obscures the source IP address of a packet accessing
a Service.
This makes some kinds of network filtering (firewalling) impossible.  The iptables
proxy mode does not
obscure in-cluster source IPs, but it does still impact clients coming through
a load balancer or node-port.

The `Type` field is designed as nested functionality - each level adds to the
previous.  This is not strictly required on all cloud providers (e.g. Google Compute Engine does
not need to allocate a `NodePort` to make `LoadBalancer` work, but AWS does)
but the current API requires it.
 --&gt;
&lt;h2 id=&#34;缺陷&#34;&gt;缺陷&lt;/h2&gt;
&lt;p&gt;使用 userspace 代理的 VIP，只适用于中小规模，但不适用于有上千 Service 的大规模集群。
更多细节见
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/1107&#34;&gt;original design proposal for portals&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使用 userspace 代理会吃掉进入 Service 的包的源 IP 地址。 这会使得某些类型的网络过虑(防火墙)
变得不可能， iptables 代理模式不会吃掉进入集群的源 IP 地址，但依然与来自负载均衡或 NodePort
的客户端有冲突。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Type&lt;/code&gt; 在设计上是可以嵌套的，每一级添加到前一级上。 但这不是所有的云提供商都严格必须要的(例如
GCE 就不会分配一个 &lt;code&gt;NodePort&lt;/code&gt; 来让 &lt;code&gt;LoadBalancer&lt;/code&gt; 工作，但 AWS 又是需要的)但目前的
API请求必须得有它&lt;/p&gt;
&lt;!--
## Virtual IP implementation {#the-gory-details-of-virtual-ips}

The previous information should be sufficient for many people who just want to
use Services.  However, there is a lot going on behind the scenes that may be
worth understanding.
 --&gt;
&lt;h2 id=&#34;the-gory-details-of-virtual-ips&#34;&gt;虚拟 IP 的实现&lt;/h2&gt;
&lt;p&gt;The previous information should be sufficient for many people who just want to
use Services.  However, there is a lot going on behind the scenes that may be
worth understanding.
之间的信息对于许多只想用 Service 的用户来说已经足够了。但是这些信息之下还有许多值得理解的东西。&lt;/p&gt;
&lt;!--
### Avoiding collisions

One of the primary philosophies of Kubernetes is that you should not be
exposed to situations that could cause your actions to fail through no fault
of your own. For the design of the Service resource, this means not making
you choose your own port number if that choice might collide with
someone else&#39;s choice.  That is an isolation failure.

In order to allow you to choose a port number for your Services, we must
ensure that no two Services can collide. Kubernetes does that by allocating each
Service its own IP address.

To ensure each Service receives a unique IP, an internal allocator atomically
updates a global allocation map in &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/tasks/administer-cluster/configure-upgrade-etcd/&#39; target=&#39;_blank&#39;&gt;etcd&lt;span class=&#39;tooltip-text&#39;&gt;Consistent and highly-available key value store used as Kubernetes&#39; backing store for all cluster data.&lt;/span&gt;
&lt;/a&gt;
prior to creating each Service. The map object must exist in the registry for
Services to get IP address assignments, otherwise creations will
fail with a message indicating an IP address could not be allocated.

In the control plane, a background controller is responsible for creating that
map (needed to support migrating from older versions of Kubernetes that used
in-memory locking). Kubernetes also uses controllers to check for invalid
assignments (eg due to administrator intervention) and for cleaning up allocated
IP addresses that are no longer used by any Services.
 --&gt;
&lt;h3 id=&#34;避免冲突&#34;&gt;避免冲突&lt;/h3&gt;
&lt;p&gt;k8s 一个主要的宗旨就是让用户不是因为自己的错误而引起出错， 就拿 Service 资源的设计来说，就是如果
你选择的端口可能与别人选择的端口可能冲突，则不你来选择这个端口。 这是一种故障隔离。&lt;/p&gt;
&lt;p&gt;为了允许用户为 Service  选择一个端口，我们必须要确保不能有两个 Service 端口冲突。k8s 通过为每
个 Service 分配 IP 地址来避免这个问题。&lt;/p&gt;
&lt;p&gt;为了保证每个 Service 接收的 IP 地址都是唯一的，在创建每个 Service 之间一个内部分配器原子地
在 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/tasks/administer-cluster/configure-upgrade-etcd/&#39; target=&#39;_blank&#39;&gt;etcd&lt;span class=&#39;tooltip-text&#39;&gt;Consistent and highly-available key value store used as Kubernetes&#39; backing store for all cluster data.&lt;/span&gt;
&lt;/a&gt; 中更新一个全局的分配字典。 必须要能在这个映射中
为 Service 分配指定 IP， 否则就会失败，错误信息为不能分配该 IP 地址。&lt;/p&gt;
&lt;p&gt;在控制中心中，一个后台控制器负载创建这个映射(需要支持使用内存锁的老版本k8s迁移)。k8s 还使用
控制器检查每个分配(例如，因为管理员介入)并清除那些不被任何 Service 使用的 IP 地址。&lt;/p&gt;
&lt;!--
### Service IP addresses {#ips-and-vips}

Unlike Pod IP addresses, which actually route to a fixed destination,
Service IPs are not actually answered by a single host.  Instead, kube-proxy
uses iptables (packet processing logic in Linux) to define _virtual_ IP addresses
which are transparently redirected as needed.  When clients connect to the
VIP, their traffic is automatically transported to an appropriate endpoint.
The environment variables and DNS for Services are actually populated in
terms of the Service&#39;s virtual IP address (and port).

kube-proxy supports three proxy modes&amp;mdash;userspace, iptables and IPVS&amp;mdash;which
each operate slightly differently.
 --&gt;
&lt;h3 id=&#34;ips-and-vips&#34;&gt;Service IP 地址&lt;/h3&gt;
&lt;p&gt;与 Pod 的 IP 地址不同， 当 Pod 实际是路由到一个固定的地址， Service  IP 通常不是由单个主机响应的。
而是 kube-proxy 使用 iptables (Linux 中的包处理逻辑) 来定义一个 &lt;em&gt;虚拟的&lt;/em&gt; IP 地址，根据需要透明地
转发流量。 当客户端连接到 VIP 时， 它们的流量自动传输到恰当和端点上。 Pod 中被注入的环境变量
和 Service DNS 记录指向的也是 Service 的虚拟 IP 地址(和端口)。&lt;/p&gt;
&lt;p&gt;kube-proxy 支持三种代理模式 —userspace, iptables 和 IPVS， 它们之间的运转方式各有不同。&lt;/p&gt;
&lt;!--
#### Userspace

As an example, consider the image processing application described above.
When the backend Service is created, the Kubernetes master assigns a virtual
IP address, for example 10.0.0.1.  Assuming the Service port is 1234, the
Service is observed by all of the kube-proxy instances in the cluster.
When a proxy sees a new Service, it opens a new random port, establishes an
iptables redirect from the virtual IP address to this new port, and starts accepting
connections on it.

When a client connects to the Service&#39;s virtual IP address, the iptables
rule kicks in, and redirects the packets to the proxy&#39;s own port.
The &#34;Service proxy&#34; chooses a backend, and starts proxying traffic from the client to the backend.

This means that Service owners can choose any port they want without risk of
collision.  Clients can simply connect to an IP and port, without being aware
of which Pods they are actually accessing.
 --&gt;
&lt;h4 id=&#34;userspace&#34;&gt;userspace&lt;/h4&gt;
&lt;p&gt;例如， 上面提到过那个处理图片的应用。 当后端的 Service 创建时， k8s 控制中心为它分配一个虚拟
的 IP 地址，假如是 &lt;code&gt;10.0.0.1&lt;/code&gt;。 假定 Service 的端口为 &lt;code&gt;1234&lt;/code&gt;， 该 Service 会被集群中所有
的 kube-proxy 实例监控。 当 一个代理发现一个新的 Service， 它会打开一个随机端口， 创建一个
iptables 规则将虚拟 IP 地址重定向到这个新创建的端口，并开始接收连接。&lt;/p&gt;
&lt;p&gt;当有一个客户端连接到这个 Service 的虚拟 IP 时， iptables 规则工作将包转发到代理自己的端口。
然后 Service 的代理选择后端，然后开始将代理从客户端到后端的流量。&lt;/p&gt;
&lt;p&gt;这么做的意义在于 Service 拥有者可以选择任意端口这就避免的端口冲突的风险。 客户端只是简单地连接
到一个 IP 地址和端口， 并不会感知到它实际上是连接到后端的哪个 Pod。&lt;/p&gt;
&lt;!--
#### iptables

Again, consider the image processing application described above.
When the backend Service is created, the Kubernetes control plane assigns a virtual
IP address, for example 10.0.0.1.  Assuming the Service port is 1234, the
Service is observed by all of the kube-proxy instances in the cluster.
When a proxy sees a new Service, it installs a series of iptables rules which
redirect from the virtual IP address  to per-Service rules.  The per-Service
rules link to per-Endpoint rules which redirect traffic (using destination NAT)
to the backends.

When a client connects to the Service&#39;s virtual IP address the iptables rule kicks in.
A backend is chosen (either based on session affinity or randomly) and packets are
redirected to the backend.  Unlike the userspace proxy, packets are never
copied to userspace, the kube-proxy does not have to be running for the virtual
IP address to work, and Nodes see traffic arriving from the unaltered client IP
address.

This same basic flow executes when traffic comes in through a node-port or
through a load-balancer, though in those cases the client IP does get altered.
 --&gt;
&lt;h4 id=&#34;iptables&#34;&gt;iptables&lt;/h4&gt;
&lt;p&gt;再来，还是上面那个处理图片的应用。 当后端的 Service 创建后， k8s 控制中心分配了一个虚拟IP地址
，假如是 &lt;code&gt;10.0.0.1&lt;/code&gt;.假定 Service 的端口为 &lt;code&gt;1234&lt;/code&gt;， 该 Service 会被集群中所有 的 kube-proxy 实例监控。
当代理发现一个新的 Service, 它会插入一系统 iptables 规则， 这些规则将重定向到虚拟IP地址的流量
到每个 Service 的规则。 每个 Service 的规则又与每个 Endpoint 的规则相连，将流量重定向(通过目标 NAT)
到后端。&lt;/p&gt;
&lt;p&gt;当一个客户端连接到 Service 的虚拟 IP 地址时， iptables 开始插一脚。 选择一个后端(要么基于会话粘性，要么随机)
并将数据包转发到该后端。 与 userspace 代理模式不同， 网络包不会拷贝到用户空间，kube-proxy 也不
需要为为虚拟 IP 运行工作任务， 节点可以看到接收到流量中未修改的客户端 IP 地址&lt;/p&gt;
&lt;p&gt;来自 NodePort 或 负载均衡器的流量也是使用这样的基本执行流程， 在这种情况下客户端 IP 不会被修改。&lt;/p&gt;
&lt;!--
#### IPVS

iptables operations slow down dramatically in large scale cluster e.g 10,000 Services.
IPVS is designed for load balancing and based on in-kernel hash tables. So you can achieve performance consistency in large number of Services from IPVS-based kube-proxy. Meanwhile, IPVS-based kube-proxy has more sophisticated load balancing algorithms (least conns, locality, weighted, persistence).
 --&gt;
&lt;h4 id=&#34;ipvs&#34;&gt;IPVS&lt;/h4&gt;
&lt;p&gt;iptables 在大规模集群中(如，上万个 Service)是急剧下降。IPVS 设计上是基于内核内部的哈希表来
实现负载均衡的。 所以基于 IPVS 的 kube-proxy 可以实现在大量 Service 的情况下性能稳定。
同时基于 IPVS kube-proxy 也包含更丰富的负载均衡算法(最少连接，位置，权重，维持)&lt;/p&gt;
&lt;!--
## API Object

Service is a top-level resource in the Kubernetes REST API. You can find more details
about the API object at: [Service API object](/docs/reference/generated/kubernetes-api/v1.19/#service-v1-core).
 --&gt;
&lt;h2 id=&#34;api-对象&#34;&gt;API 对象&lt;/h2&gt;
&lt;p&gt;Service is a top-level resource in the Kubernetes REST API. You can find more details
about the API object at: &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubernetes-api/v1.19/#service-v1-core&#34;&gt;Service API object&lt;/a&gt;.
Service 是 k8s REST API 中的顶级资源， 更多相关信息见
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#service-v1-core&#34;&gt;Service API 对象&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Supported protocols {#protocol-support}

### TCP

You can use TCP for any kind of Service, and it&#39;s the default network protocol.

### UDP

You can use UDP for most Services. For type=LoadBalancer Services, UDP support
depends on the cloud provider offering this facility.

### HTTP

If your cloud provider supports it, you can use a Service in LoadBalancer mode
to set up external HTTP / HTTPS reverse proxying, forwarded to the Endpoints
of the Service.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You can also use &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/ingress/&#39; target=&#39;_blank&#39;&gt;Ingress&lt;span class=&#39;tooltip-text&#39;&gt;一个用于管理外部访问集群内 Service 的 API 对象，通常是 HTTP。&lt;/span&gt;
&lt;/a&gt; in place of Service
to expose HTTP / HTTPS Services.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;protocol-support&#34;&gt;支持的协议&lt;/h2&gt;
&lt;h3 id=&#34;tcp&#34;&gt;TCP&lt;/h3&gt;
&lt;p&gt;TCP 可用于任意类型的 Service， 也是 Service  的默认网络协议&lt;/p&gt;
&lt;h3 id=&#34;udp&#34;&gt;UDP&lt;/h3&gt;
&lt;p&gt;UDP 可用于大多数 Service, 对于 type=LoadBalancer 的 Service, 是否支持 UDP 基于云提供商
是否提供该功能。&lt;/p&gt;
&lt;h3 id=&#34;http&#34;&gt;HTTP&lt;/h3&gt;
&lt;p&gt;如果云提供商支持，则可以使用 type=LoadBalancer 的 Service 通过外部的 HTTP / HTTPS 反向代理
转发到 Service 的 Endpoint&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 在 Service 暴露的是 HTTP / HTTPS 时也可以使用 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/ingress/&#39; target=&#39;_blank&#39;&gt;Ingress&lt;span class=&#39;tooltip-text&#39;&gt;一个用于管理外部访问集群内 Service 的 API 对象，通常是 HTTP。&lt;/span&gt;
&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### PROXY protocol

If your cloud provider supports it (eg, [AWS](/docs/concepts/cluster-administration/cloud-providers/#aws)),
you can use a Service in LoadBalancer mode to configure a load balancer outside
of Kubernetes itself, that will forward connections prefixed with
[PROXY protocol](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt).

The load balancer will send an initial series of octets describing the
incoming connection, similar to this example

```
PROXY TCP4 192.0.2.202 10.0.42.7 12345 7\r\n
```
followed by the data from the client.
 --&gt;
&lt;h3 id=&#34;proxy-协议&#34;&gt;PROXY 协议&lt;/h3&gt;
&lt;p&gt;如果云提供商支持， (比如 , &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/cluster-administration/cloud-providers/#aws&#34;&gt;AWS&lt;/a&gt;)，
可以在使用 LoadBalancer 类型的 Service 时在 k8s 外部配置负载均衡器， 它会转发带前缀
&lt;a href=&#34;https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt&#34;&gt;PROXY 协议&lt;/a&gt;.
的连接。&lt;/p&gt;
&lt;p&gt;负载均衡器会发送一个初始化八进制序列描述进入的连接，类似如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;PROXY TCP4 192.0.2.202 10.0.42.7 12345 7\r\n
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;紧哪关就是客户端发送的数据。&lt;/p&gt;
&lt;!--
### SCTP






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;



Kubernetes supports SCTP as a `protocol` value in Service, Endpoints, EndpointSlice, NetworkPolicy and Pod definitions. As a beta feature, this is enabled by default. To disable SCTP at a cluster level, you (or your cluster administrator) will need to disable the `SCTPSupport` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) for the API server with `--feature-gates=SCTPSupport=false,…`.

When the feature gate is enabled, you can set the `protocol` field of a Service, Endpoints, EndpointSlice, NetworkPolicy or Pod to `SCTP`. Kubernetes sets up the network accordingly for the SCTP associations, just like it does for TCP connections.
 --&gt;
&lt;h3 id=&#34;sctp&#34;&gt;SCTP&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;k8s 支持在 Service, Endpoints, EndpointSlice, NetworkPolicy 和 Pod 定义中 &lt;code&gt;protocol&lt;/code&gt;
的值为 &lt;code&gt;SCTP&lt;/code&gt;。 因为这是一个 beta 版本的特性，所以默认是开启的。要在集群级别禁用 &lt;code&gt;SCTP&lt;/code&gt;，
需要在 api-server 上通过设置 &lt;code&gt;--feature-gates=SCTPSupport=false,…&lt;/code&gt; 禁用 &lt;code&gt;SCTPSupport&lt;/code&gt;
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;当该特性被启用时，可以设置 Service, Endpoints, EndpointSlice, NetworkPolicy, Pod 的
&lt;code&gt;protocol&lt;/code&gt; 为 &lt;code&gt;SCTP&lt;/code&gt;。 k8s 会根据 SCTP 设置网络，就像设置 TCP 连接一样。&lt;/p&gt;
&lt;!--
#### Warnings {#caveat-sctp-overview}

##### Support for multihomed SCTP associations {#caveat-sctp-multihomed}

&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; &lt;p&gt;The support of multihomed SCTP associations requires that the CNI plugin can support the assignment of multiple interfaces and IP addresses to a Pod.&lt;/p&gt;
&lt;p&gt;NAT for multihomed SCTP associations requires special logic in the corresponding kernel modules.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;


##### Service with type=LoadBalancer {#caveat-sctp-loadbalancer-service-type}

&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; You can only create a Service with &lt;code&gt;type&lt;/code&gt; LoadBalancer plus &lt;code&gt;protocol&lt;/code&gt; SCTP if the cloud provider&amp;rsquo;s load balancer implementation supports SCTP as a protocol. Otherwise, the Service creation request is rejected. The current set of cloud load balancer providers (Azure, AWS, CloudStack, GCE, OpenStack) all lack support for SCTP.&lt;/div&gt;
&lt;/blockquote&gt;


##### Windows {#caveat-sctp-windows-os}

&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; SCTP is not supported on Windows based nodes.&lt;/div&gt;
&lt;/blockquote&gt;


##### Userspace kube-proxy {#caveat-sctp-kube-proxy-userspace}

&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; The kube-proxy does not support the management of SCTP associations when it is in userspace mode.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h4 id=&#34;caveat-sctp-overview&#34;&gt;警告&lt;/h4&gt;
&lt;h5 id=&#34;caveat-sctp-multihomed&#34;&gt;SCTP 多重连接支持&lt;/h5&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; SCTP 多重连接支持的前提是 CNI 插件支持为一个 Pod 分配多个网上和IP地址
SCTP 多重连接的 NAT 需要在对应的逻辑模块中有特殊逻辑&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h5 id=&#34;caveat-sctp-loadbalancer-service-type&#34;&gt;type=LoadBalancer 的 Service&lt;/h5&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; 如有在云提供商的负载均衡器实现了对 &lt;code&gt;SCTP&lt;/code&gt; 协议支持时才能够创建一个类型为 LoadBalancer 并且
&lt;code&gt;protocol&lt;/code&gt; 为 SCTP 的 Service. 否则 Service 的创建请求会被拒绝。 目前的云负载均衡提供者
(Azure, AWS, CloudStack, GCE, OpenStack) 都缺乏对 SCTP 的支持。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h5 id=&#34;caveat-sctp-windows-os&#34;&gt;Windows&lt;/h5&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; 基于 Windows 的节点不支持 SCTP&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h5 id=&#34;caveat-sctp-kube-proxy-userspace&#34;&gt;userspace kube-proxy&lt;/h5&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; 使用 userspace 模式的 kube-proxy 不支持对 SCTP 连接的管理&lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;对SCTP不了解，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/connect-applications-service/&#34;&gt;通过 Service 连接应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/ingress/&#34;&gt;Ingress&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/endpoint-slices/&#34;&gt;EndpointSlices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Service Topology</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service-topology/</link>
      <pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service-topology/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- johnbelamaric
- imroc
title: Service Topology
feature:
  title: Service Topology
  description: &gt;
    Routing of service traffic based upon cluster topology.

content_type: concept
weight: 10
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [alpha]&lt;/code&gt;
&lt;/div&gt;



_Service Topology_ enables a service to route traffic based upon the Node
topology of the cluster. For example, a service can specify that traffic be
preferentially routed to endpoints that are on the same Node as the client, or
in the same availability zone.
 --&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;em&gt;Service Topology&lt;/em&gt; 让 Service 可以根据集群中节点的拓扑结构来路由流量。 例如， 一个 Service
可以配置为 流量优先路由到与客户端相同的节点或在同一个可用区的 Endpoint.&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Introduction

By default, traffic sent to a `ClusterIP` or `NodePort` Service may be routed to
any backend address for the Service. Since Kubernetes 1.7 it has been possible
to route &#34;external&#34; traffic to the Pods running on the Node that received the
traffic, but this is not supported for `ClusterIP` Services, and more complex
topologies &amp;mdash; such as routing zonally &amp;mdash; have not been possible. The
_Service Topology_ feature resolves this by allowing the Service creator to
define a policy for routing traffic based upon the Node labels for the
originating and destination Nodes.

By using Node label matching between the source and destination, the operator
may designate groups of Nodes that are &#34;closer&#34; and &#34;farther&#34; from one another,
using whatever metric makes sense for that operator&#39;s requirements. For many
operators in public clouds, for example, there is a preference to keep service
traffic within the same zone, because interzonal traffic has a cost associated
with it, while intrazonal traffic does not. Other common needs include being able
to route traffic to a local Pod managed by a DaemonSet, or keeping traffic to
Nodes connected to the same top-of-rack switch for the lowest latency.
 --&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;默认情况下， 发送到 Service  &lt;code&gt;ClusterIP&lt;/code&gt; 或 &lt;code&gt;NodePort&lt;/code&gt; 的流量会路由到 Service 的任意一个
后端实例地址。 从 k8s 1.7 开始让外部流量路由到接收到流量的节点上运行的 Pod 上成功可能。
但是这不支持 &lt;code&gt;ClusterIP&lt;/code&gt; 类型的 Service。 更复杂的拓扑结构  — 例如根据分区路由 —
也不可能。 &lt;em&gt;Service Topology&lt;/em&gt;  提供了让 Service 创建者基于流量源和目标节点标签定义一个流量路由策略来解决定个问题&lt;/p&gt;
&lt;p&gt;通过比对源和目标节点的标签， 使用一些必要的度量，就可以推断出这一组节点相对于另一组节点是更近还是更远，
许多在公有云的操作器，例如, 优先将 Service 的流量保持在同一个区里面， 因为一般在公有云跨区流量是
收费的，但区内流量是免费的。其它常见的包括能够路由流量到一个由 DaemonSet 管理的本地 Pod， 或
保持流量在同一个架顶式交换机以达到较低的延迟。&lt;/p&gt;
&lt;!--
## Using Service Topology

If your cluster has Service Topology enabled, you can control Service traffic
routing by specifying the `topologyKeys` field on the Service spec. This field
is a preference-order list of Node labels which will be used to sort endpoints
when accessing this Service. Traffic will be directed to a Node whose value for
the first label matches the originating Node&#39;s value for that label. If there is
no backend for the Service on a matching Node, then the second label will be
considered, and so forth, until no labels remain.

If no match is found, the traffic will be rejected, just as if there were no
backends for the Service at all. That is, endpoints are chosen based on the first
topology key with available backends. If this field is specified and all entries
have no backends that match the topology of the client, the service has no
backends for that client and connections should fail. The special value `&#34;*&#34;` may
be used to mean &#34;any topology&#34;. This catch-all value, if used, only makes sense
as the last value in the list.

If `topologyKeys` is not specified or empty, no topology constraints will be applied.

Consider a cluster with Nodes that are labeled with their hostname, zone name,
and region name. Then you can set the `topologyKeys` values of a service to direct
traffic as follows.

* Only to endpoints on the same node, failing if no endpoint exists on the node:
  `[&#34;kubernetes.io/hostname&#34;]`.
* Preferentially to endpoints on the same node, falling back to endpoints in the
  same zone, followed by the same region, and failing otherwise: `[&#34;kubernetes.io/hostname&#34;,
  &#34;topology.kubernetes.io/zone&#34;, &#34;topology.kubernetes.io/region&#34;]`.
  This may be useful, for example, in cases where data locality is critical.
* Preferentially to the same zone, but fallback on any available endpoint if
  none are available within this zone:
  `[&#34;topology.kubernetes.io/zone&#34;, &#34;*&#34;]`.

 --&gt;
&lt;h2 id=&#34;使用-service-topology&#34;&gt;使用 Service Topology&lt;/h2&gt;
&lt;p&gt;如果集群启用了 Service Topology， 可以通过 Service 配置的 &lt;code&gt;topologyKeys&lt;/code&gt; 字段来控制
Service 流量路由方式。 这个字段是一个用于在访问该 Service 时对 Endpoint 排序的一个节点标签的
优先顺序列表。 流量会优先转发到第一个标签与源节点同一个标签有相同值的节点。 如果 Service 后端
的所有节点都没有匹配到，就会尝试使用下一个，直至所有标签都试完。&lt;/p&gt;
&lt;p&gt;如果最终一个匹配都没有找到，则这个流量就会被拒绝，就像 Service 根本就没有后端一样。 也就是说，
Endpoint 是基于第一个有可用后端的拓扑键来选择的。 如果设置了这个字段，但所以有条目都没有
与客户端的条目相匹配的，则 Service 便就有针对该客户端的后端，连接就会失败。 有一个特殊的值可以
用来表示 &amp;ldquo;任意拓扑&amp;rdquo;。 这是一个匹配所有的值，如果要使用，只有作为列表的最后一个值才有意义。&lt;/p&gt;
&lt;p&gt;如果 &lt;code&gt;topologyKeys&lt;/code&gt; 没有设置或值为空，则不会应用任何拓扑约束。&lt;/p&gt;
&lt;p&gt;假定有一个集群中的节点都打上它们的主机名，分区名，地区名。则可以设置 &lt;code&gt;topologyKeys&lt;/code&gt; 如果值来转发流量&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[&amp;quot;kubernetes.io/hostname&amp;quot;]&lt;/code&gt;: 只匹配同一个节点上的 Endpoint, 如果节点上没有对应的 Endpoint 则失败&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;优先使用同节点上匹配的 Endpoint, 如果没则使用同分区匹配的 Endpoint, 要是还没有则使用同地区
匹配的 Endpoint, 最后都没匹配则失败。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[&amp;quot;topology.kubernetes.io/zone&amp;quot;, &amp;quot;*&amp;quot;]&lt;/code&gt;: 优先使用同分区匹配的 Endpoint，
如果没则任意该 Service 的可用 Endpoint 都可以。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Constraints

* Service topology is not compatible with `externalTrafficPolicy=Local`, and
  therefore a Service cannot use both of these features. It is possible to use
  both features in the same cluster on different Services, just not on the same
  Service.

* Valid topology keys are currently limited to `kubernetes.io/hostname`,
  `topology.kubernetes.io/zone`, and `topology.kubernetes.io/region`, but will
  be generalized to other node labels in the future.

* Topology keys must be valid label keys and at most 16 keys may be specified.

* The catch-all value, `&#34;*&#34;`, must be the last value in the topology keys, if
  it is used.
 --&gt;
&lt;h2 id=&#34;限制&#34;&gt;限制&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Service Topology 与 &lt;code&gt;externalTrafficPolicy=Local&lt;/code&gt; 不兼容，因此同一个 Serice
不能同时使用这两个特性。 但可以在同一个集群中的不同的 Service 中分别使用一个特性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;目前有效的键只有 &lt;code&gt;kubernetes.io/hostname&lt;/code&gt;, &lt;code&gt;topology.kubernetes.io/zone&lt;/code&gt;,
&lt;code&gt;topology.kubernetes.io/region&lt;/code&gt;，未来可能包含其它的节点标签&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Topology 必须是有效的标签键，最多只能有 16 个键&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;匹配所有的 &lt;code&gt;&amp;quot;*&amp;quot;&lt;/code&gt;，如果要用，只能用作最后一个键&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Examples

The following are common examples of using the Service Topology feature.

### Only Node Local Endpoints

A Service that only routes to node local endpoints. If no endpoints exist on the node, traffic is dropped:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  topologyKeys:
    - &#34;kubernetes.io/hostname&#34;
```
 --&gt;
&lt;h2 id=&#34;示例&#34;&gt;示例&lt;/h2&gt;
&lt;p&gt;The following are common examples of using the Service Topology feature.
以下为 Service Topology 特性常见用法的示例&lt;/p&gt;
&lt;h3 id=&#34;仅限当前节点的-endpoint&#34;&gt;仅限当前节点的 Endpoint&lt;/h3&gt;
&lt;p&gt;这个 Service 只路由到本节点的 Endpoint， 如果本节点没有匹配的 Endpoint，流量被丢弃:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-app&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;topologyKeys&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/hostname&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Prefer Node Local Endpoints

A Service that prefers node local Endpoints but falls back to cluster wide endpoints if node local endpoints do not exist:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  topologyKeys:
    - &#34;kubernetes.io/hostname&#34;
    - &#34;*&#34;
```
 --&gt;
&lt;h3 id=&#34;优先使用本节点的-endpoint&#34;&gt;优先使用本节点的 Endpoint&lt;/h3&gt;
&lt;p&gt;这个 Service 优先使用本节点匹配的 Endpoint，如果没则使用集群中可用的 Endpoint:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-app&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;topologyKeys&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/hostname&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Only Zonal or Regional Endpoints

A Service that prefers zonal then regional endpoints. If no endpoints exist in either, traffic is dropped.


```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  topologyKeys:
    - &#34;topology.kubernetes.io/zone&#34;
    - &#34;topology.kubernetes.io/region&#34;
```
--&gt;
&lt;h3 id=&#34;仅限本分区或本地区的-endpoint&#34;&gt;仅限本分区或本地区的 Endpoint&lt;/h3&gt;
&lt;p&gt;这个 Service 优先使用本分区匹配的 Endpoint, 要是没有则使用同地区匹配的 Endpoint, 要是还没有就丢弃流量。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-app&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;topologyKeys&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;topology.kubernetes.io/zone&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;topology.kubernetes.io/region&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Prefer Node Local, Zonal, then Regional Endpoints

A Service that prefers node local, zonal, then regional endpoints but falls back to cluster wide endpoints.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  topologyKeys:
    - &#34;kubernetes.io/hostname&#34;
    - &#34;topology.kubernetes.io/zone&#34;
    - &#34;topology.kubernetes.io/region&#34;
    - &#34;*&#34;
```
 --&gt;
&lt;h3 id=&#34;本节点本分区本地区依次优先&#34;&gt;本节点，本分区，本地区依次优先&lt;/h3&gt;
&lt;p&gt;这个 Service 按照 本节点，本分区，本地区 的顺序依次匹配，如果三个都没有匹配到，则匹配全集群的 Endpoint&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-app&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;topologyKeys&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/hostname&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;topology.kubernetes.io/zone&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;topology.kubernetes.io/region&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/administer-cluster/enabling-service-topology&#34;&gt;启用 Service Topology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/connect-applications-service/&#34;&gt;通过 Service 连接应用&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 持久化卷(PV)</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/persistent-volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/persistent-volumes/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jsafrane
- saad-ali
- thockin
- msau42
- xing-yang
title: Persistent Volumes
feature:
  title: Storage orchestration
  description: &gt;
    Automatically mount the storage system of your choice, whether from local storage, a public cloud provider such as &lt;a href=&#34;https://cloud.google.com/storage/&#34;&gt;GCP&lt;/a&gt; or &lt;a href=&#34;https://aws.amazon.com/products/storage/&#34;&gt;AWS&lt;/a&gt;, or a network storage system such as NFS, iSCSI, Gluster, Ceph, Cinder, or Flocker.

content_type: concept
weight: 20
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This document describes the current state of _persistent volumes_ in Kubernetes. Familiarity with [volumes](/docs/concepts/storage/volumes/) is suggested.
 --&gt;
&lt;p&gt;本文主要介绍当前 k8s 中 &lt;em&gt;持久化卷(persistent volume)&lt;/em&gt; 的状态。
建议先熟悉 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/storage/volumes/&#34;&gt;volumes&lt;/a&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Introduction

Managing storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed. To do this, we introduce two new API resources:  PersistentVolume and PersistentVolumeClaim.

A _PersistentVolume_ (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using [Storage Classes](/docs/concepts/storage/storage-classes/). It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.

A _PersistentVolumeClaim_ (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory).  Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see [AccessModes](#access-modes)).

While PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than just size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the _StorageClass_ resource.

See the [detailed walkthrough with working examples](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/).
 --&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;管理存储和管理实例是两个不同的问题。 PersistentVolume 子系统为用户和管理员提供了一套 API 将
存储是怎么提供的细节从存储使用中抽象出来。 而为了做到这一个我们要介绍两个新的 API 资源:
PersistentVolume 和 PersistentVolumeClaim.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PersistentVolume&lt;/em&gt; (PV) 是集群中的一块存储，它可能由管理员管理或使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-classes/&#34;&gt;StorageClass&lt;/a&gt;
动态管理。它是集群中的一个资源，就像节点是集群中的一个资源一样。 PV 是与卷(Volume)类似的卷插件，
但它的生命周期与使用它的 Pod 的生命周期是相互独立的。这个 API 对象中包含了存储的实现细节，包含
NFS, iSCSI, 云服务提供的存储系统。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PersistentVolumeClaim&lt;/em&gt; (PVC) 是一个用户对存储的请求。 它就像是一个 Pod。Pod 使用的是节点上的资源
而 PVC 使用的是 PV 资源。 Pod 可以申请指定级别的资源(CPU 和 Memory)。 PVC 可以申请指定容量的存储
和访问模式 (如，它们可以以 &lt;code&gt;ReadWriteOnce&lt;/code&gt;, &lt;code&gt;ReadOnlyMany&lt;/code&gt; 或 &lt;code&gt;ReadWriteMany&lt;/code&gt; 方式挂载，
见 &lt;a href=&#34;#access-modes&#34;&gt;AccessMode&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;因为 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 允许用户使用抽象的存储资源，通常用户都需要 PersistentVolume
中包含许多属性，如性能，来应对不同的问题。 集群管理员需要能够提供多样的 PersistentVolume，而不
仅仅是容量和访问模式，还需要向用户提供这些卷的实现细节。 为了满足这些需求，我们就有了 &lt;em&gt;StorageClass&lt;/em&gt; 资源。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-persistent-volume-storage/&#34;&gt;亲自上手试试&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Lifecycle of a volume and claim

PVs are resources in the cluster. PVCs are requests for those resources and also act as claim checks to the resource. The interaction between PVs and PVCs follows this lifecycle:
 --&gt;
&lt;h2 id=&#34;pv-和-pvc-的生命周期&#34;&gt;PV 和 PVC 的生命周期&lt;/h2&gt;
&lt;p&gt;PV 是集群中的资源。 PVC 是对这些资源的申请同时也表现为对这些资源的检查声明。PV 和 PVC 之间的
相互影响遵守以下生命周期:&lt;/p&gt;
&lt;!--
### Provisioning

There are two ways PVs may be provisioned: statically or dynamically.
 --&gt;
&lt;h3 id=&#34;供给&#34;&gt;供给&lt;/h3&gt;
&lt;p&gt;PV 可以被两个方式提供: 静态供给 或 动态供给&lt;/p&gt;
&lt;!--
#### Static

A cluster administrator creates a number of PVs. They carry the details of the real storage, which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.
 --&gt;
&lt;h4 id=&#34;静态供给&#34;&gt;静态供给&lt;/h4&gt;
&lt;p&gt;集群管理创建一系列 PV。 它们包含真实存储的细节，可以被集群用户使用。
它们存在于 k8s API 中，可以被取用。&lt;/p&gt;
&lt;!--
#### Dynamic

When none of the static PVs the administrator created match a user&#39;s PersistentVolumeClaim,
the cluster may try to dynamically provision a volume specially for the PVC.
This provisioning is based on StorageClasses: the PVC must request a
[storage class](/docs/concepts/storage/storage-classes/) and
the administrator must have created and configured that class for dynamic
provisioning to occur. Claims that request the class `&#34;&#34;` effectively disable
dynamic provisioning for themselves.

To enable dynamic storage provisioning based on storage class, the cluster administrator
needs to enable the `DefaultStorageClass` [admission controller](/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)
on the API server. This can be done, for example, by ensuring that `DefaultStorageClass` is
among the comma-delimited, ordered list of values for the `--enable-admission-plugins` flag of
the API server component. For more information on API server command-line flags,
check [kube-apiserver](/docs/admin/kube-apiserver/) documentation.
 --&gt;
&lt;h4 id=&#34;动态供给&#34;&gt;动态供给&lt;/h4&gt;
&lt;p&gt;当管理创建的 PV 不能满足用户的 PersistentVolumeClaim 时，集群可能就会尝试为这个 PVC 动态
提供一个卷。这种供给基于 &lt;code&gt;StorageClass&lt;/code&gt;: PVC 必须要申请一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/storage/storage-classes/&#34;&gt;StorageClass&lt;/a&gt;
并且在动态供给发生之前管理员必须要完成该 &lt;code&gt;StorageClass&lt;/code&gt; 的创建和配置。
如果 PVC 的 &lt;code&gt;StorageClass&lt;/code&gt; 被设置为 &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt; 实际上表示关闭自身的动态供给。&lt;/p&gt;
&lt;p&gt;要启用基于 &lt;code&gt;StorageClass&lt;/code&gt; 的动态存储供给需要集群管理在 api-server 中的
&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass&#34;&gt;准入控制&lt;/a&gt;
中启用 &lt;code&gt;DefaultStorageClass&lt;/code&gt;。 具体的配置方法就是确认 api-server 组件的 &lt;code&gt;--enable-admission-plugins&lt;/code&gt; 选项
的值中有没有 &lt;code&gt;DefaultStorageClass&lt;/code&gt;。 更多关于 api-server 命令行参数见
&lt;a href=&#34;https://kubernetes.io/docs/admin/kube-apiserver/&#34;&gt;kube-apiserver&lt;/a&gt; 文档&lt;/p&gt;
&lt;!--
### Binding

A user creates, or in the case of dynamic provisioning, has already created, a PersistentVolumeClaim with a specific amount of storage requested and with certain access modes. A control loop in the master watches for new PVCs, finds a matching PV (if possible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim.

Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster.
 --&gt;
&lt;h3 id=&#34;binding&#34;&gt;绑定&lt;/h3&gt;
&lt;p&gt;当一个指定容量和访问模式的 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 被创建后。 主控中心中的一个控制回环会监控新创建的 PVC，
如果有匹配的 PV 存在，则将它们绑定在一起。 如果这个 PV 是动态供给给这个新建的 PVC 的， 那么控制
回环将会始终将这个 PV 绑定到这个 PVC。 否则用户会得到至少满足其请求的卷，但这个卷容量可能实际是超出请求的。
当绑定完成，就会确定绑定关系，不管它们是怎么绑定的。 PVC 到 PV 的绑定是一对一关系，PV 使用的 &lt;code&gt;spec.claimRef&lt;/code&gt;
实现 &lt;code&gt;PersistentVolume&lt;/code&gt; 与 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 的双向绑定。&lt;/p&gt;
&lt;p&gt;如果没有匹配的 PV 存在，PVC 会永远保持在未绑定状态。 当有可用的匹配 PV 出现时 PVC 就会绑定。
例如， 集群中提供许多 50Gi PV 是不会被一个申请 100Gi 的 PVC 匹配到的。当集群中添加了一个
100Gi PV 时，这个 PVC 就可以绑定了。&lt;/p&gt;
&lt;!--
### Using

Pods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a Pod. For volumes that support multiple access modes, the user specifies which mode is desired when using their claim as a volume in a Pod.

Once a user has a claim and that claim is bound, the bound PV belongs to the user for as long as they need it. Users schedule Pods and access their claimed PVs by including a `persistentVolumeClaim` section in a Pod&#39;s `volumes` block. See [Claims As Volumes](#claims-as-volumes) for more details on this.
 --&gt;
&lt;h3 id=&#34;使用&#34;&gt;使用&lt;/h3&gt;
&lt;p&gt;Pod 会将 PVC 当作卷来使用。 集群会检视这个 PVC 找到它绑定的 PV 然后将这个 PV 挂载到 Pod 中。
对于支持多次访问模式的卷，用户在 Pod 当作卷的 PVC 中指定需要的访问模式。&lt;/p&gt;
&lt;p&gt;当一个用户拥有一个 PVC 并且这个 PVC 完成绑定，则这个被绑定的 PV 在用户需要时始终属于该用户。
用户通过 Pod 中的 &lt;code&gt;volumes&lt;/code&gt; 配置区中添加 &lt;code&gt;persistentVolumeClaim&lt;/code&gt; 配置区来访问这些由
PVC 管理的 PV。
更多信息见 &lt;a href=&#34;#claims-as-volumes&#34;&gt;将 PVC 当作卷(PV)&lt;/a&gt;&lt;/p&gt;
&lt;!--
### Storage Object in Use Protection
The purpose of the Storage Object in Use Protection feature is to ensure that PersistentVolumeClaims (PVCs) in active use by a Pod and PersistentVolume (PVs) that are bound to PVCs are not removed from the system, as this may result in data loss.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; PVC is in active use by a Pod when a Pod object exists that is using the PVC.&lt;/div&gt;
&lt;/blockquote&gt;


If a user deletes a PVC in active use by a Pod, the PVC is not removed immediately. PVC removal is postponed until the PVC is no longer actively used by any Pods. Also, if an admin deletes a PV that is bound to a PVC, the PV is not removed immediately. PV removal is postponed until the PV is no longer bound to a PVC.

You can see that a PVC is protected when the PVC&#39;s status is `Terminating` and the `Finalizers` list includes `kubernetes.io/pvc-protection`:

```shell
kubectl describe pvc hostpath
Name:          hostpath
Namespace:     default
StorageClass:  example-hostpath
Status:        Terminating
Volume:
Labels:        &lt;none&gt;
Annotations:   volume.beta.kubernetes.io/storage-class=example-hostpath
               volume.beta.kubernetes.io/storage-provisioner=example.com/hostpath
Finalizers:    [kubernetes.io/pvc-protection]
...
```

You can see that a PV is protected when the PV&#39;s status is `Terminating` and the `Finalizers` list includes `kubernetes.io/pv-protection` too:

```shell
kubectl describe pv task-pv-volume
Name:            task-pv-volume
Labels:          type=local
Annotations:     &lt;none&gt;
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    standard
Status:          Terminating
Claim:
Reclaim Policy:  Delete
Access Modes:    RWO
Capacity:        1Gi
Message:
Source:
    Type:          HostPath (bare host directory volume)
    Path:          /tmp/data
    HostPathType:
Events:            &lt;none&gt;
```
 --&gt;
&lt;h3 id=&#34;使用保护模式的存储对象&#34;&gt;使用保护模式的存储对象&lt;/h3&gt;
&lt;p&gt;保护模式的存储对象特性的目的是保证被 Pod 使用的有效的 PVC 和与 PVC 绑定的 PV 能会被系统删除，
从而可能导致数据丢失。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 当一个使用 PVC 的 Pod 对象存在时就表示 PVC 被 Pod 有效使用。&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;如果用户删除了一个被 Pod 有效使用的 PVC， 这个 PVC 不会立马被删除。 PVC 会延迟到没有被任何
Pod 有效使用后才会删除。 同样，如果管理员删除了一个与 PVC 绑定的 PV， 这个 PV 也不会被立马删除，
PV 的删除行为会被延迟到不再与 PVC 绑定时。&lt;/p&gt;
&lt;p&gt;当 PVC 的状态是 &lt;code&gt;Terminating&lt;/code&gt; 并且 &lt;code&gt;Finalizers&lt;/code&gt; 列表中包含 &lt;code&gt;kubernetes.io/pvc-protection&lt;/code&gt;
就表示这个 PVC 是被保护的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe pvc hostpath
Name:          hostpath
Namespace:     default
StorageClass:  example-hostpath
Status:        Terminating
Volume:
Labels:        &amp;lt;none&amp;gt;
Annotations:   volume.beta.kubernetes.io/storage-class&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;example-hostpath
               volume.beta.kubernetes.io/storage-provisioner&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;example.com/hostpath
Finalizers:    &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;kubernetes.io/pvc-protection&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;同样，当 PV 的状态是 &lt;code&gt;Terminating&lt;/code&gt; 并且 &lt;code&gt;Finalizers&lt;/code&gt; 列表中包含 &lt;code&gt;kubernetes.io/pvc-protection&lt;/code&gt;
就表示这个 PV 是被保护的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe pv task-pv-volume
Name:            task-pv-volume
Labels:          type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;local
Annotations:     &amp;lt;none&amp;gt;
Finalizers:      &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;kubernetes.io/pv-protection&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
StorageClass:    standard
Status:          Terminating
Claim:
Reclaim Policy:  Delete
Access Modes:    RWO
Capacity:        1Gi
Message:
Source:
    Type:          HostPath &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;bare host directory volume&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
    Path:          /tmp/data
    HostPathType:
Events:            &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Reclaiming

When a user is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained, Recycled, or Deleted.
 --&gt;
&lt;h3 id=&#34;回收&#34;&gt;回收&lt;/h3&gt;
&lt;p&gt;当用于不再需要一个卷时，可以通过 API 删除这个 PVC 对象，这样就允许对对应资源的回收。 PV 的回收
策略让集群可以在 PV 释放以后使用对应的方式回收。 目前，卷的回收策略有 保留(&lt;code&gt;Retain&lt;/code&gt;),
循环使用(&lt;code&gt;Recycle&lt;/code&gt;), 删除 (&lt;code&gt;Delete&lt;/code&gt;).&lt;/p&gt;
&lt;!--
#### Retain

The `Retain` reclaim policy allows for manual reclamation of the resource. When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered &#34;released&#34;. But it is not yet available for another claim because the previous claimant&#39;s data remains on the volume. An administrator can manually reclaim the volume with the following steps.

1. Delete the PersistentVolume. The associated storage asset in external infrastructure (such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume) still exists after the PV is deleted.
1. Manually clean up the data on the associated storage asset accordingly.
1. Manually delete the associated storage asset, or if you want to reuse the same storage asset, create a new PersistentVolume with the storage asset definition.
 --&gt;
&lt;h4 id=&#34;保留retained&#34;&gt;保留(&lt;code&gt;Retained&lt;/code&gt;)&lt;/h4&gt;
&lt;p&gt;保留(&lt;code&gt;Retained&lt;/code&gt;) 回收策略允许对资源手动回收。 当 PVC 被删除后，其之前绑定的 PV 依然存在并被
认为是已经释放。 但是它还不被另一个 PVC 使用，因为其中还有上一个 PVC 时的数据。 管理员可以
通过以下步骤手动回收这个卷:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;删除 PV 对象。 它所关联的外部存储设施(如 AWS EBS, GCE PD, Azure Disk, Cinder 卷)依然存在。&lt;/li&gt;
&lt;li&gt;根据需要手动清理对应存储资源上的数据&lt;/li&gt;
&lt;li&gt;手动删除对应的存储资源，如果想要重新使用该存储资源，可以再创建一个新的 PV 对象与之关联。&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
#### Delete

For volume plugins that support the `Delete` reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume. Volumes that were dynamically provisioned inherit the [reclaim policy of their StorageClass](#reclaim-policy), which defaults to `Delete`. The administrator should configure the StorageClass according to users&#39; expectations; otherwise, the PV must be edited or patched after it is created. See [Change the Reclaim Policy of a PersistentVolume](/docs/tasks/administer-cluster/change-pv-reclaim-policy/).
 --&gt;
&lt;h4 id=&#34;删除-delete&#34;&gt;删除 (&lt;code&gt;Delete&lt;/code&gt;)&lt;/h4&gt;
&lt;p&gt;对于支持 删除 (&lt;code&gt;Delete&lt;/code&gt;) 回收策略的卷插件，在删除 PVC 对象之后与其郑的 PV 对象也会被删除，
而且对应的外部存储资源，如 AWS EBS, GCE PD, Azure Disk, Cinder 卷也会一同被删除。
动态提供的卷的回收策略继承自 &lt;a href=&#34;#reclaim-policy&#34;&gt;它们的 StorageClass 的回收策略&lt;/a&gt;，默认为 删除 (&lt;code&gt;Delete&lt;/code&gt;)
管理应该根据用户期望设置 StorageClass 的回收策略，否则 PV 在创建后再需要再次修改才行。
见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/change-pv-reclaim-policy/&#34;&gt;修改 PersistentVolume 的回收策略&lt;/a&gt;.&lt;/p&gt;
&lt;!--
#### Recycle

&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; The &lt;code&gt;Recycle&lt;/code&gt; reclaim policy is deprecated. Instead, the recommended approach is to use dynamic provisioning.&lt;/div&gt;
&lt;/blockquote&gt;


If supported by the underlying volume plugin, the `Recycle` reclaim policy performs a basic scrub (`rm -rf /thevolume/*`) on the volume and makes it available again for a new claim.

However, an administrator can configure a custom recycler Pod template using
the Kubernetes controller manager command line arguments as described in the
[reference](/docs/reference/command-line-tools-reference/kube-controller-manager/).
The custom recycler Pod template must contain a `volumes` specification, as
shown in the example below:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pv-recycler
  namespace: default
spec:
  restartPolicy: Never
  volumes:
  - name: vol
    hostPath:
      path: /any/path/it/will/be/replaced
  containers:
  - name: pv-recycler
    image: &#34;k8s.gcr.io/busybox&#34;
    command: [&#34;/bin/sh&#34;, &#34;-c&#34;, &#34;test -e /scrub &amp;&amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  &amp;&amp; test -z \&#34;$(ls -A /scrub)\&#34; || exit 1&#34;]
    volumeMounts:
    - name: vol
      mountPath: /scrub
```

However, the particular path specified in the custom recycler Pod template in the `volumes` part is replaced with the particular path of the volume that is being recycled.
--&gt;
&lt;h4 id=&#34;循环使用recycle&#34;&gt;循环使用(&lt;code&gt;Recycle&lt;/code&gt;)&lt;/h4&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; 循环使用(&lt;code&gt;Recycle&lt;/code&gt;) 回收策略已经废弃。 推荐使用动态供给方式。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果底层卷插件支持， 循环使用(&lt;code&gt;Recycle&lt;/code&gt;) 回收策略在卷上执行基础的清理操作 (&lt;code&gt;rm -rf /thevolume/*&lt;/code&gt;)
然后它就可以再次被新的 PVC 使用了。&lt;/p&gt;
&lt;p&gt;但是，管理也可以使用 k8s 控制管理器的命令行参数配置一个自定义的回收器 Pod 模板。具体见
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/
&#34;&gt;这里&lt;/a&gt;.
这个自定义的回收器 Pod 模板必须要包含 &lt;code&gt;volumes&lt;/code&gt; 定义，下面就是一个示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pv-recycler&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vol&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/any/path/it/will/be/replaced&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pv-recycler&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;k8s.gcr.io/busybox&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/bin/sh&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;test -e /scrub &amp;amp;&amp;amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  &amp;amp;&amp;amp; test -z \&amp;#34;$(ls -A /scrub)\&amp;#34; || exit 1&amp;#34;&lt;/span&gt;]
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vol&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/scrub&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;回收器 Pod 模板中的 &lt;code&gt;volumes&lt;/code&gt; 部分指定的路径，就是要被回收的卷&lt;/p&gt;
&lt;!--
### Reserving a PersistentVolume

The control plane can [bind PersistentVolumeClaims to matching PersistentVolumes](#binding) in the
cluster. However, if you want a PVC to bind to a specific PV, you need to pre-bind them.

By specifying a PersistentVolume in a PersistentVolumeClaim, you declare a binding between that specific PV and PVC.
If the PersistentVolume exists and has not reserved PersistentVolumeClaims through its `claimRef` field, then the PersistentVolume and PersistentVolumeClaim will be bound.

The binding happens regardless of some volume matching criteria, including node affinity.
The control plane still checks that [storage class](/docs/concepts/storage/storage-classes/), access modes, and requested storage size are valid.

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: foo-pvc
  namespace: foo
spec:
  storageClassName: &#34;&#34; # Empty string must be explicitly set otherwise default StorageClass will be set
  volumeName: foo-pv
  ...
```

This method does not guarantee any binding privileges to the PersistentVolume. If other PersistentVolumeClaims could use the PV that you specify, you first need to reserve that storage volume. Specify the relevant PersistentVolumeClaim in the `claimRef` field of the PV so that other PVCs can not bind to it.

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: foo-pv
spec:
  storageClassName: &#34;&#34;
  claimRef:
    name: foo-pvc
    namespace: foo
  ...
```

This is useful if you want to consume PersistentVolumes that have their `claimPolicy` set
to `Retain`, including cases where you are reusing an existing PV.
 --&gt;
&lt;h3 id=&#34;保留-persistentvolume&#34;&gt;保留 &lt;code&gt;PersistentVolume&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;控制中心可以在集群中将&lt;a href=&#34;#binding&#34;&gt; &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 与匹配的 &lt;code&gt;PersistentVolume&lt;/code&gt; 绑定&lt;/a&gt;。
但是，如果想要将 PVC 与指定 PV 绑定， 则需要预先绑定(而不是自动绑定)。&lt;/p&gt;
&lt;p&gt;通过在 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 中指定一个 &lt;code&gt;PersistentVolume&lt;/code&gt;，就可以将 PV 绑定到这个 PVC 上。
如果 PV 已经存在， 可能通过设置它的 &lt;code&gt;claimRef&lt;/code&gt; 字段指定一个 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;，
这样 PV 和 PVC 就会绑定。&lt;/p&gt;
&lt;p&gt;这种绑定会忽略一些卷匹配条件，包括节点亲和性。 控制中心还是会检测，
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/storage/storage-classes/&#34;&gt;StorageClass&lt;/a&gt;, 访问模式，申请容量是否是有效的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo-pvc&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# 这里必须要显示地设置，否则就会使用默认的 StorageClass&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo-pv&lt;/span&gt;
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这种方式不对绑定 PV 的优先级做任何保证。 如果其它的 PVC 能够使用过这个 PV，必须要先保留存储卷。
在需要绑定的 PV &lt;code&gt;claimRef&lt;/code&gt; 的 PVC 这样其它的 PVC 才不能绑定这个 PV。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo-pv&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;claimRef&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo-pvc&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo&lt;/span&gt;
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果想要使用 &lt;code&gt;claimPolicy&lt;/code&gt; 是 &lt;code&gt;Retain&lt;/code&gt; 的 PV 这一招很有用， 包括重复使用那些已经存在的 PV。&lt;/p&gt;
&lt;!--
### Expanding Persistent Volumes Claims






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [beta]&lt;/code&gt;
&lt;/div&gt;



Support for expanding PersistentVolumeClaims (PVCs) is now enabled by default. You can expand
the following types of volumes:

* gcePersistentDisk
* awsElasticBlockStore
* Cinder
* glusterfs
* rbd
* Azure File
* Azure Disk
* Portworx
* FlexVolumes
* CSI

You can only expand a PVC if its storage class&#39;s `allowVolumeExpansion` field is set to true.

``` yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gluster-vol-default
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: &#34;http://192.168.10.100:8080&#34;
  restuser: &#34;&#34;
  secretNamespace: &#34;&#34;
  secretName: &#34;&#34;
allowVolumeExpansion: true
```

To request a larger volume for a PVC, edit the PVC object and specify a larger
size. This triggers expansion of the volume that backs the underlying PersistentVolume. A
new PersistentVolume is never created to satisfy the claim. Instead, an existing volume is resized.
 --&gt;
&lt;h3 id=&#34;扩展-pvc&#34;&gt;扩展 PVC&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;对 PVC 的扩展默认是启用的。 可以对以下类型的卷进行扩展:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;gcePersistentDisk&lt;/li&gt;
&lt;li&gt;awsElasticBlockStore&lt;/li&gt;
&lt;li&gt;Cinder&lt;/li&gt;
&lt;li&gt;glusterfs&lt;/li&gt;
&lt;li&gt;rbd&lt;/li&gt;
&lt;li&gt;Azure File&lt;/li&gt;
&lt;li&gt;Azure Disk&lt;/li&gt;
&lt;li&gt;Portworx&lt;/li&gt;
&lt;li&gt;FlexVolumes&lt;/li&gt;
&lt;li&gt;CSI&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;只能对 StorageClass 的 &lt;code&gt;allowVolumeExpansion&lt;/code&gt; 字段为 &lt;code&gt;true&lt;/code&gt; PVC 进行扩展&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gluster-vol-default&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/glusterfs&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;resturl&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://192.168.10.100:8080&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;restuser&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;secretNamespace&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;allowVolumeExpansion&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;想要为 PVC 申请一个更大的卷，只需要修改 PVC 对象，设置一个更大的容量。 这会触发卷底层的 PV 的扩充。
这样做不会创建一个新的 PV 来满足这个 PVC， 而是通过修改原来的容量来实现。&lt;/p&gt;
&lt;!--
#### CSI Volume expansion






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [beta]&lt;/code&gt;
&lt;/div&gt;



Support for expanding CSI volumes is enabled by default but it also requires a specific CSI driver to support volume expansion. Refer to documentation of the specific CSI driver for more information.
 --&gt;
&lt;h4 id=&#34;csi-卷扩容&#34;&gt;CSI 卷扩容&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;对 CSI 卷扩展 扩容默认是开启的，但也是需要相应的 CSI 驱动支持卷扩容。 具体信息请参阅相应
CSI 驱动的文档。&lt;/p&gt;
&lt;!--
#### Resizing a volume containing a file system

You can only resize volumes containing a file system if the file system is XFS, Ext3, or Ext4.

When a volume contains a file system, the file system is only resized when a new Pod is using
the PersistentVolumeClaim in `ReadWrite` mode. File system expansion is either done when a Pod is starting up
or when a Pod is running and the underlying file system supports online expansion.

FlexVolumes allow resize if the driver is set with the `RequiresFSResize` capability to `true`.
The FlexVolume can be resized on Pod restart.
 --&gt;
&lt;h4 id=&#34;修改包含文件系统的卷的容量&#34;&gt;修改包含文件系统的卷的容量&lt;/h4&gt;
&lt;p&gt;如果郑文件系统是  XFS, Ext3, Ext4 之一，则可以修改其容量。&lt;/p&gt;
&lt;p&gt;当卷中包含文件文件系统时，只有在
新的 Pod 使用 &lt;code&gt;ReadWrite&lt;/code&gt; 模式的 PVC 时才会变更容量。文件系统的扩容只有在 Pod 启动或
正在运行的 Pod 底层的文件系统支持在线扩容时才能生效。&lt;/p&gt;
&lt;p&gt;FlexVolume 允许在 驱动的 &lt;code&gt;RequiresFSResize&lt;/code&gt; 设置为 &lt;code&gt;true&lt;/code&gt; 时变更容量。
FlexVolume 的容量变更只能在 Pod 重启时生效。&lt;/p&gt;
&lt;!--
#### Resizing an in-use PersistentVolumeClaim






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.15 [beta]&lt;/code&gt;
&lt;/div&gt;



&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Expanding in-use PVCs is available as beta since Kubernetes 1.15, and as alpha since 1.11. The &lt;code&gt;ExpandInUsePersistentVolumes&lt;/code&gt; feature must be enabled, which is the case automatically for many clusters for beta features. Refer to the &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;feature gate&lt;/a&gt; documentation for more information.&lt;/div&gt;
&lt;/blockquote&gt;


In this case, you don&#39;t need to delete and recreate a Pod or deployment that is using an existing PVC.
Any in-use PVC automatically becomes available to its Pod as soon as its file system has been expanded.
This feature has no effect on PVCs that are not in use by a Pod or deployment. You must create a Pod that
uses the PVC before the expansion can complete.


Similar to other volume types - FlexVolume volumes can also be expanded when in-use by a Pod.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; FlexVolume resize is possible only when the underlying driver supports resize.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Expanding EBS volumes is a time-consuming operation. Also, there is a per-volume quota of one modification every 6 hours.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h4 id=&#34;对使用中的-persistentvolumeclaim-变更容量&#34;&gt;对使用中的 PersistentVolumeClaim 变更容量&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.15 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 对使用中的 PersistentVolumeClaim 变更容量 在 k8s &lt;code&gt;v1.15&lt;/code&gt; 是 &lt;code&gt;beta&lt;/code&gt; 状态，
&lt;code&gt;v1.11&lt;/code&gt; 是 &lt;code&gt;alpha&lt;/code&gt; 状态, &lt;code&gt;ExpandInUsePersistentVolumes&lt;/code&gt; 在 &lt;code&gt;beta&lt;/code&gt; 时是默认打开的。
更多信息请见
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;在这种情况下，不需要删除或重新创建使用现有 PVC 的 Pod 或 Deployment. 任意使用中的 PVC 在
文件系统扩容后在 Pod 中自动变得可用。 这个特性对那些没有被 Pod 或 Deployment 的 PVC 无效。
要想扩容生效必须要有一个使用 PVC 的 Pod。&lt;/p&gt;
&lt;p&gt;与其它的卷类型类似， FlexVolume 也可以在被 Pod 使用时扩容。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; FlexVolume 变量容量只能在底层驱动支持的情况下才行。&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 对 EBS 卷扩容是一个耗时的操作。并且还有一个每个卷每 6 个小时只能扩容的限制&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
#### Recovering from Failure when Expanding Volumes

If expanding underlying storage fails, the cluster administrator can manually recover the Persistent Volume Claim (PVC) state and cancel the resize requests. Otherwise, the resize requests are continuously retried by the controller without administrator intervention.

1. Mark the PersistentVolume(PV) that is bound to the PersistentVolumeClaim(PVC) with `Retain` reclaim policy.
2. Delete the PVC. Since PV has `Retain` reclaim policy - we will not lose any data when we recreate the PVC.
3. Delete the `claimRef` entry from PV specs, so as new PVC can bind to it. This should make the PV `Available`.
4. Re-create the PVC with smaller size than PV and set `volumeName` field of the PVC to the name of the PV. This should bind new PVC to existing PV.
5. Don&#39;t forget to restore the reclaim policy of the PV.
 --&gt;
&lt;h4 id=&#34;从卷扩展失败中恢复&#34;&gt;从卷扩展失败中恢复&lt;/h4&gt;
&lt;p&gt;如果底层存储扩容的失败，集群管理可以通过手动恢复 PVC 的状态，取消扩容请求。否则，在没管理员中止
的情况下控制器会不断地重试容量变更请求。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将与 PersistentVolumeClaim(PVC) 绑定的 PersistentVolume(PV) 回收策略改为 &lt;code&gt;Retain&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;删除 PVC。 因为 PV 的回收策略是 &lt;code&gt;Retain&lt;/code&gt;，所以在重新创建 PVC 之前数据不会丢失。&lt;/li&gt;
&lt;li&gt;删除 PV 对象中的 &lt;code&gt;claimRef&lt;/code&gt; 实体， 这样新的 PVC 才可以与它绑定， 可以可以将 PV 变为 &lt;code&gt;Available&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;以小于 PV 的容量重新创建一个 PVC，这个 PVC 的 &lt;code&gt;volumeName&lt;/code&gt; 设置 PV 的名称。这样就可以将
原来的 PV 绑定到新的 PVC 上&lt;/li&gt;
&lt;li&gt;不要忘记将 PV 的回收策略也改加原来的值&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
## Types of Persistent Volumes

PersistentVolume types are implemented as plugins.  Kubernetes currently supports the following plugins:

* GCEPersistentDisk
* AWSElasticBlockStore
* AzureFile
* AzureDisk
* CSI
* FC (Fibre Channel)
* FlexVolume
* Flocker
* NFS
* iSCSI
* RBD (Ceph Block Device)
* CephFS
* Cinder (OpenStack block storage)
* Glusterfs
* VsphereVolume
* Quobyte Volumes
* HostPath (Single node testing only -- local storage is not supported in any way and WILL NOT WORK in a multi-node cluster)
* Portworx Volumes
* ScaleIO Volumes
* StorageOS
 --&gt;
&lt;h2 id=&#34;持久化卷的类型&#34;&gt;持久化卷的类型&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;PersistentVolume&lt;/code&gt; 的类型随同实现插件。 目前 k8s 支持以下插件:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GCEPersistentDisk&lt;/li&gt;
&lt;li&gt;AWSElasticBlockStore&lt;/li&gt;
&lt;li&gt;AzureFile&lt;/li&gt;
&lt;li&gt;AzureDisk&lt;/li&gt;
&lt;li&gt;CSI&lt;/li&gt;
&lt;li&gt;FC (Fibre Channel)&lt;/li&gt;
&lt;li&gt;FlexVolume&lt;/li&gt;
&lt;li&gt;Flocker&lt;/li&gt;
&lt;li&gt;NFS&lt;/li&gt;
&lt;li&gt;iSCSI&lt;/li&gt;
&lt;li&gt;RBD (Ceph Block Device)&lt;/li&gt;
&lt;li&gt;CephFS&lt;/li&gt;
&lt;li&gt;Cinder (OpenStack block storage)&lt;/li&gt;
&lt;li&gt;Glusterfs&lt;/li&gt;
&lt;li&gt;VsphereVolume&lt;/li&gt;
&lt;li&gt;Quobyte Volumes&lt;/li&gt;
&lt;li&gt;HostPath (只在单节点上测试过 &amp;ndash; 本地存储现在不支持多节点集群，将来也永远不会支持)&lt;/li&gt;
&lt;li&gt;Portworx Volumes&lt;/li&gt;
&lt;li&gt;ScaleIO Volumes&lt;/li&gt;
&lt;li&gt;StorageOS&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Persistent Volumes

Each PV contains a spec and status, which is the specification and status of the volume.
The name of a PersistentVolume object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /tmp
    server: 172.17.0.2
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Helper programs relating to the volume type may be required for consumption of a PersistentVolume within a cluster.  In this example, the PersistentVolume is of type NFS and the helper program /sbin/mount.nfs is required to support the mounting of NFS filesystems.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;persistentvolume&#34;&gt;PersistentVolume&lt;/h2&gt;
&lt;p&gt;每个 PV 包含一个 &lt;code&gt;spec&lt;/code&gt; 和 &lt;code&gt;status&lt;/code&gt;, 分别是对这个卷的配置定义和状态。 &lt;code&gt;PersistentVolume&lt;/code&gt;
对象的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pv0003&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;capacity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Filesystem&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeReclaimPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Recycle&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;mountOptions&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;hard&lt;/span&gt;
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;nfsvers=4.1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;nfs&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/tmp&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;server&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;172.17.0.2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Helper programs relating to the volume type may be required for consumption of a PersistentVolume within a cluster.  In this example, the PersistentVolume is of type NFS and the helper program /sbin/mount.nfs is required to support the mounting of NFS filesystems.
集群中辅助程序应该能够正确处理相关类型的卷。 在上面的例子中，PersistentVolume 的类型是 NFS
辅助程序 &lt;code&gt;/sbin/mount.nfs&lt;/code&gt; 就需要支持挂载 NFS 文件系统&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Capacity

Generally, a PV will have a specific storage capacity.  This is set using the PV&#39;s `capacity` attribute.  See the Kubernetes [Resource Model](https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md) to understand the units expected by `capacity`.

Currently, storage size is the only resource that can be set or requested.  Future attributes may include IOPS, throughput, etc.
 --&gt;
&lt;h3 id=&#34;容量capacity&#34;&gt;容量(Capacity)&lt;/h3&gt;
&lt;p&gt;通常，一个 PV 是有指定存储容量的。 这是通过 PV 的 &lt;code&gt;capacity&lt;/code&gt; 属性设置的。 理解受 &lt;code&gt;capacity&lt;/code&gt;
见 &lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md&#34;&gt;Resource Model&lt;/a&gt;
Currently, storage size is the only resource that can be set or requested.  Future attributes may include IOPS, throughput, etc.&lt;/p&gt;
&lt;!--
### Volume Mode






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;



Kubernetes supports two `volumeModes` of PersistentVolumes: `Filesystem` and `Block`.

`volumeMode` is an optional API parameter.
`Filesystem` is the default mode used when `volumeMode` parameter is omitted.

A volume with `volumeMode: Filesystem` is *mounted* into Pods into a directory. If the volume
is backed by a block device and the device is empty, Kuberneretes creates a filesystem
on the device before mounting it for the first time.

You can set the value of `volumeMode` to `Block` to use a volume as a raw block device.
Such volume is presented into a Pod as a block device, without any filesystem on it.
This mode is useful to provide a Pod the fastest possible way to access a volume, without
any filesystem layer between the Pod and the volume. On the other hand, the application
running in the Pod must know how to handle a raw block device.
See [Raw Block Volume Support](#raw-block-volume-support)
for an example on how to use a volume with `volumeMode: Block` in a Pod.
 --&gt;
&lt;h3 id=&#34;卷模式&#34;&gt;卷模式&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;k8s 支持两种 PersistentVolume 卷模式(&lt;code&gt;volumeModes&lt;/code&gt;): 文件系统(&lt;code&gt;Filesystem&lt;/code&gt;) 和 块设备(&lt;code&gt;Block&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;volumeMode&lt;/code&gt; 是一个可选 API 参数
如果没有设置 &lt;code&gt;volumeMode&lt;/code&gt;， 则默认使用 &lt;code&gt;Filesystem&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;一个卷模式为 &lt;code&gt;volumeMode: Filesystem&lt;/code&gt; 的卷 &lt;em&gt;挂载&lt;/em&gt; 到 Pod 中的一个目录。 如果卷的后端是
一个块设备并且这个设备是空的， k8s 会在第一次挂载之前在块设备上创建一个文件系统。&lt;/p&gt;
&lt;p&gt;也可以将 &lt;code&gt;volumeMode&lt;/code&gt; 的值设置为 &lt;code&gt;Block&lt;/code&gt; 以块设备的方式来使用这个卷。 这个卷就会以块设备的
形式存在于 Pod 中， 其中不会有任何文件系统。 这种方式在为 Pod 提供该卷可能最佳的访问速度，
在 Pod 和 卷之间没有任何文件系统层。 另一方面， Pod 中运行的应用必须要能正确使用块设备。&lt;/p&gt;
&lt;p&gt;可以在
&lt;a href=&#34;#raw-block-volume-support&#34;&gt;块设备卷支持&lt;/a&gt;
查看使用 &lt;code&gt;volumeMode: Block&lt;/code&gt; 卷模式卷的 Pod 。&lt;/p&gt;
&lt;!--
### Access Modes

A PersistentVolume can be mounted on a host in any way supported by the resource provider. As shown in the table below, providers will have different capabilities and each PV&#39;s access modes are set to the specific modes supported by that particular volume.  For example, NFS can support multiple read/write clients, but a specific NFS PV might be exported on the server as read-only. Each PV gets its own set of access modes describing that specific PV&#39;s capabilities.

The access modes are:

* ReadWriteOnce -- the volume can be mounted as read-write by a single node
* ReadOnlyMany -- the volume can be mounted read-only by many nodes
* ReadWriteMany -- the volume can be mounted as read-write by many nodes

In the CLI, the access modes are abbreviated to:

* RWO - ReadWriteOnce
* ROX - ReadOnlyMany
* RWX - ReadWriteMany

&gt; __Important!__ A volume can only be mounted using one access mode at a time, even if it supports many.  For example, a GCEPersistentDisk can be mounted as ReadWriteOnce by a single node or ReadOnlyMany by many nodes, but not at the same time.


| Volume Plugin        | ReadWriteOnce          | ReadOnlyMany          | ReadWriteMany|
| :---                 | :---:                  | :---:                 | :---:        |
| AWSElasticBlockStore | &amp;#x2713;               | -                     | -            |
| AzureFile            | &amp;#x2713;               | &amp;#x2713;              | &amp;#x2713;     |
| AzureDisk            | &amp;#x2713;               | -                     | -            |
| CephFS               | &amp;#x2713;               | &amp;#x2713;              | &amp;#x2713;     |
| Cinder               | &amp;#x2713;               | -                     | -            |
| CSI                  | depends on the driver  | depends on the driver | depends on the driver |
| FC                   | &amp;#x2713;               | &amp;#x2713;              | -            |
| FlexVolume           | &amp;#x2713;               | &amp;#x2713;              | depends on the driver |
| Flocker              | &amp;#x2713;               | -                     | -            |
| GCEPersistentDisk    | &amp;#x2713;               | &amp;#x2713;              | -            |
| Glusterfs            | &amp;#x2713;               | &amp;#x2713;              | &amp;#x2713;     |
| HostPath             | &amp;#x2713;               | -                     | -            |
| iSCSI                | &amp;#x2713;               | &amp;#x2713;              | -            |
| Quobyte              | &amp;#x2713;               | &amp;#x2713;              | &amp;#x2713;     |
| NFS                  | &amp;#x2713;               | &amp;#x2713;              | &amp;#x2713;     |
| RBD                  | &amp;#x2713;               | &amp;#x2713;              | -            |
| VsphereVolume        | &amp;#x2713;               | -                     | - (works when Pods are collocated)  |
| PortworxVolume       | &amp;#x2713;               | -                     | &amp;#x2713;     |
| ScaleIO              | &amp;#x2713;               | &amp;#x2713;              | -            |
| StorageOS            | &amp;#x2713;               | -                     | -            |
 --&gt;
&lt;h3 id=&#34;访问模式&#34;&gt;访问模式&lt;/h3&gt;
&lt;p&gt;A PersistentVolume can be mounted on a host in any way supported by the resource provider. As shown in the table below, providers will have different capabilities and each PV&amp;rsquo;s access modes are set to the specific modes supported by that particular volume.  For example, NFS can support multiple read/write clients, but a specific NFS PV might be exported on the server as read-only. Each PV gets its own set of access modes describing that specific PV&amp;rsquo;s capabilities.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PersistentVolume&lt;/code&gt; 可以以任意资源提供者支持的方式挂载到主机中。 如下表所示， 不同的提供者
拥有不同能力，每个 PV 的模式可以在对应卷上设置支持的模式。 例如， NFS 支持多客户端读写,但是可以
将一个 NFS PV 以只读方式提供。 每个 PV 都可以独立设置各自的方式模式。&lt;/p&gt;
&lt;p&gt;访问模式有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ReadWriteOnce &amp;ndash; 卷只能被一个消费者以只读方式挂载&lt;/li&gt;
&lt;li&gt;ReadOnlyMany &amp;ndash; 卷能被多个消费者以只读方式挂载&lt;/li&gt;
&lt;li&gt;ReadWriteMany &amp;ndash; 卷能被多个消费者以读写方式挂载&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在命令，这些模式的简写如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RWO - ReadWriteOnce&lt;/li&gt;
&lt;li&gt;ROX - ReadOnlyMany&lt;/li&gt;
&lt;li&gt;RWX - ReadWriteMany&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;重要!&lt;/strong&gt; 一个卷一次只能以一种访问模式挂载，即便它是支持多种的。 例如， 一个 GCEPersistentDisk 可以
在一个消费者挂载为 ReadWriteOnce 或可以在多个消费者挂载为 ReadOnlyMany， 但不能同时挂载
ReadWriteOnce 和 ReadOnlyMany。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Volume Plugin&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteOnce&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadOnlyMany&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteMany&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AWSElasticBlockStore&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AzureFile&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AzureDisk&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CephFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Cinder&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CSI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;基于驱动&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;基于驱动&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;基于驱动&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FC&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FlexVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;基于驱动&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Flocker&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;GCEPersistentDisk&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Glusterfs&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HostPath&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;iSCSI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Quobyte&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;RBD&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;VsphereVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;- (works when Pods are collocated)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PortworxVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ScaleIO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;StorageOS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
### Class

A PV can have a class, which is specified by setting the
`storageClassName` attribute to the name of a
[StorageClass](/docs/concepts/storage/storage-classes/).
A PV of a particular class can only be bound to PVCs requesting
that class. A PV with no `storageClassName` has no class and can only be bound
to PVCs that request no particular class.

In the past, the annotation `volume.beta.kubernetes.io/storage-class` was used instead
of the `storageClassName` attribute. This annotation is still working; however,
it will become fully deprecated in a future Kubernetes release.
 --&gt;
&lt;h3 id=&#34;class&#34;&gt;类别&lt;/h3&gt;
&lt;p&gt;一个 PV 可以有一个类别， 可以通过 &lt;code&gt;storageClassName&lt;/code&gt; 属性来设置一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/storage/storage-classes/&#34;&gt;StorageClass&lt;/a&gt;.
一个指定类别的 PV 只能与请求对应类的 PVC 相绑定。 没有设置 &lt;code&gt;storageClassName&lt;/code&gt; 的 PV 是没有类别的
并且只能与没有请求类别的 PVC 绑定。&lt;/p&gt;
&lt;p&gt;在过去是通过 &lt;code&gt;volume.beta.kubernetes.io/storage-class&lt;/code&gt; 注解设置类别而不是 &lt;code&gt;storageClassName&lt;/code&gt;。
这个注解目前还能用；但在未来的版本中会完全废弃。&lt;/p&gt;
&lt;!--
### Reclaim Policy

Current reclaim policies are:

* Retain -- manual reclamation
* Recycle -- basic scrub (`rm -rf /thevolume/*`)
* Delete -- associated storage asset such as AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder volume is deleted

Currently, only NFS and HostPath support recycling. AWS EBS, GCE PD, Azure Disk, and Cinder volumes support deletion.
 --&gt;
&lt;h3 id=&#34;reclain-policy&#34;&gt;回收策略&lt;/h3&gt;
&lt;p&gt;目前支持的回收策略:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Retain &amp;ndash; 手动回收&lt;/li&gt;
&lt;li&gt;Recycle &amp;ndash; 基础清理 (&lt;code&gt;rm -rf /thevolume/*&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Delete &amp;ndash; 关联存储资源如 AWS EBS, GCE PD, Azure Disk, OpenStack Cinder 卷也会被删除。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前只有 NFS 和 HostPath 支持 &lt;code&gt;Recycle&lt;/code&gt;
AWS EBS, GCE PD, Azure Disk, and Cinder 卷支持 &lt;code&gt;Delete&lt;/code&gt;&lt;/p&gt;
&lt;!--
### Mount Options

A Kubernetes administrator can specify additional mount options for when a Persistent Volume is mounted on a node.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Not all Persistent Volume types support mount options.&lt;/div&gt;
&lt;/blockquote&gt;


The following volume types support mount options:

* AWSElasticBlockStore
* AzureDisk
* AzureFile
* CephFS
* Cinder (OpenStack block storage)
* GCEPersistentDisk
* Glusterfs
* NFS
* Quobyte Volumes
* RBD (Ceph Block Device)
* StorageOS
* VsphereVolume
* iSCSI

Mount options are not validated, so mount will simply fail if one is invalid.

In the past, the annotation `volume.beta.kubernetes.io/mount-options` was used instead
of the `mountOptions` attribute. This annotation is still working; however,
it will become fully deprecated in a future Kubernetes release.
 --&gt;
&lt;h3 id=&#34;挂载选项&#34;&gt;挂载选项&lt;/h3&gt;
&lt;p&gt;k8s 管理员可以在 PV 挂载时指定额外的挂载选项。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 不是所有的 PV 类型都支持挂载选项&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;以下卷类型支持挂载选项:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AWSElasticBlockStore&lt;/li&gt;
&lt;li&gt;AzureDisk&lt;/li&gt;
&lt;li&gt;AzureFile&lt;/li&gt;
&lt;li&gt;CephFS&lt;/li&gt;
&lt;li&gt;Cinder (OpenStack block storage)&lt;/li&gt;
&lt;li&gt;GCEPersistentDisk&lt;/li&gt;
&lt;li&gt;Glusterfs&lt;/li&gt;
&lt;li&gt;NFS&lt;/li&gt;
&lt;li&gt;Quobyte Volumes&lt;/li&gt;
&lt;li&gt;RBD (Ceph Block Device)&lt;/li&gt;
&lt;li&gt;StorageOS&lt;/li&gt;
&lt;li&gt;VsphereVolume&lt;/li&gt;
&lt;li&gt;iSCSI&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;挂载选项不会验证，如果其中有无效选项就会挂载失败。&lt;/p&gt;
&lt;p&gt;在过去 使用的是 &lt;code&gt;volume.beta.kubernetes.io/mount-options&lt;/code&gt; 注解设置挂载选项，而不是 &lt;code&gt;mountOptions&lt;/code&gt;
这个注解目前还可以用，但在未来的版本中会完全废弃。&lt;/p&gt;
&lt;!--
### Node Affinity

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; For most volume types, you do not need to set this field. It is automatically populated for &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/#awselasticblockstore&#34;&gt;AWS EBS&lt;/a&gt;, &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/#gcepersistentdisk&#34;&gt;GCE PD&lt;/a&gt; and &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/#azuredisk&#34;&gt;Azure Disk&lt;/a&gt; volume block types. You need to explicitly set this for &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/#local&#34;&gt;local&lt;/a&gt; volumes.&lt;/div&gt;
&lt;/blockquote&gt;


A PV can specify [node affinity](/docs/reference/generated/kubernetes-api/v1.19/#volumenodeaffinity-v1-core) to define constraints that limit what nodes this volume can be accessed from. Pods that use a PV will only be scheduled to nodes that are selected by the node affinity.
 --&gt;
&lt;h3 id=&#34;节点亲和性&#34;&gt;节点亲和性&lt;/h3&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 对于大多数卷类型， 是不需要设置这个字段的。
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/storage/volumes/#awselasticblockstore&#34;&gt;AWS EBS&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/storage/volumes/#gcepersistentdisk&#34;&gt;GCE PD&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/storage/volumes/#azuredisk&#34;&gt;Azure Disk&lt;/a&gt;
卷块设备会自动添加。
但需要为
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/storage/volumes/#local&#34;&gt;local&lt;/a&gt;
卷需要显示设置该字段&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;PV 可以通过设置
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#volumenodeaffinity-v1-core&#34;&gt;节点亲和性&lt;/a&gt;
来定义限制哪些节点能够访问这个卷。
Pod 只会被调度到节点亲和性选择的节点上。&lt;/p&gt;
&lt;!--
### Phase

A volume will be in one of the following phases:

* Available -- a free resource that is not yet bound to a claim
* Bound -- the volume is bound to a claim
* Released -- the claim has been deleted, but the resource is not yet reclaimed by the cluster
* Failed -- the volume has failed its automatic reclamation

The CLI will show the name of the PVC bound to the PV.
 --&gt;
&lt;h3 id=&#34;阶段&#34;&gt;阶段&lt;/h3&gt;
&lt;p&gt;一个卷会处于以下阶段中的一个:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Available &amp;ndash; 一个空闲资源，还没有被绑定到 PVC&lt;/li&gt;
&lt;li&gt;Bound &amp;ndash; 这个卷被绑定了一个 PVC&lt;/li&gt;
&lt;li&gt;Released &amp;ndash; 绑定的 PVC 已经被删除，但资源还没有被集群回收&lt;/li&gt;
&lt;li&gt;Failed &amp;ndash; 这个卷自动回收失败&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;命令行可以显示与 PV 绑定的 PVC 的名称&lt;/p&gt;
&lt;!--
## PersistentVolumeClaims

Each PVC contains a spec and status, which is the specification and status of the claim.
The name of a PersistentVolumeClaim object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi
  storageClassName: slow
  selector:
    matchLabels:
      release: &#34;stable&#34;
    matchExpressions:
      - {key: environment, operator: In, values: [dev]}
```
 --&gt;
&lt;h2 id=&#34;persistentvolumeclaim&#34;&gt;PersistentVolumeClaim&lt;/h2&gt;
&lt;p&gt;每个 PVC 包含 &lt;code&gt;spec&lt;/code&gt; 和 &lt;code&gt;status&lt;/code&gt;, 其中包含 PVC 的配置定义和状态。
&lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 对象的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myclaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Filesystem&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;8Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;release&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stable&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
      - {&lt;span style=&#34;color:#f92672&#34;&gt;key: environment, operator: In, values&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;dev]}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Access Modes

Claims use the same conventions as volumes when requesting storage with specific access modes.
 --&gt;
&lt;h3 id=&#34;访问模式-1&#34;&gt;访问模式&lt;/h3&gt;
&lt;p&gt;Claims use the same conventions as volumes when requesting storage with specific access modes.
PVC 的访问模式与卷在请求存储时指定的访问模式一至&lt;/p&gt;
&lt;!--
### Volume Modes

Claims use the same convention as volumes to indicate the consumption of the volume as either a filesystem or block device.
 --&gt;
&lt;h3 id=&#34;卷模式-1&#34;&gt;卷模式&lt;/h3&gt;
&lt;p&gt;PVC 的卷模式与卷指定的卷模式一至，可以是 文件系统 或 块设备&lt;/p&gt;
&lt;!--
### Resources

Claims, like Pods, can request specific quantities of a resource. In this case, the request is for storage. The same [resource model](https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md) applies to both volumes and claims.
 --&gt;
&lt;h3 id=&#34;资源&#34;&gt;资源&lt;/h3&gt;
&lt;p&gt;PVC 与 Pod 类似， 可以请求指定数量的资源。 在这种情况下， 请求是提存储。
&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md&#34;&gt;资源模式&lt;/a&gt;
可应用到卷和 PVC&lt;/p&gt;
&lt;!--
### Selector

Claims can specify a [label selector](/docs/concepts/overview/working-with-objects/labels/#label-selectors) to further filter the set of volumes. Only the volumes whose labels match the selector can be bound to the claim. The selector can consist of two fields:

* `matchLabels` - the volume must have a label with this value
* `matchExpressions` - a list of requirements made by specifying key, list of values, and operator that relates the key and values. Valid operators include In, NotIn, Exists, and DoesNotExist.

All of the requirements, from both `matchLabels` and `matchExpressions`, are ANDed together – they must all be satisfied in order to match.
 --&gt;
&lt;h3 id=&#34;选择器&#34;&gt;选择器&lt;/h3&gt;
&lt;p&gt;PVC 可以指定一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/#label-selectors&#34;&gt;标签选择器&lt;/a&gt;
来选择卷集合。 只有与选择器匹配的卷可以与 PVC 绑定。 选择器可以包含以下两个字段&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;matchLabels&lt;/code&gt; - 卷必须要拥有与其对应的标签&lt;/li&gt;
&lt;li&gt;&lt;code&gt;matchExpressions&lt;/code&gt; - 一个由指定键和值列表，以及键和值相应的操作符组成的条件列表，
可用的操作符包含 &lt;code&gt;In&lt;/code&gt;, &lt;code&gt;NotIn&lt;/code&gt;, &lt;code&gt;Exists&lt;/code&gt;, &lt;code&gt;DoesNotExist&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;来自 &lt;code&gt;matchLabels&lt;/code&gt; 和 &lt;code&gt;matchExpressions&lt;/code&gt; 的所有条件以逻辑与方式组合。
必须要满足所有条件才能匹配到。&lt;/p&gt;
&lt;!--
### Class

A claim can request a particular class by specifying the name of a
[StorageClass](/docs/concepts/storage/storage-classes/)
using the attribute `storageClassName`.
Only PVs of the requested class, ones with the same `storageClassName` as the PVC, can
be bound to the PVC.

PVCs don&#39;t necessarily have to request a class. A PVC with its `storageClassName` set
equal to `&#34;&#34;` is always interpreted to be requesting a PV with no class, so it
can only be bound to PVs with no class (no annotation or one set equal to
`&#34;&#34;`). A PVC with no `storageClassName` is not quite the same and is treated differently
by the cluster, depending on whether the
[`DefaultStorageClass` admission plugin](/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)
is turned on.

* If the admission plugin is turned on, the administrator may specify a
  default StorageClass. All PVCs that have no `storageClassName` can be bound only to
  PVs of that default. Specifying a default StorageClass is done by setting the
  annotation `storageclass.kubernetes.io/is-default-class` equal to `true` in
  a StorageClass object. If the administrator does not specify a default, the
  cluster responds to PVC creation as if the admission plugin were turned off. If
  more than one default is specified, the admission plugin forbids the creation of
  all PVCs.
* If the admission plugin is turned off, there is no notion of a default
  StorageClass. All PVCs that have no `storageClassName` can be bound only to PVs that
  have no class. In this case, the PVCs that have no `storageClassName` are treated the
  same way as PVCs that have their `storageClassName` set to `&#34;&#34;`.

Depending on installation method, a default StorageClass may be deployed
to a Kubernetes cluster by addon manager during installation.

When a PVC specifies a `selector` in addition to requesting a StorageClass,
the requirements are ANDed together: only a PV of the requested class and with
the requested labels may be bound to the PVC.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Currently, a PVC with a non-empty &lt;code&gt;selector&lt;/code&gt; can&amp;rsquo;t have a PV dynamically provisioned for it.&lt;/div&gt;
&lt;/blockquote&gt;


In the past, the annotation `volume.beta.kubernetes.io/storage-class` was used instead
of `storageClassName` attribute. This annotation is still working; however,
it won&#39;t be supported in a future Kubernetes release.
 --&gt;
&lt;h3 id=&#34;类别&#34;&gt;类别&lt;/h3&gt;
&lt;p&gt;PVC 可以通过设置 &lt;code&gt;storageClassName&lt;/code&gt; 的值为一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-classes/&#34;&gt;StorageClass&lt;/a&gt;
的名称来指定其使用该类别。
PV 与 PVC 只有在拥有相同 &lt;code&gt;storageClassName&lt;/code&gt; 的情况下才能绑定。&lt;/p&gt;
&lt;p&gt;PVC 并不是必须要设置一个类别。 PVC 可以将 &lt;code&gt;storageClassName&lt;/code&gt; 设置为  &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;， 表示请求一个
没有类别的 PV，因此它也只能与没有类别的 PV 绑定(没有类别注解或类别注解值为 &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;)。
如果 PVC 没有设置 &lt;code&gt;storageClassName&lt;/code&gt;，则会基于是否开启
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass&#34;&gt;&lt;code&gt;DefaultStorageClass&lt;/code&gt; admission plugin&lt;/a&gt;
以下情况而有不同的表现行为:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果准入插件是开启的， 则管理员可以设置一个默认的 &lt;code&gt;StorageClass&lt;/code&gt;。 所有没有设置 &lt;code&gt;storageClassName&lt;/code&gt;
的 PVC 就只能与这个默认类别的 PV 绑定。 通过将 &lt;code&gt;StorageClass&lt;/code&gt; 对象的
&lt;code&gt;storageclass.kubernetes.io/is-default-class&lt;/code&gt; 注解值设置为 &lt;code&gt;true&lt;/code&gt; 可以将其设置默认。
如果管理不有设置默认类别， 集群应答 PVC 创建操作与准入插件关闭相同。 如果设置了不只一个默认
类别，则准入插件会阻止所有 PVC 的创建&lt;/li&gt;
&lt;li&gt;如果准入插件没有打开， 那就没有默认 &lt;code&gt;StorageClass&lt;/code&gt; 这回事。 所有没有设置 &lt;code&gt;storageClassName&lt;/code&gt;
的 PVC 就只能与没有设置类型的 PV 绑定。 在这种情况下， 没有设置 &lt;code&gt;storageClassName&lt;/code&gt; 与
将 &lt;code&gt;storageClassName&lt;/code&gt; 设置为 &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt; 的处理方式是一样的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;基于安装方式， 默认 &lt;code&gt;StorageClass&lt;/code&gt; 可以被插件管理器在安装时加入集群。&lt;/p&gt;
&lt;p&gt;当 PVC 设置 &lt;code&gt;selector&lt;/code&gt; 来请求 StorageClass， 所有条件是逻辑与关系: 只有包含所有选择器需要
的标签的类型才能被绑定的 PVC。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 目前，如果 PVC 的 &lt;code&gt;selector&lt;/code&gt; 是空，则不能实现动态管理 PV&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;在过去， 用的是 &lt;code&gt;volume.beta.kubernetes.io/storage-class&lt;/code&gt; 注解而不是 &lt;code&gt;storageClassName&lt;/code&gt; 属性。
这个注解目前还有用，但在未来版本中会完全废弃。&lt;/p&gt;
&lt;!--
## Claims As Volumes

Pods access storage by using the claim as a volume. Claims must exist in the same namespace as the Pod using the claim. The cluster finds the claim in the Pod&#39;s namespace and uses it to get the PersistentVolume backing the claim. The volume is then mounted to the host and into the Pod.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: &#34;/var/www/html&#34;
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim
```
 --&gt;
&lt;h2 id=&#34;claims-as-volumes&#34;&gt;将 PVC 当作卷(PV)&lt;/h2&gt;
&lt;p&gt;Pod 可以将 PVC 当作卷(PV) 来用作存储。 被引用 PVC 必须要要与 Pod 在同一个命名空间。
集群会在 Pod 所在的命名空间中寻找 PVC 然后使用与其绑定的 PV。 最后将 PV 挂载到主机，最终挂载到
Pod 中。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myfrontend&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/var/www/html&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypd&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypd&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaim&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;claimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myclaim&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### A Note on Namespaces

PersistentVolumes binds are exclusive, and since PersistentVolumeClaims are namespaced objects, mounting claims with &#34;Many&#34; modes (`ROX`, `RWX`) is only possible within one namespace.
 --&gt;
&lt;h3 id=&#34;一个需要注意命名空间的问题&#34;&gt;一个需要注意命名空间的问题&lt;/h3&gt;
&lt;p&gt;PersistentVolume 的绑定是独占的，而又因为 PVC 是命名空间级别的对象，因此在对 PVC 进行多节点
模式(&lt;code&gt;ROX&lt;/code&gt;, &lt;code&gt;RWX&lt;/code&gt;)挂载时只能针对同一个命名空间的节点。&lt;/p&gt;
&lt;!--
## Raw Block Volume Support






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;



The following volume plugins support raw block volumes, including dynamic provisioning where
applicable:

* AWSElasticBlockStore
* AzureDisk
* CSI
* FC (Fibre Channel)
* GCEPersistentDisk
* iSCSI
* Local volume
* OpenStack Cinder
* RBD (Ceph Block Device)
* VsphereVolume
 --&gt;
&lt;h2 id=&#34;raw-block-volume-support&#34;&gt;块设备卷支持&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;以下卷插件支持块设备卷，包含适配的动态管理:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AWSElasticBlockStore&lt;/li&gt;
&lt;li&gt;AzureDisk&lt;/li&gt;
&lt;li&gt;CSI&lt;/li&gt;
&lt;li&gt;FC (Fibre Channel)&lt;/li&gt;
&lt;li&gt;GCEPersistentDisk&lt;/li&gt;
&lt;li&gt;iSCSI&lt;/li&gt;
&lt;li&gt;Local volume&lt;/li&gt;
&lt;li&gt;OpenStack Cinder&lt;/li&gt;
&lt;li&gt;RBD (Ceph Block Device)&lt;/li&gt;
&lt;li&gt;VsphereVolume&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### PersistentVolume using a Raw Block Volume {#persistent-volume-using-a-raw-block-volume}

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: block-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  persistentVolumeReclaimPolicy: Retain
  fc:
    targetWWNs: [&#34;50060e801049cfd1&#34;]
    lun: 0
    readOnly: false
```
 --&gt;
&lt;h3 id=&#34;persistent-volume-using-a-raw-block-volume&#34;&gt;使用块设备卷的 PersistentVolume&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;block-pv&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;capacity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Block&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeReclaimPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Retain&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;fc&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;targetWWNs&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;50060e801049cfd1&amp;#34;&lt;/span&gt;]
    &lt;span style=&#34;color:#f92672&#34;&gt;lun&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### PersistentVolumeClaim requesting a Raw Block Volume {#persistent-volume-claim-requesting-a-raw-block-volume}

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 10Gi
```
 --&gt;
&lt;h3 id=&#34;persistent-volume-claim-requesting-a-raw-block-volume&#34;&gt;申请块设备卷的 PersistentVolumeClaim&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;block-pvc&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Block&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10Gi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Pod specification adding Raw Block Device path in container

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: [&#34;/bin/sh&#34;, &#34;-c&#34;]
      args: [ &#34;tail -f /dev/null&#34; ]
      volumeDevices:
        - name: data
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: block-pvc
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; When adding a raw block device for a Pod, you specify the device path in the container instead of a mount path.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;在-pod-定义中添加容器中的块设备路径&#34;&gt;在 Pod 定义中添加容器中的块设备路径&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod-with-block-volume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fc-container&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fedora:26&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/bin/sh&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;]
      &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tail -f /dev/null&amp;#34;&lt;/span&gt; ]
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeDevices&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;devicePath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/dev/xvda&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;data&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaim&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;claimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;block-pvc&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 在为一个 Pod 添加块设备时，在容器中指定的是设备路径，而不是挂载路径&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Binding Block Volumes

If a user requests a raw block volume by indicating this using the `volumeMode` field in the PersistentVolumeClaim spec, the binding rules differ slightly from previous releases that didn&#39;t consider this mode as part of the spec.
Listed is a table of possible combinations the user and admin might specify for requesting a raw block device. The table indicates if the volume will be bound or not given the combinations:
Volume binding matrix for statically provisioned volumes:

| PV volumeMode | PVC volumeMode  | Result           |
| --------------|:---------------:| ----------------:|
|   unspecified | unspecified     | BIND             |
|   unspecified | Block           | NO BIND          |
|   unspecified | Filesystem      | BIND             |
|   Block       | unspecified     | NO BIND          |
|   Block       | Block           | BIND             |
|   Block       | Filesystem      | NO BIND          |
|   Filesystem  | Filesystem      | BIND             |
|   Filesystem  | Block           | NO BIND          |
|   Filesystem  | unspecified     | BIND             |

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Only statically provisioned volumes are supported for alpha release. Administrators should take care to consider these values when working with raw block devices.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;绑定块设备&#34;&gt;绑定块设备&lt;/h3&gt;
&lt;p&gt;如果用户在 PVC 配置中使用 &lt;code&gt;volumeMode&lt;/code&gt; 字段指定申请一个块设备卷，则绑定规则与之前版本中
没有在配置中指定 &lt;code&gt;volumeMode&lt;/code&gt; 是有点不一样的。&lt;/p&gt;
&lt;p&gt;以下为用户/管理员在申请块设备时可能的组合列表。 这个表展示了这个组合卷是否能绑定
静态管理卷的绑定矩阵:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;PV volumeMode&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;PVC volumeMode&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Result&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NO BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NO BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NO BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NO BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 在 alpha 特性中只有静态管理的卷被支持。 管理员在操作块设备需要仔细考虑这些值。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Volume Snapshot and Restore Volume from Snapshot Support






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;



Volume snapshot feature was added to support CSI Volume Plugins only. For details, see [volume snapshots](/docs/concepts/storage/volume-snapshots/).

To enable support for restoring a volume from a volume snapshot data source, enable the
`VolumeSnapshotDataSource` feature gate on the apiserver and controller-manager.
 --&gt;
&lt;h2 id=&#34;卷快照和快照恢复支持&#34;&gt;卷快照和快照恢复支持&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;卷快照特性只对 CSI 卷插件支持。详细信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/storage/volume-snapshots/&#34;&gt;卷快照&lt;/a&gt;.
要启用对从卷快照恢复的支持，需要在 &lt;code&gt;apiserver&lt;/code&gt; &lt;code&gt;controller-manager&lt;/code&gt; 打开
&lt;code&gt;VolumeSnapshotDataSource&lt;/code&gt; 功能阀&lt;/p&gt;
&lt;!--
### Create a PersistentVolumeClaim from a Volume Snapshot {#create-persistent-volume-claim-from-volume-snapshot}

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: restore-pvc
spec:
  storageClassName: csi-hostpath-sc
  dataSource:
    name: new-snapshot-test
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```
 --&gt;
&lt;h3 id=&#34;create-persistent-volume-claim-from-volume-snapshot&#34;&gt;基于卷快照创建 PVC&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;restore-pvc&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;csi-hostpath-sc&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dataSource&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;new-snapshot-test&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshot&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;apiGroup&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10Gi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Volume Cloning

[Volume Cloning](/docs/concepts/storage/volume-pvc-datasource/) only available for CSI volume plugins.
 --&gt;
&lt;h2 id=&#34;卷克隆&#34;&gt;卷克隆&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/storage/volume-pvc-datasource/&#34;&gt;卷克隆&lt;/a&gt; 只存在于 CSI 卷插件&lt;/p&gt;
&lt;!--
### Create PersistentVolumeClaim from an existing PVC {#create-persistent-volume-claim-from-an-existing-pvc}

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cloned-pvc
spec:
  storageClassName: my-csi-plugin
  dataSource:
    name: existing-src-pvc-name
    kind: PersistentVolumeClaim
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```
 --&gt;
&lt;h3 id=&#34;create-persistent-volume-claim-from-an-existing-pvc&#34;&gt;基于存在的 PVC 创建新的 PVC&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cloned-pvc&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-csi-plugin&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dataSource&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;existing-src-pvc-name&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10Gi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Writing Portable Configuration

If you&#39;re writing configuration templates or examples that run on a wide range of clusters
and need persistent storage, it is recommended that you use the following pattern:

- Include PersistentVolumeClaim objects in your bundle of config (alongside
  Deployments, ConfigMaps, etc).
- Do not include PersistentVolume objects in the config, since the user instantiating
  the config may not have permission to create PersistentVolumes.
- Give the user the option of providing a storage class name when instantiating
  the template.
  - If the user provides a storage class name, put that value into the
    `persistentVolumeClaim.storageClassName` field.
    This will cause the PVC to match the right storage
    class if the cluster has StorageClasses enabled by the admin.
  - If the user does not provide a storage class name, leave the
    `persistentVolumeClaim.storageClassName` field as nil. This will cause a
    PV to be automatically provisioned for the user with the default StorageClass
    in the cluster. Many cluster environments have a default StorageClass installed,
    or administrators can create their own default StorageClass.
- In your tooling, watch for PVCs that are not getting bound after some time
  and surface this to the user, as this may indicate that the cluster has no
  dynamic storage support (in which case the user should create a matching PV)
  or the cluster has no storage system (in which case the user cannot deploy
  config requiring PVCs).

  ## 相关资料


* Learn more about [Creating a PersistentVolume](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume).
* Learn more about [Creating a PersistentVolumeClaim](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim).
* Read the [Persistent Storage design document](https://git.k8s.io/community/contributors/design-proposals/storage/persistent-storage.md).
 --&gt;
&lt;h2 id=&#34;编写可移植的配置&#34;&gt;编写可移植的配置&lt;/h2&gt;
&lt;p&gt;如果要编写运行在大范围集群中并且需要使用到持久化存储的模板或示例配置，建议依照以下模式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在配置 (随同 Deployments, ConfigMaps, 等)时在同一个配置文件中包含其使用的 PVC 对象。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果使用配置的用户可能没有创建 PV 的权限，则不要在配置中包含 PV 对象。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在用户使用模板时，提供设置 StorageClass 名称的选项&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果用户通过 &lt;code&gt;persistentVolumeClaim.storageClassName&lt;/code&gt;  字段设置了 StorageClass 名称。
当集群管理员启用了该 StorageClasses 时， PVC 就能正确使用存储类别。&lt;/li&gt;
&lt;li&gt;如果用户没提供 &lt;code&gt;StorageClass&lt;/code&gt; 名称。 这会导致 &lt;code&gt;persistentVolumeClaim.storageClassName&lt;/code&gt;
值为空。 这样集群中 PV 就会使用默认 &lt;code&gt;StorageClass&lt;/code&gt; 自动管理。 许多集群环境中都都有安装一个
默认的 &lt;code&gt;StorageClass&lt;/code&gt;或管理可能创建自己的默认 &lt;code&gt;StorageClass&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在工具中，在一个时间后观测未绑定的 PVC 并将其传递给用户。 这可能是集群不支持动态存储(这样用户
就要自己创建相应的 PV) 或者集群中没有存储系统(这种情况用户就不能部署包含 PVC 的配置)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
* Learn more about [Creating a PersistentVolume](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume).
* Learn more about [Creating a PersistentVolumeClaim](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim).
* Read the [Persistent Storage design document](https://git.k8s.io/community/contributors/design-proposals/storage/persistent-storage.md).
 --&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume&#34;&gt;创建 PersistentVolume&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim&#34;&gt;创建 PersistentVolumeClaim&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;参阅 &lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/storage/persistent-storage.md&#34;&gt;持久化存储设计文稿&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#persistentvolume-v1-core&#34;&gt;PersistentVolume&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#persistentvolumespec-v1-core&#34;&gt;PersistentVolumeSpec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#persistentvolumeclaim-v1-core&#34;&gt;PersistentVolumeClaim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#persistentvolumeclaimspec-v1-core&#34;&gt;PersistentVolumeClaimSpec&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Service 和 Pod 的 DNS</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/dns-pod-service/</link>
      <pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/dns-pod-service/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- davidopp
- thockin
title: DNS for Services and Pods
content_type: concept
weight: 20
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This page provides an overview of DNS support by Kubernetes.
 --&gt;
&lt;p&gt;本文简述 k8s 对 DNS 的支持&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Introduction

Kubernetes DNS schedules a DNS Pod and Service on the cluster, and configures
the kubelets to tell individual containers to use the DNS Service&#39;s IP to
resolve DNS names
 --&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;k8s DNS 先在集群中部署了一个 DNS 的 Pod 和 Service， 并配置 kubelet 让每一个容器都使用
DNS Service 的 IP 来解析 DNS 名称。&lt;/p&gt;
&lt;!--
### What things get DNS names?

Every Service defined in the cluster (including the DNS server itself) is
assigned a DNS name.  By default, a client Pod&#39;s DNS search list will
include the Pod&#39;s own namespace and the cluster&#39;s default domain.  This is best
illustrated by example:

Assume a Service named `foo` in the Kubernetes namespace `bar`.  A Pod running
in namespace `bar` can look up this service by simply doing a DNS query for
`foo`.  A Pod running in namespace `quux` can look up this service by doing a
DNS query for `foo.bar`.

The following sections detail the supported record types and layout that is
supported.  Any other layout or names or queries that happen to work are
considered implementation details and are subject to change without warning.
For more up-to-date specification, see
[Kubernetes DNS-Based Service Discovery](https://github.com/kubernetes/dns/blob/master/docs/specification.md).
 --&gt;
&lt;h3 id=&#34;啥东西会有-dns-名称&#34;&gt;啥东西会有 DNS 名称?&lt;/h3&gt;
&lt;p&gt;集群中定义的每一个 Service (包括 DNS 服务本身) 都会分配一个 DNS 名称。 默认情况下，一个客户端
Pod 的 DNS 检索列表会包含 Pod 自己所在的命名空间和集群的默认域。 一例胜千言:&lt;/p&gt;
&lt;p&gt;假设在 k8s 的 &lt;code&gt;bar&lt;/code&gt; 命名空间有一个叫 &lt;code&gt;foo&lt;/code&gt; 的 Service. 一个运行在 &lt;code&gt;bar&lt;/code&gt; 命名空间的 Pod
只需要简单地使用 &lt;code&gt;foo&lt;/code&gt; 作为 DNS 查询条目就可以找到这个 Service。 另一个运行在 &lt;code&gt;quux&lt;/code&gt; 命名空间的 Pod
同要为查询这个 Service。 DNS 的查询条目就需要是 &lt;code&gt;foo.bar&lt;/code&gt; (带上命名空间的名称)&lt;/p&gt;
&lt;p&gt;接下来的章节会详细介绍支持的 DNS 记录类型的规划。 (这一句没懂)
最新的规格说明书见
&lt;a href=&#34;https://github.com/kubernetes/dns/blob/master/docs/specification.md&#34;&gt;Kubernetes DNS-Based Service Discovery&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Services

### A/AAAA records

&#34;Normal&#34; (not headless) Services are assigned a DNS A or AAAA record,
depending on the IP family of the service, for a name of the form
`my-svc.my-namespace.svc.cluster-domain.example`.  This resolves to the cluster IP
of the Service.

&#34;Headless&#34; (without a cluster IP) Services are also assigned a DNS A or AAAA record,
depending on the IP family of the service, for a name of the form
`my-svc.my-namespace.svc.cluster-domain.example`.  Unlike normal
Services, this resolves to the set of IPs of the pods selected by the Service.
Clients are expected to consume the set or else use standard round-robin
selection from the set.

### SRV records

SRV Records are created for named ports that are part of normal or [Headless
Services](/docs/concepts/services-networking/service/#headless-services).
For each named port, the SRV record would have the form
`_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster-domain.example`.
For a regular service, this resolves to the port number and the domain name:
`my-svc.my-namespace.svc.cluster-domain.example`.
For a headless service, this resolves to multiple answers, one for each pod
that is backing the service, and contains the port number and the domain name of the pod
of the form `auto-generated-name.my-svc.my-namespace.svc.cluster-domain.example`.
 --&gt;
&lt;h2 id=&#34;service&#34;&gt;Service&lt;/h2&gt;
&lt;h3 id=&#34;aaaaa-记录&#34;&gt;A/AAAA 记录&lt;/h3&gt;
&lt;p&gt;&amp;ldquo;普通的&amp;rdquo;(不是无头(headless)的) Service 会基于使用的是 IPv4 还是 IPv6 分配一个 DNS A 或 AAAA 记录，
记录名为 &lt;code&gt;my-svc.my-namespace.svc.cluster-domain.example&lt;/code&gt;。 记录的值是 Service 的
集群 IP (cluster IP)&lt;/p&gt;
&lt;p&gt;无头(headless)(没有设置 &lt;code&gt;clusterIP&lt;/code&gt;) Service 也会基于使用的是 IPv4 还是 IPv6
分配一个 DNS A 或 AAAA 记录，记录名为 &lt;code&gt;my-svc.my-namespace.svc.cluster-domain.example&lt;/code&gt;，
与普通 Service 不同的是记录值是由 Service 选择的 Pod 的IP 的集合。 客户端在设计要需要能能够
接受 IP 集合或使用标准轮询 IP 集合。&lt;/p&gt;
&lt;h3 id=&#34;srv-记录&#34;&gt;SRV 记录&lt;/h3&gt;
&lt;p&gt;SRV 记录是为命名端口创建的，是普通或
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service/#headless-services&#34;&gt;Headless Services&lt;/a&gt;
的一部分。 对于每个命名端口的 SRV 记录格式如下:
&lt;code&gt;_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster-domain.example&lt;/code&gt;.
对于普通 Service , 解析的结果为 端口号和 域名:
&lt;code&gt;my-svc.my-namespace.svc.cluster-domain.example&lt;/code&gt;.
对于 无头(headless) 的 Service, 解析结果有多个应答， 一个是 Service 后端的每个 Pod。
另一个则包含端口号和类似这种格式
&lt;code&gt;auto-generated-name.my-svc.my-namespace.svc.cluster-domain.example&lt;/code&gt;
的 Pod 的域名&lt;/p&gt;
&lt;!--
## Pods

### A/AAAA records

In general a pod has the following DNS resolution:

`pod-ip-address.my-namespace.pod.cluster-domain.example`.

For example, if a pod in the `default` namespace has the IP address 172.17.0.3,
and the domain name for your cluster is `cluster.local`, then the Pod has a DNS name:

`172-17-0-3.default.pod.cluster.local`.

Any pods created by a Deployment or DaemonSet exposed by a Service have the
following DNS resolution available:

`pod-ip-address.deployment-name.my-namespace.svc.cluster-domain.example`.
 --&gt;
&lt;h2 id=&#34;pod&#34;&gt;Pod&lt;/h2&gt;
&lt;h3 id=&#34;aaaaa-记录-1&#34;&gt;A/AAAA 记录&lt;/h3&gt;
&lt;p&gt;通常 Pod 的 DNS 记录名为如下格式:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pod-ip-address.my-namespace.pod.cluster-domain.example&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;例如， 如果有一个在 &lt;code&gt;default&lt;/code&gt; 命名空间的 Pod， 它的 IP 地址为 &lt;code&gt;172.17.0.3&lt;/code&gt;， 集群配置的
域名叫 &lt;code&gt;cluster.local&lt;/code&gt;， 这样这个 Pod 的 DNS 名称就是:
&lt;code&gt;172-17-0-3.default.pod.cluster.local&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;任意由 Deployment 或 DaemonSet 创建，由 Service 暴露的 Pod 会拥有一个如下的 DNS 名称:
&lt;code&gt;pod-ip-address.deployment-name.my-namespace.svc.cluster-domain.example&lt;/code&gt;.&lt;/p&gt;
&lt;!--
### Pod&#39;s hostname and subdomain fields

Currently when a pod is created, its hostname is the Pod&#39;s `metadata.name` value.

The Pod spec has an optional `hostname` field, which can be used to specify the
Pod&#39;s hostname. When specified, it takes precedence over the Pod&#39;s name to be
the hostname of the pod. For example, given a Pod with `hostname` set to
&#34;`my-host`&#34;, the Pod will have its hostname set to &#34;`my-host`&#34;.

The Pod spec also has an optional `subdomain` field which can be used to specify
its subdomain. For example, a Pod with `hostname` set to &#34;`foo`&#34;, and `subdomain`
set to &#34;`bar`&#34;, in namespace &#34;`my-namespace`&#34;, will have the fully qualified
domain name (FQDN) &#34;`foo.bar.my-namespace.svc.cluster-domain.example`&#34;.

Example:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: default-subdomain
spec:
  selector:
    name: busybox
  clusterIP: None
  ports:
  - name: foo # Actually, no port is needed.
    port: 1234
    targetPort: 1234
---
apiVersion: v1
kind: Pod
metadata:
  name: busybox1
  labels:
    name: busybox
spec:
  hostname: busybox-1
  subdomain: default-subdomain
  containers:
  - image: busybox:1.28
    command:
      - sleep
      - &#34;3600&#34;
    name: busybox
---
apiVersion: v1
kind: Pod
metadata:
  name: busybox2
  labels:
    name: busybox
spec:
  hostname: busybox-2
  subdomain: default-subdomain
  containers:
  - image: busybox:1.28
    command:
      - sleep
      - &#34;3600&#34;
    name: busybox
```

If there exists a headless service in the same namespace as the pod and with
the same name as the subdomain, the cluster&#39;s DNS Server also returns an A or AAAA
record for the Pod&#39;s fully qualified hostname.
For example, given a Pod with the hostname set to &#34;`busybox-1`&#34; and the subdomain set to
&#34;`default-subdomain`&#34;, and a headless Service named &#34;`default-subdomain`&#34; in
the same namespace, the pod will see its own FQDN as
&#34;`busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example`&#34;. DNS serves an
A or AAAA record at that name, pointing to the Pod&#39;s IP. Both pods &#34;`busybox1`&#34; and
&#34;`busybox2`&#34; can have their distinct A or AAAA records.

The Endpoints object can specify the `hostname` for any endpoint addresses,
along with its IP.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Because A or AAAA records are not created for Pod names, &lt;code&gt;hostname&lt;/code&gt; is required for the Pod&amp;rsquo;s A or AAAA
record to be created. A Pod with no &lt;code&gt;hostname&lt;/code&gt; but with &lt;code&gt;subdomain&lt;/code&gt; will only create the
A or AAAA record for the headless service (&lt;code&gt;default-subdomain.my-namespace.svc.cluster-domain.example&lt;/code&gt;),
pointing to the Pod&amp;rsquo;s IP address. Also, Pod needs to become ready in order to have a
record unless &lt;code&gt;publishNotReadyAddresses=True&lt;/code&gt; is set on the Service.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;pod-的-hostname-和-subdomain-字段&#34;&gt;Pod 的 &lt;code&gt;hostname&lt;/code&gt; 和 &lt;code&gt;subdomain&lt;/code&gt; 字段&lt;/h3&gt;
&lt;p&gt;目前，当一个 Pod 创建后， 它的主机名是 Pod 的 &lt;code&gt;metadata.name&lt;/code&gt; 字段的值。&lt;/p&gt;
&lt;p&gt;Pod 的定义中有一个可选字段 &lt;code&gt;hostname&lt;/code&gt;， 可以用来指定 Pod 的主机名。 当设置了主机名时，它的优先
级是高于 Pod 名称作为 Pod 的主机名的。 例如， 设置一个 Pod 的 &lt;code&gt;hostname&lt;/code&gt; 为 &lt;code&gt;my-host&lt;/code&gt;，
这个 Pod 的主机名就会设置为 &lt;code&gt;my-host&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pod 定义中还有一个可选择字段 &lt;code&gt;subdomain&lt;/code&gt;， 可以用来指定它的子域名。 例如， 一个 Pod 的
&lt;code&gt;hostname&lt;/code&gt; 设置为 &lt;code&gt;foo&lt;/code&gt;， &lt;code&gt;subdomain&lt;/code&gt; 设置为 &lt;code&gt;bar&lt;/code&gt;， 处理 &lt;code&gt;my-namespace&lt;/code&gt; 命名空间。
它的全限定名(FQDN) 就是 &lt;code&gt;foo.bar.my-namespace.svc.cluster-domain.example&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-subdomain&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;clusterIP&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;None&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo # 实际上是不需要端口的&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1234&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1234&lt;/span&gt;
---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;hostname&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox-1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;subdomain&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-subdomain&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox:1.28&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;sleep&lt;/span&gt;
      - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3600&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox2&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;hostname&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox-2&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;subdomain&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-subdomain&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox:1.28&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;sleep&lt;/span&gt;
      - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3600&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果存在这样一个无头(headless) 的 Service, 它与另一个 Pod 在同一个命名空间，且 Pod 的
&lt;code&gt;subdomain&lt;/code&gt; 与这个 Service 的名称是一样的。 集群 DNS 服务一样会返回 Pod 全限制名的 A/AAAA 记录。
例如，假定一个 Pod 的 &lt;code&gt;hostname&lt;/code&gt; 设置为 &lt;code&gt;busybox-1&lt;/code&gt;， &lt;code&gt;subdomain&lt;/code&gt; 设置为 &lt;code&gt;default-subdomain&lt;/code&gt;，
一个无头(headless) 的 Service 名字是 &lt;code&gt;default-subdomain&lt;/code&gt;， 它们在同一个命名空间。
Pod 就可以看到它自己的全限定名(FQDN)为
&lt;code&gt;busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example&lt;/code&gt;。
DNS 服务为这个名称提供一个 A/AAAA 记录， 指向该 Pod 的 IP。 &lt;code&gt;busybox1&lt;/code&gt; 和 &lt;code&gt;busybox2&lt;/code&gt;
都能有它们各自的 A/AAAA 记录。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;这里需要配个完整的例子&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;Endpoint 对象可以将任意端点地址和IP 设置为 &lt;code&gt;hostname&lt;/code&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;这里不理解，并且需要一个例子&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 因为 A/AAAA 记录不是为 Pod 名称创建的， &lt;code&gt;hostname&lt;/code&gt; 是 Pod A/AAAA 记录创建所必须的。
一个没有 &lt;code&gt;hostname&lt;/code&gt; 字段，但是有 &lt;code&gt;subdomain&lt;/code&gt; 字段的 Pod 只会为无头(headless)Service
创建 A/AAAA 记录(&lt;code&gt;default-subdomain.my-namespace.svc.cluster-domain.example&lt;/code&gt;)
该记录指向的是该 Pod 的 IP 地址。还有，如果 Service 上没有设置 &lt;code&gt;publishNotReadyAddresses=True&lt;/code&gt;
则 Pod 状态变为就绪后才的 DNS 记录&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Pod&#39;s setHostnameAsFQDN field {#pod-sethostnameasfqdn-field}






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [alpha]&lt;/code&gt;
&lt;/div&gt;



**Prerequisites**: The `SetHostnameAsFQDN` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
must be enabled for the
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API Server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt;

When a Pod is configured to have fully qualified domain name (FQDN), its hostname is the short hostname. For example, if you have a Pod with the fully qualified domain name `busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example`, then by default the `hostname` command inside that Pod returns `busybox-1` and  the `hostname --fqdn` command returns the FQDN.

When you set `setHostnameAsFQDN: true` in the Pod spec, the kubelet writes the Pod&#39;s FQDN into the hostname for that Pod&#39;s namespace. In this case, both `hostname` and `hostname --fqdn` return the Pod&#39;s FQDN.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;In Linux, the hostname field of the kernel (the &lt;code&gt;nodename&lt;/code&gt; field of &lt;code&gt;struct utsname&lt;/code&gt;) is limited to 64 characters.&lt;/p&gt;
&lt;p&gt;If a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start. The Pod will remain in &lt;code&gt;Pending&lt;/code&gt; status (&lt;code&gt;ContainerCreating&lt;/code&gt; as seen by &lt;code&gt;kubectl&lt;/code&gt;) generating error events, such as Failed to construct FQDN from pod hostname and cluster domain, FQDN &lt;code&gt;long-FDQN&lt;/code&gt; is too long (64 characters is the max, 70 characters requested). One way of improving user experience for this scenario is to create an &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks&#34;&gt;admission webhook controller&lt;/a&gt; to control FQDN size when users create top level objects, for example, Deployment.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;pod-sethostnameasfqdn-field&#34;&gt;Pod 的 setHostnameAsFQDN 字段&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;前提条件&lt;/strong&gt;:
需要在 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API Server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 启用
&lt;code&gt;SetHostnameAsFQDN&lt;/code&gt; &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;当一个 Pod 配置了全限定名(FQDN)时，它的主机名是短主机名。 例如， 如果有一个全限定名为
&lt;code&gt;busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example&lt;/code&gt; 的 Pod，
在 Pod 内使用默认的 &lt;code&gt;hostname&lt;/code&gt; 命令时返回的是 &lt;code&gt;busybox-1&lt;/code&gt;， 而 &lt;code&gt;hostname --fqdn&lt;/code&gt; 命令
返回的是全限定名(FQDN)。&lt;/p&gt;
&lt;p&gt;当在 Pod 的定义中设置 &lt;code&gt;setHostnameAsFQDN: true&lt;/code&gt; 时， kubelet 会将 Pod 的 全限定名(FQDN)
写到那个 Pod 命名空间的 hostname. 在这种情况下 &lt;code&gt;hostname&lt;/code&gt; 和 &lt;code&gt;hostname --fqdn&lt;/code&gt; 两个命令
返回的都是全限定名。&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 在 Linux 中， 内核中的 &lt;code&gt;hostname&lt;/code&gt; 字段(&lt;code&gt;struct utsname&lt;/code&gt; 的 &lt;code&gt;nodename&lt;/code&gt; 字段)限制最多只能有 64 个字符。&lt;/div&gt;
&lt;/blockquote&gt;

如果一个 Pod 开启了该特性，并且它的 全限定名(FQDN) 长度大于 64 个字符，就会启动失败。
Pod 会一停在 &lt;code&gt;Pending&lt;/code&gt; 状态(通过 &lt;code&gt;kubectl&lt;/code&gt; 看到的是 &lt;code&gt;ContainerCreating&lt;/code&gt;)，最终会产生一个
错误事件，错误信息类似基于 Pod 主机名和集群域构建全限定名(FQDN)失败， &lt;code&gt;long-FDQN&lt;/code&gt;  FQDN 太长了
(最长只能有 64 个字符，但实际有 70 个字符)。 在这种情况下改善用户体验的一种方式是创建一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks&#34;&gt;admission webhook controller&lt;/a&gt;
来在创建顶级对象(如，Deployment)时控制全限定名(FQDN)的长度&lt;/p&gt;
&lt;!--
### Pod&#39;s DNS Policy

DNS policies can be set on a per-pod basis. Currently Kubernetes supports the
following pod-specific DNS policies. These policies are specified in the
`dnsPolicy` field of a Pod Spec.

- &#34;`Default`&#34;: The Pod inherits the name resolution configuration from the node
  that the pods run on.
  See [related discussion](/docs/tasks/administer-cluster/dns-custom-nameservers/#inheriting-dns-from-the-node)
  for more details.
- &#34;`ClusterFirst`&#34;: Any DNS query that does not match the configured cluster
  domain suffix, such as &#34;`www.kubernetes.io`&#34;, is forwarded to the upstream
  nameserver inherited from the node. Cluster administrators may have extra
  stub-domain and upstream DNS servers configured.
  See [related discussion](/docs/tasks/administer-cluster/dns-custom-nameservers/#effects-on-pods)
  for details on how DNS queries are handled in those cases.
- &#34;`ClusterFirstWithHostNet`&#34;: For Pods running with hostNetwork, you should
  explicitly set its DNS policy &#34;`ClusterFirstWithHostNet`&#34;.
- &#34;`None`&#34;: It allows a Pod to ignore DNS settings from the Kubernetes
  environment. All DNS settings are supposed to be provided using the
  `dnsConfig` field in the Pod Spec.
  See [Pod&#39;s DNS config](#pod-dns-config) subsection below.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &amp;ldquo;Default&amp;rdquo; is not the default DNS policy. If &lt;code&gt;dnsPolicy&lt;/code&gt; is not
explicitly specified, then &amp;ldquo;ClusterFirst&amp;rdquo; is used.&lt;/div&gt;
&lt;/blockquote&gt;



The example below shows a Pod with its DNS policy set to
&#34;`ClusterFirstWithHostNet`&#34; because it has `hostNetwork` set to `true`.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox:1.28
    command:
      - sleep
      - &#34;3600&#34;
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always
  hostNetwork: true
  dnsPolicy: ClusterFirstWithHostNet
```
 --&gt;
&lt;h3 id=&#34;pod-的-dns-策略&#34;&gt;Pod 的 DNS 策略&lt;/h3&gt;
&lt;p&gt;DNS policies can be set on a per-pod basis. Currently Kubernetes supports the
following pod-specific DNS policies. These policies are specified in the
&lt;code&gt;dnsPolicy&lt;/code&gt; field of a Pod Spec.
DNS 策略可以在 Pod 级别设置， 目前 k8s 支持以下的 Pod 级别 DNS 策略。 这个策略通过 Pod
定义的 &lt;code&gt;dnsPolicy&lt;/code&gt; 字段指定。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;&lt;code&gt;Default&lt;/code&gt;&amp;quot;: Pod 从它自己运行的节点上继承域名解析配置。更多信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/administer-cluster/dns-custom-nameservers/#inheriting-dns-from-the-node&#34;&gt;相关讨论&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;&lt;code&gt;ClusterFirst&lt;/code&gt;&amp;quot;: 任何与配置的集群域后缀匹配的 DNS 查询，如   &amp;ldquo;&lt;code&gt;www.kubernetes.io&lt;/code&gt;&amp;quot;，
会被转发到由节点继承的上游域名服务器。 集群管理员可以配置额外的 存根域(stub-domain) 和上游
DNS 服务器。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;&lt;code&gt;ClusterFirstWithHostNet&lt;/code&gt;&amp;quot;: 以 &lt;code&gt;hostNetwork&lt;/code&gt; 运行的 Pod，需要显式的设置它的 DNS
策略为 &amp;ldquo;&lt;code&gt;ClusterFirstWithHostNet&lt;/code&gt;&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;&lt;code&gt;None&lt;/code&gt;&amp;quot;: 这种策略允许 Pod 无视 k8s 环境的 DNS 配置。 所有的 DNS 配置都应该由 Pod 定义中
的 &lt;code&gt;dnsConfig&lt;/code&gt; 字段提供。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &amp;ldquo;Default&amp;rdquo; 并不是默认的 DNS 策略， 如果没有显式的设置 &lt;code&gt;dnsPolicy&lt;/code&gt;，则使用 &amp;ldquo;ClusterFirst&amp;rdquo;&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以面的例子中的 Pod 的 DNS 策略被设置为 &amp;ldquo;&lt;code&gt;ClusterFirstWithHostNet&lt;/code&gt;&amp;rdquo; 因为它的 &lt;code&gt;hostNetwork&lt;/code&gt;
被设置为了 &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox:1.28&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;sleep&lt;/span&gt;
      - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3600&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;imagePullPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IfNotPresent&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Always&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;hostNetwork&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dnsPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ClusterFirstWithHostNet&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Pod&#39;s DNS Config {#pod-dns-config}

Pod&#39;s DNS Config allows users more control on the DNS settings for a Pod.

The `dnsConfig` field is optional and it can work with any `dnsPolicy` settings.
However, when a Pod&#39;s `dnsPolicy` is set to &#34;`None`&#34;, the `dnsConfig` field has
to be specified.

Below are the properties a user can specify in the `dnsConfig` field:

- `nameservers`: a list of IP addresses that will be used as DNS servers for the
  Pod. There can be at most 3 IP addresses specified. When the Pod&#39;s `dnsPolicy`
  is set to &#34;`None`&#34;, the list must contain at least one IP address, otherwise
  this property is optional.
  The servers listed will be combined to the base nameservers generated from the
  specified DNS policy with duplicate addresses removed.
- `searches`: a list of DNS search domains for hostname lookup in the Pod.
  This property is optional. When specified, the provided list will be merged
  into the base search domain names generated from the chosen DNS policy.
  Duplicate domain names are removed.
  Kubernetes allows for at most 6 search domains.
- `options`: an optional list of objects where each object may have a `name`
  property (required) and a `value` property (optional). The contents in this
  property will be merged to the options generated from the specified DNS policy.
  Duplicate entries are removed.

The following is an example Pod with custom DNS settings:



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingcustom-dnsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/custom-dns.yaml&#34; download=&#34;service/networking/custom-dns.yaml&#34;&gt;
                    &lt;code&gt;service/networking/custom-dns.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingcustom-dnsyaml&#39;)&#34; title=&#34;Copy service/networking/custom-dns.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;dns-example&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dnsPolicy&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;None&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dnsConfig&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nameservers&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;1.2.3.4&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;searches&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;ns1.svc.cluster-domain.example&lt;/span&gt;
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;my.dns.search.suffix&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;options&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ndots&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2&amp;#34;&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;edns0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



When the Pod above is created, the container `test` gets the following contents
in its `/etc/resolv.conf` file:

```
nameserver 1.2.3.4
search ns1.svc.cluster-domain.example my.dns.search.suffix
options ndots:2 edns0
```

For IPv6 setup, search path and name server should be setup like this:

```shell
kubectl exec -it dns-example -- cat /etc/resolv.conf
```
The output is similar to this:
```shell
nameserver fd00:79:30::a
search default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.example
options ndots:5
```
 --&gt;
&lt;h3 id=&#34;pod-dns-config&#34;&gt;Pod 的 DNS 配置&lt;/h3&gt;
&lt;p&gt;Pod&amp;rsquo;s DNS Config allows users more control on the DNS settings for a Pod.
Pod 的 DNS 配置让用户可以更多地控制 Pod 上的 DNS 配置。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;dnsConfig&lt;/code&gt; 是一个可选字段， 它可以与 &lt;code&gt;dnsPolicy&lt;/code&gt; 配置配合使用。 但是当一个 Pod 的 &lt;code&gt;dnsPolicy&lt;/code&gt;
设置为 &amp;ldquo;&lt;code&gt;None&lt;/code&gt;&amp;ldquo;时，就必须要设置 &lt;code&gt;dnsConfig&lt;/code&gt; 字段。
以下是 &lt;code&gt;dnsConfig&lt;/code&gt; 字段中用户可以配置的字段:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;nameservers&lt;/code&gt;: Pod 用作 DNS 服务的一个 IP 地址列表。 最多可以指定三个 IP 地址。
当 Pod &lt;code&gt;dnsPolicy&lt;/code&gt; 设置为 &amp;ldquo;&lt;code&gt;None&lt;/code&gt;&amp;quot;， 这个列表中至少包含一个 IP 地址，其它情况下这个字段为可选。
这个 DNS 列表会与 DNS 策略配置产生的基础 DNS 服务器合并，重复的会被删除。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;searches&lt;/code&gt;: 一个 DNS 检索列表，用于 Pod 中的主机名查找。 这个属性为可选。当设置这个字段时，
这个列表会合并到 DNS 策略配置产生的DNS 检索域名中，重复的条目会被删除。 k8s 允许最多 6 个检索域名。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;options&lt;/code&gt;: 一个可选的对象列表， 其中的每个对象有一个必要的 &lt;code&gt;name&lt;/code&gt; 属性和一个可选的 &lt;code&gt;value&lt;/code&gt; 属性。
这些属性会被合并到配置的 DNS 策略生成的选项中&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下来一个包含自定义 DNS 配置的 Pod 的示例:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingcustom-dnsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/custom-dns.yaml&#34; download=&#34;service/networking/custom-dns.yaml&#34;&gt;
                    &lt;code&gt;service/networking/custom-dns.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingcustom-dnsyaml&#39;)&#34; title=&#34;Copy service/networking/custom-dns.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;dns-example&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dnsPolicy&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;None&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dnsConfig&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nameservers&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;1.2.3.4&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;searches&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;ns1.svc.cluster-domain.example&lt;/span&gt;
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;my.dns.search.suffix&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;options&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ndots&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2&amp;#34;&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;edns0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;当上面这个 Pod 被创建后，这个叫  &lt;code&gt;test&lt;/code&gt; 容器中的 &lt;code&gt;/etc/resolv.conf&lt;/code&gt; 就会有下面的这些内容:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nameserver 1.2.3.4
search ns1.svc.cluster-domain.example my.dns.search.suffix
options ndots:2 edns0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;For IPv6 setup, search path and name server should be setup like this:
如果设置了 IPv6， 检索路径和DNS服务应该这么配置:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl exec -it dns-example -- cat /etc/resolv.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nameserver fd00:79:30::a
search default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.example
options ndots:5
&lt;/code&gt;&lt;/pre&gt;&lt;!--
### Feature availability

The availability of Pod DNS Config and DNS Policy &#34;`None`&#34; is shown as below.

| k8s version | Feature support |
| :---------: |:-----------:|
| 1.14 | Stable |
| 1.10 | Beta (on by default)|
| 1.9 | Alpha |
 --&gt;
&lt;h3 id=&#34;feature-可用性&#34;&gt;Feature 可用性&lt;/h3&gt;
&lt;p&gt;Pod DNS 配置 和  DNS 策略的 &amp;ldquo;&lt;code&gt;None&lt;/code&gt;&amp;rdquo; 的可用性如下:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;k8s 版本&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;特性可用性&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.14&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Stable&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.10&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Beta (默认启用)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.9&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Alpha&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
For guidance on administering DNS configurations, check
[Configure DNS Service](/docs/tasks/administer-cluster/dns-custom-nameservers/)
 --&gt;
&lt;p&gt;DNS 配置的管理指导见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/dns-custom-nameservers/&#34;&gt;Configure DNS Service&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 通过 Service 连接应用</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/connect-applications-service/</link>
      <pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/connect-applications-service/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- caesarxuchao
- lavalamp
- thockin
title: Connecting Applications with Services
content_type: concept
weight: 30
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
## The Kubernetes model for connecting containers

Now that you have a continuously running, replicated application you can expose it on a network. Before discussing the Kubernetes approach to networking, it is worthwhile to contrast it with the &#34;normal&#34; way networking works with Docker.

By default, Docker uses host-private networking, so containers can talk to other containers only if they are on the same machine. In order for Docker containers to communicate across nodes, there must be allocated ports on the machine&#39;s own IP address, which are then forwarded or proxied to the containers. This obviously means that containers must either coordinate which ports they use very carefully or ports must be allocated dynamically.

Coordinating port allocations across multiple developers or teams that provide containers is very difficult to do at scale, and exposes users to cluster-level issues outside of their control. Kubernetes assumes that pods can communicate with other pods, regardless of which host they land on. Kubernetes gives every pod its own cluster-private IP address, so you do not need to explicitly create links between pods or map container ports to host ports. This means that containers within a Pod can all reach each other&#39;s ports on localhost, and all pods in a cluster can see each other without NAT. The rest of this document elaborates on how you can run reliable services on such a networking model.

This guide uses a simple nginx server to demonstrate proof of concept.
 --&gt;
&lt;h2 id=&#34;连接到容器的-k8s-模型&#34;&gt;连接到容器的 k8s 模型&lt;/h2&gt;
&lt;p&gt;到目前为止我们有一个持续运行的多副本应用，我们可以把它暴露到一个网络中。 在讨论 k8s 的网络实现前，
值得花点时间来看看 Docker 中普通的网络是怎么工作的。&lt;/p&gt;
&lt;p&gt;默认情况下， Docker 使用私有网络，因此只有同一个主机上的容器之间可以相互通信。要使不同主机之间
的容器能够通信，必须要通过主机自己的 IP 地址加上分配端口以转发或代理的方式连接到容器。
通过这种方式很明显的一个问题就是不同容器必须要十分小心地协调怎么分配端口，或者必须动态地分配端口。&lt;/p&gt;
&lt;p&gt;大规模在多个开发者或项目组之间为容器协调端口分配是十分困难的， 并且也直接将用户暴露在无法控制的
集群级问题中。 k8s 确保 Pod 无论他们在哪个主机上，其相互之间都可以通信。k8s 会为每个 Pod 分配
集群内的私有 IP 地址，因此不需要用户显示地创建 Pod 之间的连接，也不需要做容器端口与主机端口的映射关系。
这就是说在一个 Pod 中的容器可以通过本地回环(localhost)以端口实现相互之间的通信，并且集群中
所有的 Pod 都可以在没有 NAT 的情况互相通信。 本文接下来的部分将说结阐怎么通过这个网络模型运行
可靠的服务。&lt;/p&gt;
&lt;p&gt;本文使用一个简单的 nginx 服务来演示证明这个观点。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Exposing pods to the cluster

We did this in a previous example, but let&#39;s do it once again and focus on the networking perspective.
Create an nginx Pod, and note that it has a container port specification:



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingrun-my-nginxyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/run-my-nginx.yaml&#34; download=&#34;service/networking/run-my-nginx.yaml&#34;&gt;
                    &lt;code&gt;service/networking/run-my-nginx.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingrun-my-nginxyaml&#39;)&#34; title=&#34;Copy service/networking/run-my-nginx.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



This makes it accessible from any node in your cluster. Check the nodes the Pod is running on:

```shell
kubectl apply -f ./run-my-nginx.yaml
kubectl get pods -l run=my-nginx -o wide
```
```
NAME                        READY     STATUS    RESTARTS   AGE       IP            NODE
my-nginx-3800858182-jr4a2   1/1       Running   0          13s       10.244.3.4    kubernetes-minion-905m
my-nginx-3800858182-kna2y   1/1       Running   0          13s       10.244.2.5    kubernetes-minion-ljyd
```

Check your pods&#39; IPs:

```shell
kubectl get pods -l run=my-nginx -o yaml | grep podIP
    podIP: 10.244.3.4
    podIP: 10.244.2.5
```

You should be able to ssh into any node in your cluster and curl both IPs. Note that the containers are *not* using port 80 on the node, nor are there any special NAT rules to route traffic to the pod. This means you can run multiple nginx pods on the same node all using the same containerPort and access them from any other pod or node in your cluster using IP. Like Docker, ports can still be published to the host node&#39;s interfaces, but the need for this is radically diminished because of the networking model.

You can read more about [how we achieve this](/docs/concepts/cluster-administration/networking/#how-to-achieve-this) if you&#39;re curious.
 --&gt;
&lt;h2 id=&#34;将-pod-暴露到集群中&#34;&gt;将 Pod 暴露到集群中&lt;/h2&gt;
&lt;p&gt;这是我们之间用过的一个盒子，让我们再做一次，但这次专注于网络的角度。 创建一个 nginx 的 Pod，
注意这个定义中有关于端口的定义:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingrun-my-nginxyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/run-my-nginx.yaml&#34; download=&#34;service/networking/run-my-nginx.yaml&#34;&gt;
                    &lt;code&gt;service/networking/run-my-nginx.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingrun-my-nginxyaml&#39;)&#34; title=&#34;Copy service/networking/run-my-nginx.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f ./run-my-nginx.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这使用它可以从集群中的任意节点访问。通过以下命令查看 Pod 运行在哪个节点上&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods -l run&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;my-nginx -o wide
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME                        READY     STATUS    RESTARTS   AGE       IP            NODE
my-nginx-3800858182-jr4a2   1/1       Running   0          13s       10.244.3.4    kubernetes-minion-905m
my-nginx-3800858182-kna2y   1/1       Running   0          13s       10.244.2.5    kubernetes-minion-ljyd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看 Pod 的 IP 地址:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods -l run&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;my-nginx -o yaml | grep podIP
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;podIP: 10.244.3.4
podIP: 10.244.2.5
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用户可以通过 ssh 连接到集群中的任意节点，并通过 curl 命令访问以上两个 IP。 要注意容器并 &lt;em&gt;没有&lt;/em&gt;
使用节点上的 80 端口，也没有任何特殊的 NAT 规则将流量路由到 Pod。 也就是说用户可以在同一个节点
上使用一样的 &lt;code&gt;containerPort&lt;/code&gt; 运行多个 nginx 的 Pod， 并且可以从集群中任意 Pod 或 节点使用
它的 IP 访问它。 与 Docker 一样， 端口也可以发布到主机的网卡上， 因为网络模型的原因，应该尽量减少这
种方式的使用。&lt;/p&gt;
&lt;p&gt;如果用户比较好奇这个是怎么实现的见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/cluster-administration/networking/#how-to-achieve-this&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Creating a Service

So we have pods running nginx in a flat, cluster wide, address space. In theory, you could talk to these pods directly, but what happens when a node dies? The pods die with it, and the Deployment will create new ones, with different IPs. This is the problem a Service solves.

A Kubernetes Service is an abstraction which defines a logical set of Pods running somewhere in your cluster, that all provide the same functionality. When created, each Service is assigned a unique IP address (also called clusterIP). This address is tied to the lifespan of the Service, and will not change while the Service is alive. Pods can be configured to talk to the Service, and know that communication to the Service will be automatically load-balanced out to some pod that is a member of the Service.

You can create a Service for your 2 nginx replicas with `kubectl expose`:

```shell
kubectl expose deployment/my-nginx
```
```
service/my-nginx exposed
```

This is equivalent to `kubectl apply -f` the following yaml:



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnginx-svcyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/nginx-svc.yaml&#34; download=&#34;service/networking/nginx-svc.yaml&#34;&gt;
                    &lt;code&gt;service/networking/nginx-svc.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnginx-svcyaml&#39;)&#34; title=&#34;Copy service/networking/nginx-svc.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



This specification will create a Service which targets TCP port 80 on any Pod
with the `run: my-nginx` label, and expose it on an abstracted Service port
(`targetPort`: is the port the container accepts traffic on, `port`: is the
abstracted Service port, which can be any port other pods use to access the
Service).
View [Service](/docs/reference/generated/kubernetes-api/v1.19/#service-v1-core)
API object to see the list of supported fields in service definition.
Check your Service:

```shell
kubectl get svc my-nginx
```
```
NAME       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
my-nginx   ClusterIP   10.0.162.149   &lt;none&gt;        80/TCP    21s
```

As mentioned previously, a Service is backed by a group of Pods. These Pods are
exposed through `endpoints`. The Service&#39;s selector will be evaluated continuously
and the results will be POSTed to an Endpoints object also named `my-nginx`.
When a Pod dies, it is automatically removed from the endpoints, and new Pods
matching the Service&#39;s selector will automatically get added to the endpoints.
Check the endpoints, and note that the IPs are the same as the Pods created in
the first step:

```shell
kubectl describe svc my-nginx
```
```
Name:                my-nginx
Namespace:           default
Labels:              run=my-nginx
Annotations:         &lt;none&gt;
Selector:            run=my-nginx
Type:                ClusterIP
IP:                  10.0.162.149
Port:                &lt;unset&gt; 80/TCP
Endpoints:           10.244.2.5:80,10.244.3.4:80
Session Affinity:    None
Events:              &lt;none&gt;
```
```shell
kubectl get ep my-nginx
```
```
NAME       ENDPOINTS                     AGE
my-nginx   10.244.2.5:80,10.244.3.4:80   1m
```

You should now be able to curl the nginx Service on `&lt;CLUSTER-IP&gt;:&lt;PORT&gt;` from
any node in your cluster. Note that the Service IP is completely virtual, it
never hits the wire. If you&#39;re curious about how this works you can read more
about the [service proxy](/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies).
 --&gt;
&lt;h2 id=&#34;创建一个-service&#34;&gt;创建一个 Service&lt;/h2&gt;
&lt;p&gt;这样我们就一个集群范围以平铺方式地址空间运行 nginx 的 Pod. 理论上，用户可以直接与这些 Pod 通信，
但是要是一个节点挂了会发生什么呢? 节点上面的 Pod 也会跟着一起挂掉， 然后 Deployment 会创建
新的 Pod， 但这些 Pod 的 IP 已经不是之前的了。 这就是 Service 要解决的问题。&lt;/p&gt;
&lt;p&gt;一个 k8s 的 Service 就是对定义了一个集群中运行的提供同样功能的 Pod 的逻辑集合的抽象。每个
Service 被创建了以后都会被分配一个唯一的 IP 地址(也被称为集群IP(clusterIP)). 这个地址与该
Service 一生相伴，在 Service 的整个生命期内都不会改变。 Pod 也可以被配置为与 Service 通信，
访问 Service 的请求会被负载均衡到该 Service 所属的一个 Pod 上。&lt;/p&gt;
&lt;p&gt;可以通过如下 &lt;code&gt;kubectl expose&lt;/code&gt; 为上面的两个 nginx 副本创建一个 Service:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl expose deployment/my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;service/my-nginx exposed
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个命令与 &lt;code&gt;kubectl apply -f&lt;/code&gt; 下面这个 yaml 文件等效:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnginx-svcyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/nginx-svc.yaml&#34; download=&#34;service/networking/nginx-svc.yaml&#34;&gt;
                    &lt;code&gt;service/networking/nginx-svc.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnginx-svcyaml&#39;)&#34; title=&#34;Copy service/networking/nginx-svc.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;这个配置文件中定义会创建一个 Service, 这个 Service 的目标为任意一个标签为 &lt;code&gt;run: my-nginx&lt;/code&gt;
的 Pod 的 TCP 80 端口，并将这些端口暴露在一个抽象的 Service 端口(&lt;code&gt;targetPort&lt;/code&gt;: 是容器
接收流量的端口，&lt;code&gt;port&lt;/code&gt;: 是抽象 Service 的端口，可以是任意端口，用于其它的 Pod 访问该 Service)
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#service-v1-core&#34;&gt;Service&lt;/a&gt;
的 API 对象中有所有 Service 定义支持的字段列表。&lt;/p&gt;
&lt;p&gt;查看创建的 Service:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get svc my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
my-nginx   ClusterIP   10.0.162.149   &amp;lt;none&amp;gt;        80/TCP    21s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;就如之前提到的，一个 Service 背后都有一组(个) Pod. 这些 Pod 是通过 &lt;code&gt;Endpoint&lt;/code&gt; 来暴露的。
Service 会持续执行然后将结果通过 POST 请求到一个也叫 &lt;code&gt;my-nginx&lt;/code&gt; 的 Endpoint 对象。当有一个
Pod 挂掉时，该 Pod 的端点会自动从 Endpoint 上移除，当有一个新的 Pod 匹配 Service 的选择器
时也会自动添加到 Endpoint 上。 检查 Endpoint， 可以看到上面的 IP 地址与第一步 Pod 创建时的 IP 地址
是一样的&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe svc my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Name:                my-nginx
Namespace:           default
Labels:              run=my-nginx
Annotations:         &amp;lt;none&amp;gt;
Selector:            run=my-nginx
Type:                ClusterIP
IP:                  10.0.162.149
Port:                &amp;lt;unset&amp;gt; 80/TCP
Endpoints:           10.244.2.5:80,10.244.3.4:80
Session Affinity:    None
Events:              &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get ep my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME       ENDPOINTS                     AGE
my-nginx   10.244.2.5:80,10.244.3.4:80   1m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个时候就能在集群中的任意一个 Pod 中通过 curl 访问 &lt;code&gt;&amp;lt;CLUSTER-IP&amp;gt;:&amp;lt;PORT&amp;gt;&lt;/code&gt; 来访问这个 nginx
的 Service.  要注意 Service 的 IP 完全是虚拟的。 如果用户对此有兴趣可以看看
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/#virtual-ips-and-service-proxies&#34;&gt;service proxy&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Accessing the Service

Kubernetes supports 2 primary modes of finding a Service - environment variables
and DNS. The former works out of the box while the latter requires the
[CoreDNS cluster addon](https://releases.k8s.io/master/cluster/addons/dns/coredns).
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If the service environment variables are not desired (because possible clashing with expected program ones,
too many variables to process, only using DNS, etc) you can disable this mode by setting the &lt;code&gt;enableServiceLinks&lt;/code&gt;
flag to &lt;code&gt;false&lt;/code&gt; on the &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubernetes-api/v1.19/#pod-v1-core&#34;&gt;pod spec&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;访问这个-service&#34;&gt;访问这个 Service&lt;/h2&gt;
&lt;p&gt;k8s 主要有两种发现 Service 的模式 - 环境变量和 DNS。 前一个是直接可以用的(要注意顺序问题)
后一种则需要
&lt;a href=&#34;https://releases.k8s.io/master/cluster/addons/dns/coredns&#34;&gt;CoreDNS 集群插件&lt;/a&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果不需要 Service 的环境变量(因为可能与程序有冲突，需要处理的变量太多，只使用 DNS 等)， 可以
通过在
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#pod-v1-core&#34;&gt;Pod 定义中&lt;/a&gt;
将 &lt;code&gt;enableServiceLinks&lt;/code&gt; 字段的值设置为 &lt;code&gt;false&lt;/code&gt; 来禁用这种模式&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;!--
### Environment Variables

When a Pod runs on a Node, the kubelet adds a set of environment variables for
each active Service. This introduces an ordering problem. To see why, inspect
the environment of your running nginx Pods (your Pod name will be different):

```shell
kubectl exec my-nginx-3800858182-jr4a2 -- printenv | grep SERVICE
```
```
KUBERNETES_SERVICE_HOST=10.0.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
```

Note there&#39;s no mention of your Service. This is because you created the replicas
before the Service. Another disadvantage of doing this is that the scheduler might
put both Pods on the same machine, which will take your entire Service down if
it dies. We can do this the right way by killing the 2 Pods and waiting for the
Deployment to recreate them. This time around the Service exists *before* the
replicas. This will give you scheduler-level Service spreading of your Pods
(provided all your nodes have equal capacity), as well as the right environment
variables:

```shell
kubectl scale deployment my-nginx --replicas=0; kubectl scale deployment my-nginx --replicas=2;

kubectl get pods -l run=my-nginx -o wide
```
```
NAME                        READY     STATUS    RESTARTS   AGE     IP            NODE
my-nginx-3800858182-e9ihh   1/1       Running   0          5s      10.244.2.7    kubernetes-minion-ljyd
my-nginx-3800858182-j4rm4   1/1       Running   0          5s      10.244.3.8    kubernetes-minion-905m
```

You may notice that the pods have different names, since they are killed and recreated.

```shell
kubectl exec my-nginx-3800858182-e9ihh -- printenv | grep SERVICE
```
```
KUBERNETES_SERVICE_PORT=443
MY_NGINX_SERVICE_HOST=10.0.162.149
KUBERNETES_SERVICE_HOST=10.0.0.1
MY_NGINX_SERVICE_PORT=80
KUBERNETES_SERVICE_PORT_HTTPS=443
```
 --&gt;
&lt;h3 id=&#34;环境变量&#34;&gt;环境变量&lt;/h3&gt;
&lt;p&gt;当一个 Pod 在一个节点上运行时， kubelet 为会每个活跃的 Service 添加一系列环境变量。这会引入
一个顺序问题。为啥呢，先看看现在运行的 nginx Pod 的环境变量(Pod 名称根据实际会不同)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl exec my-nginx-3800858182-jr4a2 -- printenv | grep SERVICE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;KUBERNETES_SERVICE_HOST=10.0.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以看到没有 nginx Service 的信息。 这是因为这些 Pod 副本是在 Service 之前创建的。另一个缺点
是调度器可能将两个 Pod 都丢到一个节点点上，如果这个节点挂了，则整个 Service 也就挂了。这时候
先杀掉这两个 Pod 然后等 Deployment 重建。 这样 Service 就在这些 Pod 创建 &lt;em&gt;之前&lt;/em&gt; 就存在了.
这样就会让 Pod 在 Service 的调度级别(所有节点均匀分布)，同时环境变量也有了:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl scale deployment my-nginx --replicas&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0; kubectl scale deployment my-nginx --replicas&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;2;

kubectl get pods -l run&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;my-nginx -o wide
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME                        READY     STATUS    RESTARTS   AGE     IP            NODE
my-nginx-3800858182-e9ihh   1/1       Running   0          5s      10.244.2.7    kubernetes-minion-ljyd
my-nginx-3800858182-j4rm4   1/1       Running   0          5s      10.244.3.8    kubernetes-minion-905m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这时候可以看到 Pod 的名称也变了，因为它们被干掉然后重建了。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl exec my-nginx-3800858182-e9ihh -- printenv | grep SERVICE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;KUBERNETES_SERVICE_PORT=443
MY_NGINX_SERVICE_HOST=10.0.162.149
KUBERNETES_SERVICE_HOST=10.0.0.1
MY_NGINX_SERVICE_PORT=80
KUBERNETES_SERVICE_PORT_HTTPS=443
&lt;/code&gt;&lt;/pre&gt;&lt;!--
### DNS

Kubernetes offers a DNS cluster addon Service that automatically assigns dns names to other Services. You can check if it&#39;s running on your cluster:

```shell
kubectl get services kube-dns --namespace=kube-system
```
```
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   10.0.0.10    &lt;none&gt;        53/UDP,53/TCP   8m
```

The rest of this section will assume you have a Service with a long lived IP
(my-nginx), and a DNS server that has assigned a name to that IP. Here we use the CoreDNS cluster addon (application name `kube-dns`), so you can talk to the Service from any pod in your cluster using standard methods (e.g. `gethostbyname()`). If CoreDNS isn&#39;t running, you can enable it referring to the [CoreDNS README](https://github.com/coredns/deployment/tree/master/kubernetes) or [Installing CoreDNS](/docs/tasks/administer-cluster/coredns/#installing-coredns). Let&#39;s run another curl application to test this:

```shell
kubectl run curl --image=radial/busyboxplus:curl -i --tty
```
```
Waiting for pod default/curl-131556218-9fnch to be running, status is Pending, pod ready: false
Hit enter for command prompt
```

Then, hit enter and run `nslookup my-nginx`:

```shell
[ root@curl-131556218-9fnch:/ ]$ nslookup my-nginx
Server:    10.0.0.10
Address 1: 10.0.0.10

Name:      my-nginx
Address 1: 10.0.162.149
```
 --&gt;
&lt;h3 id=&#34;dns&#34;&gt;DNS&lt;/h3&gt;
&lt;p&gt;k8s 提供了一个 DNS 集群插件的 Service, 它会自动地为其它的 Service 分配 DNS 名称。
可以通过以下命令查看集群中是否运行了该服务:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get services kube-dns --namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   10.0.0.10    &amp;lt;none&amp;gt;        53/UDP,53/TCP   8m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;本节接下来部分假设有一个拥有长期存在 IP 的 Service (my-nginx), 并且 DNS 服务为这个 IP
分配了一个名称。 这里我们使用的是 CoreDNS 集群插件(应用名称为 &lt;code&gt;kube-dns&lt;/code&gt;)， 因此可以在集群中
的任意一个 Pod 通过标准方式(例如，&lt;code&gt;gethostbyname()&lt;/code&gt; ) 与这个 Service 通信。 如果没有 CoreDNS，
可以参考
&lt;a href=&#34;https://github.com/coredns/deployment/tree/master/kubernetes&#34;&gt;CoreDNS README&lt;/a&gt;
或
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/administer-cluster/coredns/#installing-coredns&#34;&gt;安装 CoreDNS&lt;/a&gt;.
这时候再运行 curl 应用来验证:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl run curl --image&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;radial/busyboxplus:curl -i --tty
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Waiting for pod default/curl-131556218-9fnch to be running, status is Pending, pod ready: false
Hit enter for command prompt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;看到以上提供，再次敲回车键，然后运行 &lt;code&gt;nslookup my-nginx&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt; root@curl-131556218-9fnch:/ &lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;$ nslookup my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Server:    10.0.0.10
Address 1: 10.0.0.10

Name:      my-nginx
Address 1: 10.0.162.149
&lt;/code&gt;&lt;/pre&gt;&lt;!--
## Securing the Service

Till now we have only accessed the nginx server from within the cluster. Before exposing the Service to the internet, you want to make sure the communication channel is secure. For this, you will need:

* Self signed certificates for https (unless you already have an identity certificate)
* An nginx server configured to use the certificates
* A [secret](/docs/concepts/configuration/secret/) that makes the certificates accessible to pods

You can acquire all these from the [nginx https example](https://github.com/kubernetes/examples/tree/master/staging/https-nginx/). This requires having go and make tools installed. If you don&#39;t want to install those, then follow the manual steps later. In short:

```shell
make keys KEY=/tmp/nginx.key CERT=/tmp/nginx.crt
kubectl create secret tls nginxsecret --key /tmp/nginx.key --cert /tmp/nginx.crt
```
```
secret/nginxsecret created
```
```shell
kubectl get secrets
```
```
NAME                  TYPE                                  DATA      AGE
default-token-il9rc   kubernetes.io/service-account-token   1         1d
nginxsecret           kubernetes.io/tls                     2         1m
```
And also the configmap:
```shell
kubectl create configmap nginxconfigmap --from-file=default.conf
```
```
configmap/nginxconfigmap created
```
```shell
kubectl get configmaps
```
```
NAME             DATA   AGE
nginxconfigmap   1      114s
```
Following are the manual steps to follow in case you run into problems running make (on windows for example):

```shell
# Create a public private key pair
openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /d/tmp/nginx.key -out /d/tmp/nginx.crt -subj &#34;/CN=my-nginx/O=my-nginx&#34;
# Convert the keys to base64 encoding
cat /d/tmp/nginx.crt | base64
cat /d/tmp/nginx.key | base64
```
Use the output from the previous commands to create a yaml file as follows. The base64 encoded value should all be on a single line.

```yaml
apiVersion: &#34;v1&#34;
kind: &#34;Secret&#34;
metadata:
  name: &#34;nginxsecret&#34;
  namespace: &#34;default&#34;
type: kubernetes.io/tls
data:
  tls.crt: &#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURIekNDQWdlZ0F3SUJBZ0lKQUp5M3lQK0pzMlpJTUEwR0NTcUdTSWIzRFFFQkJRVUFNQ1l4RVRBUEJnTlYKQkFNVENHNW5hVzU0YzNaak1SRXdEd1lEVlFRS0V3aHVaMmx1ZUhOMll6QWVGdzB4TnpFd01qWXdOekEzTVRKYQpGdzB4T0RFd01qWXdOekEzTVRKYU1DWXhFVEFQQmdOVkJBTVRDRzVuYVc1NGMzWmpNUkV3RHdZRFZRUUtFd2h1CloybHVlSE4yWXpDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBSjFxSU1SOVdWM0IKMlZIQlRMRmtobDRONXljMEJxYUhIQktMSnJMcy8vdzZhU3hRS29GbHlJSU94NGUrMlN5ajBFcndCLzlYTnBwbQppeW1CL3JkRldkOXg5UWhBQUxCZkVaTmNiV3NsTVFVcnhBZW50VWt1dk1vLzgvMHRpbGhjc3paenJEYVJ4NEo5Ci82UVRtVVI3a0ZTWUpOWTVQZkR3cGc3dlVvaDZmZ1Voam92VG42eHNVR0M2QURVODBpNXFlZWhNeVI1N2lmU2YKNHZpaXdIY3hnL3lZR1JBRS9mRTRqakxCdmdONjc2SU90S01rZXV3R0ljNDFhd05tNnNTSzRqYUNGeGpYSnZaZQp2by9kTlEybHhHWCtKT2l3SEhXbXNhdGp4WTRaNVk3R1ZoK0QrWnYvcW1mMFgvbVY0Rmo1NzV3ajFMWVBocWtsCmdhSXZYRyt4U1FVQ0F3RUFBYU5RTUU0d0hRWURWUjBPQkJZRUZPNG9OWkI3YXc1OUlsYkROMzhIYkduYnhFVjcKTUI4R0ExVWRJd1FZTUJhQUZPNG9OWkI3YXc1OUlsYkROMzhIYkduYnhFVjdNQXdHQTFVZEV3UUZNQU1CQWY4dwpEUVlKS29aSWh2Y05BUUVGQlFBRGdnRUJBRVhTMW9FU0lFaXdyMDhWcVA0K2NwTHI3TW5FMTducDBvMm14alFvCjRGb0RvRjdRZnZqeE04Tzd2TjB0clcxb2pGSW0vWDE4ZnZaL3k4ZzVaWG40Vm8zc3hKVmRBcStNZC9jTStzUGEKNmJjTkNUekZqeFpUV0UrKzE5NS9zb2dmOUZ3VDVDK3U2Q3B5N0M3MTZvUXRUakViV05VdEt4cXI0Nk1OZWNCMApwRFhWZmdWQTRadkR4NFo3S2RiZDY5eXM3OVFHYmg5ZW1PZ05NZFlsSUswSGt0ejF5WU4vbVpmK3FqTkJqbWZjCkNnMnlwbGQ0Wi8rUUNQZjl3SkoybFIrY2FnT0R4elBWcGxNSEcybzgvTHFDdnh6elZPUDUxeXdLZEtxaUMwSVEKQ0I5T2wwWW5scE9UNEh1b2hSUzBPOStlMm9KdFZsNUIyczRpbDlhZ3RTVXFxUlU9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K&#34;
  tls.key: &#34;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&#34;
```
Now create the secrets using the file:

```shell
kubectl apply -f nginxsecrets.yaml
kubectl get secrets
```
```
NAME                  TYPE                                  DATA      AGE
default-token-il9rc   kubernetes.io/service-account-token   1         1d
nginxsecret           kubernetes.io/tls                     2         1m
```

Now modify your nginx replicas to start an https server using the certificate in the secret, and the Service, to expose both ports (80 and 443):



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnginx-secure-appyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/nginx-secure-app.yaml&#34; download=&#34;service/networking/nginx-secure-app.yaml&#34;&gt;
                    &lt;code&gt;service/networking/nginx-secure-app.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnginx-secure-appyaml&#39;)&#34; title=&#34;Copy service/networking/nginx-secure-app.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NodePort&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;443&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginxsecret&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;configmap-volume&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;configMap&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginxconfigmap&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginxhttps&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bprashanth/nginxhttps:1.0&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;443&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/etc/nginx/ssl&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/etc/nginx/conf.d&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;configmap-volume&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



Noteworthy points about the nginx-secure-app manifest:

- It contains both Deployment and Service specification in the same file.
- The [nginx server](https://github.com/kubernetes/examples/tree/master/staging/https-nginx/default.conf)
  serves HTTP traffic on port 80 and HTTPS traffic on 443, and nginx Service
  exposes both ports.
- Each container has access to the keys through a volume mounted at `/etc/nginx/ssl`.
  This is setup *before* the nginx server is started.

```shell
kubectl delete deployments,svc my-nginx; kubectl create -f ./nginx-secure-app.yaml
```

At this point you can reach the nginx server from any node.

```shell
kubectl get pods -o yaml | grep -i podip
    podIP: 10.244.3.5
node $ curl -k https://10.244.3.5
...
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
```

Note how we supplied the `-k` parameter to curl in the last step, this is because we don&#39;t know anything about the pods running nginx at certificate generation time,
so we have to tell curl to ignore the CName mismatch. By creating a Service we linked the CName used in the certificate with the actual DNS name used by pods during Service lookup.
Let&#39;s test this from a pod (the same secret is being reused for simplicity, the pod only needs nginx.crt to access the Service):



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingcurlpodyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/curlpod.yaml&#34; download=&#34;service/networking/curlpod.yaml&#34;&gt;
                    &lt;code&gt;service/networking/curlpod.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingcurlpodyaml&#39;)&#34; title=&#34;Copy service/networking/curlpod.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;curl-deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;curlpod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;curlpod&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginxsecret&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;curlpod&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;:
        - &lt;span style=&#34;color:#ae81ff&#34;&gt;sh&lt;/span&gt;
        - -&lt;span style=&#34;color:#ae81ff&#34;&gt;c&lt;/span&gt;
        - &lt;span style=&#34;color:#ae81ff&#34;&gt;while true; do sleep 1; done&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;radial/busyboxplus:curl&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/etc/nginx/ssl&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



```shell
kubectl apply -f ./curlpod.yaml
kubectl get pods -l app=curlpod
```
```
NAME                               READY     STATUS    RESTARTS   AGE
curl-deployment-1515033274-1410r   1/1       Running   0          1m
```
```shell
kubectl exec curl-deployment-1515033274-1410r -- curl https://my-nginx --cacert /etc/nginx/ssl/tls.crt
...
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
...
```
 --&gt;
&lt;h2 id=&#34;为-service-加安全层&#34;&gt;为 Service 加安全层&lt;/h2&gt;
&lt;p&gt;到目前为止我们都是在集群内访问这个 nginx 服务。 在将这个 Service 暴露到互联网之前，需要确保
连接是安全。 因此，必须要:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Self signed certificates for https (unless you already have an identity certificate)&lt;/li&gt;
&lt;li&gt;An nginx server configured to use the certificates&lt;/li&gt;
&lt;li&gt;A &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/secret/&#34;&gt;secret&lt;/a&gt; that makes the certificates accessible to pods&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;创建一个自签的 https 证书(除非已经有对应的证书)&lt;/li&gt;
&lt;li&gt;配置一个使用该证书的 nginx 服务&lt;/li&gt;
&lt;li&gt;创建一个 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/secret/&#34;&gt;Secret&lt;/a&gt;， 让证书在 Pod 中可用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有的这些都可以参考
&lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/https-nginx&#34;&gt;nginx https 示例&lt;/a&gt;
这个需要有 go 环境和安装打包工具。如果不想安装这些， 则跟着下面步骤手动操作， 简单说:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;make keys KEY&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/tmp/nginx.key CERT&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/tmp/nginx.crt
kubectl create secret tls nginxsecret --key /tmp/nginx.key --cert /tmp/nginx.crt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;secret/nginxsecret created
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get secrets
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME                  TYPE                                  DATA      AGE
default-token-il9rc   kubernetes.io/service-account-token   1         1d
nginxsecret           kubernetes.io/tls                     2         1m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;再创建 Configmap:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create configmap nginxconfigmap --from-file&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;default.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;configmap/nginxconfigmap created
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get configmaps
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME             DATA   AGE
nginxconfigmap   1      114s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果运行 make 有问题，则跟随下面的手动步骤操作(比如在 windows 上):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a public private key pair&lt;/span&gt;
openssl req -x509 -nodes -days &lt;span style=&#34;color:#ae81ff&#34;&gt;365&lt;/span&gt; -newkey rsa:2048 -keyout /d/tmp/nginx.key -out /d/tmp/nginx.crt -subj &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/CN=my-nginx/O=my-nginx&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Convert the keys to base64 encoding&lt;/span&gt;
cat /d/tmp/nginx.crt | base64
cat /d/tmp/nginx.key | base64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Use the output from the previous commands to create a yaml file as follows. The base64 encoded value should all be on a single line.
使用上面的输出创建以下的 yaml 配置文件。 base64 编码的内容应该只有一行。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;v1&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Secret&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nginxsecret&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/tls&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;tls.crt&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURIekNDQWdlZ0F3SUJBZ0lKQUp5M3lQK0pzMlpJTUEwR0NTcUdTSWIzRFFFQkJRVUFNQ1l4RVRBUEJnTlYKQkFNVENHNW5hVzU0YzNaak1SRXdEd1lEVlFRS0V3aHVaMmx1ZUhOMll6QWVGdzB4TnpFd01qWXdOekEzTVRKYQpGdzB4T0RFd01qWXdOekEzTVRKYU1DWXhFVEFQQmdOVkJBTVRDRzVuYVc1NGMzWmpNUkV3RHdZRFZRUUtFd2h1CloybHVlSE4yWXpDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBSjFxSU1SOVdWM0IKMlZIQlRMRmtobDRONXljMEJxYUhIQktMSnJMcy8vdzZhU3hRS29GbHlJSU94NGUrMlN5ajBFcndCLzlYTnBwbQppeW1CL3JkRldkOXg5UWhBQUxCZkVaTmNiV3NsTVFVcnhBZW50VWt1dk1vLzgvMHRpbGhjc3paenJEYVJ4NEo5Ci82UVRtVVI3a0ZTWUpOWTVQZkR3cGc3dlVvaDZmZ1Voam92VG42eHNVR0M2QURVODBpNXFlZWhNeVI1N2lmU2YKNHZpaXdIY3hnL3lZR1JBRS9mRTRqakxCdmdONjc2SU90S01rZXV3R0ljNDFhd05tNnNTSzRqYUNGeGpYSnZaZQp2by9kTlEybHhHWCtKT2l3SEhXbXNhdGp4WTRaNVk3R1ZoK0QrWnYvcW1mMFgvbVY0Rmo1NzV3ajFMWVBocWtsCmdhSXZYRyt4U1FVQ0F3RUFBYU5RTUU0d0hRWURWUjBPQkJZRUZPNG9OWkI3YXc1OUlsYkROMzhIYkduYnhFVjcKTUI4R0ExVWRJd1FZTUJhQUZPNG9OWkI3YXc1OUlsYkROMzhIYkduYnhFVjdNQXdHQTFVZEV3UUZNQU1CQWY4dwpEUVlKS29aSWh2Y05BUUVGQlFBRGdnRUJBRVhTMW9FU0lFaXdyMDhWcVA0K2NwTHI3TW5FMTducDBvMm14alFvCjRGb0RvRjdRZnZqeE04Tzd2TjB0clcxb2pGSW0vWDE4ZnZaL3k4ZzVaWG40Vm8zc3hKVmRBcStNZC9jTStzUGEKNmJjTkNUekZqeFpUV0UrKzE5NS9zb2dmOUZ3VDVDK3U2Q3B5N0M3MTZvUXRUakViV05VdEt4cXI0Nk1OZWNCMApwRFhWZmdWQTRadkR4NFo3S2RiZDY5eXM3OVFHYmg5ZW1PZ05NZFlsSUswSGt0ejF5WU4vbVpmK3FqTkJqbWZjCkNnMnlwbGQ0Wi8rUUNQZjl3SkoybFIrY2FnT0R4elBWcGxNSEcybzgvTHFDdnh6elZPUDUxeXdLZEtxaUMwSVEKQ0I5T2wwWW5scE9UNEh1b2hSUzBPOStlMm9KdFZsNUIyczRpbDlhZ3RTVXFxUlU9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;tls.key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;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&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;然后使用这引文件创建 Secret：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f nginxsecrets.yaml
kubectl get secrets
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME                  TYPE                                  DATA      AGE
default-token-il9rc   kubernetes.io/service-account-token   1         1d
nginxsecret           kubernetes.io/tls                     2         1m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这时候修改 nginx 副本使用 Secret 中的证书启动一个 https 服务。 Service 也需要同时暴露 80 和 443 端口:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnginx-secure-appyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/nginx-secure-app.yaml&#34; download=&#34;service/networking/nginx-secure-app.yaml&#34;&gt;
                    &lt;code&gt;service/networking/nginx-secure-app.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnginx-secure-appyaml&#39;)&#34; title=&#34;Copy service/networking/nginx-secure-app.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NodePort&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;443&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginxsecret&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;configmap-volume&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;configMap&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginxconfigmap&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginxhttps&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bprashanth/nginxhttps:1.0&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;443&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/etc/nginx/ssl&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/etc/nginx/conf.d&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;configmap-volume&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;需要重点注意 nginx-secure-app 的地方:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在同一个文件中包含了 Deployment 和 Service 的定义配置。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/https-nginx/default.conf&#34;&gt;nginx 服务器&lt;/a&gt;
同时在 80 端口提供 HTTP 流量和 443 端口提供 HTTPS 流量， nginx Service 同时暴露了这两个端口。&lt;/li&gt;
&lt;li&gt;每个容器都通过挂载在  &lt;code&gt;/etc/nginx/ssl&lt;/code&gt; 的数据卷来访问证书
因此需要在 nginx 服务启动 &lt;em&gt;之前&lt;/em&gt; 配置好&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl delete deployments,svc my-nginx; kubectl create -f ./nginx-secure-app.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这时候就可以在任意一个节点上访问这个 nginx 服务&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods -o yaml | grep -i podip
    podIP: 10.244.3.5
node $ curl -k https://10.244.3.5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;...
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;注意这里最后一步中的 curl 使用用了 &lt;code&gt;-k&lt;/code&gt;， 因为在创建证书的时候不会知道运行 nginx 的 Pod的
任何信息， 因此要让 curl 忽略 CName 不匹配的问题。通过创建一个 Service, 就可以把证书中的
CName 和 Pod 在对 Service 解析使用的 DNS 名称统一起来。
在一个 Pod 中再测试一下(为了简单使用同一个 Secret, 这个 Pod 访问 Service 只需要 nginx.crt)&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingcurlpodyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/curlpod.yaml&#34; download=&#34;service/networking/curlpod.yaml&#34;&gt;
                    &lt;code&gt;service/networking/curlpod.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingcurlpodyaml&#39;)&#34; title=&#34;Copy service/networking/curlpod.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;curl-deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;curlpod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;curlpod&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginxsecret&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;curlpod&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;:
        - &lt;span style=&#34;color:#ae81ff&#34;&gt;sh&lt;/span&gt;
        - -&lt;span style=&#34;color:#ae81ff&#34;&gt;c&lt;/span&gt;
        - &lt;span style=&#34;color:#ae81ff&#34;&gt;while true; do sleep 1; done&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;radial/busyboxplus:curl&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/etc/nginx/ssl&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f ./curlpod.yaml
kubectl get pods -l app&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;curlpod
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME                               READY     STATUS    RESTARTS   AGE
curl-deployment-1515033274-1410r   1/1       Running   0          1m
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl exec curl-deployment-1515033274-1410r -- curl https://my-nginx --cacert /etc/nginx/ssl/tls.crt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;...
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;!--
## Exposing the Service

For some parts of your applications you may want to expose a Service onto an
external IP address. Kubernetes supports two ways of doing this: NodePorts and
LoadBalancers. The Service created in the last section already used `NodePort`,
so your nginx HTTPS replica is ready to serve traffic on the internet if your
node has a public IP.

```shell
kubectl get svc my-nginx -o yaml | grep nodePort -C 5
  uid: 07191fb3-f61a-11e5-8ae5-42010af00002
spec:
  clusterIP: 10.0.162.149
  ports:
  - name: http
    nodePort: 31704
    port: 8080
    protocol: TCP
    targetPort: 80
  - name: https
    nodePort: 32453
    port: 443
    protocol: TCP
    targetPort: 443
  selector:
    run: my-nginx
```
```shell
kubectl get nodes -o yaml | grep ExternalIP -C 1
    - address: 104.197.41.11
      type: ExternalIP
    allocatable:
--
    - address: 23.251.152.56
      type: ExternalIP
    allocatable:
...

$ curl https://&lt;EXTERNAL-IP&gt;:&lt;NODE-PORT&gt; -k
...
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
```

Let&#39;s now recreate the Service to use a cloud load balancer, just change the `Type` of `my-nginx` Service from `NodePort` to `LoadBalancer`:

```shell
kubectl edit svc my-nginx
kubectl get svc my-nginx
```
```
NAME       TYPE           CLUSTER-IP     EXTERNAL-IP        PORT(S)               AGE
my-nginx   LoadBalancer   10.0.162.149     xx.xxx.xxx.xxx     8080:30163/TCP        21s
```
```
curl https://&lt;EXTERNAL-IP&gt; -k
...
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
```

The IP address in the `EXTERNAL-IP` column is the one that is available on the public internet.  The `CLUSTER-IP` is only available inside your
cluster/private cloud network.

Note that on AWS, type `LoadBalancer` creates an ELB, which uses a (long)
hostname, not an IP.  It&#39;s too long to fit in the standard `kubectl get svc`
output, in fact, so you&#39;ll need to do `kubectl describe service my-nginx` to
see it.  You&#39;ll see something like this:

```shell
kubectl describe service my-nginx
...
LoadBalancer Ingress:   a320587ffd19711e5a37606cf4a74574-1142138393.us-east-1.elb.amazonaws.com
...
```
 --&gt;
&lt;h2 id=&#34;暴露这个-service&#34;&gt;暴露这个 Service&lt;/h2&gt;
&lt;p&gt;For some parts of your applications you may want to expose a Service onto an
external IP address. Kubernetes supports two ways of doing this: NodePorts and
LoadBalancers. The Service created in the last section already used &lt;code&gt;NodePort&lt;/code&gt;,
so your nginx HTTPS replica is ready to serve traffic on the internet if your
node has a public IP.&lt;/p&gt;
&lt;p&gt;对于应用中的一部分，用户可能希望通过一个 Service 将其暴露到一个公网 IP 地址上。 k8s 支持两种方式:
NodePort 和 负载均衡器。 最后创建的这个 Service 已经使用了 &lt;code&gt;NodePort&lt;/code&gt;， 所以如果集群中的
节点有公网IP，这个 nginx 的 HTTPS 就可以通过公网访问。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get svc my-nginx -o yaml | grep nodePort -C &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;uid: 07191fb3-f61a-11e5-8ae5-42010af00002
spec:
clusterIP: 10.0.162.149
ports:
- name: http
  nodePort: 31704
  port: 8080
  protocol: TCP
  targetPort: 80
- name: https
  nodePort: 32453
  port: 443
  protocol: TCP
  targetPort: 443
selector:
  run: my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get nodes -o yaml | grep ExternalIP -C &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;- address: 104.197.41.11
  type: ExternalIP
allocatable:
--
- address: 23.251.152.56
  type: ExternalIP
allocatable:
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ curl https://&amp;lt;EXTERNAL-IP&amp;gt;:&amp;lt;NODE-PORT&amp;gt; -k
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;...
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;现在修改 Service 使它使用云负载均衡器，只需要将 &lt;code&gt;my-nginx&lt;/code&gt; Service 的 &lt;code&gt;Type&lt;/code&gt; 由 &lt;code&gt;NodePort&lt;/code&gt; 改为 &lt;code&gt;LoadBalancer&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl edit svc my-nginx
kubectl get svc my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME       TYPE           CLUSTER-IP     EXTERNAL-IP        PORT(S)               AGE
my-nginx   LoadBalancer   10.0.162.149     xx.xxx.xxx.xxx     8080:30163/TCP        21s
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl https://&amp;lt;EXTERNAL-IP&amp;gt; -k
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;...
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The IP address in the &lt;code&gt;EXTERNAL-IP&lt;/code&gt; column is the one that is available on the public internet.  The &lt;code&gt;CLUSTER-IP&lt;/code&gt; is only available inside your
cluster/private cloud network.
&lt;code&gt;EXTERNAL-IP&lt;/code&gt; 列中的 IP 地址就是一个公网可用的地址。 &lt;code&gt;CLUSTER-IP&lt;/code&gt; 只能在集群内部使用。
Note that on AWS, type &lt;code&gt;LoadBalancer&lt;/code&gt; creates an ELB, which uses a (long)
hostname, not an IP.  It&amp;rsquo;s too long to fit in the standard &lt;code&gt;kubectl get svc&lt;/code&gt;
output, in fact, so you&amp;rsquo;ll need to do &lt;code&gt;kubectl describe service my-nginx&lt;/code&gt; to
see it.  You&amp;rsquo;ll see something like this:&lt;/p&gt;
&lt;p&gt;要注意在 AWS 上， &lt;code&gt;LoadBalancer&lt;/code&gt; 类型的 Service 会创建一个 ELB, 它会使用一个(很长的)主机名，
而不是一个 IP 地址。 因为这个主机名太长而不适配标准的 &lt;code&gt;kubectl get svc&lt;/code&gt; 输出，实际上需要使用
&lt;code&gt;kubectl describe service my-nginx&lt;/code&gt; 命令才能看当输出的这个主机名，样子就类似下面的:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe service my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;...
LoadBalancer Ingress:   a320587ffd19711e5a37606cf4a74574-1142138393.us-east-1.elb.amazonaws.com
...
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/access-application-cluster/service-access-application-cluster/&#34;&gt;使用 Service 让集群中的一个应用可以访问&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/access-application-cluster/connecting-frontend-backend/&#34;&gt;使用 Service 连接前后端应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/access-application-cluster/create-external-load-balancer/&#34;&gt;创建一个外部负载均衡器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: EndpointSlice</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/endpoint-slices/</link>
      <pubDate>Thu, 17 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/endpoint-slices/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- freehan
title: EndpointSlices
content_type: concept
weight: 35
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;



_EndpointSlices_ provide a simple way to track network endpoints within a
Kubernetes cluster. They offer a more scalable and extensible alternative to
Endpoints.
 --&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;em&gt;EndpointSlice&lt;/em&gt; 提供了一种简单的方式来在 k8s 集群跟踪网络端点。 它提供了比 Endpoint 拥有更
好的伸缩性和扩展性的替代方案&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Motivation

The Endpoints API has provided a simple and straightforward way of
tracking network endpoints in Kubernetes. Unfortunately as Kubernetes clusters
and &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Services&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; have grown to handle and
send more traffic to more backend Pods, limitations of that original API became
more visible.
Most notably, those included challenges with scaling to larger numbers of
network endpoints.

Since all network endpoints for a Service were stored in a single Endpoints
resource, those resources could get quite large. That affected the performance
of Kubernetes components (notably the master control plane) and resulted in
significant amounts of network traffic and processing when Endpoints changed.
EndpointSlices help you mitigate those issues as well as provide an extensible
platform for additional features such as topological routing.
 --&gt;
&lt;h2 id=&#34;动机&#34;&gt;动机&lt;/h2&gt;
&lt;p&gt;Endpoint 的 API 提供了一个简单的直接的方式来跟踪 k8s 中的网络端点。不幸的是当 k8s 集群和
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 处理和发送更多的流量到更多的后端 Pod 时，
这个 API 的限制就越来越明显了.  特别是当集群中扩充到很大数量的网络端点时，这些挑战就越发明显。&lt;/p&gt;
&lt;p&gt;当一个 Service 所有的网络端点都被存储在一个 Endpoint 资源时，这个资源就会变得很大。这会影响
到 k8s 组件(特别是控制中心)的性能， 并且的 Endpoint 发生变化时导致大量的网络流量和相关处理业务。
EndpointSlice 缓解这些问题的同时提供了一个可扩展的平台，这个平台上还有包括拓扑路由等特性。&lt;/p&gt;
&lt;!--
## EndpointSlice resources {#endpointslice-resource}

In Kubernetes, an EndpointSlice contains references to a set of network
endpoints. The control plane automatically creates EndpointSlices
for any Kubernetes Service that has a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/&#39; target=&#39;_blank&#39;&gt;selector&lt;span class=&#39;tooltip-text&#39;&gt;允许用户基于标签过滤资源列表&lt;/span&gt;
&lt;/a&gt; specified. These EndpointSlices include
references to all the Pods that match the Service selector. EndpointSlices group
network endpoints together by unique combinations of protocol, port number, and
Service name.  
The name of a EndpointSlice object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

As an example, here&#39;s a sample EndpointSlice resource for the `example`
Kubernetes Service.

```yaml
apiVersion: discovery.k8s.io/v1beta1
kind: EndpointSlice
metadata:
  name: example-abc
  labels:
    kubernetes.io/service-name: example
addressType: IPv4
ports:
  - name: http
    protocol: TCP
    port: 80
endpoints:
  - addresses:
      - &#34;10.1.2.3&#34;
    conditions:
      ready: true
    hostname: pod-1
    topology:
      kubernetes.io/hostname: node-1
      topology.kubernetes.io/zone: us-west2-a
```

By default, the control plane creates and manages EndpointSlices to have no
more than 100 endpoints each. You can configure this with the
`--max-endpoints-per-slice`
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/kube-controller-manager/&#39; target=&#39;_blank&#39;&gt;kube-controller-manager&lt;span class=&#39;tooltip-text&#39;&gt;Control Plane component that runs controller processes.&lt;/span&gt;
&lt;/a&gt;
flag, up to a maximum of 1000.

EndpointSlices can act as the source of truth for
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/kube-proxy/&#39; target=&#39;_blank&#39;&gt;kube-proxy&lt;span class=&#39;tooltip-text&#39;&gt;kube-proxy is a network proxy that runs on each node in the cluster.&lt;/span&gt;
&lt;/a&gt; when it comes to
how to route internal traffic. When enabled, they should provide a performance
improvement for services with large numbers of endpoints.
 --&gt;
&lt;h2 id=&#34;endpointslice-resource&#34;&gt;EndpointSlice 资源&lt;/h2&gt;
&lt;p&gt;在 k8s 中， 一个 EndpointSlice 包含一组网络端点集合的引用。 控制中心会自动为任意包含
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/&#39; target=&#39;_blank&#39;&gt;选择器&lt;span class=&#39;tooltip-text&#39;&gt;允许用户基于标签过滤资源列表&lt;/span&gt;
&lt;/a&gt;
的 Service 创建 EndpointSlice。 这些 EndpointSlice 包含所有匹配该 Service 的 Pod 的引用。
EndpointSlice 通过协议，端口号，Service 名称的唯一组合将这些网络端点组织在一起。
EndpointSlice 对象的名称必须是一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;下面的例子中，是一个叫 &lt;code&gt;example&lt;/code&gt; 的 Service 的 EndpointSlice 资源的简单示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;discovery.k8s.io/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;EndpointSlice&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;example-abc&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;kubernetes.io/service-name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;example&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;addressType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IPv4&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;endpoints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;addresses&lt;/span&gt;:
      - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10.1.2.3&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;conditions&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;ready&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostname&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod-1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topology&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;kubernetes.io/hostname&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;node-1&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;topology.kubernetes.io/zone&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;us-west2-a&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;默认情况下， 控制中心创建和管理的 EndpointSlice 每个不超过 100 个网络端点。用户可以通过
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/kube-controller-manager/&#39; target=&#39;_blank&#39;&gt;kube-controller-manager&lt;span class=&#39;tooltip-text&#39;&gt;Control Plane component that runs controller processes.&lt;/span&gt;
&lt;/a&gt;
的 &lt;code&gt;--max-endpoints-per-slice&lt;/code&gt; 参数配置，最高可以到 1000。&lt;/p&gt;
&lt;p&gt;EndpointSlices 可以在路由内部流量时被认作
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/kube-proxy/&#39; target=&#39;_blank&#39;&gt;kube-proxy&lt;span class=&#39;tooltip-text&#39;&gt;kube-proxy is a network proxy that runs on each node in the cluster.&lt;/span&gt;
&lt;/a&gt; 的真实的来源。
当启用后，可以改善拥有大量网络端点的 Service 的性能。&lt;/p&gt;
&lt;!--
### Address types

EndpointSlices support three address types:

* IPv4
* IPv6
* FQDN (Fully Qualified Domain Name)
 --&gt;
&lt;h3 id=&#34;地址类型&#34;&gt;地址类型&lt;/h3&gt;
&lt;p&gt;EndpointSlice 支持以下三种类型:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;IPv4&lt;/li&gt;
&lt;li&gt;IPv6&lt;/li&gt;
&lt;li&gt;FQDN (全限定名)&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Topology information {#topology}

Each endpoint within an EndpointSlice can contain relevant topology information.
This is used to indicate where an endpoint is, containing information about the
corresponding Node, zone, and region. When the values are available, the
control plane sets the following Topology labels for EndpointSlices:

* `kubernetes.io/hostname` - The name of the Node this endpoint is on.
* `topology.kubernetes.io/zone` - The zone this endpoint is in.
* `topology.kubernetes.io/region` - The region this endpoint is in.

The values of these labels are derived from resources associated with each
endpoint in a slice. The hostname label represents the value of the NodeName
field on the corresponding Pod. The zone and region labels represent the value
of the labels with the same names on the corresponding Node.
 --&gt;
&lt;h3 id=&#34;topology&#34;&gt;拓扑信息&lt;/h3&gt;
&lt;p&gt;EndpointSlice 中的每一个网络端点都可以有一个有意义的拓扑信息。 用于指示这个网络端点在哪，包含对应的
节点信息，分区信息和地区信息。 当这些值存在时，控制中心会为 EndpointSlice 打上如下标签:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kubernetes.io/hostname&lt;/code&gt; - 这个网络端点所在的节点名称.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;topology.kubernetes.io/zone&lt;/code&gt; - 这个网络端点所在分区.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;topology.kubernetes.io/region&lt;/code&gt; - 这个网络端点所在地区.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些标签的值源于 EndpointSlice 中的每个网络端点对应的资源。  hostname 标签表示 对应 Pod
的 NodeName 字段的值。 &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;region&lt;/code&gt; 标签与对应节点同一标签一致。&lt;/p&gt;
&lt;!--
### Management

Most often, the control plane (specifically, the endpoint slice
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;controller&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt;) creates and
manages EndpointSlice objects. There are a variety of other use cases for
EndpointSlices, such as service mesh implementations, that could result in other
entities or controllers managing additional sets of EndpointSlices.

To ensure that multiple entities can manage EndpointSlices without interfering
with each other, Kubernetes defines the
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels&#39; target=&#39;_blank&#39;&gt;label&lt;span class=&#39;tooltip-text&#39;&gt;在对象上标上对用户有意义和有关联的属性&lt;/span&gt;
&lt;/a&gt;
`endpointslice.kubernetes.io/managed-by`, which indicates the entity managing
an EndpointSlice.
The endpoint slice controller sets `endpointslice-controller.k8s.io` as the value
for this label on all EndpointSlices it manages. Other entities managing
EndpointSlices should also set a unique value for this label.
 --&gt;
&lt;h3 id=&#34;管理&#34;&gt;管理&lt;/h3&gt;
&lt;p&gt;大多数时候，控制中心(确切的说，EndpointSlice &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt;)
创建和管理 EndpointSlice 对象。 除此之外 EndpointSlice 还有其它多种应用场景， 例如服务网格的实现，
由此可以会导致其它的实体或控制顺管理额外的 EndpointSlice 集合。&lt;/p&gt;
&lt;p&gt;为了能保证多个实体在管理 EndpointSlice 时不会相互影响， k8s 定义了 &lt;code&gt;endpointslice.kubernetes.io/managed-by&lt;/code&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels&#39; target=&#39;_blank&#39;&gt;label&lt;span class=&#39;tooltip-text&#39;&gt;在对象上标上对用户有意义和有关联的属性&lt;/span&gt;
&lt;/a&gt;，
这个标签用于指示是哪个实体在管理这个 EndpointSlice。 EndpointSlice 控制器会为所有它管理的
EndpointSlice 设置 &lt;code&gt;endpointslice-controller.k8s.io&lt;/code&gt; 标签。 其它实体管理 EndpointSlice
也应当为该标签设置唯一的值。&lt;/p&gt;
&lt;!--
### Ownership

In most use cases, EndpointSlices are owned by the Service that the endpoint
slice object tracks endpoints for. This ownership is indicated by an owner
reference on each EndpointSlice as well as a `kubernetes.io/service-name`
label that enables simple lookups of all EndpointSlices belonging to a Service.
 --&gt;
&lt;h3 id=&#34;所有权&#34;&gt;所有权&lt;/h3&gt;
&lt;p&gt;在大多数情况下， EndpointSlice 的所有者是它跟踪的网络端点对应的 Service。 这个所有权会通过
EndpointSlice 上的 &lt;code&gt;kubernetes.io/service-name&lt;/code&gt; 标签显示，标签的值为其所属 Service 的名称&lt;/p&gt;
&lt;!--
### EndpointSlice mirroring

In some cases, applications create custom Endpoints resources. To ensure that
these applications do not need to concurrently write to both Endpoints and
EndpointSlice resources, the cluster&#39;s control plane mirrors most Endpoints
resources to corresponding EndpointSlices.

The control plane mirrors Endpoints resources unless:

* the Endpoints resource has a `endpointslice.kubernetes.io/skip-mirror` label
  set to `true`.
* the Endpoints resource has a `control-plane.alpha.kubernetes.io/leader`
  annotation.
* the corresponding Service resource does not exist.
* the corresponding Service resource has a non-nil selector.

Individual Endpoints resources may translate into multiple EndpointSlices. This
will occur if an Endpoints resource has multiple subsets or includes endpoints
with multiple IP families (IPv4 and IPv6). A maximum of 1000 addresses per
subset will be mirrored to EndpointSlices.
 --&gt;
&lt;h3 id=&#34;endpointslice-镜像&#34;&gt;EndpointSlice 镜像&lt;/h3&gt;
&lt;p&gt;在某些情况下，应用会创建自定义的 Endpoint。 为了保证这些应用不会同时写入到 Endpoint 和 EndpointSlice
资源， 集群控制中心会为大多数 Endpoint 创建对应的 EndpointSlice。&lt;/p&gt;
&lt;p&gt;以下情况控制中心不会镜像 Endpoint:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Endpoint 资源有 &lt;code&gt;endpointslice.kubernetes.io/skip-mirror&lt;/code&gt; 标签，且值为 &lt;code&gt;true&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Endpoint 资源有 &lt;code&gt;control-plane.alpha.kubernetes.io/leader&lt;/code&gt; 注解。&lt;/li&gt;
&lt;li&gt;对应的 Service 资源不存在&lt;/li&gt;
&lt;li&gt;对应的 Service 资源有非空选择器&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个 Endpoint 可能会被转化为多个 EndpointSlice。发生这种情况的原因有可能是 Endpoint 资源
包含多个子网或包含的网络端点同时包含 IPv4 和 IPv6。 每个子网最多可以把 1000 个地址镜像到一个 EndpointSlice&lt;/p&gt;
&lt;!--
### Distribution of EndpointSlices

Each EndpointSlice has a set of ports that applies to all endpoints within the
resource. When named ports are used for a Service, Pods may end up with
different target port numbers for the same named port, requiring different
EndpointSlices. This is similar to the logic behind how subsets are grouped
with Endpoints.

The control plane tries to fill EndpointSlices as full as possible, but does not
actively rebalance them. The logic is fairly straightforward:

1. Iterate through existing EndpointSlices, remove endpoints that are no longer
   desired and update matching endpoints that have changed.
2. Iterate through EndpointSlices that have been modified in the first step and
   fill them up with any new endpoints needed.
3. If there&#39;s still new endpoints left to add, try to fit them into a previously
   unchanged slice and/or create new ones.

Importantly, the third step prioritizes limiting EndpointSlice updates over a
perfectly full distribution of EndpointSlices. As an example, if there are 10
new endpoints to add and 2 EndpointSlices with room for 5 more endpoints each,
this approach will create a new EndpointSlice instead of filling up the 2
existing EndpointSlices. In other words, a single EndpointSlice creation is
preferrable to multiple EndpointSlice updates.

With kube-proxy running on each Node and watching EndpointSlices, every change
to an EndpointSlice becomes relatively expensive since it will be transmitted to
every Node in the cluster. This approach is intended to limit the number of
changes that need to be sent to every Node, even if it may result with multiple
EndpointSlices that are not full.

In practice, this less than ideal distribution should be rare. Most changes
processed by the EndpointSlice controller will be small enough to fit in an
existing EndpointSlice, and if not, a new EndpointSlice is likely going to be
necessary soon anyway. Rolling updates of Deployments also provide a natural
repacking of EndpointSlices with all Pods and their corresponding endpoints
getting replaced.
 --&gt;
&lt;h3 id=&#34;endpointslice-分布&#34;&gt;EndpointSlice 分布&lt;/h3&gt;
&lt;p&gt;每个 EndpointSlice 都有一系列端口，这些端口会被应用到该资源内的所有的网络端点上。当一个
Service 使用命名端口时， Pod 最终可以会有同一个名称的端口的目标端口不一样的情况，这样就需要不
同的 EndpointSlice。 这与子网是怎么对 Endpoint 分组的逻辑是类似的。&lt;/p&gt;
&lt;p&gt;控制中心会尽量将 EndpointSlice 装满，但不会主动地重新平衡它们。 其中逻辑是相同简单的:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;迭代所有存在的 EndpointSlice，称除不需要的网络端点，更新发生变化的网络端点&lt;/li&gt;
&lt;li&gt;迭代每一步中被修改的 EndpointSlice ，添加所有需要的新的网络端点&lt;/li&gt;
&lt;li&gt;如果还有新的网络端点需要添加，尝试添加到之间没变更的 EndpointSlice 或者(同时)创建一个新的。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;重要的是， 在第三步的优先级会限制 EndpointSlice 更新到一个完美分布。 例如，如果有 10 个新增的
网络端点要添加到两个 EndpointSlice，并且这两个 EndpointSlice 都还有 5 个空位，这种方式会
创建一个新的 EndpointSlice， 而不是装满这两个已经存在的 EndpointSlice。 换句话说，创建一个新的
EndpointSlice 优先于更新两个 EndpointSlice。&lt;/p&gt;
&lt;p&gt;当 kube-proxy 运行在每个节点上并监听 EndpointSlice， 每一次对 EndpointSlice 修改都会变得
相对来多代价比较高的，因为每个更新都会传达到信念中的每一个节点上。这种方式旨在限制发送到每个节点
的变更次数，尽管它会导致产生多个没有被装满的 EndpointSlice&lt;/p&gt;
&lt;p&gt;在实践中，这种不太理想分配应该是比较少见的。 大多数由 EndpointSlice 处理的变量应该都足够小到适应
已经存在的 EndpointSlice， 如果没，也就是创建一个很快就也必须要创建的新的 EndpointSlice。
Deployment 的滚动更新也提供了一个自然的  EndpointSlice 重新打包，因为所有的 Pod 和它们对
应的网络端点也是被替换。&lt;/p&gt;
&lt;!--
### Duplicate endpoints

Due to the nature of EndpointSlice changes, endpoints may be represented in more
than one EndpointSlice at the same time. This naturally occurs as changes to
different EndpointSlice objects can arrive at the Kubernetes client watch/cache
at different times. Implementations using EndpointSlice must be able to have the
endpoint appear in more than one slice. A reference implementation of how to
perform endpoint deduplication can be found in the `EndpointSliceCache`
implementation in `kube-proxy`.
 --&gt;
&lt;h3 id=&#34;重复的网络端点&#34;&gt;重复的网络端点&lt;/h3&gt;
&lt;p&gt;由于 EndpointSlice 的自然变更，网络端点可能同时存在于不同的 EndpointSlice 中。 这些自然地
发生在对不同 EndpointSlice 对象的变更，可能在不同的时间到到 k8s 客户端的监听或缓存。使用
EndpointSlice 的实现必须要能处理这种同一个网络端点出现在多个 EndpointSlice 的情况。
怎么处理重复网络端点的实现参考可以在 &lt;code&gt;kube-proxy&lt;/code&gt; 中的 &lt;code&gt;EndpointSliceCache&lt;/code&gt; 实现中找到。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/enabling-endpointslices&#34;&gt;开启 EndpointSlices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/connect-applications-service/&#34;&gt;通过 Service 连接应用&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ingress</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/ingress/</link>
      <pubDate>Mon, 21 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/ingress/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- bprashanth
title: Ingress
content_type: concept
weight: 40
--- --&gt;
&lt;!-- overview --&gt;
&lt;p&gt;




&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;一个用于管理外部访问集群内 Service 的 API 对象，通常是 HTTP。&lt;/p&gt;
&lt;p&gt;Ingress 还可以提供负载均衡，SSL 和基于名称的虚拟主机。&lt;/p&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Terminology

For clarity, this guide defines the following terms:

* Node: A worker machine in Kubernetes, part of a cluster.
* Cluster: A set of Nodes that run containerized applications managed by Kubernetes. For this example, and in most common Kubernetes deployments, nodes in the cluster are not part of the public internet.
* Edge router: A router that enforces the firewall policy for your cluster. This could be a gateway managed by a cloud provider or a physical piece of hardware.
* Cluster network: A set of links, logical or physical, that facilitate communication within a cluster according to the Kubernetes [networking model](/docs/concepts/cluster-administration/networking/).
* Service: A Kubernetes &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; that identifies a set of Pods using &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels&#39; target=&#39;_blank&#39;&gt;label&lt;span class=&#39;tooltip-text&#39;&gt;在对象上标上对用户有意义和有关联的属性&lt;/span&gt;
&lt;/a&gt; selectors. Unless mentioned otherwise, Services are assumed to have virtual IPs only routable within the cluster network.
 --&gt;
&lt;h2 id=&#34;术语&#34;&gt;术语&lt;/h2&gt;
&lt;p&gt;为了明确，本文定义了如下术语:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节点(Node): k8s 中的一个工作机，集群的一部分。&lt;/li&gt;
&lt;li&gt;集群(Cluster): 一组由运行由 k8s 管理的容器化应用的节点集合。在本例和大多数常用的 k8s 部署
中，集群中的节点是不在公网上的。&lt;/li&gt;
&lt;li&gt;边缘路由: 一个为集群执行防火墙策略的路由。 可以是由云提供商管理的网关或一个物理硬件。&lt;/li&gt;
&lt;li&gt;集群网络: 一组逻辑的或物理的连接，这些连接根据 k8s
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/cluster-administration/networking/&#34;&gt;网络模型&lt;/a&gt;简化集群内的通信&lt;/li&gt;
&lt;li&gt;Service: 一个 k8s &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 就是使用
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels&#39; target=&#39;_blank&#39;&gt;标签&lt;span class=&#39;tooltip-text&#39;&gt;在对象上标上对用户有意义和有关联的属性&lt;/span&gt;
&lt;/a&gt; 选择器区分一组 Pod 的抽象概念。 如果没有特别
说明， Service 假定是有一个只能在集群内路由的虚拟 IP 的。&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## What is Ingress?

[Ingress](/docs/reference/generated/kubernetes-api/v1.19/#ingress-v1-networking-k8s-io) exposes HTTP and HTTPS routes from outside the cluster to
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service/&#34; target=&#34;_blank&#34;&gt;services&lt;/a&gt; within the cluster.
Traffic routing is controlled by rules defined on the Ingress resource.

```none
    internet
        |
   [ Ingress ]
   --|-----|--
   [ Services ]
```

An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. An [Ingress controller](/docs/concepts/services-networking/ingress-controllers) is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic.

An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically
uses a service of type [Service.Type=NodePort](/docs/concepts/services-networking/service/#nodeport) or
[Service.Type=LoadBalancer](/docs/concepts/services-networking/service/#loadbalancer).
 --&gt;
&lt;h2 id=&#34;ingress-是什么&#34;&gt;Ingress 是什么?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#ingress-v1-networking-k8s-io&#34;&gt;Ingress&lt;/a&gt;
将集群内部的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#34; target=&#34;_blank&#34;&gt;Service&lt;/a&gt;
通过 HTTP 和 HTTPS 路由暴露到集群外部。
流量路由由 Ingress 资源内部定义的规则控制。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-none&#34; data-lang=&#34;none&#34;&gt;    internet
        |
   [ Ingress ]
   --|-----|--
   [ Services ]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以配置一个 Ingress 为 Service 提供外部可以访问的 URL, 负载均衡，SSL / TLS, 提供基于名称的
虚拟主机名。
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/ingress-controllers&#34;&gt;Ingress 控制器&lt;/a&gt;将负责
实现 Ingress， 通常是通过负载均衡器，也可能是通过配置边缘路由或额外的前端组件来帮助处理流量。&lt;/p&gt;
&lt;p&gt;Ingress 不是暴露任意的端口或协议。 要暴露非 HTTP / HTTPS 的 Service 通常使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/#nodeport&#34;&gt;Service.Type=NodePort&lt;/a&gt;
或
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/#loadbalancer&#34;&gt;Service.Type=LoadBalancer&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Prerequisites

You must have an [Ingress controller](/docs/concepts/services-networking/ingress-controllers) to satisfy an Ingress. Only creating an Ingress resource has no effect.

You may need to deploy an Ingress controller such as [ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/). You can choose from a number of
[Ingress controllers](/docs/concepts/services-networking/ingress-controllers).

Ideally, all Ingress controllers should fit the reference specification. In reality, the various Ingress
controllers operate slightly differently.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Make sure you review your Ingress controller&amp;rsquo;s documentation to understand the caveats of choosing it.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;前置条件&#34;&gt;前置条件&lt;/h2&gt;
&lt;p&gt;要使用 Ingress 首先得有一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/ingress-controllers&#34;&gt;Ingress 控制器&lt;/a&gt;。
不然只创建一个 Ingress 资源是没有效果的。&lt;/p&gt;
&lt;p&gt;需要部署一个 Ingress 控制器，例如
&lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/deploy/&#34;&gt;ingress-nginx&lt;/a&gt;.
也可以从
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/ingress-controllers&#34;&gt;Ingress 控制器&lt;/a&gt;
中选几个.&lt;/p&gt;
&lt;p&gt;理想情况下，所有的 Ingress 控制器都应该是符合参考规格说明的。 实际上，不同的 Ingress 运转方式
各自都有点不同。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 在选择 Ingress 控制器前请仔细阅读相关文档，确定理解了相关注意事项。&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;!--
## The Ingress resource

A minimal Ingress resource example:



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingminimal-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/minimal-ingress.yaml&#34; download=&#34;service/networking/minimal-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/minimal-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingminimal-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/minimal-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;minimal-ingress&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nginx.ingress.kubernetes.io/rewrite-target&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/testpath&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



As with all other Kubernetes resources, an Ingress needs `apiVersion`, `kind`, and `metadata` fields.
The name of an Ingress object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
For general information about working with config files, see [deploying applications](/docs/tasks/run-application/run-stateless-application-deployment/), [configuring containers](/docs/tasks/configure-pod-container/configure-pod-configmap/), [managing resources](/docs/concepts/cluster-administration/manage-deployment/).
 Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of which
 is the [rewrite-target annotation](https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/rewrite/README.md).
Different [Ingress controller](/docs/concepts/services-networking/ingress-controllers) support different annotations. Review the documentation for
 your choice of Ingress controller to learn which annotations are supported.

The Ingress [spec](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status)
has all the information needed to configure a load balancer or proxy server. Most importantly, it
contains a list of rules matched against all incoming requests. Ingress resource only supports rules
for directing HTTP(S) traffic.
 --&gt;
&lt;h2 id=&#34;ingress-资源&#34;&gt;Ingress 资源&lt;/h2&gt;
&lt;p&gt;一个最小化 Ingress 资源的示例:


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingminimal-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/minimal-ingress.yaml&#34; download=&#34;service/networking/minimal-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/minimal-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingminimal-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/minimal-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;minimal-ingress&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nginx.ingress.kubernetes.io/rewrite-target&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/testpath&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

&lt;/p&gt;
&lt;p&gt;与所有其它的 k8s 资源一样， 一个 Ingress 的必要字段有 &lt;code&gt;apiVersion&lt;/code&gt;, &lt;code&gt;kind&lt;/code&gt;, &lt;code&gt;metadata&lt;/code&gt;。
Ingress 对象的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.
关于配置文件的通用信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/run-stateless-application-deployment/&#34;&gt;部署应用&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-pod-configmap/&#34;&gt;配置容器&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/cluster-administration/manage-deployment/&#34;&gt;管理资源&lt;/a&gt;.
Ingress 经常使用注解(annotation) 来配置一些基于 Ingress 控制器的可选配置，这里有一个例子
&lt;a href=&#34;https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/rewrite/README.md&#34;&gt;rewrite-target 注解&lt;/a&gt;.
不同的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/ingress-controllers&#34;&gt;Ingress 控制器&lt;/a&gt;
会支持不同的注解。 查看对应的控制文档了解其支持的注解(annotation)。&lt;/p&gt;
&lt;p&gt;Ingress &lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;spec&lt;/a&gt;
中包含了配置负载均衡器或代理服务器的所有信息。 最重要的是它包含了一个匹配进入请求的规则列表。
Ingress 资源只支持对 HTTP(S) 流量的转发。&lt;/p&gt;
&lt;!--
### Ingress rules

Each HTTP rule contains the following information:

* An optional host. In this example, no host is specified, so the rule applies to all inbound
  HTTP traffic through the IP address specified. If a host is provided (for example,
  foo.bar.com), the rules apply to that host.
* A list of paths (for example, `/testpath`), each of which has an associated
  backend defined with a `service.name` and a `service.port.name` or
  `service.port.number`. Both the host and path must match the content of an
  incoming request before the load balancer directs traffic to the referenced
  Service.
* A backend is a combination of Service and port names as described in the
  [Service doc](/docs/concepts/services-networking/service/) or a [custom resource backend](#resource-backend) by way of a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/&#39; target=&#39;_blank&#39;&gt;CRD&lt;span class=&#39;tooltip-text&#39;&gt;Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server.&lt;/span&gt;
&lt;/a&gt;. HTTP (and HTTPS) requests to the
  Ingress that matches the host and path of the rule are sent to the listed backend.

A `defaultBackend` is often configured in an Ingress controller to service any requests that do not
match a path in the spec.
 --&gt;
&lt;h3 id=&#34;ingress-rules&#34;&gt;Ingress 规则&lt;/h3&gt;
&lt;p&gt;每个 HTTP 规则包含以下信息:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个可选的主机名。 在这个例子中就没有配，所以这个规则应用于所有通过指定 IP 的流量。如果主机名
有设置(例如，foo.bar.com), 这个规则就应用于这个主机名。&lt;/li&gt;
&lt;li&gt;一个路径列表(如例子中的 &lt;code&gt;/testpath&lt;/code&gt;)， 每个路径都与一个后台通过一个 &lt;code&gt;service.name&lt;/code&gt; 和
一个 &lt;code&gt;service.port.name&lt;/code&gt; 或 &lt;code&gt;service.port.number&lt;/code&gt; 相关联。 必须要主机名和路径都匹配的
进入请求才会重定向到相应的 Service&lt;/li&gt;
&lt;li&gt;一个后端就是一个 Service 和 端口名称的组合，就如
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service/&#34;&gt;Service&lt;/a&gt;
或
&lt;a href=&#34;#resource-backend&#34;&gt;自定义资源后端&lt;/a&gt;通过
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/&#39; target=&#39;_blank&#39;&gt;CRD&lt;span class=&#39;tooltip-text&#39;&gt;Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server.&lt;/span&gt;
&lt;/a&gt;
方式中所描述的一样。
到达 Ingress 的 HTTP (和 HTTPS) 请求匹配到对应规则的主机名和路径就会发送到列举的后端。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个 &lt;code&gt;defaultBackend&lt;/code&gt; 通常是配置在一个 Ingress 控制器中，用于接收所有没有匹配到任何配置中的
路径的请求。&lt;/p&gt;
&lt;!--
### DefaultBackend {#default-backend}

An Ingress with no rules sends all traffic to a single default backend. The `defaultBackend` is conventionally a configuration option
of the [Ingress controller](/docs/concepts/services-networking/ingress-controllers) and is not specified in your Ingress resources.

If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is
routed to your default backend.
 --&gt;
&lt;h3 id=&#34;default-backend&#34;&gt;默认后端(DefaultBackend)&lt;/h3&gt;
&lt;p&gt;一个没有任何规则的 Ingress 会将所有的流量发送到一个默认的后端。 &lt;code&gt;defaultBackend&lt;/code&gt; 是一个很方便
的配置在  &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/ingress-controllers&#34;&gt;Ingress 控制器&lt;/a&gt;
并不需要在 Ingress 中配置。&lt;/p&gt;
&lt;p&gt;如果没有一个 Ingress 中的主机名或路径匹配到的 HTTP 请求，这些请求就会路由到默认后端&lt;/p&gt;
&lt;!--
### Resource backends {#resource-backend}

A `Resource` backend is an ObjectRef to another Kubernetes resource within the
same namespace as the Ingress object. A `Resource` is a mutually exclusive
setting with Service, and will fail validation if both are specified. A common
usage for a `Resource` backend is to ingress data to an object storage backend
with static assets.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingingress-resource-backendyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/ingress-resource-backend.yaml&#34; download=&#34;service/networking/ingress-resource-backend.yaml&#34;&gt;
                    &lt;code&gt;service/networking/ingress-resource-backend.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingingress-resource-backendyaml&#39;)&#34; title=&#34;Copy service/networking/ingress-resource-backend.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ingress-resource-backend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;defaultBackend&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;resource&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;apiGroup&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.example.com&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageBucket&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;static-assets&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/icons&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ImplementationSpecific&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;resource&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;apiGroup&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.example.com&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageBucket&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;icon-assets&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



After creating the Ingress above, you can view it with the following command:

```bash
kubectl describe ingress ingress-resource-backend
```

```
Name:             ingress-resource-backend
Namespace:        default
Address:
Default backend:  APIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assets
Rules:
  Host        Path  Backends
  ----        ----  --------
  *
              /icons   APIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assets
Annotations:  &lt;none&gt;
Events:       &lt;none&gt;
```
 --&gt;
&lt;h3 id=&#34;resource-backend&#34;&gt;资源(Resource) 后端&lt;/h3&gt;
&lt;p&gt;一个 &lt;code&gt;Resource&lt;/code&gt; 后端是一个与 Ingress 对象在同一个命名空间的另一个 k8s 资源的引用。
&lt;code&gt;Resource&lt;/code&gt; 与 Service 是互斥的，如果同时配置了两者，则不会通过验证。 &lt;code&gt;Resource&lt;/code&gt; 的一个常见
应用场景是进入一个存放静态资源的对象存储后端。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingingress-resource-backendyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/ingress-resource-backend.yaml&#34; download=&#34;service/networking/ingress-resource-backend.yaml&#34;&gt;
                    &lt;code&gt;service/networking/ingress-resource-backend.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingingress-resource-backendyaml&#39;)&#34; title=&#34;Copy service/networking/ingress-resource-backend.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ingress-resource-backend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;defaultBackend&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;resource&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;apiGroup&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.example.com&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageBucket&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;static-assets&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/icons&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ImplementationSpecific&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;resource&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;apiGroup&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.example.com&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageBucket&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;icon-assets&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;在创建以上定义的 Ingress 后，可以通过以下命令查看:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl describe ingress ingress-resource-backend
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Name:             ingress-resource-backend
Namespace:        default
Address:
Default backend:  APIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assets
Rules:
  Host        Path  Backends
  ----        ----  --------
  *
              /icons   APIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assets
Annotations:  &amp;lt;none&amp;gt;
Events:       &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;!--
### Path types

Each path in an Ingress is required to have a corresponding path type. Paths
that do not include an explicit `pathType` will fail validation. There are three
supported path types:

* `ImplementationSpecific`: With this path type, matching is up to the
  IngressClass. Implementations can treat this as a separate `pathType` or treat
  it identically to `Prefix` or `Exact` path types.

* `Exact`: Matches the URL path exactly and with case sensitivity.

* `Prefix`: Matches based on a URL path prefix split by `/`. Matching is case
  sensitive and done on a path element by element basis. A path element refers
  to the list of labels in the path split by the `/` separator. A request is a
  match for path _p_ if every _p_ is an element-wise prefix of _p_ of the
  request path.

  &lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If the last element of the path is a substring of the last
element in request path, it is not a match (for example: &lt;code&gt;/foo/bar&lt;/code&gt;
matches&lt;code&gt;/foo/bar/baz&lt;/code&gt;, but does not match &lt;code&gt;/foo/barbaz&lt;/code&gt;).&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;路径path类型&#34;&gt;路径(Path)类型&lt;/h3&gt;
&lt;p&gt;Ingress 中的每一个路径都需要有一个相应的路径类型。 没有显示设置 &lt;code&gt;pathType&lt;/code&gt; 是没办法通过验证的。
有以下三种支持的路径类型:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;ImplementationSpecific&lt;/code&gt;: 这个路径类型的匹配是由 IngressClass 决定。 具体实现可以将其认为是
独立的 &lt;code&gt;pathType&lt;/code&gt; 或认为是 &lt;code&gt;Prefix&lt;/code&gt; 或 &lt;code&gt;Exact&lt;/code&gt; 路径类型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Exact&lt;/code&gt;:  完全匹配路径，区分大小写。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Prefix&lt;/code&gt;: 基于由 &lt;code&gt;/&lt;/code&gt; 分割的 URL 路径前缀匹配。匹配区分大小写，并且逐个元素匹配。
一个元素就是通过 &lt;code&gt;/&lt;/code&gt; 分割后列表中的每一个元素。 一个请求与路径 &lt;em&gt;p&lt;/em&gt; 相匹配则请求中的元素可以
依次匹配完路径 &lt;em&gt;p&lt;/em&gt; 的每一个元素。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果路径的最后一个元素是请求路径的最后一个元素的子串，则不能匹配(例如：&lt;code&gt;/foo/bar&lt;/code&gt; 可以匹配
请求路径 &lt;code&gt;/foo/bar/baz&lt;/code&gt;，但是不能匹配请求路径 &lt;code&gt;/foo/barbaz&lt;/code&gt; )。&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;!--
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Kind&lt;/th&gt;
&lt;th&gt;Path(s)&lt;/th&gt;
&lt;th&gt;Request path(s)&lt;/th&gt;
&lt;th&gt;Matches?&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;(all paths)&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exact&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exact&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/bar&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exact&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exact&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;, &lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;, &lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes, ignores trailing slash&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes,  matches trailing slash&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb/ccc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes, matches subpath&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbbxyz&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;No, does not match string prefix&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;, &lt;code&gt;/aaa&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/ccc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes, matches &lt;code&gt;/aaa&lt;/code&gt; prefix&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;, &lt;code&gt;/aaa&lt;/code&gt;, &lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes, matches &lt;code&gt;/aaa/bbb&lt;/code&gt; prefix&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;, &lt;code&gt;/aaa&lt;/code&gt;, &lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/ccc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes, matches &lt;code&gt;/&lt;/code&gt; prefix&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/ccc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;No, uses default backend&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mixed&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt; (Prefix), &lt;code&gt;/foo&lt;/code&gt; (Exact)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes, prefers Exact&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;ndash;&amp;gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;示例&#34;&gt;示例&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;类型&lt;/th&gt;
&lt;th&gt;路径&lt;/th&gt;
&lt;th&gt;请求路径&lt;/th&gt;
&lt;th&gt;匹配结果&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;(all paths)&lt;/td&gt;
&lt;td&gt;匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exact&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exact&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/bar&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;不匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exact&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;不匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exact&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;不匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;, &lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;, &lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;不匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配, 忽略尾部的斜线&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配, 匹配尾部的斜线&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb/ccc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配, 匹配前缀&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbbxyz&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;不匹配, 不是匹配字符串前缀&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;, &lt;code&gt;/aaa&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/ccc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配, 匹配 &lt;code&gt;/aaa&lt;/code&gt; 前缀&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;, &lt;code&gt;/aaa&lt;/code&gt;, &lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配, 匹配 &lt;code&gt;/aaa/bbb&lt;/code&gt; 前缀&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;, &lt;code&gt;/aaa&lt;/code&gt;, &lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/ccc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配, 匹配 &lt;code&gt;/&lt;/code&gt; 前缀&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/ccc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;不匹配, 使用默认后端&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mixed&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt; (Prefix), &lt;code&gt;/foo&lt;/code&gt; (Exact)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配, 优先完全匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
#### Multiple matches
In some cases, multiple paths within an Ingress will match a request. In those
cases precedence will be given first to the longest matching path. If two paths
are still equally matched, precedence will be given to paths with an exact path
type over prefix path type.
 --&gt;
&lt;h4 id=&#34;多匹配&#34;&gt;多匹配&lt;/h4&gt;
&lt;p&gt;在某些情况下，一个 Ingress 中的多个路径都能与请求路径相匹配。在这种情况下优先级最高的是最长匹配
路径。 如果还有两个路径匹配长度匹配相同，则完全匹配优先级高于前端匹配。&lt;/p&gt;
&lt;!--
## Hostname wildcards
Hosts can be precise matches (for example “`foo.bar.com`”) or a wildcard (for
example “`*.foo.com`”). Precise matches require that the HTTP `host` header
matches the `host` field. Wildcard matches require the HTTP `host` header is
equal to the suffix of the wildcard rule.

| Host        | Host header       | Match?                                            |
| ----------- |-------------------| --------------------------------------------------|
| `*.foo.com` | `bar.foo.com`     | Matches based on shared suffix                    |
| `*.foo.com` | `baz.bar.foo.com` | No match, wildcard only covers a single DNS label |
| `*.foo.com` | `foo.com`         | No match, wildcard only covers a single DNS label |



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingingress-wildcard-hostyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/ingress-wildcard-host.yaml&#34; download=&#34;service/networking/ingress-wildcard-host.yaml&#34;&gt;
                    &lt;code&gt;service/networking/ingress-wildcard-host.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingingress-wildcard-hostyaml&#39;)&#34; title=&#34;Copy service/networking/ingress-wildcard-host.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ingress-wildcard-host&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;foo.bar.com&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/bar&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*.foo.com&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/foo&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


 --&gt;
&lt;h2 id=&#34;主机名通配符&#34;&gt;主机名通配符&lt;/h2&gt;
&lt;p&gt;主机名可以是精确匹配(例如 &lt;code&gt;foo.bar.com&lt;/code&gt;)，也可以有通配符(例如 &lt;code&gt;*.foo.com&lt;/code&gt;)。
精确匹配必须要 HTTP 请求头中的 &lt;code&gt;host&lt;/code&gt; 与规则的 &lt;code&gt;host&lt;/code&gt; 字段完全匹配。
通配符则要 HTTP 请求头中的 &lt;code&gt;host&lt;/code&gt; 与通配符规则的后缀相同。&lt;/p&gt;
&lt;p&gt;例如:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;规则&lt;/th&gt;
&lt;th&gt;请求头&lt;code&gt;host&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;匹配结果&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;*.foo.com&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bar.foo.com&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配，因为后缀相同&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;*.foo.com&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;baz.bar.foo.com&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;不匹配, 通配符只能包含一级子域名&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;*.foo.com&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;foo.com&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;不匹配, 通配符只能包含一级子域名&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingingress-wildcard-hostyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/ingress-wildcard-host.yaml&#34; download=&#34;service/networking/ingress-wildcard-host.yaml&#34;&gt;
                    &lt;code&gt;service/networking/ingress-wildcard-host.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingingress-wildcard-hostyaml&#39;)&#34; title=&#34;Copy service/networking/ingress-wildcard-host.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ingress-wildcard-host&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;foo.bar.com&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/bar&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*.foo.com&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/foo&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;!--
## Ingress class

Ingresses can be implemented by different controllers, often with different
configuration. Each Ingress should specify a class, a reference to an
IngressClass resource that contains additional configuration including the name
of the controller that should implement the class.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingexternal-lbyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/external-lb.yaml&#34; download=&#34;service/networking/external-lb.yaml&#34;&gt;
                    &lt;code&gt;service/networking/external-lb.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingexternal-lbyaml&#39;)&#34; title=&#34;Copy service/networking/external-lb.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IngressClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;external-lb&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;controller&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;example.com/ingress-controller&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;apiGroup&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.example.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IngressParameters&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;external-lb&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



IngressClass resources contain an optional parameters field. This can be used to
reference additional configuration for this class.
 --&gt;
&lt;h2 id=&#34;ingressclass&#34;&gt;IngressClass&lt;/h2&gt;
&lt;p&gt;Ingress 可以由不同的控制器实现，通常也会有不同的配置。 每个 Ingress 都需要指定一个类型，
它是一个 IngressClass 资源的引用， 其中包含如实现该类型的控制器的名称等额外配置。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingexternal-lbyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/external-lb.yaml&#34; download=&#34;service/networking/external-lb.yaml&#34;&gt;
                    &lt;code&gt;service/networking/external-lb.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingexternal-lbyaml&#39;)&#34; title=&#34;Copy service/networking/external-lb.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IngressClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;external-lb&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;controller&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;example.com/ingress-controller&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;apiGroup&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.example.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IngressParameters&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;external-lb&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;IngressClass 资源中包含一个可选参数字段。 可以用设置该类型额外配置的引用&lt;/p&gt;
&lt;!--

### Deprecated annotation

Before the IngressClass resource and `ingressClassName` field were added in
Kubernetes 1.18, Ingress classes were specified with a
`kubernetes.io/ingress.class` annotation on the Ingress. This annotation was
never formally defined, but was widely supported by Ingress controllers.

The newer `ingressClassName` field on Ingresses is a replacement for that
annotation, but is not a direct equivalent. While the annotation was generally
used to reference the name of the Ingress controller that should implement the
Ingress, the field is a reference to an IngressClass resource that contains
additional Ingress configuration, including the name of the Ingress controller.
 --&gt;
&lt;h3 id=&#34;废弃的注解&#34;&gt;废弃的注解&lt;/h3&gt;
&lt;p&gt;在 k8s 1.18 版本中添加 IngressClass 资源和 &lt;code&gt;ingressClassName&lt;/code&gt; 资源前，Ingress 的类型是
通过其上的 &lt;code&gt;kubernetes.io/ingress.class&lt;/code&gt; 注解指定的。 这个注解从来没有正式的定义过，但它
在 Ingress 控制器中被广泛支持。&lt;/p&gt;
&lt;p&gt;Ingress 中的新字段 &lt;code&gt;ingressClassName&lt;/code&gt; 就是对这个注解的替代，但它们也不是直接等价的。
其中注解通常是用来指定实现该 Ingress 的控制器的名称， 新字段则是指定一个 IngressClass 资源的引用。
这个 IngressClass 资源中包含一额外的 Ingress 配置参数和 Ingress 控制器的名称。&lt;/p&gt;
&lt;!--
### Default IngressClass {#default-ingress-class}

You can mark a particular IngressClass as default for your cluster. Setting the
`ingressclass.kubernetes.io/is-default-class` annotation to `true` on an
IngressClass resource will ensure that new Ingresses without an
`ingressClassName` field specified will be assigned this default IngressClass.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; If you have more than one IngressClass marked as the default for your cluster,
the admission controller prevents creating new Ingress objects that don&amp;rsquo;t have
an &lt;code&gt;ingressClassName&lt;/code&gt; specified. You can resolve this by ensuring that at most 1
IngressClass is marked as default in your cluster.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;default-ingress-class&#34;&gt;默认 IngressClass&lt;/h3&gt;
&lt;p&gt;用户可以将某个 IngressClass 设置为集群的默认 IngressClass。
在 IngressClass 资源上设置 &lt;code&gt;ingressclass.kubernetes.io/is-default-class&lt;/code&gt; 注解值为 &lt;code&gt;true&lt;/code&gt;。
这样新创建 Ingress 如果没有设置 &lt;code&gt;ingressClassName&lt;/code&gt; 字段则会分配这个默认的 IngressClass。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 如果集群中有不止一个 IngressClass 被标记为默认，准入控制器(admission controller) 就会禁止
创建没有设置 &lt;code&gt;ingressClassName&lt;/code&gt; 的 Ingress。 需要用户手动解决，确保集群中只有一个默认
IngressClass。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Types of Ingress

### Ingress backed by a single Service {#single-service-ingress}

There are existing Kubernetes concepts that allow you to expose a single Service
(see [alternatives](#alternatives)). You can also do this with an Ingress by specifying a
*default backend* with no rules.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingtest-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/test-ingress.yaml&#34; download=&#34;service/networking/test-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/test-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingtest-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/test-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;defaultBackend&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



If you create it using `kubectl apply -f` you should be able to view the state
of the Ingress you just added:

```bash
kubectl get ingress test-ingress
```

```
NAME           CLASS         HOSTS   ADDRESS         PORTS   AGE
test-ingress   external-lb   *       203.0.113.123   80      59s
```

Where `203.0.113.123` is the IP allocated by the Ingress controller to satisfy
this Ingress.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Ingress controllers and load balancers may take a minute or two to allocate an IP address.
Until that time, you often see the address listed as &lt;code&gt;&amp;lt;pending&amp;gt;&lt;/code&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;ingress-的类型&#34;&gt;Ingress 的类型&lt;/h2&gt;
&lt;h3 id=&#34;single-service-ingress&#34;&gt;后端只有一个 Service 的 Ingress &lt;/h3&gt;
&lt;p&gt;k8s 存在概念可以让用户暴露单个 Service (见 &lt;a href=&#34;#alternatives&#34;&gt;替代方案&lt;/a&gt;).
也可以通过一个指定 &lt;em&gt;默认后端&lt;/em&gt; 但没有配置规则的 Ingress 达到同样的效果。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingtest-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/test-ingress.yaml&#34; download=&#34;service/networking/test-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/test-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingtest-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/test-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;defaultBackend&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;通过 &lt;code&gt;kubectl apply -f&lt;/code&gt; 创建 Ingress，并通过以下命令查看:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get ingress test-ingress
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME           CLASS         HOSTS   ADDRESS         PORTS   AGE
test-ingress   external-lb   *       203.0.113.123   80      59s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中 &lt;code&gt;203.0.113.123&lt;/code&gt; 是由 Ingress 控制器分配来满足这个 Ingress 的 IP 地址。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Ingress 控制器和负载均衡器可能需要一两分钟为分配一个 IP 地址。在这之前，通常看到地址列的值为 &lt;code&gt;&amp;lt;pending&amp;gt;&lt;/code&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Simple fanout

A fanout configuration routes traffic from a single IP address to more than one Service,
based on the HTTP URI being requested. An Ingress allows you to keep the number of load balancers
down to a minimum. For example, a setup like:

```
foo.bar.com -&gt; 178.91.123.132 -&gt; / foo    service1:4200
                                 / bar    service2:8080
```

would require an Ingress such as:



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingsimple-fanout-exampleyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/simple-fanout-example.yaml&#34; download=&#34;service/networking/simple-fanout-example.yaml&#34;&gt;
                    &lt;code&gt;service/networking/simple-fanout-example.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingsimple-fanout-exampleyaml&#39;)&#34; title=&#34;Copy service/networking/simple-fanout-example.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;simple-fanout-example&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/foo&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4200&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/bar&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



When you create the Ingress with `kubectl apply -f`:

```shell
kubectl describe ingress simple-fanout-example
```

```
Name:             simple-fanout-example
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:4200 (10.8.0.90:4200)
               /bar   service2:8080 (10.8.0.91:8080)
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     22s                loadbalancer-controller  default/test
```

The Ingress controller provisions an implementation-specific load balancer
that satisfies the Ingress, as long as the Services (`service1`, `service2`) exist.
When it has done so, you can see the address of the load balancer at the
Address field.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Depending on the &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/ingress-controllers/&#34;&gt;Ingress controller&lt;/a&gt;
you are using, you may need to create a default-http-backend
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service/&#34;&gt;Service&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;简单分散fanout&#34;&gt;简单分散(fanout)&lt;/h3&gt;
&lt;p&gt;一个分散就是基于请求的 HTTP URI 配置路由流量从一单个 IP 地址到多个 Service。
一个 Ingress 可以使负载均衡器的数量减少到最低。 例如如下配置:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;foo.bar.com -&amp;gt; 178.91.123.132 -&amp;gt; / foo    service1:4200
                                 / bar    service2:8080
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;需要要一个如下 Ingress:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingsimple-fanout-exampleyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/simple-fanout-example.yaml&#34; download=&#34;service/networking/simple-fanout-example.yaml&#34;&gt;
                    &lt;code&gt;service/networking/simple-fanout-example.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingsimple-fanout-exampleyaml&#39;)&#34; title=&#34;Copy service/networking/simple-fanout-example.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;simple-fanout-example&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/foo&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4200&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/bar&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;在使用 &lt;code&gt;kubectl apply -f&lt;/code&gt; 命令创建 Ingress 后，通过以下命令查看详情:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe ingress simple-fanout-example
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Name:             simple-fanout-example
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:4200 (10.8.0.90:4200)
               /bar   service2:8080 (10.8.0.91:8080)
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     22s                loadbalancer-controller  default/test
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Ingress 控制器会在目标 Service (&lt;code&gt;service1&lt;/code&gt;, &lt;code&gt;service2&lt;/code&gt;) 还存在时，
根据本身实现提供一个满足 Ingress 的负载均衡器直到。
当创建好后，可以在地址字段看到负载均衡器的地址。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 基于使用的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/ingress-controllers/&#34;&gt;Ingress 控制器&lt;/a&gt;
可能需要创建一个 default-http-backend
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#34;&gt;Service&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Name based virtual hosting

Name-based virtual hosts support routing HTTP traffic to multiple host names at the same IP address.

```none
foo.bar.com --|                 |-&gt; foo.bar.com service1:80
              | 178.91.123.132  |
bar.foo.com --|                 |-&gt; bar.foo.com service2:80
```

The following Ingress tells the backing load balancer to route requests based on
the [Host header](https://tools.ietf.org/html/rfc7230#section-5.4).



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingname-virtual-host-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/name-virtual-host-ingress.yaml&#34; download=&#34;service/networking/name-virtual-host-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/name-virtual-host-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingname-virtual-host-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/name-virtual-host-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;name-virtual-host-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar.foo.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



If you create an Ingress resource without any hosts defined in the rules, then any
web traffic to the IP address of your Ingress controller can be matched without a name based
virtual host being required.

For example, the following Ingress routes traffic
requested for `first.bar.com` to `service1`, `second.foo.com` to `service2`, and any traffic
to the IP address without a hostname defined in request (that is, without a request header being
presented) to `service3`.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingname-virtual-host-ingress-no-third-hostyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/name-virtual-host-ingress-no-third-host.yaml&#34; download=&#34;service/networking/name-virtual-host-ingress-no-third-host.yaml&#34;&gt;
                    &lt;code&gt;service/networking/name-virtual-host-ingress-no-third-host.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingname-virtual-host-ingress-no-third-hostyaml&#39;)&#34; title=&#34;Copy service/networking/name-virtual-host-ingress-no-third-host.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;name-virtual-host-ingress-no-third-host&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;first.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;second.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service3&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


 --&gt;
&lt;h3 id=&#34;基于名称的虚拟主机&#34;&gt;基于名称的虚拟主机&lt;/h3&gt;
&lt;p&gt;基于名称的虚拟主机支持在同一个IP上将 HTTP 流量路由到多个主机名上。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-none&#34; data-lang=&#34;none&#34;&gt;foo.bar.com --|                 |-&amp;gt; foo.bar.com service1:80
              | 178.91.123.132  |
bar.foo.com --|                 |-&amp;gt; bar.foo.com service2:80
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The following Ingress tells the backing load balancer to route requests based on
the &lt;a href=&#34;https://tools.ietf.org/html/rfc7230#section-5.4&#34;&gt;Host header&lt;/a&gt;.
以下 Ingress 告诉后端的负载均衡器基于
&lt;a href=&#34;https://tools.ietf.org/html/rfc7230#section-5.4&#34;&gt;Host 头字段&lt;/a&gt;
路由请求。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingname-virtual-host-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/name-virtual-host-ingress.yaml&#34; download=&#34;service/networking/name-virtual-host-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/name-virtual-host-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingname-virtual-host-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/name-virtual-host-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;name-virtual-host-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar.foo.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;If you create an Ingress resource without any hosts defined in the rules, then any
web traffic to the IP address of your Ingress controller can be matched without a name based
virtual host being required.
如果创建的 Ingress 资源中没有定义任何主机规则，则所有到达这个 Ingress 控制 IP 的的 web 流量
则不需要基于名称的虚拟主机就能匹配。&lt;/p&gt;
&lt;p&gt;例如， 以下 Ingress 路由 &lt;code&gt;first.bar.com&lt;/code&gt; 的请求到 &lt;code&gt;service1&lt;/code&gt;，
&lt;code&gt;second.foo.com&lt;/code&gt; 的请求到 &lt;code&gt;service2&lt;/code&gt;，其它任意到达这个 IP 地址，但在请求中没有包含主机名
(也就是没的请求头(或请求头中没有 Host 字段))的请求到 &lt;code&gt;service3&lt;/code&gt;&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingname-virtual-host-ingress-no-third-hostyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/name-virtual-host-ingress-no-third-host.yaml&#34; download=&#34;service/networking/name-virtual-host-ingress-no-third-host.yaml&#34;&gt;
                    &lt;code&gt;service/networking/name-virtual-host-ingress-no-third-host.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingname-virtual-host-ingress-no-third-hostyaml&#39;)&#34; title=&#34;Copy service/networking/name-virtual-host-ingress-no-third-host.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;name-virtual-host-ingress-no-third-host&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;first.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;second.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service3&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;!--
### TLS

You can secure an Ingress by specifying a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.&lt;/span&gt;
&lt;/a&gt;
that contains a TLS private key and certificate. The Ingress resource only
supports a single TLS port, 443, and assumes TLS termination at the ingress point
(traffic to the Service and its Pods is in plaintext).
If the TLS configuration section in an Ingress specifies different hosts, they are
multiplexed on the same port according to the hostname specified through the
SNI TLS extension (provided the Ingress controller supports SNI). The TLS secret
must contain keys named `tls.crt` and `tls.key` that contain the certificate
and private key to use for TLS. For example:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: testsecret-tls
  namespace: default
data:
  tls.crt: base64 encoded cert
  tls.key: base64 encoded key
type: kubernetes.io/tls
```

Referencing this secret in an Ingress tells the Ingress controller to
secure the channel from the client to the load balancer using TLS. You need to make
sure the TLS secret you created came from a certificate that contains a Common
Name (CN), also known as a Fully Qualified Domain Name (FQDN) for `sslexample.foo.com`.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingtls-example-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/tls-example-ingress.yaml&#34; download=&#34;service/networking/tls-example-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/tls-example-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingtls-example-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/tls-example-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;tls-example-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;tls&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;hosts&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;https-example.foo.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;testsecret-tls&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https-example.foo.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; There is a gap between TLS features supported by various Ingress
controllers. Please refer to documentation on
&lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/tls/&#34;&gt;nginx&lt;/a&gt;,
&lt;a href=&#34;https://git.k8s.io/ingress-gce/README.md#frontend-https&#34;&gt;GCE&lt;/a&gt;, or any other
platform specific Ingress controller to understand how TLS works in your environment.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;tls&#34;&gt;TLS&lt;/h3&gt;
&lt;p&gt;用户可以能过一个包含 TLS 私钥和证书的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.&lt;/span&gt;
&lt;/a&gt; 为 Ingress
添加安全层。 Ingress 只支持一个 TLS 端口，&lt;code&gt;443&lt;/code&gt;， 并且假定 TLS 在 Ingress 终结(也就是
到达 Service 及其 Pod 的流量是明文的)。
如果 Ingress TLS 配置区中包含了多个主机名，那么它们根据 SNI TLS 扩展(需要 Ingress 控制器
支持 SNI)中指定的主机名来区分请求。 TLS &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.&lt;/span&gt;
&lt;/a&gt;
中必须要包含名叫 &lt;code&gt;tls.crt&lt;/code&gt; 和 &lt;code&gt;tls.key&lt;/code&gt; 的键，其它分别存放 TLS 的证书和私钥。 例如:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;testsecret-tls&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;tls.crt&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;base64 encoded cert&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;tls.key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;base64 encoded key&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/tls&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在一个 Ingress 中引用该 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.&lt;/span&gt;
&lt;/a&gt; 保证客户端到负载均衡器之间的
连接是使用 TLS。必须要保证创建 TLS Secret 的证书中包含公用名(CN), 也就是 &lt;code&gt;sslexample.foo.com&lt;/code&gt;
的全限定名(FQDN).&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingtls-example-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/tls-example-ingress.yaml&#34; download=&#34;service/networking/tls-example-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/tls-example-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingtls-example-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/tls-example-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;tls-example-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;tls&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;hosts&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;https-example.foo.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;testsecret-tls&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https-example.foo.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 不同的 Ingress 控制器对 TLS 特性的支持有较大差异。 请查阅
&lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/tls/&#34;&gt;nginx&lt;/a&gt;,
&lt;a href=&#34;https://git.k8s.io/ingress-gce/README.md#frontend-https&#34;&gt;GCE&lt;/a&gt;,
或其它平台相应的 Ingress 控制的说明文档，以理解在你的环境中 TLS 是怎么工作的。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Load balancing {#load-balancing}

An Ingress controller is bootstrapped with some load balancing policy settings
that it applies to all Ingress, such as the load balancing algorithm, backend
weight scheme, and others. More advanced load balancing concepts
(e.g. persistent sessions, dynamic weights) are not yet exposed through the
Ingress. You can instead get these features through the load balancer used for
a Service.

It&#39;s also worth noting that even though health checks are not exposed directly
through the Ingress, there exist parallel concepts in Kubernetes such as
[readiness probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)
that allow you to achieve the same end result. Please review the controller
specific documentation to see how they handle health checks (for example:
[nginx](https://git.k8s.io/ingress-nginx/README.md), or
[GCE](https://git.k8s.io/ingress-gce/README.md#health-checks)).
 --&gt;
&lt;h3 id=&#34;load-balancing&#34;&gt;负载均衡&lt;/h3&gt;
&lt;p&gt;Ingress 控制器天生带有一些会应用到所有 Ingress 负载均衡策略设置，如 负载均衡算法，后端权重，等。
更高级的负载均衡概念(如，持久会化，动态权重)目前还没能过 Ingress 实现。 要实现这些特性可以通过
Service 使用的负载均衡器。&lt;/p&gt;
&lt;p&gt;还有值得注意的是健康检查不是直接通过 Ingress 暴露的。 k8s 中存在如
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/&#34;&gt;存活探针&lt;/a&gt;
的平行概念，允许用户达成同样的结果。 请查阅控制的说明文档，看看是怎么处理健康检查的(例如
&lt;a href=&#34;https://git.k8s.io/ingress-nginx/README.md&#34;&gt;nginx&lt;/a&gt;, or
&lt;a href=&#34;https://git.k8s.io/ingress-gce/README.md#health-checks&#34;&gt;GCE&lt;/a&gt;).
)&lt;/p&gt;
&lt;!--
## Updating an Ingress

To update an existing Ingress to add a new Host, you can update it by editing the resource:

```shell
kubectl describe ingress test
```

```
Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     35s                loadbalancer-controller  default/test
```

```shell
kubectl edit ingress test
```

This pops up an editor with the existing configuration in YAML format.
Modify it to include the new Host:

```yaml
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - backend:
          service:
            name: service1
            port:
              number: 80
        path: /foo
        pathType: Prefix
  - host: bar.baz.com
    http:
      paths:
      - backend:
          service:
            name: service2
            port:
              number: 80
        path: /foo
        pathType: Prefix
..
```

After you save your changes, kubectl updates the resource in the API server, which tells the
Ingress controller to reconfigure the load balancer.

Verify this:

```shell
kubectl describe ingress test
```

```
Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
  bar.baz.com
               /foo   service2:80 (10.8.0.91:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     45s                loadbalancer-controller  default/test
```

You can achieve the same outcome by invoking `kubectl replace -f` on a modified Ingress YAML file.
 --&gt;
&lt;h2 id=&#34;更新-ingress&#34;&gt;更新 Ingress&lt;/h2&gt;
&lt;p&gt;要在已经存在的 Ingress 中添加一个新的主机规则，可以通过编辑 Ingress 资源实现:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe ingress test
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     35s                loadbalancer-controller  default/test
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl edit ingress test
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这条命令会用文本编辑器打开已经存在的 Ingress  YAML 格式的配置文件，在其中添加新的主机规则配置:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/foo&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar.baz.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/foo&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
&lt;span style=&#34;color:#ae81ff&#34;&gt;..&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在保存修改后， kubectl 就会在 API-server 中更新该资源，这会使得 Ingress 控制器重新配置
负载均衡器&lt;/p&gt;
&lt;p&gt;通过以下命令验证:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe ingress test
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
  bar.baz.com
               /foo   service2:80 (10.8.0.91:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     45s                loadbalancer-controller  default/test
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;也可以通过在外部修改 Ingress YAML 配置后，使用 &lt;code&gt;kubectl replace -f&lt;/code&gt; 命令可以达成相同的效果。&lt;/p&gt;
&lt;!--
## Failing across availability zones

Techniques for spreading traffic across failure domains differ between cloud providers.
Please check the documentation of the relevant [Ingress controller](/docs/concepts/services-networking/ingress-controllers) for details.
 --&gt;
&lt;h2 id=&#34;跨可用区失效&#34;&gt;跨可用区失效&lt;/h2&gt;
&lt;p&gt;在故障域之间传播流量的方法在各家云提供商上的方式都不一样。
请查看 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/ingress-controllers&#34;&gt;Ingress 控制器&lt;/a&gt;文档了解详细信息&lt;/p&gt;
&lt;!--
## Alternatives

You can expose a Service in multiple ways that don&#39;t directly involve the Ingress resource:

* Use [Service.Type=LoadBalancer](/docs/concepts/services-networking/service/#loadbalancer)
* Use [Service.Type=NodePort](/docs/concepts/services-networking/service/#nodeport)
 --&gt;
&lt;h2 id=&#34;alternatives&#34;&gt;替代方案&lt;/h2&gt;
&lt;p&gt;在不直接使用 Ingress 的情况下还有几种暴露 Service 的方法:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/#loadbalancer&#34;&gt;Service.Type=LoadBalancer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/#nodeport&#34;&gt;Service.Type=NodePort&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;查阅 &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#ingress-v1beta1-networking-k8s-io&#34;&gt;Ingress API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/ingress-controllers/&#34;&gt;Ingress 控制器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/access-application-cluster/ingress-minikube/&#34;&gt;使用 NGINX 控制器在 Minikube 上设置 Ingress&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ingress 控制器</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/ingress-controllers/</link>
      <pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/ingress-controllers/</guid>
      <description>
        
        
        &lt;!--
---
title: Ingress Controllers
reviewers:
content_type: concept
weight: 40
--- --&gt;
&lt;!-- overview --&gt;
&lt;!--
In order for the Ingress resource to work, the cluster must have an ingress controller running.

Unlike other types of controllers which run as part of the `kube-controller-manager` binary, Ingress controllers
are not started automatically with a cluster. Use this page to choose the ingress controller implementation
that best fits your cluster.

Kubernetes as a project currently supports and maintains [GCE](https://git.k8s.io/ingress-gce/README.md) and
  [nginx](https://git.k8s.io/ingress-nginx/README.md) controllers.
 --&gt;
&lt;p&gt;要使一个 Ingress 工作的前提是集群中必须要有一个 Ingress 控制器在运行。&lt;/p&gt;
&lt;p&gt;与其它类型的控制器作为 &lt;code&gt;kube-controller-manager&lt;/code&gt; 的一部分运行不同， Ingress 不会自动在集群中
运行。 使用本文选择最适合你的集群的 Ingress 控制器实现。&lt;/p&gt;
&lt;p&gt;k8s 项目目前支持和维护
&lt;a href=&#34;https://git.k8s.io/ingress-gce/README.md&#34;&gt;GCE&lt;/a&gt;
和
&lt;a href=&#34;https://git.k8s.io/ingress-nginx/README.md&#34;&gt;nginx&lt;/a&gt;
控制器&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Additional controllers

* [AKS Application Gateway Ingress Controller](https://github.com/Azure/application-gateway-kubernetes-ingress) is an ingress controller that enables ingress to [AKS clusters](https://docs.microsoft.com/azure/aks/kubernetes-walkthrough-portal) using the [Azure Application Gateway](https://docs.microsoft.com/azure/application-gateway/overview).
* [Ambassador](https://www.getambassador.io/) API Gateway is an [Envoy](https://www.envoyproxy.io) based ingress
  controller with [community](https://www.getambassador.io/docs) or
  [commercial](https://www.getambassador.io/pro/) support from [Datawire](https://www.datawire.io/).
* [AppsCode Inc.](https://appscode.com) offers support and maintenance for the most widely used [HAProxy](https://www.haproxy.org/) based ingress controller [Voyager](https://appscode.com/products/voyager).
* [AWS ALB Ingress Controller](https://github.com/kubernetes-sigs/aws-alb-ingress-controller) enables ingress using the [AWS Application Load Balancer](https://aws.amazon.com/elasticloadbalancing/).
* [Contour](https://projectcontour.io/) is an [Envoy](https://www.envoyproxy.io/) based ingress controller
  provided and supported by VMware.
* Citrix provides an [Ingress Controller](https://github.com/citrix/citrix-k8s-ingress-controller) for its hardware (MPX), virtualized (VPX) and [free containerized (CPX) ADC](https://www.citrix.com/products/citrix-adc/cpx-express.html) for [baremetal](https://github.com/citrix/citrix-k8s-ingress-controller/tree/master/deployment/baremetal) and [cloud](https://github.com/citrix/citrix-k8s-ingress-controller/tree/master/deployment) deployments.
* F5 Networks provides [support and maintenance](https://support.f5.com/csp/article/K86859508)
  for the [F5 BIG-IP Container Ingress Services for Kubernetes](https://clouddocs.f5.com/containers/latest/userguide/kubernetes/).
* [Gloo](https://gloo.solo.io) is an open-source ingress controller based on [Envoy](https://www.envoyproxy.io) which offers API Gateway functionality with enterprise support from [solo.io](https://www.solo.io).  
* [HAProxy Ingress](https://haproxy-ingress.github.io) is a highly customizable community-driven ingress controller for HAProxy.
* [HAProxy Technologies](https://www.haproxy.com/) offers support and maintenance for the [HAProxy Ingress Controller for Kubernetes](https://github.com/haproxytech/kubernetes-ingress). See the [official documentation](https://www.haproxy.com/documentation/hapee/1-9r1/traffic-management/kubernetes-ingress-controller/).
* [Istio](https://istio.io/) based ingress controller
  [Control Ingress Traffic](https://istio.io/docs/tasks/traffic-management/ingress/).
* [Kong](https://konghq.com/) offers [community](https://discuss.konghq.com/c/kubernetes) or
  [commercial](https://konghq.com/kong-enterprise/) support and maintenance for the
  [Kong Ingress Controller for Kubernetes](https://github.com/Kong/kubernetes-ingress-controller).
* [NGINX, Inc.](https://www.nginx.com/) offers support and maintenance for the
  [NGINX Ingress Controller for Kubernetes](https://www.nginx.com/products/nginx/kubernetes-ingress-controller).
* [Skipper](https://opensource.zalando.com/skipper/kubernetes/ingress-controller/) HTTP router and reverse proxy for service composition, including use cases like Kubernetes Ingress, designed as a library to build your custom proxy
* [Traefik](https://github.com/containous/traefik) is a fully featured ingress controller
  ([Let&#39;s Encrypt](https://letsencrypt.org), secrets, http2, websocket), and it also comes with commercial
  support by [Containous](https://containo.us/services).
 --&gt;
&lt;h2 id=&#34;其它的控制器&#34;&gt;其它的控制器&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Azure/application-gateway-kubernetes-ingress&#34;&gt;AKS Application Gateway Ingress Controller&lt;/a&gt;
是使用
&lt;a href=&#34;https://docs.microsoft.com/azure/application-gateway/overview&#34;&gt;Azure Application Gateway&lt;/a&gt;
为
&lt;a href=&#34;https://docs.microsoft.com/azure/aks/kubernetes-walkthrough-portal&#34;&gt;AKS 集群&lt;/a&gt;
提供入口的 Ingress 控制器&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.getambassador.io/&#34;&gt;Ambassador&lt;/a&gt; API 网关
是一个基于
&lt;a href=&#34;https://www.envoyproxy.io&#34;&gt;Envoy&lt;/a&gt;
的 Ingress 控制器
有 &lt;a href=&#34;https://www.getambassador.io/docs&#34;&gt;社区&lt;/a&gt; 支持和
来自
&lt;a href=&#34;https://www.datawire.io/&#34;&gt;Datawire&lt;/a&gt;
的
&lt;a href=&#34;https://www.getambassador.io/pro/&#34;&gt;商业&lt;/a&gt; 支持&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://appscode.com&#34;&gt;AppsCode Inc.&lt;/a&gt; 提供了最广泛使用的基于
&lt;a href=&#34;https://www.haproxy.org/&#34;&gt;HAProxy&lt;/a&gt;
的 Ingress 控制器
&lt;a href=&#34;https://appscode.com/products/voyager&#34;&gt;Voyager&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/aws-alb-ingress-controller&#34;&gt;AWS ALB Ingress Controller&lt;/a&gt;
使用
&lt;a href=&#34;https://aws.amazon.com/elasticloadbalancing/&#34;&gt;AWS Application Load Balancer&lt;/a&gt;
实现 Ingress&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://projectcontour.io/&#34;&gt;Contour&lt;/a&gt;
由 VMware 提供和支持的基于 &lt;a href=&#34;https://www.envoyproxy.io/&#34;&gt;Envoy&lt;/a&gt; 的 Ingress 控制器&lt;/li&gt;
&lt;li&gt;Citrix 为它的
&lt;a href=&#34;https://github.com/citrix/citrix-k8s-ingress-controller/tree/master/deployment/baremetal&#34;&gt;baremetal&lt;/a&gt;
和
&lt;a href=&#34;https://github.com/citrix/citrix-k8s-ingress-controller/tree/master/deployment&#34;&gt;cloud&lt;/a&gt;
上部署的硬件(MPX),虚拟化(VPX)和
&lt;a href=&#34;https://www.citrix.com/products/citrix-adc/cpx-express.html&#34;&gt;free containerized (CPX) ADC&lt;/a&gt;
提供的 &lt;a href=&#34;https://github.com/citrix/citrix-k8s-ingress-controller&#34;&gt;Ingress 控制器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;F5 Networks&lt;/code&gt; 为
&lt;a href=&#34;https://clouddocs.f5.com/containers/latest/userguide/kubernetes/&#34;&gt;F5 BIG-IP Container Ingress Services for Kubernetes&lt;/a&gt;
提供 &lt;a href=&#34;https://support.f5.com/csp/article/K86859508&#34;&gt;支持和维护&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gloo.solo.io&#34;&gt;Gloo&lt;/a&gt; 是基于
&lt;a href=&#34;https://www.envoyproxy.io&#34;&gt;Envoy&lt;/a&gt; 的开源 Ingress 控制器
提供了来自 &lt;a href=&#34;https://www.solo.io&#34;&gt;solo.io&lt;/a&gt; 企业支持的 API 网络功能&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://haproxy-ingress.github.io&#34;&gt;HAProxy Ingress&lt;/a&gt; 用于 HAProxy 调度可定制的社区驱动的 Ingress 控制器&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.haproxy.com/&#34;&gt;HAProxy Technologies&lt;/a&gt; 为
&lt;a href=&#34;https://github.com/haproxytech/kubernetes-ingress&#34;&gt;HAProxy Ingress Controller for Kubernetes&lt;/a&gt;
提供支持和维护。 详见
&lt;a href=&#34;https://www.haproxy.com/documentation/hapee/1-9r1/traffic-management/kubernetes-ingress-controller/&#34;&gt;官方文档&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;基于 &lt;a href=&#34;https://istio.io/&#34;&gt;Istio&lt;/a&gt; 的
&lt;a href=&#34;https://istio.io/docs/tasks/traffic-management/ingress/&#34;&gt;Control Ingress Traffic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://konghq.com/&#34;&gt;Kong&lt;/a&gt; 为
&lt;a href=&#34;https://github.com/Kong/kubernetes-ingress-controller&#34;&gt;Kong Ingress Controller for Kubernetes&lt;/a&gt;
提供
&lt;a href=&#34;https://discuss.konghq.com/c/kubernetes&#34;&gt;社区&lt;/a&gt;
或
&lt;a href=&#34;https://konghq.com/kong-enterprise/&#34;&gt;商业&lt;/a&gt;的
支持和维护
&lt;a href=&#34;https://www.nginx.com/products/nginx/kubernetes-ingress-controller&#34;&gt;NGINX Ingress Controller for Kubernetes&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nginx.com/&#34;&gt;NGINX, Inc.&lt;/a&gt; 为
&lt;a href=&#34;https://www.nginx.com/products/nginx/kubernetes-ingress-controller&#34;&gt;NGINX Ingress Controller for Kubernetes&lt;/a&gt;
提供支持和维护&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://opensource.zalando.com/skipper/kubernetes/ingress-controller/&#34;&gt;Skipper&lt;/a&gt;
为 Service 提供 HTTP 路由 和反向代理。还包含了类似 k8s Ingress 的使用场景，
设计上可以作为自定义代理的库&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/containous/traefik&#34;&gt;Traefik&lt;/a&gt; 是个功能齐全的 Ingress 控制器
(&lt;a href=&#34;https://letsencrypt.org&#34;&gt;Let&amp;rsquo;s Encrypt&lt;/a&gt;, secrets, http2, websocket)
还有来自 &lt;a href=&#34;https://containo.us/services&#34;&gt;Containous&lt;/a&gt; 的商业支持&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Using multiple Ingress controllers

You may deploy [any number of ingress controllers](https://git.k8s.io/ingress-nginx/docs/user-guide/multiple-ingress.md#multiple-ingress-controllers)
within a cluster. When you create an ingress, you should annotate each ingress with the appropriate
[`ingress.class`](https://git.k8s.io/ingress-gce/docs/faq/README.md#how-do-i-run-multiple-ingress-controllers-in-the-same-cluster)
to indicate which ingress controller should be used if more than one exists within your cluster.

If you do not define a class, your cloud provider may use a default ingress controller.

Ideally, all ingress controllers should fulfill this specification, but the various ingress
controllers operate slightly differently.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Make sure you review your ingress controller&amp;rsquo;s documentation to understand the caveats of choosing it.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;using-multiple-ingress-controllers&#34;&gt;Using multiple Ingress controllers&lt;/h2&gt;
&lt;p&gt;用户可以在集群中部署
&lt;a href=&#34;https://git.k8s.io/ingress-nginx/docs/user-guide/multiple-ingress.md#multiple-ingress-controllers&#34;&gt;任意数量的 Ingress 控制器&lt;/a&gt;
如果集群中有多个 Ingress 控制器，在创建 Ingress 时需要为每个 Ingress 标注恰当的
&lt;a href=&#34;https://git.k8s.io/ingress-gce/docs/faq/README.md#how-do-i-run-multiple-ingress-controllers-in-the-same-cluster&#34;&gt;&lt;code&gt;ingress.class&lt;/code&gt;&lt;/a&gt;
来指示使用哪个 Ingress 控制器。&lt;/p&gt;
&lt;p&gt;如果在 Ingress 中没有定义控制器类型，云提供商可能使用一个默认的 Ingress 控制器。&lt;/p&gt;
&lt;p&gt;理想情况下，所有的 Ingress 控制器都应当满足这个规范，但是很多 Ingress 控制器动作方式都有点不同。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 一定要仔细阅读你选择的 Ingress 控制器的文档，确保理解了相关注意事项。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/ingress/&#34;&gt;Ingress&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/access-application-cluster/ingress-minikube&#34;&gt;使用 NGINX 控制器在 Minikube 上设置 Ingress&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 网络策略</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/network-policies/</link>
      <pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/network-policies/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- thockin
- caseydavenport
- danwinship
title: Network Policies
content_type: concept
weight: 50
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), then you might consider using Kubernetes NetworkPolicies for particular applications in your cluster.  NetworkPolicies are an application-centric construct which allow you to specify how a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; is allowed to communicate with various network &#34;entities&#34; (we use the word &#34;entity&#34; here to avoid overloading the more common terms such as &#34;endpoints&#34; and &#34;services&#34;, which have specific Kubernetes connotations) over the network.

The entities that a Pod can communicate with are identified through a combination of the following 3 identifiers:

1. Other pods that are allowed (exception: a pod cannot block access to itself)
2. Namespaces that are allowed
3. IP blocks (exception: traffic to and from the node where a Pod is running is always allowed, regardless of the IP address of the Pod or the node)

When defining a pod- or namespace- based NetworkPolicy, you use a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/&#39; target=&#39;_blank&#39;&gt;selector&lt;span class=&#39;tooltip-text&#39;&gt;允许用户基于标签过滤资源列表&lt;/span&gt;
&lt;/a&gt; to specify what traffic is allowed to and from the Pod(s) that match the selector.

Meanwhile, when IP based NetworkPolicies are created, we define policies based on IP blocks (CIDR ranges).
 --&gt;
&lt;p&gt;如果想要在 IP 地址或端口层(OSI 3 或 4 层)控制网络流量，可以考虑对集群中的特定应用使用 k8s 网络策略。
网络策略是一个应用为中心的构造，它允许用户指定怎么控制一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 以允许它与其它多种网络实体(这里使用 实体(entity)避免与 &amp;ldquo;Endpoint&amp;rdquo; &amp;ldquo;Service&amp;rdquo;
这此在 k8s 中有明确含义的词混淆)通信。&lt;/p&gt;
&lt;p&gt;Pod 可以与之通信的实体可以由以下三个标识组合来识别:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;其它被允许的 Pod (例外: Pod 不可以禁止与它本身通信)&lt;/li&gt;
&lt;li&gt;被允许的命名空间&lt;/li&gt;
&lt;li&gt;IP 段(例外: Pod 所在的节点进出流量都是允许的，Pod 和 节点的 IP 如果在禁用段则会被忽略)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在定义一个基于 Pod 或 命名空间的网络策略时，可以使用&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/&#39; target=&#39;_blank&#39;&gt;选择器&lt;span class=&#39;tooltip-text&#39;&gt;允许用户基于标签过滤资源列表&lt;/span&gt;
&lt;/a&gt;
来指定只与选择器相匹配的 Pod 在允许与之进行流量往来。&lt;/p&gt;
&lt;p&gt;同时，当创建基于 IP 的网络策略时，定义的策略基于 IP 段(CIDR 范围)。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Prerequisites

Network policies are implemented by the [network plugin](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/). To use network policies, you must be using a networking solution which supports NetworkPolicy. Creating a NetworkPolicy resource without a controller that implements it will have no effect.
--&gt;
&lt;h2 id=&#34;前置条件&#34;&gt;前置条件&lt;/h2&gt;
&lt;p&gt;网络策略是由
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/extend-kubernetes/compute-storage-net/network-plugins/&#34;&gt;网络插件&lt;/a&gt;
实现的。 要使用网络策略，就必要使用一个支持网络策略的网络解决方案。 创建一个没有实现的控制器的
&lt;code&gt;NetworkPolicy&lt;/code&gt; 资源是没有效果的&lt;/p&gt;
&lt;!--
## Isolated and Non-isolated Pods

By default, pods are non-isolated; they accept traffic from any source.

Pods become isolated by having a NetworkPolicy that selects them. Once there is any NetworkPolicy in a namespace selecting a particular pod, that pod will reject any connections that are not allowed by any NetworkPolicy. (Other pods in the namespace that are not selected by any NetworkPolicy will continue to accept all traffic.)

Network policies do not conflict; they are additive. If any policy or policies select a pod, the pod is restricted to what is allowed by the union of those policies&#39; ingress/egress rules. Thus, order of evaluation does not affect the policy result.
 --&gt;
&lt;h2 id=&#34;隔离和非隔离的-pod&#34;&gt;隔离和非隔离的 Pod&lt;/h2&gt;
&lt;p&gt;默认情况下，所有的 Pod 都是非隔离的; 它们可以接收来自任意源的流量。&lt;/p&gt;
&lt;p&gt;当 Pod 被一个 NetworkPolicy 选中时就会变成隔离的。 当在一个命名空间中有任意 NetworkPolicy
选中了某个 Pod， 这个 Pod 就会拒绝那些不被这些 NetworkPolicy 允许的连接就会被拒绝。(同一个
命名空间中其它没有被任何 NetworkPolicy 选择的 Pod 依然继续接收所有流量。)&lt;/p&gt;
&lt;p&gt;网络策略不会冲突； 它们可叠加。 如果任意策略选中了一个 Pod， 该 Pod 就会被限制在这些策略的
&lt;code&gt;ingress/egress&lt;/code&gt; 规则交集允许的范围。 因此，执行的顺序并不会影响策略的结果。&lt;/p&gt;
&lt;!--
## The NetworkPolicy resource {#networkpolicy-resource}

See the [NetworkPolicy](/docs/reference/generated/kubernetes-api/v1.19/#networkpolicy-v1-networking-k8s-io) reference for a full definition of the resource.

An example NetworkPolicy might look like this:

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; POSTing this to the API server for your cluster will have no effect unless your chosen networking solution supports network policy.&lt;/div&gt;
&lt;/blockquote&gt;


__Mandatory Fields__: As with all other Kubernetes config, a NetworkPolicy
needs `apiVersion`, `kind`, and `metadata` fields.  For general information
about working with config files, see
[Configure Containers Using a ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/),
and [Object Management](/docs/concepts/overview/working-with-objects/object-management).

__spec__: NetworkPolicy [spec](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status) has all the information needed to define a particular network policy in the given namespace.

__podSelector__: Each NetworkPolicy includes a `podSelector` which selects the grouping of pods to which the policy applies. The example policy selects pods with the label &#34;role=db&#34;. An empty `podSelector` selects all pods in the namespace.

__policyTypes__: Each NetworkPolicy includes a `policyTypes` list which may include either `Ingress`, `Egress`, or both. The `policyTypes` field indicates whether or not the given policy applies to ingress traffic to selected pod, egress traffic from selected pods, or both. If no `policyTypes` are specified on a NetworkPolicy then by default `Ingress` will always be set and `Egress` will be set if the NetworkPolicy has any egress rules.

__ingress__: Each NetworkPolicy may include a list of allowed `ingress` rules.  Each rule allows traffic which matches both the `from` and `ports` sections. The example policy contains a single rule, which matches traffic on a single port, from one of three sources, the first specified via an `ipBlock`, the second via a `namespaceSelector` and the third via a `podSelector`.

__egress__: Each NetworkPolicy may include a list of allowed `egress` rules.  Each rule allows traffic which matches both the `to` and `ports` sections. The example policy contains a single rule, which matches traffic on a single port to any destination in `10.0.0.0/24`.

So, the example NetworkPolicy:

1. isolates &#34;role=db&#34; pods in the &#34;default&#34; namespace for both ingress and egress traffic (if they weren&#39;t already isolated)
2. (Ingress rules) allows connections to all pods in the &#34;default&#34; namespace with the label &#34;role=db&#34; on TCP port 6379 from:

   * any pod in the &#34;default&#34; namespace with the label &#34;role=frontend&#34;
   * any pod in a namespace with the label &#34;project=myproject&#34;
   * IP addresses in the ranges 172.17.0.0–172.17.0.255 and 172.17.2.0–172.17.255.255 (ie, all of 172.17.0.0/16 except 172.17.1.0/24)
3. (Egress rules) allows connections from any pod in the &#34;default&#34; namespace with the label &#34;role=db&#34; to CIDR 10.0.0.0/24 on TCP port 5978

See the [Declare Network Policy](/docs/tasks/administer-cluster/declare-network-policy/) walkthrough for further examples.
--&gt;
&lt;h2 id=&#34;networkpolicy-resource&#34;&gt;网络策略资源&lt;/h2&gt;
&lt;p&gt;可以在 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubernetes-api/v1.19/#networkpolicy-v1-networking-k8s-io&#34;&gt;NetworkPolicy&lt;/a&gt; 查阅详细定义文档.&lt;/p&gt;
&lt;p&gt;一个 NetworkPolicy 示例可能长成这个样子:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-network-policy&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;role&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;db&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Egress&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ingress&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;ipBlock&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;cidr&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;172.17.0.0&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;/16&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;except&lt;/span&gt;:
        - &lt;span style=&#34;color:#ae81ff&#34;&gt;172.17.1.0&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;/24&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;namespaceSelector&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;project&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myproject&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;role&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;6379&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;egress&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;to&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;ipBlock&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;cidr&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10.0.0.0&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;/24&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5978&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果集群的网络解决方案(网络插件)不支持网络策略，POST 这个配置到集群 api-server 将是没有效果的
也就是说要使用网络策略，就必须要安装支持网络策略的网络插件。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;必要字段&lt;/strong&gt;: 与其它所有的 k8s 配置一样， NetworkPolicy 必须有 &lt;code&gt;apiVersion&lt;/code&gt;, &lt;code&gt;kind&lt;/code&gt;, &lt;code&gt;metadata&lt;/code&gt;
字段。 配置文件常用信息请见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-pod-configmap/&#34;&gt;使用 ConfigMap 配置容器&lt;/a&gt;,
和 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/object-management&#34;&gt;对象管理&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;spec&lt;/strong&gt;: NetworkPolicy &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;spec&lt;/a&gt; 包含了在指定命名空间创建特定网络策略的所有信息&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;podSelector&lt;/strong&gt;: 每个 NetworkPolicy 都包含了一个 &lt;code&gt;podSelector&lt;/code&gt; 字段，用户选择应用该策略的 Pod 组。
上面例子中的策略选择包含 &amp;ldquo;&lt;code&gt;role=db&lt;/code&gt;&amp;rdquo; 标签的 Pod。 如果 &lt;code&gt;podSelector&lt;/code&gt; 是空则选择该命名空间中所有的 Pod。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;policyTypes&lt;/strong&gt;: 每个 NetworkPolicy 包含一个 &lt;code&gt;policyTypes&lt;/code&gt; 字段，该字段值为一个列表，
这个列表的值可以是 &lt;code&gt;Ingress&lt;/code&gt;, &lt;code&gt;Egress&lt;/code&gt; 中的任意一个或同时包含两个。 该字段值是否包含
&lt;code&gt;Ingress&lt;/code&gt; 表示是否将该策略执行到输入流量到达选定的 Pod；
&lt;code&gt;Egress&lt;/code&gt; 表示是否将该策略执行到选择 Pod 输出的流量。
如果没有指定 &lt;code&gt;policyTypes&lt;/code&gt; 字段，则默认情况下 &lt;code&gt;Ingress&lt;/code&gt; 问题要设置，而 &lt;code&gt;Egress&lt;/code&gt; 则只在
NetworkPolicy 中包含 &lt;code&gt;egress&lt;/code&gt; 规则是才设置。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ingress&lt;/strong&gt;: 每个 NetworkPolicy 可能包含一个被允许的 &lt;code&gt;ingress&lt;/code&gt; 规则列表。
每个规则所允许的流量会同时匹配 &lt;code&gt;from&lt;/code&gt; 和 &lt;code&gt;ports&lt;/code&gt; 两个部分。上面例子中的策略就包含一个规则
这个规则匹配来自一个端口加上三个源(第一个是通过 &lt;code&gt;ipBlock&lt;/code&gt;指定一个 IP 段； 第三个是通过
&lt;code&gt;namespaceSelector&lt;/code&gt;；第三个是通过 &lt;code&gt;podSelector&lt;/code&gt;)中的一个的组合的流量&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;egress&lt;/strong&gt;: 每个 NetworkPolicy 可能包含一个被允许的 &lt;code&gt;egress&lt;/code&gt; 规则列表。 每个规则允许的流量
由 &lt;code&gt;to&lt;/code&gt; 和 &lt;code&gt;ports&lt;/code&gt; 组合匹配。 上面的例子中的策略包含一个规则，这个规则匹配一个端口加上
&lt;code&gt;10.0.0.0/24&lt;/code&gt; 中的任意一个目标的组合的流量。&lt;/p&gt;
&lt;p&gt;因此，上面例子中 NetworkPolicy 如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;在默认(default)命名空间中隔离 &amp;ldquo;role=db&amp;rdquo; 所选择 Pod 的 &lt;code&gt;ingress&lt;/code&gt; 和 &lt;code&gt;egress&lt;/code&gt; 流量(如果它们还没有被隔离)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Ingress 规则)允许以下范围的所有连接到默认(default)命名空间中包含 &amp;ldquo;role=db&amp;rdquo; 标签的 Pod 的 TCP 端口 6379:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;默认(default)命名空间中包含 &amp;ldquo;role=frontend&amp;rdquo; 标签的任意 Pod&lt;/li&gt;
&lt;li&gt;所在命名空间包含 &amp;ldquo;project=myproject&amp;rdquo; 标签的任意 Pod&lt;/li&gt;
&lt;li&gt;&lt;code&gt;172.17.0.0–172.17.0.255&lt;/code&gt; 和 &lt;code&gt;172.17.2.0–172.17.255.255&lt;/code&gt; IP 段中的地址(
&lt;code&gt;172.17.0.0/16&lt;/code&gt; 中除了 &lt;code&gt;172.17.1.0/24&lt;/code&gt; 之外的地址
)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;(Egress 规则) 允许 默认(default)命名空间中包含 &amp;ldquo;role=db&amp;rdquo; 的任意 Pod 到
CIDR 10.0.0.0/24 TCP 端口 5978 的所有连接&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;更多示例见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/declare-network-policy/&#34;&gt;声明网络策略&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Behavior of `to` and `from` selectors

There are four kinds of selectors that can be specified in an `ingress` `from` section or `egress` `to` section:

__podSelector__: This selects particular Pods in the same namespace as the NetworkPolicy which should be allowed as ingress sources or egress destinations.

__namespaceSelector__: This selects particular namespaces for which all Pods should be allowed as ingress sources or egress destinations.

__namespaceSelector__ *and* __podSelector__: A single `to`/`from` entry that specifies both `namespaceSelector` and `podSelector` selects particular Pods within particular namespaces. Be careful to use correct YAML syntax; this policy:

```yaml
  ...
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
      podSelector:
        matchLabels:
          role: client
  ...
```

contains a single `from` element allowing connections from Pods with the label `role=client` in namespaces with the label `user=alice`. But *this* policy:

```yaml
  ...
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
    - podSelector:
        matchLabels:
          role: client
  ...
```

contains two elements in the `from` array, and allows connections from Pods in the local Namespace with the label `role=client`, *or* from any Pod in any namespace with the label `user=alice`.

When in doubt, use `kubectl describe` to see how Kubernetes has interpreted the policy.

__ipBlock__: This selects particular IP CIDR ranges to allow as ingress sources or egress destinations. These should be cluster-external IPs, since Pod IPs are ephemeral and unpredictable.

Cluster ingress and egress mechanisms often require rewriting the source or destination IP
of packets. In cases where this happens, it is not defined whether this happens before or
after NetworkPolicy processing, and the behavior may be different for different
combinations of network plugin, cloud provider, `Service` implementation, etc.

In the case of ingress, this means that in some cases you may be able to filter incoming
packets based on the actual original source IP, while in other cases, the &#34;source IP&#34; that
the NetworkPolicy acts on may be the IP of a `LoadBalancer` or of the Pod&#39;s node, etc.

For egress, this means that connections from pods to `Service` IPs that get rewritten to
cluster-external IPs may or may not be subject to `ipBlock`-based policies.
 --&gt;
&lt;h2 id=&#34;to-和-from-选择器的行为方式&#34;&gt;&lt;code&gt;to&lt;/code&gt; 和 &lt;code&gt;from&lt;/code&gt; 选择器的行为方式&lt;/h2&gt;
&lt;p&gt;在 &lt;code&gt;ingress&lt;/code&gt; 的 &lt;code&gt;from&lt;/code&gt; 区 或 &lt;code&gt;egress&lt;/code&gt; 的 &lt;code&gt;to&lt;/code&gt; 区可以指定以下四种选择器：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;podSelector&lt;/strong&gt;:  选择与 NetworkPolicy 在同一个命名空间的指定 Pod 允许为 &lt;code&gt;ingress&lt;/code&gt; 的源() 或 &lt;code&gt;egress&lt;/code&gt; 的目的
&lt;strong&gt;namespaceSelector&lt;/strong&gt;: 选择指定名称空间，其中所有的 Pod 允许为 &lt;code&gt;ingress&lt;/code&gt; 的源或 &lt;code&gt;egress&lt;/code&gt; 的目的
&lt;strong&gt;namespaceSelector&lt;/strong&gt; &lt;em&gt;加&lt;/em&gt; &lt;strong&gt;podSelector&lt;/strong&gt;: 单个 &lt;code&gt;to&lt;/code&gt;/&lt;code&gt;from&lt;/code&gt; 条目可以同时指定 &lt;code&gt;namespaceSelector&lt;/code&gt; 和 &lt;code&gt;podSelector&lt;/code&gt;
来选择指定命名空间的指定 Pod。 需要注意正确地使用 YAML 语法。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ingress&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;namespaceSelector&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;user&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;alice&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;role&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;client&lt;/span&gt;
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上面的这个策略包含一个 &lt;code&gt;from&lt;/code&gt; 元素，它允许连接的 Pod 在包含 &lt;code&gt;user=alice&lt;/code&gt; 标签的命名空间中
并且要包含 &lt;code&gt;role=client&lt;/code&gt; 标签。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ingress&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;namespaceSelector&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;user&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;alice&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;role&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;client&lt;/span&gt;
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个策略包含两上 &lt;code&gt;from&lt;/code&gt; 元素的数据， 它允许连接的 Pod 有: 同命名空间中有 &lt;code&gt;role=client&lt;/code&gt; 标签的 Pod
&lt;em&gt;或&lt;/em&gt; 任意命名空间中包含 &lt;code&gt;user=alice&lt;/code&gt; 标签的任意 Pod&lt;/p&gt;
&lt;p&gt;当搞不清楚的时候，就使用 &lt;code&gt;kubectl describe&lt;/code&gt; 来看 k8s 是怎么来解释这个策略的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ipBlock&lt;/strong&gt;: 指定 IP CIDR 范围允许为 &lt;code&gt;ingress&lt;/code&gt; 的源或 &lt;code&gt;egress&lt;/code&gt; 的目的。这个 IP 范围
应该是集群外部 IP， 因为 Pod 的 IP 是临时的并且是不可预测的。&lt;/p&gt;
&lt;p&gt;集群的 &lt;code&gt;ingress&lt;/code&gt; 和 &lt;code&gt;egress&lt;/code&gt; 经常需要重写包的源或目的 IP。 如果发生了这种情况， 它发生在
NetworkPolicy 之前还是之后不不确定的，由网络插件，云提供商，&lt;code&gt;Service&lt;/code&gt; 实现的不同而有不同的行为方式&lt;/p&gt;
&lt;p&gt;对于 &lt;code&gt;ingress&lt;/code&gt;, 就意味着在某些情况下可能可以通过实际的原始源 IP 地址来过滤包，在另一情况下,
NetworkPolicy 处理的的 &amp;ldquo;源 IP&amp;rdquo; 可能是 &lt;code&gt;LoadBalancer&lt;/code&gt; 的 IP 或 Pod 所在节点的 IP 等。&lt;/p&gt;
&lt;p&gt;对于 &lt;code&gt;egress&lt;/code&gt;， 这就意味着 Pod 连接的目标 &lt;code&gt;Service&lt;/code&gt; IP 在重写到集群外部IP 就可能受 &lt;code&gt;ipBlock&lt;/code&gt;
策略控制，也可能不受控制。&lt;/p&gt;
&lt;!--
## Default policies

By default, if no policies exist in a namespace, then all ingress and egress traffic is allowed to and from pods in that namespace. The following examples let you change the default behavior
in that namespace.
 --&gt;
&lt;h2 id=&#34;默认策略&#34;&gt;默认策略&lt;/h2&gt;
&lt;p&gt;默认情况下，如果一个命名空间中没有策略存在，那这个命名空间中的 Pod 所有的 &lt;code&gt;ingress&lt;/code&gt; 和 &lt;code&gt;egress&lt;/code&gt;
流量都是允许的。 下面的例子让用户可以修改那个命名空间中的默认行为。&lt;/p&gt;
&lt;!--
### Default deny all ingress traffic

You can create a &#34;default&#34; isolation policy for a namespace by creating a NetworkPolicy that selects all pods but does not allow any ingress traffic to those pods.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-default-deny-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-default-deny-ingress.yaml&#34; download=&#34;service/networking/network-policy-default-deny-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-default-deny-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-default-deny-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-default-deny-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-deny-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



This ensures that even pods that aren&#39;t selected by any other NetworkPolicy will still be isolated. This policy does not change the default egress isolation behavior.
 --&gt;
&lt;h3 id=&#34;默认拒绝所有-ingress-流量&#34;&gt;默认拒绝所有 &lt;code&gt;ingress&lt;/code&gt; 流量&lt;/h3&gt;
&lt;p&gt;可以能过创建一个 NetworkPolicy 为命名空间创建一个&amp;quot;默认&amp;quot;的隔离策略。这个 NetworkPolicy
会选择所有的 Pod 并且不允许任何 &lt;code&gt;ingress&lt;/code&gt; 流量到这些 Pod。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-default-deny-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-default-deny-ingress.yaml&#34; download=&#34;service/networking/network-policy-default-deny-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-default-deny-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-default-deny-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-default-deny-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-deny-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;这样可以保证即便有些 Pod 没有被其它任意 NetworkPolicy 选择还是会被隔离。
这个策略不会影响默认的 &lt;code&gt;egress&lt;/code&gt; 隔离行为。&lt;/p&gt;
&lt;!--
### Default allow all ingress traffic

If you want to allow all traffic to all pods in a namespace (even if policies are added that cause some pods to be treated as &#34;isolated&#34;), you can create a policy that explicitly allows all traffic in that namespace.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-allow-all-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-allow-all-ingress.yaml&#34; download=&#34;service/networking/network-policy-allow-all-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-allow-all-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-allow-all-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-allow-all-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;allow-all-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;ingress&lt;/span&gt;:
  - {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


 --&gt;
&lt;h3 id=&#34;默认允许所有-ingress-流量&#34;&gt;默认允许所有 &lt;code&gt;ingress&lt;/code&gt; 流量&lt;/h3&gt;
&lt;p&gt;如果想要在一个命名空间中允许到达所有 Pod 的所有流量(即便已经添加了一些策略导致有些 Pod 被认为是隔离的)
可以通过创建一个策略显式地允许这个命名空间中的所有离题。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-allow-all-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-allow-all-ingress.yaml&#34; download=&#34;service/networking/network-policy-allow-all-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-allow-all-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-allow-all-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-allow-all-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;allow-all-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;ingress&lt;/span&gt;:
  - {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;!--
### Default deny all egress traffic

You can create a &#34;default&#34; egress isolation policy for a namespace by creating a NetworkPolicy that selects all pods but does not allow any egress traffic from those pods.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-default-deny-egressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-default-deny-egress.yaml&#34; download=&#34;service/networking/network-policy-default-deny-egress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-default-deny-egress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-default-deny-egressyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-default-deny-egress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-deny-egress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Egress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



This ensures that even pods that aren&#39;t selected by any other NetworkPolicy will not be allowed egress traffic. This policy does not
change the default ingress isolation behavior.
 --&gt;
&lt;h3 id=&#34;默认拒绝所有-egress-流量&#34;&gt;默认拒绝所有 &lt;code&gt;egress&lt;/code&gt; 流量&lt;/h3&gt;
&lt;p&gt;可以通过创建一个选择所有 Pod 并不接受任何来自这些 Pod 的 egress 流量的 NetworkPolicy 来作为
这个命名空间的默认 &lt;code&gt;egress&lt;/code&gt; 隔离策略。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-default-deny-egressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-default-deny-egress.yaml&#34; download=&#34;service/networking/network-policy-default-deny-egress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-default-deny-egress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-default-deny-egressyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-default-deny-egress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-deny-egress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Egress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;这样可以保证即便有 Pod 没有被其它任意 NetworkPolicy 选中，也不会被允许其 egress 流量。
这个策略不影响默认 &lt;code&gt;ingress&lt;/code&gt; 隔离行为。&lt;/p&gt;
&lt;!--
### Default allow all egress traffic

If you want to allow all traffic from all pods in a namespace (even if policies are added that cause some pods to be treated as &#34;isolated&#34;), you can create a policy that explicitly allows all egress traffic in that namespace.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-allow-all-egressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-allow-all-egress.yaml&#34; download=&#34;service/networking/network-policy-allow-all-egress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-allow-all-egress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-allow-all-egressyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-allow-all-egress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;allow-all-egress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;egress&lt;/span&gt;:
  - {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Egress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


 --&gt;
&lt;h3 id=&#34;默认允许所有-egress-流量&#34;&gt;默认允许所有 &lt;code&gt;egress&lt;/code&gt; 流量&lt;/h3&gt;
&lt;p&gt;如果想要允许一个命名空间中来自所有 Pod 的所有流量(即便已经添加了一些策略导致有些 Pod 被认为是隔离的)，
也可以通过创建一个策略显式地允许这个命名空间中所有的 &lt;code&gt;egress&lt;/code&gt; 流量。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-allow-all-egressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-allow-all-egress.yaml&#34; download=&#34;service/networking/network-policy-allow-all-egress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-allow-all-egress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-allow-all-egressyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-allow-all-egress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;allow-all-egress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;egress&lt;/span&gt;:
  - {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Egress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;!--
### Default deny all ingress and all egress traffic

You can create a &#34;default&#34; policy for a namespace which prevents all ingress AND egress traffic by creating the following NetworkPolicy in that namespace.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-default-deny-allyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-default-deny-all.yaml&#34; download=&#34;service/networking/network-policy-default-deny-all.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-default-deny-all.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-default-deny-allyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-default-deny-all.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-deny-all&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Egress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



This ensures that even pods that aren&#39;t selected by any other NetworkPolicy will not be allowed ingress or egress traffic.
 --&gt;
&lt;h3 id=&#34;拒绝所有-ingress-和所有-egress-流量&#34;&gt;拒绝所有 &lt;code&gt;ingress&lt;/code&gt; 和所有 &lt;code&gt;egress&lt;/code&gt; 流量&lt;/h3&gt;
&lt;p&gt;You can create a &amp;ldquo;default&amp;rdquo; policy for a namespace which prevents all ingress AND egress traffic by creating the following NetworkPolicy in that namespace.
可以在一个命名空间中创建一个如下默认的 NetworkPolicy，同时禁止所有的 &lt;code&gt;ingress&lt;/code&gt; 和 &lt;code&gt;egress&lt;/code&gt; 流量。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-default-deny-allyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-default-deny-all.yaml&#34; download=&#34;service/networking/network-policy-default-deny-all.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-default-deny-all.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-default-deny-allyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-default-deny-all.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-deny-all&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Egress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;这样确保即便有 Pod 没有被其它任意 NetworkPolicy 选中，也不会被允许其 &lt;code&gt;ingress&lt;/code&gt; 或 &lt;code&gt;egress&lt;/code&gt; 流量&lt;/p&gt;
&lt;!--
## SCTP support






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;



As a beta feature, this is enabled by default. To disable SCTP at a cluster level, you (or your cluster administrator) will need to disable the `SCTPSupport` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) for the API server with `--feature-gates=SCTPSupport=false,…`.
When the feature gate is enabled, you can set the `protocol` field of a NetworkPolicy to `SCTP`.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You must be using a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni&#39; target=&#39;_blank&#39;&gt;CNI&lt;span class=&#39;tooltip-text&#39;&gt;Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.&lt;/span&gt;
&lt;/a&gt; plugin that supports SCTP protocol NetworkPolicies.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;sctp-支持&#34;&gt;SCTP 支持&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;作为一个 &lt;code&gt;beta&lt;/code&gt; 版本的特性，默认是开启的。要在集群级别禁用 SCTP， 需要在 api-server 使用
&lt;code&gt;--feature-gates=SCTPSupport=false,…&lt;/code&gt; 禁用
&lt;code&gt;SCTPSupport&lt;/code&gt; &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能特性阀&lt;/a&gt;
当这个特性被启用时，可能在 NetworkPolicy 的 &lt;code&gt;protocol&lt;/code&gt; 字段使用 &lt;code&gt;SCTP&lt;/code&gt; 协议。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 必须要使用一个支持 NetworkPolicy SCTP 协议的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni&#39; target=&#39;_blank&#39;&gt;CNI&lt;span class=&#39;tooltip-text&#39;&gt;Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.&lt;/span&gt;
&lt;/a&gt; 插件&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;!--
# What you CAN&#39;T do with network policy&#39;s (at least, not yet)

As of Kubernetes 1.20, the following functionality does not exist in the NetworkPolicy API, but you might be able to implement workarounds using Operating System components (such as SELinux, OpenVSwitch, IPTables, and so on) or Layer 7 technologies (Ingress controllers, Service Mesh implementations) or admission controllers.  In case you are new to network security in Kubernetes, its worth noting that the following User Stories cannot (yet) be implemented using the NetworkPolicy API.  Some (but not all) of these user stories are actively being discussed for future releases of the NetworkPolicy API.

- Forcing internal cluster traffic to go through a common gateway (this might be best served with a service mesh or other proxy).
- Anything TLS related (use a service mesh or ingress controller for this).
- Node specific policies (you can use CIDR notation for these, but you cannot target nodes by their Kubernetes identities specifically).
- Targeting of namespaces or services by name (you can, however, target pods or namespaces by their&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels&#39; target=&#39;_blank&#39;&gt;labels&lt;span class=&#39;tooltip-text&#39;&gt;在对象上标上对用户有意义和有关联的属性&lt;/span&gt;
&lt;/a&gt;, which is often a viable workaround).
- Creation or management of &#34;Policy requests&#34; that are fulfilled by a third party.
- Default policies which are applied to all namespaces or pods (there are some third party Kubernetes distributions and projects which can do this).
- Advanced policy querying and reachability tooling.
- The ability to target ranges of Ports in a single policy declaration.
- The ability to log network security events (for example connections that are blocked or accepted).
- The ability to explicitly deny policies (currently the model for NetworkPolicies are deny by default, with only the ability to add allow rules).
- The ability to prevent loopback or incoming host traffic (Pods cannot currently block localhost access, nor do they have the ability to block access from their resident node).
 --&gt;
&lt;h2 id=&#34;不能通过网络策略做到的情况-至少目前还不能&#34;&gt;不能通过网络策略做到的情况 (至少目前还不能)&lt;/h2&gt;
&lt;p&gt;到 k8s v1.20 为止，以下功能在 NetworkPolicy API 中是不存在的， 但可能可能通过操作系统组件
(如 SELinux， OpenVSwitch， IPTables 等) 或者通过 7 层技术(Ingress 控制器，&lt;code&gt;Service Mesh&lt;/code&gt; 实现)
或者 准入控制器(admission controller) 来曲线救国。 假如用户对 k8s 网络安全了解比较少，
需要注意下面的用户故事(User Stories)(还)不能通过使用 NetworkPolicy API 实现。
有些(但不是所有)正在被讨论加入到未来版本中的 NetworkPolicy API 中。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;强制集群内流量通过一个通用网关(这个最好使用服务网格或其它代理来提供)&lt;/li&gt;
&lt;li&gt;所有与 TLS 相关的事(使用服务网格或 Ingress 控制器来实现)&lt;/li&gt;
&lt;li&gt;节点特有的策略(可以使用 CIDR 标注，但是不能能过 k8s 标识来特指一些节点)(说的啥？)&lt;/li&gt;
&lt;li&gt;通过名称将命名空间或 Service 作为目标(通常的做法是通过 Pod 或命名空间的
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels&#39; target=&#39;_blank&#39;&gt;标签&lt;span class=&#39;tooltip-text&#39;&gt;在对象上标上对用户有意义和有关联的属性&lt;/span&gt;
&lt;/a&gt;来筛选目标)&lt;/li&gt;
&lt;li&gt;通过第三方达成对&amp;quot;Policy requests&amp;quot;的创建的或管理&lt;/li&gt;
&lt;li&gt;应用到所有命名空间或 Pod 的默认策略(有些第三方 k8s 发行版本和项目提供该功能)&lt;/li&gt;
&lt;li&gt;高级策略查询和可到达性检查工具&lt;/li&gt;
&lt;li&gt;在一个策略定义中设置端口范围的能力&lt;/li&gt;
&lt;li&gt;输出网络安全事件的能力(如，连接是被禁止或接受)&lt;/li&gt;
&lt;li&gt;声明显示拒绝的策略的能力(目前的网络策略模型默认为拒绝，只能提供添加允许规则的能力)&lt;/li&gt;
&lt;li&gt;能够阻止回环网络或进入主机流量(目前 Pod 不能禁用通过 localhost 的访问，也不能禁止所在主机的访问)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/administer-cluster/declare-network-policy/&#34;&gt;声明网络策略&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;参阅 &lt;a href=&#34;https://github.com/ahmetb/kubernetes-network-policy-recipes&#34;&gt;recipes&lt;/a&gt;
启用网络策略的常见场景&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 通过 HostAliases 向 Pod 的 /etc/hosts 文件中添加条目</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/</link>
      <pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- rickypai
- thockin
title: Adding entries to Pod /etc/hosts with HostAliases
content_type: concept
weight: 60
min-kubernetes-server-version: 1.7
--- --&gt;
&lt;!-- overview --&gt;
&lt;!--
Adding entries to a Pod&#39;s `/etc/hosts` file provides Pod-level override of hostname resolution when DNS and other options are not applicable. You can add these custom entries with the HostAliases field in PodSpec.

Modification not using HostAliases is not suggested because the file is managed by the kubelet and can be overwritten on during Pod creation/restart.
--&gt;
&lt;p&gt;在 DNS 和其它方式都不可用时，想要向通过向 Pod 的 &lt;code&gt;/etc/hosts&lt;/code&gt; 文件添加条目的方式来重写 Pod
级别的域名解析，可能通过 &lt;code&gt;PodSpec&lt;/code&gt; 的 &lt;code&gt;HostAliases&lt;/code&gt; 字段向该 Pod 的 &lt;code&gt;/etc/hosts&lt;/code&gt; 添加自定义条目。&lt;/p&gt;
&lt;p&gt;不建议直接修改文件而不使用 &lt;code&gt;HostAliases&lt;/code&gt;， 因为这个文件是受 kubelet 管理并且在 Pod 的重新创建
或重启(能重启？)时会重写该文件，也就是直接修改文件在重建后会丢失。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Default hosts file content

Start an Nginx Pod which is assigned a Pod IP:

```shell
kubectl run nginx --image nginx
```

```
pod/nginx created
```

Examine a Pod IP:

```shell
kubectl get pods --output=wide
```

```
NAME     READY     STATUS    RESTARTS   AGE    IP           NODE
nginx    1/1       Running   0          13s    10.200.0.4   worker0
```

The hosts file content would look like this:

```shell
kubectl exec nginx -- cat /etc/hosts
```

```
# Kubernetes-managed hosts file.
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
fe00::0	ip6-mcastprefix
fe00::1	ip6-allnodes
fe00::2	ip6-allrouters
10.200.0.4	nginx
```

By default, the `hosts` file only includes IPv4 and IPv6 boilerplates like
`localhost` and its own hostname.
 --&gt;
&lt;h2 id=&#34;hosts-默认的内容&#34;&gt;hosts 默认的内容&lt;/h2&gt;
&lt;p&gt;启动一个 Nginx 的 Pod，它会被分配一个 Pod IP:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl run nginx --image nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;pod/nginx created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;检查 Pod IP:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods --output&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;wide
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME     READY     STATUS    RESTARTS   AGE    IP           NODE
nginx    1/1       Running   0          13s    10.200.0.4   worker0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过以下命令查看 hosts 文件内容&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl exec nginx -- cat /etc/hosts
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Kubernetes-managed hosts file.
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
fe00::0	ip6-mcastprefix
fe00::1	ip6-allnodes
fe00::2	ip6-allrouters
10.200.0.4	nginx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;默认情况下， &lt;code&gt;hosts&lt;/code&gt; 文件只会包含 IPv4/IPv6 如 &lt;code&gt;localhost&lt;/code&gt; 这样的模板条目和主机名条目&lt;/p&gt;
&lt;!--
## Adding additional entries with hostAliases

In addition to the default boilerplate, you can add additional entries to the
`hosts` file.
For example: to resolve `foo.local`, `bar.local` to `127.0.0.1` and `foo.remote`,
`bar.remote` to `10.1.2.3`, you can configure HostAliases for a Pod under
`.spec.hostAliases`:



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkinghostaliases-podyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/hostaliases-pod.yaml&#34; download=&#34;service/networking/hostaliases-pod.yaml&#34;&gt;
                    &lt;code&gt;service/networking/hostaliases-pod.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkinghostaliases-podyaml&#39;)&#34; title=&#34;Copy service/networking/hostaliases-pod.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hostaliases-pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;hostAliases&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;ip&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;127.0.0.1&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostnames&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;foo.local&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bar.local&amp;#34;&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;ip&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10.1.2.3&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostnames&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;foo.remote&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bar.remote&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cat-hosts&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;cat&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/etc/hosts&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



You can start a Pod with that configuration by running:

```shell
kubectl apply -f https://k8s.io/examples/service/networking/hostaliases-pod.yaml
```

```
pod/hostaliases-pod created
```

Examine a Pod&#39;s details to see its IPv4 address and its status:

```shell
kubectl get pod --output=wide
```

```
NAME                           READY     STATUS      RESTARTS   AGE       IP              NODE
hostaliases-pod                0/1       Completed   0          6s        10.200.0.5      worker0
```

The `hosts` file content looks like this:

```shell
kubectl logs hostaliases-pod
```

```
# Kubernetes-managed hosts file.
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
fe00::0	ip6-mcastprefix
fe00::1	ip6-allnodes
fe00::2	ip6-allrouters
10.200.0.5	hostaliases-pod

# Entries added by HostAliases.
127.0.0.1	foo.local	bar.local
10.1.2.3	foo.remote	bar.remote
```

with the additional entries specified at the bottom. --&gt;
&lt;h2 id=&#34;通过-hostaliases-添加额外的条目&#34;&gt;通过 hostAliases 添加额外的条目&lt;/h2&gt;
&lt;p&gt;除了模板条目，可以手动向 &lt;code&gt;hosts&lt;/code&gt; 文件添加条目
例如下面的示例中通过 Pod 的 &lt;code&gt;.spec.hostAliases&lt;/code&gt; 字段实现配置：
将 &lt;code&gt;foo.local&lt;/code&gt;, &lt;code&gt;bar.local&lt;/code&gt; 解析到 &lt;code&gt;127.0.0.1&lt;/code&gt;，
将 &lt;code&gt;foo.remote&lt;/code&gt;, &lt;code&gt;bar.remote&lt;/code&gt; 解析到 &lt;code&gt;10.1.2.3&lt;/code&gt;&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkinghostaliases-podyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/hostaliases-pod.yaml&#34; download=&#34;service/networking/hostaliases-pod.yaml&#34;&gt;
                    &lt;code&gt;service/networking/hostaliases-pod.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkinghostaliases-podyaml&#39;)&#34; title=&#34;Copy service/networking/hostaliases-pod.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hostaliases-pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;hostAliases&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;ip&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;127.0.0.1&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostnames&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;foo.local&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bar.local&amp;#34;&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;ip&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10.1.2.3&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostnames&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;foo.remote&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bar.remote&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cat-hosts&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;cat&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/etc/hosts&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;用户可以通过以下命令，使用该配置启动一个 Pod:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f https://k8s.io/examples/service/networking/hostaliases-pod.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;pod/hostaliases-pod created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过以下命令查看 Pod 的信息，查看其 IPv4 地址及其状态:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pod --output&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;wide
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME                           READY     STATUS      RESTARTS   AGE       IP              NODE
hostaliases-pod                0/1       Completed   0          6s        10.200.0.5      worker0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过以下命令查看 &lt;code&gt;hosts&lt;/code&gt; 文件的内存:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl logs hostaliases-pod
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Kubernetes-managed hosts file.
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
fe00::0	ip6-mcastprefix
fe00::1	ip6-allnodes
fe00::2	ip6-allrouters
10.200.0.5	hostaliases-pod

# Entries added by HostAliases.
127.0.0.1	foo.local	bar.local
10.1.2.3	foo.remote	bar.remote
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;额外添加的条目的文件的底部。&lt;/p&gt;
&lt;!--
## Why does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}

The kubelet [manages](https://github.com/kubernetes/kubernetes/issues/14633) the
`hosts` file for each container of the Pod to prevent Docker from
[modifying](https://github.com/moby/moby/issues/17190) the file after the
containers have already been started.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; &lt;p&gt;Avoid making manual changes to the hosts file inside a container.&lt;/p&gt;
&lt;p&gt;If you make manual changes to the hosts file,
those changes are lost when the container exits.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;why-does-kubelet-manage-the-hosts-file&#34;&gt;为啥要 kubelet 管理 hosts 文件?&lt;/h2&gt;
&lt;p&gt;使用 kubelet &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/14633&#34;&gt;管理&lt;/a&gt; the
Pod 中每个容器的 &lt;code&gt;hosts&lt;/code&gt; 文件，是防止 Docker 在容器启动后再
&lt;a href=&#34;https://github.com/moby/moby/issues/17190&#34;&gt;修改&lt;/a&gt; 该文件。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; &lt;p&gt;避免手动修改容器中的 &lt;code&gt;hosts&lt;/code&gt; 文件&lt;/p&gt;
&lt;p&gt;如果手动修改了 &lt;code&gt;hosts&lt;/code&gt; 文件， 这些改动会在容器退出时丢失。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: IPv4/IPv6 双栈</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/dual-stack/</link>
      <pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/dual-stack/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- lachie83
- khenidak
- aramase
title: IPv4/IPv6 dual-stack
feature:
  title: IPv4/IPv6 dual-stack
  description: &gt;
    Allocation of IPv4 and IPv6 addresses to Pods and Services

content_type: concept
weight: 70
---
--&gt;
&lt;!-- overview --&gt;
&lt;!--





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [alpha]&lt;/code&gt;
&lt;/div&gt;



 IPv4/IPv6 dual-stack enables the allocation of both IPv4 and IPv6 addresses to &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; and &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Services&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;.

If you enable IPv4/IPv6 dual-stack networking for your Kubernetes cluster, the cluster will support the simultaneous assignment of both IPv4 and IPv6 addresses.
 --&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;IPv4/IPv6 双栈让 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 和
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 能够同时分配到 IPv4 和 IPv6 地址。&lt;/p&gt;
&lt;p&gt;如果在 k8s 集群中启用了 IPv4/IPv6 双栈网络，则集群支持同时分配 IPv4 和 IPv6 地址。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Supported Features

Enabling IPv4/IPv6 dual-stack on your Kubernetes cluster provides the following features:

   * Dual-stack Pod networking (a single IPv4 and IPv6 address assignment per Pod)
   * IPv4 and IPv6 enabled Services (each Service must be for a single address family)
   * Pod off-cluster egress routing (eg. the Internet) via both IPv4 and IPv6 interfaces
 --&gt;
&lt;h2 id=&#34;支持的特性&#34;&gt;支持的特性&lt;/h2&gt;
&lt;p&gt;在集群中开启 IPv4/IPv6 双栈可以提供以下特性:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod 双栈网络(每个 Pod 分配一个 IPv4 和 IPv6 地址)&lt;/li&gt;
&lt;li&gt;Service 启用 IPv4 和 IPv6 (每个 Service 只能是 IPv4 或 IPv6)&lt;/li&gt;
&lt;li&gt;Pod 的出站路由(如，到互联网)会同时通过 IPv4 和 IPv6 接口&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Prerequisites

The following prerequisites are needed in order to utilize IPv4/IPv6 dual-stack Kubernetes clusters:

   * Kubernetes 1.16 or later
   * Provider support for dual-stack networking (Cloud provider or otherwise must be able to provide Kubernetes nodes with routable IPv4/IPv6 network interfaces)
   * A network plugin that supports dual-stack (such as Kubenet or Calico)
 --&gt;
&lt;h2 id=&#34;前置条件&#34;&gt;前置条件&lt;/h2&gt;
&lt;p&gt;为集群启用 IPv4/IPv6 双栈需要做以下准备:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;k8s &lt;code&gt;v1.16+&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;提供商支持双栈网络(云提供商或其它的提供者必须要为 k8s 节点提供IPv4/IPv6网络接口)&lt;/li&gt;
&lt;li&gt;一个支持双栈的网络插件(如 Kubenet 或 Calico)&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Enable IPv4/IPv6 dual-stack

To enable IPv4/IPv6 dual-stack, enable the `IPv6DualStack` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) for the relevant components of your cluster, and set dual-stack cluster network assignments:

   * kube-apiserver:
      * `--feature-gates=&#34;IPv6DualStack=true&#34;`
      * `--service-cluster-ip-range=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;`
   * kube-controller-manager:
      * `--feature-gates=&#34;IPv6DualStack=true&#34;`
      * `--cluster-cidr=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;`
      * `--service-cluster-ip-range=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;`
      * `--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6` defaults to /24 for IPv4 and /64 for IPv6
   * kubelet:
      * `--feature-gates=&#34;IPv6DualStack=true&#34;`
   * kube-proxy:
      * `--cluster-cidr=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;`
      * `--feature-gates=&#34;IPv6DualStack=true&#34;`

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;An example of an IPv4 CIDR: &lt;code&gt;10.244.0.0/16&lt;/code&gt; (though you would supply your own address range)&lt;/p&gt;
&lt;p&gt;An example of an IPv6 CIDR: &lt;code&gt;fdXY:IJKL:MNOP:15::/64&lt;/code&gt; (this shows the format but is not a valid address - see &lt;a href=&#34;https://tools.ietf.org/html/rfc4193&#34;&gt;RFC 4193&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;启用-ipv4ipv6-双栈&#34;&gt;启用 IPv4/IPv6 双栈&lt;/h2&gt;
&lt;p&gt;要启用 IPv4/IPv6 双栈, 需要打开集群中的对应组件 &lt;code&gt;IPv6DualStack&lt;/code&gt; &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;
并设置集群双栈网络分配:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kube-apiserver:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--feature-gates=&amp;quot;IPv6DualStack=true&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--service-cluster-ip-range=&amp;lt;IPv4 CIDR&amp;gt;,&amp;lt;IPv6 CIDR&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;kube-controller-manager:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--feature-gates=&amp;quot;IPv6DualStack=true&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--cluster-cidr=&amp;lt;IPv4 CIDR&amp;gt;,&amp;lt;IPv6 CIDR&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--service-cluster-ip-range=&amp;lt;IPv4 CIDR&amp;gt;,&amp;lt;IPv6 CIDR&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6&lt;/code&gt; IPv4 默认为 /24；IPv6 默认为 /64&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;kubelet:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--feature-gates=&amp;quot;IPv6DualStack=true&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;kube-proxy:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--cluster-cidr=&amp;lt;IPv4 CIDR&amp;gt;,&amp;lt;IPv6 CIDR&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--feature-gates=&amp;quot;IPv6DualStack=true&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;一个 IPv4 CIDR 示例: &lt;code&gt;10.244.0.0/16&lt;/code&gt; (应该根据需要设置IP范围)&lt;/p&gt;
&lt;p&gt;一个 IPv6 CIDR 示例: &lt;code&gt;fdXY:IJKL:MNOP:15::/64&lt;/code&gt; (这里只显示格式，但不是一个有效的值 - 具体见 &lt;a href=&#34;https://tools.ietf.org/html/rfc4193&#34;&gt;RFC 4193&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Services

If your cluster has IPv4/IPv6 dual-stack networking enabled, you can create &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Services&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; with either an IPv4 or an IPv6 address. You can choose the address family for the Service&#39;s cluster IP by setting a field, `.spec.ipFamily`, on that Service.
You can only set this field when creating a new Service. Setting the `.spec.ipFamily` field is optional and should only be used if you plan to enable IPv4 and IPv6 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Services&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; and &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/ingress/&#39; target=&#39;_blank&#39;&gt;Ingresses&lt;span class=&#39;tooltip-text&#39;&gt;一个用于管理外部访问集群内 Service 的 API 对象，通常是 HTTP。&lt;/span&gt;
&lt;/a&gt; on your cluster. The configuration of this field not a requirement for [egress](#egress-traffic) traffic.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The default address family for your cluster is the address family of the first service cluster IP range configured via the &lt;code&gt;--service-cluster-ip-range&lt;/code&gt; flag to the kube-controller-manager.&lt;/div&gt;
&lt;/blockquote&gt;


You can set `.spec.ipFamily` to either:

   * `IPv4`: The API server will assign an IP from a `service-cluster-ip-range` that is `ipv4`
   * `IPv6`: The API server will assign an IP from a `service-cluster-ip-range` that is `ipv6`

The following Service specification does not include the `ipFamily` field. Kubernetes will assign an IP address (also known as a &#34;cluster IP&#34;) from the first configured `service-cluster-ip-range` to this Service.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingdual-stack-default-svcyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/dual-stack-default-svc.yaml&#34; download=&#34;service/networking/dual-stack-default-svc.yaml&#34;&gt;
                    &lt;code&gt;service/networking/dual-stack-default-svc.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingdual-stack-default-svcyaml&#39;)&#34; title=&#34;Copy service/networking/dual-stack-default-svc.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



The following Service specification includes the `ipFamily` field. Kubernetes will assign an IPv6 address (also known as a &#34;cluster IP&#34;) from the configured `service-cluster-ip-range` to this Service.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingdual-stack-ipv6-svcyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/dual-stack-ipv6-svc.yaml&#34; download=&#34;service/networking/dual-stack-ipv6-svc.yaml&#34;&gt;
                    &lt;code&gt;service/networking/dual-stack-ipv6-svc.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingdual-stack-ipv6-svcyaml&#39;)&#34; title=&#34;Copy service/networking/dual-stack-ipv6-svc.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ipFamily&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IPv6&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



For comparison, the following Service specification will be assigned an IPv4 address (also known as a &#34;cluster IP&#34;) from the configured `service-cluster-ip-range` to this Service.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingdual-stack-ipv4-svcyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/dual-stack-ipv4-svc.yaml&#34; download=&#34;service/networking/dual-stack-ipv4-svc.yaml&#34;&gt;
                    &lt;code&gt;service/networking/dual-stack-ipv4-svc.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingdual-stack-ipv4-svcyaml&#39;)&#34; title=&#34;Copy service/networking/dual-stack-ipv4-svc.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ipFamily&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IPv4&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


 --&gt;
&lt;h2 id=&#34;service&#34;&gt;Service&lt;/h2&gt;
&lt;p&gt;如果集群启用的了 IPv4/IPv6 双栈网络，就可以创建一个带 IPv4 或 IPv6 地址的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;。
可以通过 Service 的 &lt;code&gt;.spec.ipFamily&lt;/code&gt; 来设置该 Service 是使用 IPv4 还是 IPv6 地址。
只有在创建一个新的 Service 才可以设置该字段。 &lt;code&gt;.spec.ipFamily&lt;/code&gt; 是一个可选字段，只有在需要
在 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 和 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/ingress/&#39; target=&#39;_blank&#39;&gt;Ingress&lt;span class=&#39;tooltip-text&#39;&gt;一个用于管理外部访问集群内 Service 的 API 对象，通常是 HTTP。&lt;/span&gt;
&lt;/a&gt;
上启用 IPv4 和 IPv6 时才需要配置该字段。 (这里还有一句不太明白在说啥)&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 集群默认使用的 IP 族是 kube-controller-manager 上设置 &lt;code&gt;--service-cluster-ip-range&lt;/code&gt;的
值中，前面那一个 CIDR 对应的 IP 族。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;.spec.ipFamily&lt;/code&gt; 字段的值可以是:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;IPv4&lt;/code&gt;: API server 会从 &lt;code&gt;service-cluster-ip-range&lt;/code&gt; 中分配一个 &lt;code&gt;ipv4&lt;/code&gt; 地址&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IPv6&lt;/code&gt;: API server 会从 &lt;code&gt;service-cluster-ip-range&lt;/code&gt; 中分配一个 &lt;code&gt;ipv6&lt;/code&gt; 地址&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面的这个 Service 配置文件中没有包含 &lt;code&gt;ipFamily&lt;/code&gt; 字段。 k8s 会为它分配一个 IP 地址
(也就是集群IP)，这个地址是 &lt;code&gt;service-cluster-ip-range&lt;/code&gt; 配置值中第一个个值所定义的 IP 范围内的一个 IP 地址&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingdual-stack-default-svcyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/dual-stack-default-svc.yaml&#34; download=&#34;service/networking/dual-stack-default-svc.yaml&#34;&gt;
                    &lt;code&gt;service/networking/dual-stack-default-svc.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingdual-stack-default-svcyaml&#39;)&#34; title=&#34;Copy service/networking/dual-stack-default-svc.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;下面的这个 Service 配置文件中包含 &lt;code&gt;ipFamily&lt;/code&gt; 字段。 k8s 会为它分配一个 IPv6 地址(也是集群IP)
这个地址在 &lt;code&gt;service-cluster-ip-range&lt;/code&gt; 配置的范围内&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingdual-stack-ipv6-svcyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/dual-stack-ipv6-svc.yaml&#34; download=&#34;service/networking/dual-stack-ipv6-svc.yaml&#34;&gt;
                    &lt;code&gt;service/networking/dual-stack-ipv6-svc.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingdual-stack-ipv6-svcyaml&#39;)&#34; title=&#34;Copy service/networking/dual-stack-ipv6-svc.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ipFamily&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IPv6&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;为了对应，下面的这个 Service 的配置会被分配一个 IPv4 地址(作为集群IP)，这个地址在 &lt;code&gt;service-cluster-ip-range&lt;/code&gt; 配置的范围内&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingdual-stack-ipv4-svcyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/dual-stack-ipv4-svc.yaml&#34; download=&#34;service/networking/dual-stack-ipv4-svc.yaml&#34;&gt;
                    &lt;code&gt;service/networking/dual-stack-ipv4-svc.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingdual-stack-ipv4-svcyaml&#39;)&#34; title=&#34;Copy service/networking/dual-stack-ipv4-svc.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ipFamily&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IPv4&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;!--
### Type LoadBalancer

On cloud providers which support IPv6 enabled external load balancers, setting the `type` field to `LoadBalancer` in additional to setting `ipFamily` field to `IPv6` provisions a cloud load balancer for your Service.
 --&gt;
&lt;h3 id=&#34;type-loadbalancer&#34;&gt;Type LoadBalancer&lt;/h3&gt;
&lt;p&gt;在支持 IPv6 的云提供商上启用外部负载均衡器时，在 Service 上设置 &lt;code&gt;type&lt;/code&gt; 字段值为 &lt;code&gt;LoadBalancer&lt;/code&gt; 时，同
时还需要设置 &lt;code&gt;ipFamily&lt;/code&gt; 字段的值为 &lt;code&gt;IPv6&lt;/code&gt;。&lt;/p&gt;
&lt;!--
## Egress Traffic

The use of publicly routable and non-publicly routable IPv6 address blocks is acceptable provided the underlying &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni&#39; target=&#39;_blank&#39;&gt;CNI&lt;span class=&#39;tooltip-text&#39;&gt;Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.&lt;/span&gt;
&lt;/a&gt; provider is able to implement the transport. If you have a Pod that uses non-publicly routable IPv6 and want that Pod to reach off-cluster destinations (eg. the public Internet), you must set up IP masquerading for the egress traffic and any replies. The [ip-masq-agent](https://github.com/kubernetes-incubator/ip-masq-agent) is dual-stack aware, so you can use ip-masq-agent for IP masquerading on dual-stack clusters.
 --&gt;
&lt;h2 id=&#34;egress-traffic&#34;&gt;Egress Traffic&lt;/h2&gt;
&lt;p&gt;底层实现的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni&#39; target=&#39;_blank&#39;&gt;CNI&lt;span class=&#39;tooltip-text&#39;&gt;Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.&lt;/span&gt;
&lt;/a&gt; 提供者可以提供对
公网可路由和公网不可路由的 IPv6 地址段。如果 Pod 使用的是公网不可路由的 IPv6 地址，但要求
这个 Pod 可以访问集群外的目标(如，互联网)， 需要为外出流量及其应答做 IP 地址转换。
&lt;a href=&#34;https://github.com/kubernetes-incubator/ip-masq-agent&#34;&gt;ip-masq-agent&lt;/a&gt; 支持双栈，
所以可以使用它在双栈集群中做地址转换。&lt;/p&gt;
&lt;!--
## Known Issues

   * Kubenet forces IPv4,IPv6 positional reporting of IPs (--cluster-cidr)
 --&gt;
&lt;h2 id=&#34;已知问题&#34;&gt;已知问题&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Kubenet forces IPv4,IPv6 positional reporting of IPs (&amp;ndash;cluster-cidr)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;kubenet 强制 IPv4,IPv6 位置汇报  (&amp;ndash;cluster-cidr)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/network/validate-dual-stack&#34;&gt;验证 IPv4/IPv6 双栈网络&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 节点</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/architecture/nodes/</link>
      <pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/architecture/nodes/</guid>
      <description>
        
        
        &lt;p&gt;k8s 将用户的工作负载窗口塞进 Pod 里然后运行在节点上，根据集群节点可以是虚拟机或物理机，每个节点必须要有运行 Pod 所需要的服务，并由 k8s 控制中心管理。
一般情况下一个集群会有多个节点; 在资源受限或学习的环境，可能只有一个节点。
每个节点包含的组件有 kubelet, 容器运行环境, kube-proxy&lt;/p&gt;
&lt;h2 id=&#34;节点管理&#34;&gt;节点管理&lt;/h2&gt;
&lt;p&gt;向 api-server 添加节点的方式有以下两种:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;节点上的 kubelet 服务自动注册到控制中心&lt;/li&gt;
&lt;li&gt;管理员用户手动添加节点对象&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;当管理员用户创建节点对象或 kubelet 将节点自动注册，控制中心会检查新创建的新节点是否有效。例如，可以通过以下 JSON 创建一个新节点&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;{
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;kind&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Node&amp;#34;&lt;/span&gt;,
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;apiVersion&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;v1&amp;#34;&lt;/span&gt;,
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;metadata&amp;#34;&lt;/span&gt;: {
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10.240.79.157&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;labels&amp;#34;&lt;/span&gt;: {
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;my-first-k8s-node&amp;#34;&lt;/span&gt;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;k8s 节点是内部创建的. k8s 检查通过 kubelet 注册到 api-server 的节点. 如果节点是健康的 (当所有必要的服务都正常运行), 这个节点就可以用来运行 Pod, 否则 这个节点状态在变为健康之前，这个节点会被所有集群活动忽略.&lt;/p&gt;
&lt;p&gt;注意: k8s 会一直保留无效的节点并持续检查这个节点是否变更为健康. 用户或控制器必须要显示的删除这个节点对象，这种检测才会停止。&lt;/p&gt;
&lt;p&gt;节点的名称必须要是一个有效的&lt;a href=&#34;../../00-overview/03-working-with-objects/01-names/#DNS%20%E5%AD%90%E5%9F%9F%E5%90%8D&#34;&gt;DNS 子域名&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;节点的自注册&#34;&gt;节点的自注册&lt;/h3&gt;
&lt;p&gt;当 kubelet 参数 &lt;code&gt;--register-node&lt;/code&gt; 设置为 &lt;code&gt;true&lt;/code&gt; 时(默认值)， kubelet 会自动把所在节点注册到 api-server. 这是首选的配置方式，被多数集群搭建工具使用。&lt;/p&gt;
&lt;p&gt;对于自注册的节点， kubelet 需要以下配置参数:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--kubeconfig&lt;/code&gt;  节点在 api-server 认证凭据所在目录&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--cloud-provider&lt;/code&gt; 怎么从云提供商获取节点元数据&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--register-node&lt;/code&gt; 自动注册到 api-server&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--register-with-taints&lt;/code&gt; 为注册节点添加 taints (格式为 &lt;key&gt;=&lt;value&gt;:&lt;effect&gt;，多个由逗号分隔)，如果 register-node 值设置为 false 则，该配置无操作&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--node-ip&lt;/code&gt; 节点的 IP 地址&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-node-labels&lt;/code&gt; 当节点注册是，添加到节点上的标签&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--node-status-update-frequency&lt;/code&gt; kubectl 向控制中心报告状态的频率&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当 &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/node/&#34;&gt;Node authorization mode&lt;/a&gt; 和 &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#noderestriction&#34;&gt;NodeRestriction admission&lt;/a&gt; 插件打开进, kubelet 只被授权创建/修改自己的节点资源&lt;/p&gt;
&lt;h3 id=&#34;节点手动管理&#34;&gt;节点手动管理&lt;/h3&gt;
&lt;p&gt;用户可以通过 kubectl 创建和修改节点对象
当用户需要手动创建一个节点对象时，需要 kubelet 设置参数  &lt;code&gt;--register-node=false&lt;/code&gt;
也可修改节点配置忽略 &lt;code&gt;--register-node&lt;/code&gt; 配置。 例如可以在存在的节点上设置标签或将节点标记为不可调度
可以通过节点的标签和Pod的节点标签选择器来控制调度。 例如， 限制某个 Pod 只能在某些节点上运行
标记一个节点为不可调度后，就会阻止调度器再向这个节点调度新的 Pod ， 但不会影响到节点上已经在运行的 Pod。 以下命令将标记指定节点为不可调度:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl cordon $NODENAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意: 属于 DaemonSet 的 Pod 会运行在不可调度的节点上， 因为 &lt;code&gt;DaemonSets&lt;/code&gt; 通常提供节点本地服务，所以即使节点被清空应用工作负载也应该运行在节点&lt;/p&gt;
&lt;h2 id=&#34;节点状态&#34;&gt;节点状态&lt;/h2&gt;
&lt;p&gt;节点状态包含以下信息:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;地址&lt;/li&gt;
&lt;li&gt;条件&lt;/li&gt;
&lt;li&gt;容量和可分配状态&lt;/li&gt;
&lt;li&gt;其它信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以通过以命令查看节点状态与其它更多信息:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl describe node &amp;lt;insert-node-name-here&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;每个部分详细信息&lt;/p&gt;
&lt;h3 id=&#34;地址&#34;&gt;地址&lt;/h3&gt;
&lt;p&gt;这些字段会根据节点是云提供或裸金属和等的不同而不同&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主机名: 由节点的内核提供，可以通过 kubelet 的 &amp;ndash;hostname-override 覆盖&lt;/li&gt;
&lt;li&gt;外部IP: 在外部网络(集群之外)路由可达的IP地址&lt;/li&gt;
&lt;li&gt;内部IP: 只在集群内部路由可达的IP地址&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;条件&#34;&gt;条件&lt;/h3&gt;
&lt;p&gt;conditions 字段描述的是 所有状态为 Running 的节点， 示例如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ready&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;True 则表示节点健康，可以接收调度 Pod&lt;/li&gt;
&lt;li&gt;False 则表示节点状不健康，不可接收 Pod&lt;/li&gt;
&lt;li&gt;Unknown node 控制器在最近一个 &lt;code&gt;node-monitor-grace-period&lt;/code&gt; 内(默认40s)没有收到节点的心跳&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DiskPressure&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;True 磁盘存储剩余空间紧张&lt;/li&gt;
&lt;li&gt;False 磁盘存储剩余空间充足&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MemoryPressure&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;True 节点内存紧张&lt;/li&gt;
&lt;li&gt;False 节点内存充足&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PIDPressure&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;True 节点上的进程太多了&lt;/li&gt;
&lt;li&gt;False 节点上进程数量适度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;NetworkUnavailable&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;True 节点网络配置错误&lt;/li&gt;
&lt;li&gt;False 节点网络配置正常&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注意: 通过命令行工具打印清楚节点的详细信息中， 条件信息中包含 &lt;code&gt;SchedulingDisabled&lt;/code&gt;， 但 SchedulingDisabled 不属于 k8s API, 而是节点被标记为不可调度&lt;/p&gt;
&lt;p&gt;节点的条件可以表现为JSON对象，如下是一个健康的节点的示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conditions&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;:&lt;/span&gt; [
  {
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Ready&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;status&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;True&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;reason&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;KubeletReady&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;message&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubelet is posting ready status&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;lastHeartbeatTime&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2019-06-05T18:38:35Z&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;lastTransitionTime&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2019-06-05T11:41:27Z&amp;#34;&lt;/span&gt;
  }
]

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果 &lt;code&gt;Ready&lt;/code&gt; 条件的状态为 &lt;code&gt;Unknown&lt;/code&gt; 或 &lt;code&gt;False&lt;/code&gt; 持续时间超过 &lt;code&gt;pod-eviction-timeout&lt;/code&gt; (通过  kube-controller-manager 参数配置)， 这个节点上所以的 Pod 都会被节点控制器调度为删除。 默认的踢除的超时时间为5分钟。 在某些情况下，当一个节点不可达时，api-server 就不能与节点上的 kubelet 进行通信。 在 kubelet 与 api-server 重新建立连接之前对 Pod 的删除指令传达不到 kubelet. 在这段时间内这些被调度为删除的节点可以继续在分区节点上运行。&lt;/p&gt;
&lt;p&gt;节点控制器在确认 Pod 在集群中已经停止运行前是不会强制删除的。 所以可以会出现 Pod 运行在状态为  &lt;code&gt;Terminating&lt;/code&gt; 或 &lt;code&gt;Unknown&lt;/code&gt; 的不可达节点上。 有时候 k8s 不能在节点被永久移出集群后不能从基础设施自动的移除，需要管理员手动地删除对应的节点对象。从 k8s 中删除节点对象时，会同时从 api-server 中对应删除节点上运行的所有 Pod 对象，并释放其名称&lt;/p&gt;
&lt;p&gt;节点的生命周期管理器会自动创建代理节点状态的 &lt;code&gt;Taint&lt;/code&gt;. 调度器在分配 Pod 到节点时会顾及节点上的 Taint. Pod 也可以配置容忍节点的某些 &lt;code&gt;Taint&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;了解更多 Taint 相关信息见&lt;a href=&#34;../../09-scheduling-eviction/01-taint-and-toleration.md&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;容量和可分配状态&#34;&gt;容量和可分配状态&lt;/h3&gt;
&lt;p&gt;表示节点上的可用资源: CPU, 内存， 节点可接受 Pod 数量的最大值
容量(capacity)块下的字段表示节点资源总数
可分配状态(allocatable)块下的字段表示可用于普通 Pod 的资源数&lt;/p&gt;
&lt;p&gt;了解更多关于 容量和可分配状态 的信息见&lt;a href=&#34;../../../3-tasks/01-administer-cluster/28-reserve-compute-resources.md&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;其它信息&#34;&gt;其它信息&lt;/h3&gt;
&lt;p&gt;表示邛的通用信息，如 内核版本， k8s 版本(kubelet 和 kube-proxy 的版本)， Docker 版本， OS 名称。 这些信息都是节点上的 kubelet 生成的&lt;/p&gt;
&lt;h3 id=&#34;节点控制器&#34;&gt;节点控制器&lt;/h3&gt;
&lt;p&gt;节点控制器是 k8s 控制中心的组成部分，用于管理节点的各方面功能
节点控制器在节点的生命周期类扮演多个角色。 第一个就是为节点分配 CIDR 段(当 CIDR 分配开启时)
第二个是保持节点控制器内部的节点列表与云提供商提供的可用机器列表一致， 在运行在云环境时，当一个节点状态变为不健康时， 节点控制器会向云提供商查询该节点的虚拟机是否可用。 如果不可用就会从列表中删除该节点
第三个是监控节点状态， 节点控制器负责在节点变得不可达时(如， 因为某些原因收不到心跳，比如节点宕机)，更新节点就绪状态为 &lt;code&gt;ConditionUnknown&lt;/code&gt;， 如果节点持续不可达则踢出节点上所有的Pod(使用优雅终结方式)。设置就绪状态的不可达时间为 40s, 踢除 Pod 的时间为 5 分钟。节点控制器检查节点状态的时间由 &lt;code&gt;--node-monitor-period&lt;/code&gt; 配置&lt;/p&gt;
&lt;h4 id=&#34;心跳&#34;&gt;心跳&lt;/h4&gt;
&lt;p&gt;心跳由k8s 节点发送，用于帮助判定节点是否可用
心跳的形式有两种， 一个更新 &lt;code&gt;NodeStatus&lt;/code&gt; 另一个为租约对象。每个节点在 kube-node-lease 命名空间中有一个关联的租约对象。 租约是一个轻量级资源， 用户在集群范围内改善心跳的性能
由 kubelet 负责创建更新 &lt;code&gt;NodeStatus&lt;/code&gt; 和 租约对象。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kubelet  在状态发生改变或在配置的时间间隔内没有更新时更新 &lt;code&gt;NodeStatus&lt;/code&gt;， &lt;code&gt;NodeStatus&lt;/code&gt; 更新的默认的更新间隔为 5 分钟(比不可达节点默认超时的 40 秒长很多)&lt;/li&gt;
&lt;li&gt;kubelet 创建随后每隔10秒(默认时间间隔)更新租约对象。 租约对象的更新独立与 &lt;code&gt;NodeStatus&lt;/code&gt; 更新。 如果租约更新失败， kubelet 使用从 200ms 到 7s 间的指数组补尝&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;可靠性&#34;&gt;可靠性&lt;/h4&gt;
&lt;p&gt;大多数情况下， 节点控制器限制踢除速率由 &lt;code&gt;--node-eviction-rate&lt;/code&gt;(每秒) 配置， 默认 &lt;code&gt;0.1&lt;/code&gt;，即每10秒最多能踢除一个 Pod。
节点的踢除行为会在节点所有的可用区变为不健康时发生改变。 节点控制器会检查区域内同一时间不健康(&lt;code&gt;NodeReady&lt;/code&gt; 条件为 &lt;code&gt;ConditionUnknown&lt;/code&gt; 或 &lt;code&gt;ConditionFalse&lt;/code&gt;)节点的百分比。 如果不健康的节点比达达到 &lt;code&gt;--unhealthy-zone-threshold&lt;/code&gt; (默认 0.55)时踢除速率会降低: 如果集群较小(不多于 &lt;code&gt;--large-cluster-size-threshold&lt;/code&gt; 节点, 默认 50)，踢除行为停止。否则踢除速率降低至 &lt;code&gt;--secondary-node-eviction-rate&lt;/code&gt; (默认 0.01)每秒。 这个策略会在每个分区实现因为这个可用区可能与集群分隔， 但其它部分仍正常连接。 如果集群不是分散在多个云提供商的可用区，则只有一个可用区(即整个集群)。&lt;/p&gt;
&lt;p&gt;将节点分散在不同可用区的主要原因就当一个可用区整体不可用时，可以将工作负载转移到另一个可用区。 如果一个可用区的节点都不可用时节点控制器使用正常踢除速率(&lt;code&gt;--node-eviction-rate&lt;/code&gt;). 极限情况是当所有的可用区全部变得不可用时(整个集群每有一个健康节点)， 在这种情况下节点控制器认为主节点有连接问题并停止踢除行为直到连接恢复。&lt;/p&gt;
&lt;p&gt;节点控制器负责踢除包含 NoExecute Taint 节点运行的 Pod, 除了包含容忍该 Taint 的 Pod。 节点控制器会为有问题(如不可达或未就绪的节点)添加相应的 Taint, 也就是说调度器不会向不健康的节点放置 Pod.&lt;/p&gt;
&lt;p&gt;警告: &lt;code&gt;kubectl cordon&lt;/code&gt; 命令标记一个节点为不可调度，而其作为是 服务控制器会将节点从所有负载列表中移除， 并再向该节点调度流量。&lt;/p&gt;
&lt;h4 id=&#34;节点容量&#34;&gt;节点容量&lt;/h4&gt;
&lt;p&gt;节点对象记录了节点的资源容量(如: 可用内存，CPU核心数)。 自动注册的节点在注册时会报告其容量，如果节点是手动添加，则需要在添加时提供对应容量信息。
k8s 调度器保证节点有足够的资源运行分配到其上的 Pod， 调度器检测节点不所有容器请求的资源不大于节点的容量。 请求资源总和包括由kubelet 管理的所有容器， 但不包括直接由容器运行环境启动的容器。也不包括其它所有不被kubelet 控制的进程。
注意: 如果需要为非 Pod 进程保留资源，见&lt;a href=&#34;../../../3-tasks/01-administer-cluster/28-reserve-compute-resources/&#34;&gt;为系统进程保留资源&lt;/a&gt;,系统保留部分。&lt;/p&gt;
&lt;h2 id=&#34;节点拓扑&#34;&gt;节点拓扑&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;如果通过&lt;a href=&#34;../../../reference/command-line-tools-reference/feature-gates/&#34;&gt;功能特性开关&lt;/a&gt;，开启了 &lt;code&gt;TopologyManager&lt;/code&gt;， kubelet 在作资源分配决策时会参考拓扑信息。更多信息， 见&lt;a href=&#34;../../../3-tasks/01-administer-cluster/14-topology-manager/&#34;&gt;节点拓扑控制管理策略&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 控制中心与节点的通信</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/architecture/control-plane-node-communication/</link>
      <pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/architecture/control-plane-node-communication/</guid>
      <description>
        
        
        &lt;p&gt;本文主要介结控制中心(实际就是 api-server)与 k8s 集群通信的几种途径, 目的是使用户可以选择健壮的网络配置使集群可以运行在不受信的网络环境中(或在云提供商的公网IP上)&lt;/p&gt;
&lt;h2 id=&#34;从节点到控制中心的通信&#34;&gt;从节点到控制中心的通信&lt;/h2&gt;
&lt;p&gt;k8s 使用轴辐式(hub-and-spoke) API 模式，所有节点(包括节点上运行的Pod)使用到的API都指向 api-server(其它所有的组件在设计上就不提供远程服务)。api-server 配置成安全的HTTPS端口(通常是 443)来对外提供服务，并且开启一种或多种&lt;a href=&#34;../../../reference/03-access-authn-authz/01-authentication/&#34;&gt;认证&lt;/a&gt;方式。还需要开启一种或多种&lt;a href=&#34;../../../reference/03-access-authn-authz/07-authorization/&#34;&gt;授权&lt;/a&gt;方式， 特别是 匿名请求 或 service account tokens 可用(具体见认证相关部分)。
每个节点上都需要有集群的公开根证书，这样节点才能通过有效的客户凭据安全的连接到 api-server. 例如， 在默认的 GKE 部署中， 客户端为 kubelet 提供的凭据格式为客户端证书， 自动化提供客户端证书的方式见&lt;a href=&#34;../../../reference/command-line-tools-reference/08-kubelet-tls-bootstrapping/&#34;&gt;这里&lt;/a&gt;
如果集群中的Pod想要连接到 api-server 可以借助&lt;code&gt;service account&lt;/code&gt;实现安全连接， k8s 会在 Pod 启动时自动注入公开根谈不上和令牌.
在每个命名空间下有一个叫 &lt;code&gt;kubernetes&lt;/code&gt; 的 Service， 指向一个虚拟IP地址，并重写向(通过 kube-proxy)向 api-server 的HTTPS端口上。
控制中心组件也是通过安全端口与集群的 api-server 通信。
在默认的操作模式下，节点和节点上的Pod 与控制中心的连接默认就是安全的可以信赖于不受信的网络环境中&lt;/p&gt;
&lt;h2 id=&#34;从控制中心向节点的通信&#34;&gt;从控制中心向节点的通信&lt;/h2&gt;
&lt;p&gt;由控制中心(api-server)向节点的通信路径注要有两条， 第一条从 api-server 直接连接到集群中每个节点上的 kubelet 进程。 第二条是通过 api-server 的代理功能来实现 api-server 到任意节点， Pod， Service的连接&lt;/p&gt;
&lt;h3 id=&#34;从-api-server-连接到-kubelet&#34;&gt;从 api-server 连接到 kubelet&lt;/h3&gt;
&lt;p&gt;从 api-server 到 kubelet 通信主要有以下用途:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;摘取 Pod 日志&lt;/li&gt;
&lt;li&gt;通过 kubelet 终端连接到运行的Pod&lt;/li&gt;
&lt;li&gt;为 kubelet 提供端口转发功能&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;连接指向的是 kubelet 的 HTTPS 端口， 默认 api-server 不验证 kubelet 提供的证书，这样会存在中间人攻击的风险，在不受网络环境中是不安全的。&lt;/p&gt;
&lt;p&gt;要验证这个连接，需要在 api-server 配置启动参数 &lt;code&gt;--kubelet-certificate-authority&lt;/code&gt; 指向 根证书用于验证 kubelet 的服务证书。
如果做不到， 则在非受信网络或公网中使用 SSH 遂道连接 api-server 和 kubelet.
最后，需要开启 &lt;a href=&#34;../../../reference/command-line-tools-reference/07-kubelet-authentication-authorization/&#34;&gt;kubelet 认证和授权&lt;/a&gt;，以保护 kubelet API 安全。&lt;/p&gt;
&lt;h3 id=&#34;从-api-server-到-kubelet-节点-pod--service-的连接&#34;&gt;从 api-server 到 kubelet, 节点, Pod,  Service 的连接&lt;/h3&gt;
&lt;p&gt;默认情况下 从 api-server 到 kubelet, 节点, Pod,  Service 的连接是通过 HTTP 明文，因此没有认证也没加密。在连接到 节点， Pod, Service 名称对应的 API URL时可以添加 https 前缀来使用 HTTPS 来建立安全连接，但不会验证HTTPS证书也不验证客户端提供的凭据。 因此也不能保证任何完整性。 所以目前这能连接都不能用于非受信网络或公网。&lt;/p&gt;
&lt;h3 id=&#34;ssh-遂道&#34;&gt;SSH 遂道&lt;/h3&gt;
&lt;p&gt;k8s 支持通过 SSH 遂道的方式来实现从 控制中心到节点的连接路径的安全。 在这个配置中， 由 api-server 来初始化 SSH 遂道连接到集群中每个节点(连接到ssh服务监听端口22)，并通过这个连接来传输 kubelet, 节点, Pod,  Service 所有流量， 这样可以使流量不会显露给节点运行的外部网络&lt;/p&gt;
&lt;p&gt;SSH 遂道连接方式当前已经被废弃，用户在开启该功能需要清楚为什么为开启。  Konnectivity 服务是替代方案&lt;/p&gt;
&lt;h3 id=&#34;konnectivity-服务&#34;&gt;Konnectivity 服务&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;作为 SSH 遂道 的替代方案， Konnectivity 服务 通过提供 TCP 层的代理来实现 控制中心与集群之间的通信。 Konnectivity 服务主要由两部分组成， Konnectivity 服务端和Konnectivity 代理程序，分别运行在 控制中心网络和节点网络上。 Konnectivity 代理程序初始化并维护到服务端的连接。 在开启 Konnectivity 服务后，控制中心到节的所有连接都通过这些连接。
在集群中开启 Konnectivity 服务见 &lt;a href=&#34;../../../3-tasks/09-extend-kubernetes/01-setup-konnectivity/&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 控制器</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/architecture/controller/</link>
      <pubDate>Thu, 09 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/architecture/controller/</guid>
      <description>
        
        
        &lt;p&gt;在机器人技术和自动化领域，一个控制回路就是一个控制系统状态的无限循环。
以下为控制回路的一个示例: 房间内的温度控制器
当用户设定一个温度值时，就是告知温度控制器其期望状态。 当时房间内实际的问题则是当前状态。 温度控制器就会启动相应的动作(开启或关闭相应的设备)来让当前状态逐渐趋近于期望状态。&lt;/p&gt;
&lt;p&gt;在 k8s 中，控制器就是监控集群状态的控制循环，在需要的时候执行变更或请求其它服务执行变更。每一个控制器都在尝试让当前状态向期望状态演进。&lt;/p&gt;
&lt;h2 id=&#34;control-pattern&#34;&gt;控制模式&lt;/h2&gt;
&lt;p&gt;一个控制器至少会跟踪一个k8s 资源类型。 这些&lt;a href=&#34;../../00-overview/03-working-with-objects/00-kubernetes-objects/&#34;&gt;对象&lt;/a&gt;都有一个 &lt;code&gt;spec&lt;/code&gt; 字段来定义它的期望状态。该资源所对应的控制器就负责让当前状态逐渐趋近于期望状态。控制器可以直接采取动作实现状态的变更，但在 k8s 中，通常是一个控制器会向 &lt;code&gt;api-server&lt;/code&gt; 发送消息来达到这个目的。接下来可以会有实例。&lt;/p&gt;
&lt;h3 id=&#34;通过-api-server-实现控制&#34;&gt;通过 api-server 实现控制&lt;/h3&gt;
&lt;p&gt;Job 控制器就是 k8s 内置控制器的一员. 内置控制器就是通过与集群 api-server 来实现状态管理.
Job 这种 k8s 资源类型的工作模型是，运行一个或多个 Pod 来完成某个任务，完成后就停止。
(一旦完成&lt;a href=&#34;../../09-scheduling-eviction/&#34;&gt;调度&lt;/a&gt;， Pod对蟓就会成为 &lt;code&gt;kubelet&lt;/code&gt; 期望状态的组成部署)&lt;/p&gt;
&lt;p&gt;当 Job 控制器接收到一个新的任务，就需要按照任务的需求在集群中的某些节点上运行指定数量的 Pod 来完成相应的任务。 但 Job 控制器本身并不会运行任意 Pod 或容器。 而是向 api-server 发送 创建或删除 Pod 的请求。控制中心中的其它组件就会根据请求信息(有新的 Pod 需要调度并运行)，然后最终完成任务。&lt;/p&gt;
&lt;p&gt;在用户创建一个新的 Job 后，此时的期望状态就是完成这个 Job。 Job 控制器让当前状态逐渐趋近于期望状态： 创建 Job 需要完成任务的 Pod，此时就离完成 Job 进了不步。&lt;/p&gt;
&lt;p&gt;控制器也会更新那些对控制器进行配置的对象。 例如： 当一个 Job 对应的任务完成后， Job 控制器更新对应 Job 的对象状态为 &lt;code&gt;Finished&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;(就像开时提到的温度控制器有点类似，温度的控制器关闭指示灯表示当前房间内的温度达到用户设置所期望的温度)&lt;/p&gt;
&lt;h3 id=&#34;直接控制&#34;&gt;直接控制&lt;/h3&gt;
&lt;p&gt;与 Job 控制方式不同， 有些控制需要对集群外的部分组成资源进行变更。
例如，用户使用一个控制回路来保证集群中有足够的节点， 此时这个控制器就需要一些集群外的资源来实现对节点的配置管理。
控制器也是通过 api-server 来获取外部资源的期望状态，然后再直接与外部系统通信让其状态实现向期望状态的迁移。
(实际就有一个控制器可以实现集群节点的水平扩展， 见 &lt;a href=&#34;../../../3-tasks/01-administer-cluster/10-cluster-management/#cluster-autoscaling&#34;&gt;集群扩容&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;期望状态与当前状态&#34;&gt;期望状态与当前状态&lt;/h2&gt;
&lt;p&gt;k8s 是一个云原生的系统，能够处理持续不断的变更请求。
集群可以在任何时候发变更， 控制回路会自动的修复出现的问题。 也就是从始至终集群可能都不会达到一个稳定的状态。
当控制器都在正常运行且能够过成有效的变更，全局状态是否稳定就无关紧要了
(这段意思表达得不是很顺畅)&lt;/p&gt;
&lt;h2 id=&#34;设计&#34;&gt;设计&lt;/h2&gt;
&lt;p&gt;按照设计宗旨， k8s 使用很多控制器，每个控制器管理特定方面的集群状态。大多数情况下， 一个特定的控制回路(控制器)使用一种类型的资源作为期望状态，并通过管理多种类型的资源来实现期望状态。 例如：一个 Job 控制器跟踪 Job 对象(发现新发布的任务)和 Pod 对象(用来运行任务和监测任务是否完成，什么时候完成)。 在这种情况下，其它组件创建 Job， Job 控制器再创建 Pod。
使用多个简单的的控制器而不是一个内部关联的单体控制回路集合理有优势。 因为控制器可能挂掉，k8s 在设计上也是允许这种情况发生的。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 一般可以有多个控制器创建或更新同一个类型的对象。 在这种情况下， k8s 控制器保证只关注与控制资源关联的资源。 例如： 同时使用  Deployment 和 Job， 两者都会创建 Pod， 但 Job 的控制器不能删除 Deployment 创建的 Pod， 因为控制器可以傅秀对象包含的信息(标签)来区分各自不同的 Pod。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;运行控制器的方式&#34;&gt;运行控制器的方式&lt;/h2&gt;
&lt;p&gt;k8s 自带了一系列内置的控制器，运行在 &lt;code&gt;kube-controller-manager&lt;/code&gt; 中，这些内置的控制提供了重要的核心功能。
例如 Deployment 控制器和 Job 控制器就是由 k8s 本身(内置控制器)提供的控制器。 k8s 允许运行弹性的控制中心，所以当任意内置控制器挂掉后，其它节点的控制器能接替其上任。&lt;/p&gt;
&lt;p&gt;也可以找到用于扩展k8s 功能，运行在控制中心外的控制器。 如果用户愿意也可以自己编写全新的控制器。这些控制器可以以Pod的方式运行， 也可以运行在 k8s 集群之外。 具体由控制器的功能决定。&lt;/p&gt;
&lt;h2 id=&#34;引申阅读&#34;&gt;引申阅读&lt;/h2&gt;
&lt;p&gt;这里介绍怎么编写自己的控制器, 见&lt;a href=&#34;../../11-extend-kubernetes/00-extend-cluster/#extension-patterns&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Cloud Controller Manager</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/architecture/cloud-controller/</link>
      <pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/architecture/cloud-controller/</guid>
      <description>
        
        
        




&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;云基础设施技术让用户可以在公有云，私有云，混合云上运行 k8s. k8s 倡导自动化， API 驱动，组件之间松耦合的基础设施&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cloud-controller-manager&lt;/code&gt; 是集成了云提供商控制逻辑的 k8s 控制中心组件。 云提供商控制管理器让集群与云提供商提供的 API 相关系并让与云平台交互的组件和与集群交互的组件分离。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cloud-controller-manager&lt;/code&gt; 组件通过让 k8s 与底层云基础设施的交互逻辑解耦，使得云提供上功能发布的节奏与 k8s 项目功能发的节奏分离
&lt;code&gt;cloud-controller-manager&lt;/code&gt; 以插件结构的方便让不同的云提供与可以让其平台可以与 k8s 集成。&lt;/p&gt;
&lt;h2 id=&#34;设计&#34;&gt;设计&lt;/h2&gt;
&lt;p&gt;以下为 &lt;code&gt;cloud-controller-manager&lt;/code&gt; 在 k8s 架构中的位置：
&lt;img src=&#34;https://d33wubrfki0l68.cloudfront.net/7016517375d10c702489167e704dcb99e570df85/7bb53/images/docs/components-of-kubernetes.png&#34; alt=&#34;kubernetes architecture&#34;&gt;&lt;/p&gt;
&lt;p&gt;The cloud controller manager runs in the control plane as a replicated set of processes (usually, these are containers in Pods). Each cloud-controller-manager implements multiple controllers in a single process.
云提供商控制管理器在控制中心中以副本集进程的形式运行(通常为 Pod 中的容器)。 每个 &lt;code&gt;cloud-controller-manager&lt;/code&gt; 在一个进程中实现了多个控制器。&lt;/p&gt;
&lt;p&gt;{{ &lt;node&gt; }}
云提供商控制管理器通常以插件的方式运行而不是以控制中心组件的方式运行
{{ &lt;/node&gt; }}&lt;/p&gt;
&lt;h2 id=&#34;云提供商控制管理器的功用&#34;&gt;云提供商控制管理器的功用&lt;/h2&gt;
&lt;p&gt;云提供商控制管理器包含如下控制器:&lt;/p&gt;
&lt;h3 id=&#34;节点控制器&#34;&gt;节点控制器&lt;/h3&gt;
&lt;p&gt;节点控制器当在云基础设施上创建服务器时负责创建圣训的节点对象。 节点控制器获取用户在云提供商所租赁的主机的信息。 节点控制器主要有以下功能：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为控制器通过云提供商API发现的服务器初始化节点信息&lt;/li&gt;
&lt;li&gt;在节点对象上打上云提供商相关的注解和标签，例如 节点部署的区域，和可用资源(CPU, memory, 等)&lt;/li&gt;
&lt;li&gt;获取节点的主机名和网络地址&lt;/li&gt;
&lt;li&gt;验证节点的健康状况。 当一个节点不响应时，控制器会检查云提供商的 API， 确认服务器状态是否被修改为 停用/删除/终止。 如果发现节点已经被从云端删除则从 k8s 集群中删除对应的节点对象&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;有些云提供商在实现时会将其分为两个独立的控制器，分别为节点控制器和节点生命周期控制器&lt;/p&gt;
&lt;h3 id=&#34;路由控制器&#34;&gt;路由控制器&lt;/h3&gt;
&lt;p&gt;路由控制器负责在云提供商配置适当的路由以实现不同节点之间的通信。
根据云提供商的不同， 路由控制器还可能要为 Pod 网络申请一个IP段&lt;/p&gt;
&lt;h3 id=&#34;service-控制器&#34;&gt;Service 控制器&lt;/h3&gt;
&lt;p&gt;在云环境中 Service 会与一些云基础设施组件集群，比如负载均衡，IP地址， 网络包过滤，目标健康检测。 Service 控制器在创建 Service 时调用云提供商的API 设置 Service 需要的负载均衡和其它云基础设施组件&lt;/p&gt;
&lt;h2 id=&#34;授权&#34;&gt;授权&lt;/h2&gt;
&lt;p&gt;本节将分别说明云提供商管理器执行对应操作所需要访问的各种 API 对象的&lt;/p&gt;
&lt;h3 id=&#34;节点控制器-1&#34;&gt;节点控制器&lt;/h3&gt;
&lt;p&gt;节点控制器只需要访问节点对象。需要提供节点对的的读写权限&lt;/p&gt;
&lt;p&gt;&lt;code&gt;v1/Node&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Get&lt;/li&gt;
&lt;li&gt;List&lt;/li&gt;
&lt;li&gt;Create&lt;/li&gt;
&lt;li&gt;Update&lt;/li&gt;
&lt;li&gt;Patch&lt;/li&gt;
&lt;li&gt;Watch&lt;/li&gt;
&lt;li&gt;Delete&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;路由控制器-1&#34;&gt;路由控制器&lt;/h3&gt;
&lt;p&gt;路由控制器需要监听节点对象的创建来配置对应的路由。 所以需要节点对象的 &lt;code&gt;Get&lt;/code&gt; 权限&lt;/p&gt;
&lt;p&gt;&lt;code&gt;v1/Node&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Get&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;service-控制器-1&#34;&gt;Service 控制器&lt;/h3&gt;
&lt;p&gt;Service 控制器监听 Service 对象的创建，更新，删除事件来配置对应的 Endpoint&lt;/p&gt;
&lt;p&gt;访问 Service 需要 List， Watch 权限
更新 Service 需要 Patch， Update 权限
设置 Service 对的 Endpoint 需要 Create, List, Get, Watch, Update
最终需要如下：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;v1/Service&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;List&lt;/li&gt;
&lt;li&gt;Get&lt;/li&gt;
&lt;li&gt;Watch&lt;/li&gt;
&lt;li&gt;Patch&lt;/li&gt;
&lt;li&gt;Update&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;其它权限需求&#34;&gt;其它权限需求&lt;/h3&gt;
&lt;p&gt;云提供商控制管理器核心的实现中需要创建  Event 对象的权限。 为了设置安全操作，还需要创建 ServiceAccount 的权限&lt;/p&gt;
&lt;p&gt;&lt;code&gt;v1/Event&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create&lt;/li&gt;
&lt;li&gt;Patch&lt;/li&gt;
&lt;li&gt;Update&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;v1/ServiceAccount&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最终云提供商控制管理器基于 RBAC ClusterRole 配置如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;rbac.authorization.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ClusterRole&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cloud-controller-manager&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
- &lt;span style=&#34;color:#f92672&#34;&gt;apiGroups&lt;/span&gt;:
  - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;events&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;create&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;patch&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;update&lt;/span&gt;
- &lt;span style=&#34;color:#f92672&#34;&gt;apiGroups&lt;/span&gt;:
  - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;nodes&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;:
  - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;*&amp;#39;&lt;/span&gt;
- &lt;span style=&#34;color:#f92672&#34;&gt;apiGroups&lt;/span&gt;:
  - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;nodes/status&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;patch&lt;/span&gt;
- &lt;span style=&#34;color:#f92672&#34;&gt;apiGroups&lt;/span&gt;:
  - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;services&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;list&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;patch&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;update&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;watch&lt;/span&gt;
- &lt;span style=&#34;color:#f92672&#34;&gt;apiGroups&lt;/span&gt;:
  - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;serviceaccounts&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;create&lt;/span&gt;
- &lt;span style=&#34;color:#f92672&#34;&gt;apiGroups&lt;/span&gt;:
  - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;persistentvolumes&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;get&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;list&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;update&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;watch&lt;/span&gt;
- &lt;span style=&#34;color:#f92672&#34;&gt;apiGroups&lt;/span&gt;:
  - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;endpoints&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;create&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;get&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;list&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;watch&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;update&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;接下来应该看啥&#34;&gt;接下来应该看啥&lt;/h2&gt;
&lt;p&gt;这篇讲怎么运行管理 云提供商控制管理器 &lt;a href=&#34;../../../3-tasks/01-administer-cluster/09-running-cloud-controller/#cloud-controller-manager&#34;&gt;云提供商控制管理器管理&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;想要自己开发一个 云提供商控制管理器 &lt;a href=&#34;../../../3-tasks/01-administer-cluster/18-developing-cloud-controller-manager/&#34;&gt;云提供商控制管理器开发&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: k8s 是什么</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/00-what-is-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/00-what-is-k8s/</guid>
      <description>
        
        
        &lt;p&gt;k8s 是一个可扩展，可移植的开源平台，用于管理容器化的工作负载和服务，帮助实现声明式配置与自动化，有一个区大且快速发展的生态系统
源自Google 15年的生产经验&lt;/p&gt;
&lt;h2 id=&#34;部署方式演进&#34;&gt;部署方式演进&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://d33wubrfki0l68.cloudfront.net/26a177ede4d7b032362289c6fccd448fc4a91174/eb693/images/docs/container_evolution.svg&#34; alt=&#34;deployment&#34;&gt; &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/&#34;&gt;来源 kubernetes.io&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;传统部署时代&#34;&gt;传统部署时代&lt;/h3&gt;
&lt;p&gt;应用直接部署在物理服务器上，无法设置资源限制，因而可能引起资源分配问题
如果每个应用独立部署在物理机上，资源利用不充分，维护大量物理机费用高昂&lt;/p&gt;
&lt;h3 id=&#34;虚拟化部署&#34;&gt;虚拟化部署&lt;/h3&gt;
&lt;p&gt;通过虚拟机方式实现资源隔离和应用的扩展
虚拟机会在其自己的操作系统运行一个机器所有的组件&lt;/p&gt;
&lt;h3 id=&#34;容器化部署&#34;&gt;容器化部署&lt;/h3&gt;
&lt;p&gt;容器与虚拟机类似，但有更低的隔离级别，应用之间共享操作系统，因此容器被认为更轻量，与虚拟机类似，容器有独立的文件系统，CPU,内存，进程空间等。些两者都能实现与底层基础设施解耦，因此可以在不同的操作系统与云环境之间移植
容器变得流行，因为它提供了以下额外的优势&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;相较与虚拟机镜像创建更高效，更快捷的应用创建，更宽松的部署&lt;/li&gt;
&lt;li&gt;持续开发，集成，部署能通过镜像构建变得快速可靠，部署回滚也因为镜像的不可变性而变得快速和容易&lt;/li&gt;
&lt;li&gt;分离 开发和运维的关注点 在构建和发布时创建镜像而不是在部署。实现了应用与基础设施的解耦&lt;/li&gt;
&lt;li&gt;不止可以观测到系统级的信息和指数，还可以观测到应用的健康状态及其它信息&lt;/li&gt;
&lt;li&gt;实现 开发，测试，生产各环境的一致性。 可以在开发的笔记本和云运营上的环境上运行同样的镜像&lt;/li&gt;
&lt;li&gt;实现云环境和各种操作系统之间的可移植， 可运行在 Ubuntu, RHEL, CoreOS, on-premises,主流云环境，其它环境&lt;/li&gt;
&lt;li&gt;应用指数管理： 将抽像级别在虚拟操作系统运行应用提升到在一个运行在逻辑资源上的操作系统上运行应用&lt;/li&gt;
&lt;li&gt;松耦合，分布式，灵活，开放的微服务，应用被拆分为较小，独立，可以动态部署和管理。不是运行在一个专用大机器来运行一个单体应用&lt;/li&gt;
&lt;li&gt;资源隔离： 可预期的应用性能&lt;/li&gt;
&lt;li&gt;资源利用率: 高效充分&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;为啥需要-k8s它又能做啥&#34;&gt;为啥需要 k8s，它又能做啥&lt;/h2&gt;
&lt;p&gt;以上可知，应用容器化是个好东西，但在生产环境中,需要管理容器，并保证应用一直可用。 比如当一个容器挂掉以后，需要要马上再启动一个。如果这些都由一个系统来搞定，是不是管理容器就更容易了。而 k8s 就是为些而生的。 k8s 提供了运行弹性分布式系统的框架。提供应用的扩展和容灾等如下功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;服务发现和负载均衡
在 k8s 中可以通过 DNS 名称或 IP 地址来访问容器， 如果流量较高可能负载到多个容器上&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;存储编排
k8s 允许你自动挂载你选择的存储系统，比如 本地存储或公有云存储&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;自动化更新或回滚
由你来定义部署容器的状态，k8s 通过控制器来实现这些状态&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;自动化调度
将能够运行对应容器的节点加入 k8s 集群， 为容器定义需要的 CPU和内存资源数。k8s会把容器调度到合适的节点上，并充分利用节点资源&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;自愈
k8s 能重启替换挂掉的容器，根据配置检测不响应的容器，不让未就绪的容器进度流量负载列表&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;敏感数据和配置管理
通过配置管理敏感数据，而不需要将敏感数据打入到容器镜像中&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;k8s-不xxx&#34;&gt;k8s 不XXX&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;k8s 不限制应用的类型，k8s 致力于支持各种类型的工作负载，包括 无状态应用，有状态应用，数据处理工作，如果一个应用可以在容器中运行，就可以在k8s上运行&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;k8s 不发布源码，不构建应用。 可由组织文化和技术偏好决定CI/CD的工作流&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;k8s 不提供应用层的服务，比如 中间件(eg: 消息总线)，数据处理框架(eg: Spark), 数据库(eg: mysql), 缓存服务，集群存储系统(eg: Ceph) 等作为其内置服务， 这些组件可以直接运行在 k8s 上或者通过 k8s 可移植机制(Open Service Broker)让k8s 集群中的应用访问&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;k8s 不指定 日志，监控，报警解决方案，只提供集成方式，和收集导出相关metrics的机制&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;k8s 不提供也不要求一种配置 语言/系统(e.g. jsonnet)，只提供任意形式的声明式规范的声明式的API?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;k8s 不提供也不接收？ 任何 综合机器配置，维护，管理，自愈系统
k8s 也不仅仅是一个编排系统，实事上它消除了对编排的需求。 编排的技术定义是执行一个定义的工作流：先做A,然后B,再然后C. 相反的，k8s由一组独立的，可组合的管理理进程组成，它们的任务就是保证当前的状态与期望的状态是一致的。不关心怎么从A到C. 不需要中心化的控制。 这样可以使系统更易用，更强大，健壮，有更好的弹性和扩展性&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kubernetesk8s的字面意思&#34;&gt;Kubernetes(K8s),的字面意思&lt;/h2&gt;
&lt;p&gt;源自希腊词，意思是舵手或引向员，是最高的管理和控制， k8s 是缩写&lt;/p&gt;
&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://research.google.com/pubs/pub43438.html&#34;&gt;https://research.google.com/pubs/pub43438.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/&#34;&gt;https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: k8s 组件</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/01-components/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/01-components/</guid>
      <description>
        
        
        &lt;p&gt;当完成 k8s 部署后，用户就拥有一个 k8s 集群， 一个 k8s 一般包含多台工作机，称为节点，用于运行容器化应用。 每个集群至少包含一个节点。
在节点上运行应用工作负载的组件称为 Pod. k8s 控制面板 管理工作节点和集群中的 Pod. 在生产环境中，k8s 控制面板一般会运行在多个机器上，同样一个集群也会包含多个节点，用以提高集群的容错和高可用&lt;/p&gt;
&lt;p&gt;本文主要介绍一个完整可工作的集群包含哪些组件
下图标示出了一个集群的所有组件及相互关系
&lt;img src=&#34;https://d33wubrfki0l68.cloudfront.net/7016517375d10c702489167e704dcb99e570df85/7bb53/images/docs/components-of-kubernetes.png&#34; alt=&#34;k8s components diagram&#34;&gt; &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/components/&#34;&gt;来源 kubernetes.io&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;control-plane-components&#34;&gt;控制中心 组件&lt;/h2&gt;
&lt;p&gt;控制中心负责整个集群的全局决策(例如，调度)，也包含侦听和响应集群事件(例如，当应用的副本数增加时启动一个新的 Pod)
控制中心组件可以运行在集群的任意机器上。 但为了简单，创建脚本一般都会将组件放在同一个节点上，并不在这个节点上运行用户的容器。 更多示例参见
&lt;a href=&#34;TODO&#34;&gt;高可用集群&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;kube-apiserver&#34;&gt;kube-apiserver&lt;/h3&gt;
&lt;p&gt;提供 k8s API，是k8s控制中心的前端(此前端非)
k8s API 的大多数实现都在 kube-apiserver
提供了水平扩展的能力，扩容方式为部署多个实例，并提供负载均衡 &lt;a href=&#34;TODO&#34;&gt;搭建高可能集群&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;etcd&#34;&gt;etcd&lt;/h3&gt;
&lt;p&gt;强一致性和高可用键值存储，用于 k8s 存储所有集群数据
一定要对 etcd 数据做备份计划
需要深入了解 etcd 请移步 &lt;a href=&#34;https://etcd.io/docs/&#34;&gt;官网&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;kube-scheduler&#34;&gt;kube-scheduler&lt;/h3&gt;
&lt;p&gt;侦听新创建还没有指定 Node 的 Pod，为其选择一个 Node
调度因素：包含独立，集合资源需求，硬件，软件，策略，亲和性，反亲和性规范，数据位置，inter-workload interference and deadlines (TODO 没接触过，需要研究一下)&lt;/p&gt;
&lt;h3 id=&#34;kube-controller-manager&#34;&gt;kube-controller-manager&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kube-controller-manager&lt;/code&gt; 包含控制器进程，逻辑上每个 控制器是独立的进程，但为了降低复杂度，将它们打入一个可执行文件并运行在一个进程里&lt;/p&gt;
&lt;p&gt;有如下控制器：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Node 控制器: 负责监视和响应 Node 是不是挂了&lt;/li&gt;
&lt;li&gt;Replication 控制器: 负责系统中的 Pod 维护在预期的副本数&lt;/li&gt;
&lt;li&gt;Endpoints 控制器: 负责管理 Endpoints对象(Services &amp;amp; Pods)&lt;/li&gt;
&lt;li&gt;Service Account &amp;amp; Token 控制器: 为新增的命名空间创建默认的账号和令牌&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cloud-controller-manager&#34;&gt;cloud-controller-manager&lt;/h3&gt;
&lt;p&gt;k8s 控制中心集群了云环境的控制逻辑。 云控制管理器是集群与云提供商API交互的控制器。并与只与集群交互的控制器分离。
&lt;code&gt;cloud-controller-manager&lt;/code&gt; 只运行与指定云提供商对应的控制器。 如果k8s集群没有运行在云上，则集群不包含 &lt;code&gt;cloud-controller-manager&lt;/code&gt;
与 &lt;code&gt;kube-controller-manager&lt;/code&gt; 一样，&lt;code&gt;cloud-controller-manager&lt;/code&gt; 也是由多个独立的控制器放在一个二进制文件中，并运行在一个进程里。 可能运行多个运程实现水平扩展达到扩容和容灾的目的&lt;/p&gt;
&lt;p&gt;以下控制器依赖于云提供商：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Node 控制器 当机器不响应时向云提供商询问是否被删除&lt;/li&gt;
&lt;li&gt;Route 控制器 在云提供商的基础设施上创建路由&lt;/li&gt;
&lt;li&gt;Service 控制器: 创建，更新，删除云提供商的负载均衡&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;node-组件&#34;&gt;Node 组件&lt;/h2&gt;
&lt;p&gt;Node 组件运行在每个节点上，维护 Pod 运行，提供 k8s 运行环境&lt;/p&gt;
&lt;h3 id=&#34;kubelet&#34;&gt;kubelet&lt;/h3&gt;
&lt;p&gt;保证容器在 Pod 运行
读取&lt;code&gt;PodSpecs&lt;/code&gt; 保证容器的健康运行
kubelet 不管理非 k8s 创建的容器(例如由 manifests 创建的 Pod)&lt;/p&gt;
&lt;h3 id=&#34;kube-proxy&#34;&gt;kube-proxy&lt;/h3&gt;
&lt;p&gt;kube-proxy 运行在集群中每个节点上的网络代理， 实现 Service 层抽象的组成部分
kube-proxy 维护节点上的网络规则。 这些规则允计集群内外能够与 Pod 进行网络通信
kube-proxy 优先使用系统层的包过虑，如果没有则自己转发&lt;/p&gt;
&lt;h3 id=&#34;container-runtime&#34;&gt;Container Runtime&lt;/h3&gt;
&lt;p&gt;容器的运行环境，
k8s 支持容器运行环境有: Docker, containerd, CRI-O 和 任意实现了 k8s CRI(&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md&#34;&gt;Container Runtime Interface&lt;/a&gt;)的运行环境&lt;/p&gt;
&lt;h2 id=&#34;插件&#34;&gt;插件&lt;/h2&gt;
&lt;p&gt;插件基于 k8s 资源(DaemonSet, Deployment 等)来实现集群新功能。由于提供的是集群级的功能，所有插件所属的命名空间为 &lt;code&gt;kube-system&lt;/code&gt;
下面列举了一些插件，更多请见 &lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/addons/&#34;&gt;这里&lt;/a&gt;
TODO&lt;/p&gt;
&lt;h3 id=&#34;dns&#34;&gt;DNS&lt;/h3&gt;
&lt;p&gt;其它插件不是十分必要，但这个必须要有，许多示例依赖这个插件
Cluster DNS，是一个DNS服务，是对其它DNS服务的补充，为k8s 集群 Service 提供 DNS 记录
由 k8s 启动的容器会自动包含 此 DNS 的配置&lt;/p&gt;
&lt;h3 id=&#34;web-ui-dashboard&#34;&gt;Web UI (Dashboard)&lt;/h3&gt;
&lt;p&gt;一个通用的 k8s 集群 web UI. 可用于集群和集群内应用的管理和调试
&lt;a href=&#34;https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/&#34;&gt;Dashboard&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;容器资源监控&#34;&gt;容器资源监控&lt;/h3&gt;
&lt;p&gt;记录容器的时序监控数据到到一个中心数据库，并提供查看数据的UI
&lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/&#34;&gt;Container Resource Monitoring&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;集群级的日志&#34;&gt;集群级的日志&lt;/h3&gt;
&lt;p&gt;提供集群级的容器日志采集并存储到中心日志存储的机制，并提供查询和阅览接口
&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/logging/&#34;&gt;Cluster-level Logging&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/components/&#34;&gt;https://kubernetes.io/docs/concepts/overview/components/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;引申阅读&#34;&gt;引申阅读&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/nodes/&#34;&gt;节点&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/controller/&#34;&gt;控制器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/&#34;&gt;调度器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://etcd.io/docs/&#34;&gt;etcd 官网&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/admin/high-availability/&#34;&gt;k8s 集群高可用&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: k8s API 说明</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/02-k8s-api/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/02-k8s-api/</guid>
      <description>
        
        
        &lt;p&gt;k8s API 提供查询操作 k8s 对应状态的功能。 k8s 控制中心的核心是 &lt;code&gt;api-server&lt;/code&gt; 及其提供的 HTTP API。包括用户，集群的其它组件，外部组件都是与 &lt;code&gt;api-server&lt;/code&gt; 通信&lt;/p&gt;
&lt;p&gt;全部的API约定在&lt;a href=&#34;https://git.k8s.io/community/contributors/devel/api-conventions.md&#34;&gt;这里&lt;/a&gt;
API endpoints,资源类型，示例在&lt;a href=&#34;https://kubernetes.io/docs/reference&#34;&gt;这里&lt;/a&gt;
远程访问 API 的说明在&lt;a href=&#34;https://kubernetes.io/docs/admin/accessing-the-api&#34;&gt;这里&lt;/a&gt;
k8s API 是系统声明式配置的基础， 命令行工具 &lt;code&gt;[kubectl](https://kubernetes.io/docs/user-guide/kubectl/)&lt;/code&gt; 可以用来进行对 API 对象的增删改查
k8s 以API 资源的形式也存储了序列化状态(目前使用 &lt;a href=&#34;https://coreos.com/docs/distributed-configuration/getting-started-with-etcd/&#34;&gt;etcd&lt;/a&gt;)
k8s 自身也解耦成多个模块，通过 API 进行交互&lt;/p&gt;
&lt;h2 id=&#34;api-的变化&#34;&gt;API 的变化&lt;/h2&gt;
&lt;p&gt;任何成功的系统都应该快速响应新的应用场景或现有需求的变更。 因此 k8s 在设计 API 时就提供了持续改进与成长的方式。k8s 项目主旨是不打破对已有客户端的普空性。并将此兼容性保持一段时间，让其它项目有时间来进行适配.
通常新的API 资源和新的API资源项，可以经常频繁增加， 移除资源或资源项则需要遵循 &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-policy/&#34;&gt;API废弃策略&lt;/a&gt; TODO&lt;/p&gt;
&lt;p&gt;关于怎么做兼容和怎么修改API,依照这个(&lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api_changes.md#readme&#34;&gt;https://git.k8s.io/community/contributors/devel/sig-architecture/api_changes.md#readme&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;openapi-规范&#34;&gt;OpenAPI 规范&lt;/h2&gt;
&lt;p&gt;完整 API 规范明细见 &lt;a href=&#34;https://www.openapis.org/&#34;&gt;这里&lt;/a&gt;
k8s api-server 通过 &lt;code&gt;/openapi/v2&lt;/code&gt; 提供 OpenAPI 规范， 用户可以通过以下请求头发送请求获取响应格式&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;请求头&lt;/th&gt;
&lt;th&gt;可选值&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Accept-Encoding&lt;/td&gt;
&lt;td&gt;gzip&lt;/td&gt;
&lt;td&gt;可选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Accept&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:application/com.github.proto-openapi.spec.v2@v1.0&#34;&gt;application/com.github.proto-openapi.spec.v2@v1.0&lt;/a&gt;+protobuf&lt;/td&gt;
&lt;td&gt;mainly for intra-cluster use&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;^&lt;/td&gt;
&lt;td&gt;application/json&lt;/td&gt;
&lt;td&gt;默认&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;^&lt;/td&gt;
&lt;td&gt;响应 &lt;code&gt;application/json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;也可以是任务符合OpenAPI 规范的请求头
k8s 还有一个基于基于 &lt;code&gt;Protobuf&lt;/code&gt; 序列化的API，这套API 主要用于集群内通信。具体见对应模块 Go 源码文档，文档设计准则在&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/protobuf.md&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;api-版本&#34;&gt;API 版本&lt;/h2&gt;
&lt;p&gt;为方便移除资源项或修改资源结构，k8s 同时存在多个版本的 API, 每个版本的 API 路径都不一样，比如 &lt;code&gt;/api/v1&lt;/code&gt; &lt;code&gt;/apis/extensions/v1beta1&lt;/code&gt;
这样版本就通过区分就在API这一层完成，而不用到资源或资源荐，可以做到清晰明了,并提供一致的系统资源的行为。
也可以实现对过期和实验性 API 的访问控制
JSON和Protobuf序列化的定义结构变更策略与API定义资源结构的规则一致，具体如下:&lt;/p&gt;
&lt;p&gt;API 的版本和软件的版本只是间接相关的(和 Docker 类似)，详见&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/release/versioning.md&#34;&gt;k8s 版本发布&lt;/a&gt;提议&lt;/p&gt;
&lt;p&gt;不同的 API 版本提供不同级别的稳定性和支持，关于每个级别详细描述的细则见&lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api_changes.md#alpha-beta-and-stable-versions&#34;&gt;这个文档&lt;/a&gt;
可以总结为如下几个:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Alpha 级别&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;api URL名字中带有 alpha (e.g. v1alpha1)&lt;/li&gt;
&lt;li&gt;可能有不少bug,启用这个物引可能会引入bug. 默认关闭&lt;/li&gt;
&lt;li&gt;所支持的功能可能在没有任何通知的情况下被移除&lt;/li&gt;
&lt;li&gt;在未来的版本中可能会在没有通知的情况谱得与之前不兼容&lt;/li&gt;
&lt;li&gt;只推荐用于短期实验集群，因为有较高bug多，缺乏长期支持的风险&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Beta 级别&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;api URL名字中带有 beta (e.g. v2beta3)&lt;/li&gt;
&lt;li&gt;代码是充分测试的，启用这个功能基本上是安全的，默认打开&lt;/li&gt;
&lt;li&gt;整体功能不会被删除，但细节可能会修改&lt;/li&gt;
&lt;li&gt;未来版本中可能会因资源的结构与资源项的含义出现不兼容情况，如果出现这种情况,官方会提供升级与该版本的迁移指导，可能需要删除，编辑或重建 API对象，在修改过程可能需要用户仔细思考需要的改动，此过程信赖些功能的应用可能会不可用&lt;/li&gt;
&lt;li&gt;只推荐 非关键商业 用户使用，因为在未来版本中可能存在潜在的不兼容变更，如果用户有几个可独立更新集群，也可以不受此限制&lt;/li&gt;
&lt;li&gt;强烈建议用户试用beta特性并给予反馈，因为一旦从beta 版本确认为稳定版，则特性将不可更改&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stable&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;api 版本名称模式为 &lt;code&gt;vX&lt;/code&gt;， 其中&lt;code&gt;X&lt;/code&gt;为整数&lt;/li&gt;
&lt;li&gt;稳定版功能会存在于稳定版的软件的很多个版本中&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;api-分组&#34;&gt;API 分组&lt;/h2&gt;
&lt;p&gt;为了方便API的扩展， k8s 实现了 API 分组的方式。 API 组在 REST 接口的Path 上的 apiVersion 字段
&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/api-machinery/api-group.md&#34;&gt;设计提案之API分组&lt;/a&gt;
集群中一般有如下一些组:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;核心组,也被称为经典组，REST 路径中为 &lt;code&gt;/api/v1&lt;/code&gt; 对象定义 &lt;code&gt;apiVersion: v1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;其它组 REST 路径为 &lt;code&gt;/apis/$GROUP_NAME/$VERSION&lt;/code&gt; 对象定义为 &lt;code&gt;apiVersion: $GROUP_NAME/$VERSION (e.g. apiVersion: batch/v1)&lt;/code&gt; &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/&#34;&gt;API文档&lt;/a&gt;是有完整的分级列表&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过自定义资源可以通过以下两种方式扩展API:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/&#34;&gt;CustomResourceDefinition&lt;/a&gt; 通过声明方式定义 api-server 怎么提供用户选择资源的API (TODO 这里描述不太清楚)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用户可能通过&lt;a href=&#34;https://kubernetes.io/docs/tasks/extend-kubernetes/setup-extension-api-server/&#34;&gt;实现自己的扩展api-server&lt;/a&gt;, 并通过&lt;a href=&#34;https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/&#34;&gt;聚合&lt;/a&gt;使客户端可以访问&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;api-分组开关&#34;&gt;API 分组开关&lt;/h2&gt;
&lt;p&gt;一些资源和API默认是开启的，可能通过 api-server 命令行启动参数中使用 &amp;ndash;runtime-config 进行开启或关闭
关闭 &amp;ndash;runtime-config=batch/v1=false
开启 &amp;ndash;runtime-config=batch/v2alpha1
如果有多个分组进制设置，可能key=value并用逗号分隔
注意： 开启或关闭分组或资源一需要重启 api-server 和 kube-controller-manager&lt;/p&gt;
&lt;h2 id=&#34;开启-extensionsv1beta1-组的指定资源&#34;&gt;开启 &lt;code&gt;extensions/v1beta1&lt;/code&gt; 组的指定资源&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;extensions/v1beta1&lt;/code&gt; 组默认启用资源的有 &lt;code&gt;DaemonSets&lt;/code&gt;, &lt;code&gt;Deployments&lt;/code&gt;, &lt;code&gt;StatefulSet&lt;/code&gt;, &lt;code&gt;NetworkPolicies&lt;/code&gt;, &lt;code&gt;PodSecurityPolicies&lt;/code&gt;， &lt;code&gt;ReplicaSets&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;例如要启用资源 &lt;code&gt;deployments&lt;/code&gt; 和 &lt;code&gt;daemonsets&lt;/code&gt;
也是在 api-server 启动参数中使用 &amp;ndash;runtime-config 进行开启或关闭
&lt;code&gt;--runtime-config=extensions/v1beta1/deployments=false,extensions/v1beta1/ingress=false&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;持久化&#34;&gt;持久化&lt;/h2&gt;
&lt;p&gt;k8s 将序列化的 API 资源对象保存在 etcd 中&lt;/p&gt;
&lt;h2 id=&#34;引申阅读&#34;&gt;引申阅读&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/&#34;&gt;API访问控制&lt;/a&gt;
&lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#api-conventions&#34;&gt;API约定&lt;/a&gt;
&lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/&#34;&gt;API文档&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 镜像</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/containers/00-images/</link>
      <pubDate>Sun, 19 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/containers/00-images/</guid>
      <description>
        
        
        &lt;!-- overview --&gt;
&lt;p&gt;一个容器镜像就是构建应用的二进制数据和应用所以需要的依赖。 容器镜像是一个可独立运行的可执行软件包，并定义了清楚的运行环境。
通常是用户创建一个应用的容器镜像，推送到镜像仓库，在 Pod 中引用这个镜像。&lt;/p&gt;
&lt;p&gt;本文介绍容器镜像的主要概念&lt;/p&gt;
&lt;!-- body --&gt;
&lt;h2 id=&#34;镜像名称&#34;&gt;镜像名称&lt;/h2&gt;
&lt;p&gt;镜像的名称通常是长成这些个样子的 &lt;code&gt;pause&lt;/code&gt;, &lt;code&gt;example/mycontainer&lt;/code&gt;, &lt;code&gt;kube-apiserver&lt;/code&gt;。 镜像名也可以带上镜像仓库的主机名(自建或第三方镜像仓库默认都要带上)，如：&lt;code&gt;fictional.registry.example/imagename&lt;/code&gt;。也可能有端口号，如: &lt;code&gt;fictional.registry.example:10443/imagename&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;如果镜像名称上没有镜像仓库的主机名， 则 k8s 认为使用的是 Docker 公共镜像仓库。&lt;/p&gt;
&lt;p&gt;在镜像名称的后面通常还会有一个标签(&lt;code&gt;tag&lt;/code&gt;)(就和 &lt;code&gt;docker&lt;/code&gt; 与 &lt;code&gt;podman&lt;/code&gt; 命令用的的一样)。 标签用于区分同一系列镜像的不同版本。&lt;/p&gt;
&lt;p&gt;标签名的命名规范为： 大小写字母，数字，下划线(&lt;code&gt;_&lt;/code&gt;)，点(&lt;code&gt;.&lt;/code&gt;)中划线(&lt;code&gt;-&lt;/code&gt;)，但分隔符(&lt;code&gt;_&lt;/code&gt;,&lt;code&gt;.&lt;/code&gt;,&lt;code&gt;-&lt;/code&gt;)有使用限制
如果镜像名称后没有标签，则默认使用 &lt;code&gt;latest&lt;/code&gt; 作为标签。&lt;/p&gt;
&lt;p&gt;{{ &lt;warning&gt; }}
在生产环境部署中尽量避免使用 latest 标签。 这样很难跟踪当前运行的是哪个版本，要做版本回退就更难了。
所以建议使用有含意的标签，如: &lt;code&gt;v1.42.0&lt;/code&gt;
{{ &lt;/warning&gt; }}&lt;/p&gt;
&lt;h2 id=&#34;镜像更新策略&#34;&gt;镜像更新策略&lt;/h2&gt;
&lt;p&gt;默认的更新策略为 &lt;code&gt;IfNotPresent&lt;/code&gt;, 就是让 kubelet 在找不到镜像时才从仓库拉取。 如果想要每次都强制拉取，则可以通过以下任意一种方式实现：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将容器 &lt;code&gt;imagePullPolicy&lt;/code&gt; 的值设置为 &lt;code&gt;Always&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;不设置 &lt;code&gt;imagePullPolicy&lt;/code&gt; 并让镜像使用 &lt;code&gt;latest&lt;/code&gt; 标签&lt;/li&gt;
&lt;li&gt;不设置 &lt;code&gt;imagePullPolicy&lt;/code&gt; 并镜像也不设置标签&lt;/li&gt;
&lt;li&gt;打开  &lt;a href=&#34;../../../reference/03-access-authn-authz/04-admission-controllers/#alwayspullimages&#34;&gt;AlwaysPullImages&lt;/a&gt; admission controller&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果配置了 &lt;code&gt;imagePullPolicy&lt;/code&gt; 但没有设置值，则默认为 &lt;code&gt;Always&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;带清单manifest的多架构镜像&#34;&gt;带清单(Manifest)的多架构镜像&lt;/h2&gt;
&lt;p&gt;镜像仓库在提供二进制镜像的同时也可以提供 &lt;a href=&#34;https://github.com/opencontainers/image-spec/blob/master/manifest.md&#34;&gt;容器镜像清单&lt;/a&gt;, 这个清单中列举了不同架构的镜像引用层。这样虽然镜像只有一个名字(如: &lt;code&gt;pause&lt;/code&gt;, &lt;code&gt;example/mycontainer&lt;/code&gt;, &lt;code&gt;kube-apiserve&lt;/code&gt;), 但不同的操作系统架构可以拉取到对应架构的镜像。&lt;/p&gt;
&lt;p&gt;在 k8s 中容器镜像名称是带有后缀 &lt;code&gt;-$(ARCH)&lt;/code&gt;的。 为了向后兼容， 请为旧的镜像添加后缀。 比如生成包含所有架构清单的镜像叫 &lt;code&gt;pause&lt;/code&gt;，&lt;code&gt;pause-amd64&lt;/code&gt; 就是为向后兼容旧的配置或可能存在于YAML配置文件中带后续硬编码镜像名&lt;/p&gt;
&lt;h2 id=&#34;使用私有仓库&#34;&gt;使用私有仓库&lt;/h2&gt;
&lt;p&gt;从私有仓库拉取镜像时一般都需要提供凭据。
以下为提供凭据的几种方式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在节点上配置仓库认证&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有的 Pod 都对仓库有全部访问权限&lt;/li&gt;
&lt;li&gt;需要集群管理员对节点进行配置&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;预先下载镜像&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有的 Pod 可以使用节点是缓存的镜像&lt;/li&gt;
&lt;li&gt;需要在节点上以 root 用户配置&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;基于提供商或本地插件&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果使用自定义节点配置， 用户(或云提供商)可以自己在节点上实现向镜像仓库的认证&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下为对这些认证方式更详细的说明&lt;/p&gt;
&lt;h3 id=&#34;在节点上配置仓库认证&#34;&gt;在节点上配置仓库认证&lt;/h3&gt;
&lt;p&gt;如果节点上运行的是 Docker 用户可以通过配置 Docker 运行时向私有仓库进行认证
这种方式适用于用于能近控制节点配置&lt;/p&gt;
&lt;p&gt;{{ &lt;note&gt; }}
k8s 只支持 Docker 配置的 &lt;code&gt;auths&lt;/code&gt; 和 &lt;code&gt;HttpHeaders&lt;/code&gt; 部分， Docker 凭据帮助工具(&lt;code&gt;credHelpers&lt;/code&gt; 或 &lt;code&gt;credsStore&lt;/code&gt;)是不支持的
{{ &lt;/note&gt; }}&lt;/p&gt;
&lt;p&gt;Docker 会将私有仓库的凭据存储在 &lt;code&gt;$HOME/.dockercfg&lt;/code&gt; 或 &lt;code&gt;$HOME/.docker/config.json&lt;/code&gt; 文件中，kubelet 在拉取镜像时也可以从以下列表中搜寻凭据配置文件:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;{--root-dir:-/var/lib/kubelet}/config.json&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;{cwd of kubelet}/config.json&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;${HOME}/.docker/config.json&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/.docker/config.json&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;{--root-dir:-/var/lib/kubelet}/.dockercfg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;{cwd of kubelet}/.dockercfg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;${HOME}/.dockercfg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/.dockercfg&lt;/code&gt;
{{ &lt;note&gt; }}
用户可以在 kubelet 进行的环境变量上显示地设置 HOME=/root
{{ &lt;/note&gt; }}&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下为为节点设置私有仓库凭据的推荐方式。 本示例运行在控制机/笔记本上：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为每一个想要使用的凭据执行命令 &lt;code&gt;docker login [server]&lt;/code&gt;，该命令会更本机的 &lt;code&gt;$HOME/.docker/config.json&lt;/code&gt; 文件&lt;/li&gt;
&lt;li&gt;在文本编辑器中查看 &lt;code&gt;$HOME/.docker/config.json&lt;/code&gt; 文件，保证其中只包含需要使用到的凭据。&lt;/li&gt;
&lt;li&gt;获取节点列表; 例如:
&lt;ul&gt;
&lt;li&gt;获取节点名称: &lt;code&gt;nodes=$( kubectl get nodes -o jsonpath=&#39;{range.items[*].metadata}{.name} {end}&#39; )&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;获取节点IP: &lt;code&gt;nodes=$( kubectl get nodes -o jsonpath=&#39;{range .items[*].status.addresses[?(@.type==&amp;quot;ExternalIP&amp;quot;)]}{.address} {end}&#39; )&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;将本地 &lt;code&gt;.docker/config.json&lt;/code&gt; 文件拷贝到以上列举的节点的凭据搜索目录中的一个
&lt;ul&gt;
&lt;li&gt;例如: &lt;code&gt;for n in $nodes; do scp ~/.docker/config.json root@&amp;quot;$n&amp;quot;:/var/lib/kubelet/config.json; done&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;{{ &lt;note&gt; }}
在生成环境集群中，使用配置管理工具来让配置拷贝到所有需要的节点上
{{ &lt;/note&gt; }}&lt;/p&gt;
&lt;p&gt;要验证配置是否正确，则使用私有仓库中的镜像来创建一个Pod，如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl apply -f - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt;EOF
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;apiVersion: v1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kind: Pod
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;metadata:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  name: private-image-test-1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;spec:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  containers:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    - name: uses-private-image
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      image: $PRIVATE_IMAGE_NAME
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      imagePullPolicy: Always
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      command: [ &amp;#34;echo&amp;#34;, &amp;#34;SUCCESS&amp;#34; ]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果所有配置正确，则在一会之后可以通过命令&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl logs private-image-test-1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SUCCESS
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果怀疑命令失败，则执行命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl describe pods/private-image-test-1 | grep &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Failed&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果有失败则输出类似如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; Fri, 26 Jun 2015 15:36:13 -0700    Fri, 26 Jun 2015 15:39:13 -0700    19    {kubelet node-i2hq}    spec.containers{uses-private-image}    failed        Failed to pull image &amp;quot;user/privaterepo:v1&amp;quot;: Error: image user/privaterepo:v1 not found
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;必须要保证所有节点都是使用的同一个 &lt;code&gt;.docker/config.json&lt;/code&gt; 文件， 否则 Pod 可能在某些节点成功，某些节点则失败。 例如，在使用节点自动扩容时， 每个实例模板都需要包含 &lt;code&gt;.docker/config.json&lt;/code&gt; 或挂载包含该文件的盘
当私有仓库的凭据被添加到&lt;code&gt;.docker/config.json&lt;/code&gt;后 所有的 Pod 都可以对其中配置的任意私有仓库中拉取镜像。&lt;/p&gt;
&lt;h3 id=&#34;预先拉取镜像&#34;&gt;预先拉取镜像&lt;/h3&gt;
&lt;p&gt;{{ &lt;note&gt; }}
这种方式适用于用于用户有权限对节点配置的情况，而且不适用于节点由云提供商管理且能够自动扩容的情况
{{ &lt;/note&gt; }}&lt;/p&gt;
&lt;p&gt;默认情况下， kubelet 会尝试从指定镜像仓库拉取每一个镜像， 但是在容器 imagePullPolicy 属性的值被设置为 &lt;code&gt;IfNotPresent&lt;/code&gt; 或 &lt;code&gt;Never&lt;/code&gt;时，则会使用本地镜像。 (preferentially or exclusively, respectively).&lt;/p&gt;
&lt;p&gt;如果想要用预先摘取的方式取代镜像仓库认证， 需要保证集群中所有节点上预先摘取到的所有镜像都必须一致。
这种方式可用于预先取镜像来达到提速的目的或代替私有镜像仓库认证。
所有的 Pod 都有权访问任意预先拉取的镜像&lt;/p&gt;
&lt;h3 id=&#34;在-pod-上设置-imagepullsecrets&#34;&gt;在 Pod 上设置 &lt;code&gt;ImagePullSecrets&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;{{ &lt;note&gt; }}
这是使用私有仓库镜像的推荐方式
{{ &lt;/note&gt; }}&lt;/p&gt;
&lt;p&gt;k8s 支持在 Pod 上配置私有镜像仓库凭据&lt;/p&gt;
&lt;h4 id=&#34;创建带-docker-配置的-secret&#34;&gt;创建带 Docker 配置的 Secret&lt;/h4&gt;
&lt;p&gt;替换命令中的大写值为对应的配置，并运行此命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl create secret docker-registry &amp;lt;name&amp;gt; --docker-server&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;DOCKER_REGISTRY_SERVER --docker-username&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;DOCKER_USER --docker-password&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;DOCKER_PASSWORD --docker-email&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;DOCKER_EMAIL
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果已经有 Docker 凭据，可以不用以上命令，直接使用凭据文件创建对应的 Secret.
具体配置见&lt;a href=&#34;../../../3-tasks/02-configure-pod-container/11-pull-image-private-registry/#registry-secret-existing-credentials&#34;&gt;这里&lt;/a&gt;
这种配置方式尤其适用有多个私有镜像仓库的情况，因为 &lt;code&gt;kubectl create secret docker-registry&lt;/code&gt; 创建 Secret 的方式只适用于单个私有镜像仓库的情况。
{{ &lt;note&gt; }}
Pod 只能引用当前命名空间内的 &lt;code&gt;Secret&lt;/code&gt; , 因此需要在每个命名空间都需要创建 &lt;code&gt;Secret&lt;/code&gt;
{{ &lt;/note&gt; }}&lt;/p&gt;
&lt;h3 id=&#34;在-pod-中使用-imagepullsecrets&#34;&gt;在 Pod 中使用 &lt;code&gt;imagePullSecrets&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;当 &lt;code&gt;Secret&lt;/code&gt; 创建好后，可以配置 &lt;code&gt;imagePullSecrets&lt;/code&gt; 使用该 &lt;code&gt;Secret&lt;/code&gt;, 例如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt;EOF &amp;gt; pod.yaml
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;apiVersion: v1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kind: Pod
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;metadata:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  name: foo
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  namespace: awesomeapps
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;spec:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  containers:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    - name: foo
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      image: janedoe/awesomeapp:v1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  imagePullSecrets:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    - name: myregistrykey
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;

cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; ./kustomization.yaml
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;resources:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;- pod.yaml
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;需要在每个用到私有镜像仓库的 Pod 都需要该配置。
也可以在 &lt;code&gt;ServiceAccount&lt;/code&gt; 设置 &lt;code&gt;imagePullSecrets&lt;/code&gt; 可以使用对应的 Pod 自动添加该属性
在 &lt;code&gt;ServiceAccount&lt;/code&gt; 设置 &lt;code&gt;imagePullSecrets&lt;/code&gt;具体见&lt;a href=&#34;../../../3-tasks/02-configure-pod-container/10-configure-service-account/#add-imagepullsecrets-to-a-service-account&#34;&gt;这里&lt;/a&gt;
这个配置可与节点上的 &lt;code&gt;.docker/config.json&lt;/code&gt; 配合使用，两个配置的凭据全合并到一起。&lt;/p&gt;
&lt;h2 id=&#34;应用场景&#34;&gt;应用场景&lt;/h2&gt;
&lt;p&gt;配置私有仓库镜像的方式有多种，以下为常用应用场景和推荐方案。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;集群只使用开放(如：开源)镜像， 不需要私有仓库。
- 使用 Docker Hub 上的公有镜像
&lt;ul&gt;
&lt;li&gt;不需要配置&lt;/li&gt;
&lt;li&gt;一些云提供商会自动缓存或镜像开放镜像， 可以提高可用性并减少拉取镜像的时间&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;集群使用到的镜像对外私有，对内公开
- 使用自建私有镜像仓库
&lt;ul&gt;
&lt;li&gt;可以托管在 Docker Hub 或其它地方&lt;/li&gt;
&lt;li&gt;使用上面介结的在每个节点配置 &lt;code&gt;.docker/config.json&lt;/code&gt; 的方式
- 在内网运行一个开放的内部私有镜像仓库&lt;/li&gt;
&lt;li&gt;不需要在 k8s 上做配置
- 使用一个有访问控制的镜像仓库&lt;/li&gt;
&lt;li&gt;在节点自动扩容的场景下会比手动更佳
- 在节点配置不方便的集群中使用 &lt;code&gt;imagePullSecrets&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;镜像仓库需要更严格的访问控制
- 需要打开  &lt;a href=&#34;../../../reference/03-access-authn-authz/04-admission-controllers/#alwayspullimages&#34;&gt;AlwaysPullImages&lt;/a&gt; admission controller， 否则所有 Pod 默认对所有镜像有访问权限
- 将敏感数据放的 Secret 中， 还是是打在镜像中&lt;/li&gt;
&lt;li&gt;多租户集群，每个租户需要独立的私有镜像仓库
- 需要打开  &lt;a href=&#34;../../../reference/03-access-authn-authz/04-admission-controllers/#alwayspullimages&#34;&gt;AlwaysPullImages&lt;/a&gt; admission controller， 否则所有租户的所有 Pod 默认对所有镜像有访问权限
- 私有镜像仓库需要有认证系统
- 为每个租户生成私有镜像仓库凭据，并在每个租户的命名空间中创建对应 Secret.
- 各命名空间的租户将名称的 Secret 配置到 &lt;code&gt;imagePullSecrets&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果用到的多个私有镜像仓库， 可以对每个仓库创建一个 Secret, kubelet 会将所有 imagePullSecrets 合并到一个虚拟的 &lt;code&gt;.docker/config.json&lt;/code&gt; 中。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 容器环境</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/containers/01-container-environment/</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/containers/01-container-environment/</guid>
      <description>
        
        
        &lt;!-- overview --&gt;
&lt;p&gt;本文主要介绍以环境提供容器的资源信息&lt;/p&gt;
&lt;!-- body --&gt;
&lt;h2 id=&#34;容器内的环境变量&#34;&gt;容器内的环境变量&lt;/h2&gt;
&lt;p&gt;k8s 环境为容器提供了一些重要的资源信息：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个包含&lt;a href=&#34;../00-images/&#34;&gt;镜像&lt;/a&gt;和一个或多个&lt;a href=&#34;../../05-storage/00-volumes/&#34;&gt;卷&lt;/a&gt;组合成的文件系统&lt;/li&gt;
&lt;li&gt;关于容器自身的相关信息&lt;/li&gt;
&lt;li&gt;关于集群中其它对象的相关信息&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;容器信息&#34;&gt;容器信息&lt;/h3&gt;
&lt;p&gt;容器的主机名，也就是容器运行的 Pod 的名称。 可以通过 &lt;code&gt;hostname&lt;/code&gt; 命令或 &lt;code&gt;libc&lt;/code&gt; 库中的 &lt;code&gt;gethostname&lt;/code&gt; 函数获得
Pod 的名称和所在的命名空间可能 &lt;a href=&#34;../../../3-tasks/04-inject-data-application/04-downward-api-volume-expose-pod-information/#the-downward-api&#34;&gt;downward API&lt;/a&gt;以环境变量方式访问
在 Pod 定义中用户自定义的环境变量和Docker 镜像中的环境变量都会成为容器中的环境变量。&lt;/p&gt;
&lt;h3 id=&#34;集群信息&#34;&gt;集群信息&lt;/h3&gt;
&lt;p&gt;在容器创建之前的所有 Service 列表会以环境变量方式加入容器。 这些环境变量与 Docker link 的语法一致。
例如 在一个叫 bar 的容器中加一个叫 foo 的 Service, 会加入以下环境变量:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-env&#34; data-lang=&#34;env&#34;&gt;FOO_SERVICE_HOST=&amp;lt;the host the service is running on&amp;gt;
FOO_SERVICE_PORT=&amp;lt;the port the service is running on&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果集群开启了 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/&#34;&gt;DNS 插件&lt;/a&gt;，容器也可能通过 DNS 方式获取 Service 的 IP 地址。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../03-container-lifecycle-hooks/&#34;&gt;容器生命周期钩子&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../../3-tasks/02-configure-pod-container/16-attach-handler-lifecycle-event/&#34;&gt;生命周期事件处理器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Runtime Class</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/containers/02-runtime-class/</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/containers/02-runtime-class/</guid>
      <description>
        
        
        &lt;!-- overview --&gt;
&lt;p&gt;




&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [beta]&lt;/code&gt;
&lt;/div&gt;


本文介绍 &lt;code&gt;RuntimeClass&lt;/code&gt; 资源和运行时选择机制。
&lt;code&gt;RuntimeClass&lt;/code&gt; 是一个选择容器运行时配置的特性。而容器运行时配置则用于运行 Pod 中的容器的。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;用户可以在不同的 Pod 上配置不同的 &lt;code&gt;RuntimeClass&lt;/code&gt; 以达成性能与安全之间的平衡。 例如，有一部分工作负载需要一个高级别的信息安全保降。这时就需要选择装 Pod 调度到运行在物理虚拟化的容器运行时。 这样才有相对其它运行时更高的隔离级别同时也会有额外的资源开销。
&lt;code&gt;RuntimeClass&lt;/code&gt; 也可以用于在同一个运行时上，对不同的 Pod 使用不同的配置。&lt;/p&gt;
&lt;h2 id=&#34;设置&#34;&gt;设置&lt;/h2&gt;
&lt;p&gt;确保 &lt;code&gt;RuntimeClass&lt;/code&gt; 功能特性是开启的(默认开启)。关于如何开启或关闭功能特性见&lt;a href=&#34;../../../reference/command-line-tools-reference/feature-gates/&#34;&gt;这里&lt;/a&gt;。 apiserver 和 kubelet 的&lt;code&gt;RuntimeClass&lt;/code&gt; 功能特性必须要开启。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;配置节点上的 CRI 实现(各种运行时配置方式不同)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建相应的 &lt;code&gt;RuntimeClass&lt;/code&gt; 资源&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置节点上的 CRI 实现&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;基于 RuntimeClass 的配置因 容器运行时接口(CRI)具体实现的不同而不同。 具体配置文档见&lt;a href=&#34;#cri-configuration&#34;&gt;下面章节&lt;/a&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 默认情况下 &lt;code&gt;RuntimeClass&lt;/code&gt; 假定整个集群中所有节点的配置是相同的(也就是说所有节点针对容器运行时的配置是一样的)，为了支持这样的配置请见&lt;a href=&#34;#scheduling&#34;&gt;下面章节-调度&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

配置上有一个想对应的 handler 的名称， 被 &lt;code&gt;RuntimeClass&lt;/code&gt; 所引用。 这个字段的命名必须符合 DNS-1123 标签的标准(字母，数字，中划线(&lt;code&gt;-&lt;/code&gt;)组成)&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;创建相应的 &lt;code&gt;RuntimeClass&lt;/code&gt; 资源&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上一步提到的每个配置都有对应的 &lt;code&gt;handler&lt;/code&gt; 名称，用于区分不同的配置。 每一个 &lt;code&gt;handler&lt;/code&gt; 都会创建一个对就的 &lt;code&gt;RuntimeClass&lt;/code&gt; 对象。
&lt;code&gt;RuntimeClass&lt;/code&gt; 资源目前只有两个有意义的字段： 名称 (&lt;code&gt;metadata.name&lt;/code&gt;)和处理器 (&lt;code&gt;handler&lt;/code&gt;), 对象定义类似如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;node.k8s.io/v1beta1  # RuntimeClass 定义在 node.k8s.io API 组中&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;RuntimeClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myclass  # The name the RuntimeClass will be referenced by&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;# RuntimeClass is a non-namespaced resource&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;handler&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myconfiguration  # The name of the corresponding CRI configuration&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;RuntimeClass&lt;/code&gt; 对象命名必须是一个有效的 &lt;a href=&#34;../../00-overview/03-working-with-objects/01-names/#dns-subdomain-names&#34;&gt;DNS 子域名格式&lt;/a&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 建议只有集群管理员对 &lt;code&gt;RuntimeClass&lt;/code&gt; 对象有写权限(create/update/patch/delete). 一般情况这是默认配置。 更多信息见 &lt;a href=&#34;../../../reference/03-access-authn-authz/07-authorization/&#34;&gt;授权概览&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;h2 id=&#34;使用说明&#34;&gt;使用说明&lt;/h2&gt;
&lt;p&gt;当在集群中配置好了 &lt;code&gt;RuntimeClasses&lt;/code&gt;， 使用就是一件简单的事情，只需要在 Pod 定义中添加 &lt;code&gt;runtimeClassName&lt;/code&gt;就行， 例：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;runtimeClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myclass&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个配置会让 kublet 使用对应名称的 &lt;code&gt;RuntimeClass&lt;/code&gt; 来运行这个 Pod， 如果没有叫这个名称的 &lt;code&gt;RuntimeClass&lt;/code&gt;， 或者 CRI 不能运行这个名称对应的处理器，则这个进入 &lt;code&gt;Failed&lt;/code&gt; &lt;a href=&#34;../../03-workloads/00-pods/00-pod-lifecycle/#pod-phase&#34;&gt;状态&lt;/a&gt; 。 错误信息在对应的&lt;a href=&#34;../../../3-tasks/08-debug-application-cluster/00-debug-application-introspection/&#34;&gt;事件&lt;/a&gt;对象中
如果没有 runtimeClassName 字段则使用默认的运行时处理器，与 &lt;code&gt;RuntimeClass&lt;/code&gt; 功能特性关闭等同&lt;/p&gt;
&lt;h3 id=&#34;cri-配置&#34;&gt;CRI 配置&lt;/h3&gt;
&lt;p&gt;更多关于 CRI 运行时配置见&lt;a href=&#34;../../../1-setup/02-production-environment/00-container-runtimes/&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;dockershim&#34;&gt;dockershim&lt;/h4&gt;
&lt;p&gt;k8s 内置的 dockershim CRI 不支持运行时处理器&lt;/p&gt;
&lt;h4 id=&#34;containerd&#34;&gt;containerd&lt;/h4&gt;
&lt;p&gt;运行时处理器通过 containerd 的 &lt;code&gt;/etc/containerd/config.toml&lt;/code&gt; 配置文件时行配置， 有效的处理器配置在运行时配置区:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;[&lt;span style=&#34;color:#a6e22e&#34;&gt;plugins&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;cri&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;containerd&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;runtimes&lt;/span&gt;.&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt;{&lt;span style=&#34;color:#a6e22e&#34;&gt;HANDLER_NAME&lt;/span&gt;}]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;更多 containerd 的配置见文档:  &lt;a href=&#34;https://github.com/containerd/cri/blob/master/docs/config.md&#34;&gt;https://github.com/containerd/cri/blob/master/docs/config.md&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;cri-o&#34;&gt;CRI-O&lt;/h4&gt;
&lt;p&gt;运行时处理器通过 CRI-O 的 &lt;code&gt;/etc/crio/crio.conf&lt;/code&gt; 配置文件时行配置，有效的处理器配置在 &lt;a href=&#34;https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md#crioruntime-table&#34;&gt;crio.runtime table&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[crio.runtime.runtimes.${HANDLER_NAME}]
  runtime_path = &amp;quot;${PATH_TO_BINARY}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;更多 CRI-O 的配置见文档: &lt;a href=&#34;https://raw.githubusercontent.com/cri-o/cri-o/9f11d1d/docs/crio.conf.5.md&#34;&gt;https://raw.githubusercontent.com/cri-o/cri-o/9f11d1d/docs/crio.conf.5.md&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;调度&#34;&gt;调度&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;k8s v1.16， RuntimeClass 通过 scheduling 字段实现了对异构系统集群的支持. 通过对这些字段的配置实现让带有对应 RuntimeClass Pod 被调度到受到支持的节点上。而要实现支持这个调度，需要开启 &lt;a href=&#34;../../../reference/03-access-authn-authz/04-admission-controllers/#runtimeclass&#34;&gt;RuntimeClass 准入控制器&lt;/a&gt;(k8s v1.16 默认开启)&lt;/p&gt;
&lt;p&gt;为了保证 Pod 会落到 RuntimeClass 指定的节点上，需要要在节点上设置通用标签，让 &lt;code&gt;runtimeclass.scheduling.nodeSelector&lt;/code&gt; 可以选择到对应的节点。RuntimeClass 的节点选择器与 Pod 的节点选择器的交集会作为最终的节点选择条件。 如果两都有冲突，则 Pod 会被拒绝。
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

如果支持的节点上有防止其它 RuntimeClass 的 Pod 在其上运行的一毒点(Taint), 则需要在 RuntimeClass 上添加 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/&#39; target=&#39;_blank&#39;&gt;耐受(tolerations)&lt;span class=&#39;tooltip-text&#39;&gt;A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint.&lt;/span&gt;
&lt;/a&gt; 与 nodeSelector 相似， RuntimeClass 的耐受(tolerations) 会与 Pod 的 耐受(tolerations)的并集组成最终的节点&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/&#39; target=&#39;_blank&#39;&gt;耐受(tolerations)&lt;span class=&#39;tooltip-text&#39;&gt;A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint.&lt;/span&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;要了解更多关于节点选择与&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/&#39; target=&#39;_blank&#39;&gt;耐受(tolerations)&lt;span class=&#39;tooltip-text&#39;&gt;A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint.&lt;/span&gt;
&lt;/a&gt;信息，见 &lt;a href=&#34;../../09-scheduling-eviction/02-assign-pod-node/&#34;&gt;分配 Pod 到节点&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;pod-overhead&#34;&gt;Pod Overhead&lt;/h2&gt;
&lt;p&gt;




&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [beta]&lt;/code&gt;
&lt;/div&gt;


用户可以在 Pod 上配置 &lt;code&gt;overhead&lt;/code&gt; 进行资源限制。 声明 &lt;code&gt;overhead&lt;/code&gt; 可以让集群(包括调度器)在调度 Pod 时计算合适的资源。
要使用 Pod &lt;code&gt;overhead&lt;/code&gt; 需要打开功&lt;a href=&#34;../../../reference/command-line-tools-reference/feature-gates/&#34;&gt;能特性开关&lt;/a&gt;(默认开启) PodOverhead
Pod &lt;code&gt;overhead&lt;/code&gt; 是通过 RuntimeClass 的 overhead 字段进行定义的， 用户可以利用 RuntimeClass 来设置正在运行的 Pod的 &lt;code&gt;overhead&lt;/code&gt;， 并保证k8s 能够计算到这些 &lt;code&gt;overhead&lt;/code&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/runtime-class.md&#34;&gt;RuntimeClass Design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/runtime-class-scheduling.md&#34;&gt;RuntimeClass Scheduling Design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Read about the &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/pod-overhead/&#34;&gt;Pod Overhead&lt;/a&gt; concept&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/20190226-pod-overhead.md&#34;&gt;PodOverhead Feature Design&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 容器生命周期钩子</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/containers/container-lifecycle-hooks/</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/containers/container-lifecycle-hooks/</guid>
      <description>
        
        
        &lt;!-- overview --&gt;
&lt;!--
This page describes how kubelet managed Containers can use the Container lifecycle hook framework
to run code triggered by events during their management lifecycle.
--&gt;
&lt;p&gt;本文主要介绍如何使用容器生命周期钩子框架来实现由 kubelet 管理的容器通过在管理过程中的生命周期事件来触发代理运行。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;
&lt;!--
Analogous to many programming language frameworks that have component lifecycle hooks, such as Angular,
Kubernetes provides Containers with lifecycle hooks.
The hooks enable Containers to be aware of events in their management lifecycle
and run code implemented in a handler when the corresponding lifecycle hook is executed.
--&gt;
&lt;p&gt;与许多编程语言的框架的组件有生命周期钩子(如: &lt;code&gt;Angular&lt;/code&gt;)一样， k8s 也为容器提供了生命周期钩子。
生命周期钩子可以是容器收到自身生命周期管理时发生的事件，当这些事件发生时就会触发对应的钩子调用执行处理器中的代码实现&lt;/p&gt;
&lt;h2 id=&#34;容器钩子&#34;&gt;容器钩子&lt;/h2&gt;
&lt;!--
There are two hooks that are exposed to Containers:

`PostStart`

This hook executes immediately after a container is created.
However, there is no guarantee that the hook will execute before the container ENTRYPOINT.
No parameters are passed to the handler.

`PreStop`

This hook is called immediately before a container is terminated due to an API request or management event such as liveness probe failure, preemption, resource contention and others. A call to the preStop hook fails if the container is already in terminated or completed state.
It is blocking, meaning it is synchronous,
so it must complete before the call to delete the container can be sent.
No parameters are passed to the handler.

A more detailed description of the termination behavior can be found in
[Termination of Pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination).

--&gt;
&lt;p&gt;容器钩子有以下两个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;PostStart&lt;/code&gt;
这个钩子在容器创建后马上执行。
但是不能保证鑫子会在容器执行 &lt;code&gt;ENTRYPOINT&lt;/code&gt; 之前执行。&lt;/p&gt;
&lt;p&gt;这个钩子不会向处理器传递任何参数&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;PreStop&lt;/code&gt;
在容器被终结，比如因为一个 API 请求或诸如存活探针失败，优先级较低，资源竞争等原因，会立即执行这个钩子。 如果容器已经为终止或完成状态则调用 &lt;code&gt;preStop&lt;/code&gt; 钩子会失败。
该钩子会造成阻塞，也就是说它是同步的，所以必须在钩子调用处理器执行完成后，才能发送删除容器指令。&lt;/p&gt;
&lt;p&gt;这个钩子不会向处理器传递任何参数&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;钩子处理器的实现方式&#34;&gt;钩子处理器的实现方式&lt;/h3&gt;
&lt;!--
Containers can access a hook by implementing and registering a handler for that hook.
There are two types of hook handlers that can be implemented for Containers:

* Exec - Executes a specific command, such as `pre-stop.sh`, inside the cgroups and namespaces of the Container.
Resources consumed by the command are counted against the Container.
* HTTP - Executes an HTTP request against a specific endpoint on the Container.
--&gt;
&lt;p&gt;容器可以用过为钩子实现并注册处理器来达成钩子的使用，以下为容器可实现的两种类型的钩子处理器&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exec - 在容器的 &lt;code&gt;cgroups&lt;/code&gt; 和 &lt;code&gt;namespaces&lt;/code&gt; 权限内执行指定的命令，例如 &lt;code&gt;pre-stop.sh&lt;/code&gt;， 所需要的资源包含在容器申请的资源中&lt;/li&gt;
&lt;li&gt;HTTP - 向容器上的指定接口发送一个请求&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;钩子处理器的执行&#34;&gt;钩子处理器的执行&lt;/h3&gt;
&lt;!--
When a Container lifecycle management hook is called,
the Kubernetes management system executes the handler in the Container registered for that hook. 

Hook handler calls are synchronous within the context of the Pod containing the Container.
This means that for a `PostStart` hook,
the Container ENTRYPOINT and hook fire asynchronously.
However, if the hook takes too long to run or hangs,
the Container cannot reach a `running` state.

The behavior is similar for a `PreStop` hook.
If the hook hangs during execution,
the Pod phase stays in a `Terminating` state and is killed after `terminationGracePeriodSeconds` of pod ends.
If a `PostStart` or `PreStop` hook fails,
it kills the Container.

Users should make their hook handlers as lightweight as possible.
There are cases, however, when long running commands make sense,
such as when saving state prior to stopping a Container.
--&gt;
&lt;p&gt;当一个容器的生命周期管理钩子被调用时， k8s 管理系统就会执行注册到那个钩子上的处理器。&lt;/p&gt;
&lt;p&gt;钩子处理器调用在容器所在的 Pod 的上下文中是同步的。 也就是说对于一个 &lt;code&gt;PostStart&lt;/code&gt; 钩子， 它与容器的 &lt;code&gt;ENTRYPOINT&lt;/code&gt; 是异步执行的。
但是，如果钩子执行时间过长或挂起，容器便不会进入到 &lt;code&gt;running&lt;/code&gt; 的状态。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PreStop&lt;/code&gt; 钩子的行为模式也是相似的。 如果钩子在执行过程中挂起，则 Pod 在终结前的 &lt;code&gt;terminationGracePeriodSeconds&lt;/code&gt; 时间内都是 &lt;code&gt;Terminating&lt;/code&gt; 状态。
如果 &lt;code&gt;PostStart&lt;/code&gt; 或 &lt;code&gt;PreStop&lt;/code&gt; 执行失败，会导致对应容器被杀死&lt;/p&gt;
&lt;p&gt;用户应该尽可能地让钩子处理器越轻量越好。 但是也有些场景，运行长时间的钩子命令是有意义的，比如当保存状态比停止容器更重要时&lt;/p&gt;
&lt;h3 id=&#34;钩子的送达可靠性&#34;&gt;钩子的送达可靠性&lt;/h3&gt;
&lt;!--
Hook delivery is intended to be *at least once*,
which means that a hook may be called multiple times for any given event,
such as for `PostStart` or `PreStop`.
It is up to the hook implementation to handle this correctly.

Generally, only single deliveries are made.
If, for example, an HTTP hook receiver is down and is unable to take traffic,
there is no attempt to resend.
In some rare cases, however, double delivery may occur.
For instance, if a kubelet restarts in the middle of sending a hook,
the hook might be resent after the kubelet comes back up.
--&gt;
&lt;p&gt;对于 &lt;code&gt;PostStart&lt;/code&gt; 或 &lt;code&gt;PreStop&lt;/code&gt; 钩子投递被定为 &lt;em&gt;至少一次&lt;/em&gt;， 也就是说对于任意一个事件可能多次调用钩子。 这就需要实现的钩子处理器需要正确的应对这种情况。&lt;/p&gt;
&lt;p&gt;通常情况下，只能投递一次。如果钩子所调用的 HTTP 目标接口不可用，这种情况下不会尝试重发。 在有些不常见的场景中，也可能发生两次投递的情况。到目前为止，如果 kubelet 在钩子发送过程中重启，这个钩子可能在 kubelet 启动后再次发送&lt;/p&gt;
&lt;h3 id=&#34;如何调试钩子处理器&#34;&gt;如何调试钩子处理器&lt;/h3&gt;
&lt;!--
The logs for a Hook handler are not exposed in Pod events.
If a handler fails for some reason, it broadcasts an event.
For `PostStart`, this is the `FailedPostStartHook` event,
and for `PreStop`, this is the `FailedPreStopHook` event.
You can see these events by running `kubectl describe pod &lt;pod_name&gt;`.
Here is some example output of events from running this command:
--&gt;
&lt;p&gt;钩子处理器的日志不会输出到 Pod 的事件中。 如果一个处理器因为某些原因失败，会发送一个广播事件。 &lt;code&gt;PostStart&lt;/code&gt; 是 &lt;code&gt;FailedPostStartHook&lt;/code&gt;事件， &lt;code&gt;PreStop&lt;/code&gt;是 &lt;code&gt;FailedPreStopHook&lt;/code&gt;事件。 可以通过执行命令 &lt;code&gt;kubectl describe pod &amp;lt;pod_name&amp;gt;&lt;/code&gt; 查看这些事件， 命令输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Events:
  FirstSeen  LastSeen  Count  From                                                   SubObjectPath          Type      Reason               Message
  ---------  --------  -----  ----                                                   -------------          --------  ------               -------
  1m         1m        1      {default-scheduler }                                                          Normal    Scheduled            Successfully assigned test-1730497541-cq1d2 to gke-test-cluster-default-pool-a07e5d30-siqd
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Pulling              pulling image &amp;quot;test:1.0&amp;quot;
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Created              Created container with docker id 5c6a256a2567; Security:[seccomp=unconfined]
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Pulled               Successfully pulled image &amp;quot;test:1.0&amp;quot;
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Started              Started container with docker id 5c6a256a2567
  38s        38s       1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Killing              Killing container with docker id 5c6a256a2567: PostStart handler: Error executing in Docker Container: 1
  37s        37s       1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Killing              Killing container with docker id 8df9fdfd7054: PostStart handler: Error executing in Docker Container: 1
  38s        37s       2      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}                         Warning   FailedSync           Error syncing pod, skipping: failed to &amp;quot;StartContainer&amp;quot; for &amp;quot;main&amp;quot; with RunContainerError: &amp;quot;PostStart handler: Error executing in Docker Container: 1&amp;quot;
  1m         22s       2      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Warning   FailedPostStartHook
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实践指导
&lt;a href=&#34;../../../3-tasks/02-configure-pod-container/16-attach-handler-lifecycle-event/&#34;&gt;挂载处理器到容器生命周期事件&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: k8s 对象介绍</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/00-kubernetes-objects/</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/00-kubernetes-objects/</guid>
      <description>
        
        
        &lt;p&gt;本文介绍 k8s 对象 是怎么在 k8s API 中表示的，怎么以 &lt;code&gt;.yaml&lt;/code&gt; 格式输出 k8s 对象&lt;/p&gt;
&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;
&lt;p&gt;k8s 对象是 k8s 系统中持久化的实体， k8s 使用这些实体来表示集群的状态，它们具体可以表示如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;哪些容器的应用在(如个节点上)运行&lt;/li&gt;
&lt;li&gt;可用于运行应用的资源&lt;/li&gt;
&lt;li&gt;应用的行为策略，比如重启策略，升级，容错性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;k8s 对象是对用户一个意图的记录，当用户创建一个对象后，系统需要保证对象持续存在。用户通过创建一个来告知 k8s 需要一个什么样的 工作负载(workload), 也就是集群的期望状态(desired state)&lt;/p&gt;
&lt;p&gt;要实现对 k8s 对象的管理，比如增删改查都需要调用 k8s API, 例如，当用户可以通过 kubectl 命令来实现对API的调用。也可以通过自己写程序实现对 k8s API 的调用，调用库详见(&lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/client-libraries/&#34;&gt;https://kubernetes.io/docs/reference/using-api/client-libraries/&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;对象的-specstatus-属性&#34;&gt;对象的 &lt;code&gt;Spec&lt;/code&gt;，&lt;code&gt;Status&lt;/code&gt; 属性&lt;/h2&gt;
&lt;p&gt;基本上所有的 k8s 都包含两个嵌套对象作为属性用于管理对象的配置，其中一个对象为 &lt;code&gt;spec&lt;/code&gt;， 另一个为 &lt;code&gt;status&lt;/code&gt;， 用户在创建对象进设置 &lt;code&gt;spec&lt;/code&gt; 来定义所需资源的特性，也就是集群的期望状态。&lt;/p&gt;
&lt;p&gt;而 &lt;code&gt;status&lt;/code&gt; 字段，则是对象的当前的实际状态，由 k8s 系统及其组件进行提供和修改。 k8s 控制中心的任务就是始终让所有对象的实际状态与期望状态一致。&lt;/p&gt;
&lt;p&gt;例如: 在 k8s 中， 一个  Deployment 对象表示运行有用户集群中的一个应用，当用户创建一个 Deployment 并在 spec 对象中设置应用副本数为 3时， k8s 会读取对象属性，启动用户所期望的三个实例并更新相应的状态以达成与 spec 配置的一致。 如果其它任意一个实例失效(某一状态发生变化)， k8s 将会对 spec 与 status 之间的差异采取行动，在当前描述的情况下就会再启动一个实例代替失效的实例。更多关于对象 &lt;code&gt;spec&lt;/code&gt;, &lt;code&gt;status&lt;/code&gt;, &lt;code&gt;metadata&lt;/code&gt; 相关信息看&lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;怎么描述一个-k8s-对象&#34;&gt;怎么描述一个 k8s 对象&lt;/h2&gt;
&lt;p&gt;用户在创建 k8s 对象时，必须要提供描述期望状态的 &lt;code&gt;spec&lt;/code&gt;, 同时还需要该对象的基础信息，比如名称。 当用户使用 API 创建对象时(无论是直接调用还是通过 &lt;code&gt;kubectl&lt;/code&gt; ), 对象信息都为以JSON格式作为请求的消息体发送给 API.&lt;/p&gt;
&lt;p&gt;一般情况下使用 &lt;code&gt;yaml&lt;/code&gt; 文件为 &lt;code&gt;kubectl&lt;/code&gt;提供信息, 此时kubectl 会将 &lt;code&gt;.yaml&lt;/code&gt; 格式转化为JSON格式然后对 k8s API 发起请求&lt;/p&gt;
&lt;p&gt;以下是示例为创建一个 Deployment 必要字段和对象 &lt;code&gt;spec&lt;/code&gt;的 &lt;code&gt;yaml&lt;/code&gt; 文件：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1 # 集群版本 &amp;lt; 1.9.0 使用 apps/v1beta2&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx-deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# 需要按以下模板运行 3 个 Pod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx:1.14.2&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;要使用以上的 &lt;code&gt;yaml&lt;/code&gt; 文件创建一个 Deployment，一种方式是通过 &lt;code&gt;kubectl apply&lt;/code&gt; 命令，并将这个 &lt;code&gt;yaml&lt;/code&gt; 文件作为参数&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl apply -f nginx-deployment.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;命令输出如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment created --record
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;必要字段&#34;&gt;必要字段&lt;/h2&gt;
&lt;p&gt;在使用 &lt;code&gt;.yaml&lt;/code&gt; 创建对象时，以下是必要字段:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;apiVersion&lt;/code&gt; 使用哪个版本的 k8s API 创建这个对象&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kind&lt;/code&gt; 创建对象的类型&lt;/li&gt;
&lt;li&gt;&lt;code&gt;metadata&lt;/code&gt; 唯一标识对象的信息，
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;name&lt;/code&gt; 字符串&lt;/li&gt;
&lt;li&gt;&lt;code&gt;UID&lt;/code&gt; (系统会生成?)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;namespace&lt;/code&gt;, 可选，默认 default&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spec&lt;/code&gt; 对象实际定义(期望)， 每类对象不一样&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中 &lt;code&gt;spec&lt;/code&gt; 字段值是一个嵌套对象，其字段因不同的对象类型而有不同。&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/&#34;&gt;这个文档&lt;/a&gt;包含k8s 所有对象的创建, 比如 &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#podspec-v1-core&#34;&gt;这里是Pod的 spec 详情&lt;/a&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#deploymentspec-v1-apps&#34;&gt;这里是Deployment的 spec 详情&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;源文件&#34;&gt;源文件&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/&#34;&gt;https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;引申阅读&#34;&gt;引申阅读&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/api-overview/&#34;&gt;k8s API 概念说明&lt;/a&gt;
&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/&#34;&gt;k8s 最重要基础概念 Pod&lt;/a&gt;
&lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/controller/&#34;&gt;k8s 的控制器&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 对象命令与ID</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/names/</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/names/</guid>
      <description>
        
        
        &lt;p&gt;在 k8s 的对象中，同一类型的对象名称唯一， 比如在一个 &lt;a href=&#34;./02-namespace&#34;&gt;命名空间&lt;/a&gt;中只能有一个叫 &lt;code&gt;myapp-1234&lt;/code&gt; 在 Pod， 但不同类型的资源可以有相同的名称，比如还可以定义一个叫 &lt;code&gt;myapp-1234&lt;/code&gt; 在 Deployment。
每个对象也有一个 &lt;code&gt;UID&lt;/code&gt; 这个 &lt;code&gt;UID&lt;/code&gt; 整个集群全局唯一
用户需要定义非唯一的，用户自定义的属性，则可通过 &lt;a href=&#34;./03-label-selectors&#34;&gt;标签&lt;/a&gt; 和 &lt;a href=&#34;04-annotation&#34;&gt;注解&lt;/a&gt; 实现&lt;/p&gt;
&lt;h1 id=&#34;对象名称&#34;&gt;对象名称&lt;/h1&gt;
&lt;p&gt;对象的名称是一个字符串，体现在对象的 URL中， 比如 &lt;code&gt;/api/v1/pods/some-name&lt;/code&gt;
在同一时间，一个类型的对象名称必须唯一，但如果用户删除的这个对象，则可以用这个名称再创建一个新的对象&lt;/p&gt;
&lt;p&gt;以下是命令规范三类限制&lt;/p&gt;
&lt;h3 id=&#34;dns-subdomain-names&#34;&gt;DNS 子域名&lt;/h3&gt;
&lt;p&gt;多数类型的资源命令必须可以作为 DNS 子域名，定义在这里(&lt;a href=&#34;https://tools.ietf.org/html/rfc1123),&#34;&gt;https://tools.ietf.org/html/rfc1123),&lt;/a&gt; 总结如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不能多于 253个字符&lt;/li&gt;
&lt;li&gt;只能包含小写字母，数字， &lt;code&gt;-&lt;/code&gt;(中划线)，&lt;code&gt;.&lt;/code&gt;(点)&lt;/li&gt;
&lt;li&gt;只能以字母数字开头&lt;/li&gt;
&lt;li&gt;只能以字母数字结尾&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更多资料见 &lt;a href=&#34;https://en.wikipedia.org/wiki/Subdomain&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;dns-标签名&#34;&gt;DNS 标签名&lt;/h3&gt;
&lt;p&gt;有些资源类型命令遵循 DNS 标签名称， 规则如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最多 63 个字符&lt;/li&gt;
&lt;li&gt;只能包含小写字母，数字， &lt;code&gt;-&lt;/code&gt;(中划线)&lt;/li&gt;
&lt;li&gt;只能以字母数字开头&lt;/li&gt;
&lt;li&gt;只能以字母数字结尾&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;路径分段名&#34;&gt;路径分段名&lt;/h3&gt;
&lt;p&gt;有些资源类型的命令必须要能够编码到 路径的一段上，所以名称不能包含 &lt;code&gt;.&lt;/code&gt; 或 &lt;code&gt;..&lt;/code&gt;, 也不能包含 &lt;code&gt;/&lt;/code&gt; 或 &lt;code&gt;%&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;以下为一个命令为 &lt;code&gt;nginx-demo&lt;/code&gt; 的 Pod 示例&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx-demo&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx:1.14.2&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意: 还有一些资源类型有更严格的命名规则&lt;/p&gt;
&lt;h2 id=&#34;uids&#34;&gt;UIDs&lt;/h2&gt;
&lt;p&gt;由系统创建的对象的唯一标识，类型为字符串
k8s 集群整个生命周期内，创建的每一个对象的UID都是唯一的，用于区分可能存在或曾今过的相似对象
k8s UID 是 UUID， 使用标准为  &lt;code&gt;ISO/IEC 9834-8&lt;/code&gt;  和 &lt;code&gt;ITU-T X.667&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;引申阅读&#34;&gt;引申阅读&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;./03-label-selectors&#34;&gt;k8s 标签&lt;/a&gt;
&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/architecture/identifiers.md&#34;&gt;k8s 标识符与命令设计文档&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 命名空间(Namespaces)</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/02-namespace/</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/02-namespace/</guid>
      <description>
        
        
        &lt;p&gt;k8s 可以在一个物理集群上创建多个虚拟集群, 每个命名空间就是一个虚拟集群&lt;/p&gt;
&lt;h2 id=&#34;啥时候用命名空间&#34;&gt;啥时候用命名空间&lt;/h2&gt;
&lt;p&gt;命名空间是为用户分散于有多个组或项目下的场景设计的。 如果只有二三十个用户就不用想了，当需要用到命名空间的特性时才考虑用命名空间
命名空间是对象名称的一个作用域，对象命名只需要在一个命名空间唯一即可，命名空间不可以嵌套且一个资源对象只能属于一个命名空间
命名空间是集群中多个(组)用户分配资源的一个方式(&lt;a href=&#34;../../../08-policy/01-resource-quotas&#34;&gt;通过资源配额&lt;/a&gt;)
未来版本，一个命名空间下的对象可能有相同的默认访问控制策略
不必要用命名空间来区分差异较小的资源，比如同一个软件的不同版本，可以同一个命名空间下使用标签(labels)区分这些对象&lt;/p&gt;
&lt;h2 id=&#34;管理命名空间&#34;&gt;管理命名空间&lt;/h2&gt;
&lt;p&gt;命名空间的创建和删除请见&lt;a href=&#34;../../../../3-tasks/01-administer-cluster/34-namespaces&#34;&gt;管理指南命名空间部分&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;注意: 在自定义命名空间是，避免使用 &lt;code&gt;kube-&lt;/code&gt; 作为前缀， 这个前缀是 k8s 命名空间保留字&lt;/p&gt;
&lt;h3 id=&#34;查看&#34;&gt;查看&lt;/h3&gt;
&lt;p&gt;可以通过以下命令查看当前集群所有命名空间:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get namespaces
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME              STATUS   AGE
default           Active   1d
kube-node-lease   Active   1d
kube-public       Active   1d
kube-system       Active   1d
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;k8s 启动时会初始化创建以下4个命名空间:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;default：&lt;/code&gt; 当对象不指定命名空间时所属的命名空间&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kube-system&lt;/code&gt;： 由系统创建对象所在的命名空间&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kube-public&lt;/code&gt;：可以被所有人访问(包括未授权用户)，一般只能被系统使用，在这个命名空间的对象，整个集群都可访问，公开只是为了方便，不是强制要求&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kube-node-lease&lt;/code&gt;: 用于放置用于存放与每个节点关联的租约对象，在集群扩容时改善节点心跳性能&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;在请求中添加命名空间&#34;&gt;在请求中添加命名空间&lt;/h3&gt;
&lt;p&gt;为当前请求添加命名空间 使用 &lt;code&gt;--namespace&lt;/code&gt; 参数
示例如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl run nginx --image&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx --namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&amp;lt;insert-namespace-name-here&amp;gt;
kubectl get pods --namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&amp;lt;insert-namespace-name-here&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;设置默认命名空间&#34;&gt;设置默认命名空间&lt;/h3&gt;
&lt;p&gt;持久化配置 kubectl 默认操作的命名空间&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl config set-context --current --namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&amp;lt;insert-namespace-name-here&amp;gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Validate it&lt;/span&gt;
kubectl config view --minify | grep namespace:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;命名空间与dns的关系&#34;&gt;命名空间与DNS的关系&lt;/h3&gt;
&lt;p&gt;当用户创建 &lt;a href=&#34;../../../services-networking/service/&#34;&gt;Service&lt;/a&gt; 时会对应生成一条 &lt;a href=&#34;../../../services-networking/03-dns-pod-service/&#34;&gt;DNS 记录&lt;/a&gt;, 而这条记录中的格式为 &lt;code&gt;&amp;lt;service-name&amp;gt;.&amp;lt;namespace-name&amp;gt;.svc.cluster.local&lt;/code&gt; 也就是说 通过 &lt;code&gt;&amp;lt;service-name&amp;gt;&lt;/code&gt; 只能解析到本命名空间的服务，这在不同命名空间使用同一套配置时相关有用，比如开发，演示，生产等不同环境。如果跨命名空间访问 Service 需要使用全限定名(FQDN)，一般来说只需要 &lt;code&gt;&amp;lt;service-name&amp;gt;.&amp;lt;namespace-name&amp;gt;&lt;/code&gt; 也是可以的&lt;/p&gt;
&lt;h2 id=&#34;那些不属于任何命名空间的对象&#34;&gt;那些不属于任何命名空间的对象&lt;/h2&gt;
&lt;p&gt;大多数 k8s 资源( pod, service, replication controller 等)都会属于某一个命名空间。 而 命名空间 资源则不属于任何命名空间。 还有一个底层资源 比如 &lt;a href=&#34;../../../01-architecture/00-nodes/&#34;&gt;节点&lt;/a&gt;， persistentVolumes 也不属于任何命名空间&lt;/p&gt;
&lt;p&gt;以下命令可以查看资源是否属于命名空间:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 在一个命名空间中&lt;/span&gt;
kubectl api-resources --namespaced&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;true

&lt;span style=&#34;color:#75715e&#34;&gt;# 不属于任何命名空间&lt;/span&gt;
kubectl api-resources --namespaced&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Node&lt;/li&gt;
&lt;li&gt;persistentVolumes&lt;/li&gt;
&lt;li&gt;Events: 根据事件关联的对象，可能有属于命名空间也可能不属于命名空间&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 标签和标签选择器</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/labels/</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/labels/</guid>
      <description>
        
        
        &lt;p&gt;标签是附加在对象上的键值对，应该与所在的对象有关且具有意义
标签可以用于组织和筛选一组对象
标签可以在对象创建时就定义，也可以在对象创建后任何时间添加或修改
一个对象可以有多个标签，但同一个对象所有标签的名称必须唯一
标签可以让UI或命令行工具快速地查询和监听对象，所以标签不适用于非标识性的内容，非标识性内容应该使用注解(&lt;a href=&#34;../04-annotation/&#34;&gt;annotations&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;动机&#34;&gt;动机&lt;/h2&gt;
&lt;p&gt;标签让用户可以以松耦合的形式把组织架构映射到系统对象上，而不需要客户端存在这些映射
服务部署和批处理流水线通常胡是多维的实体(比如: 多个分区或部署， 多条发布线， 多个层级， 每个次级又有多个微服务)。 要管理这些对象经常需要多维度分割操作， 这就需要打破严格的层级结构的表现形式， 特别是由基础设施而不是由用户决定的死板的层级结构&lt;/p&gt;
&lt;p&gt;常用标签示例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;release&amp;quot; : &amp;quot;stable&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;release&amp;quot; : &amp;quot;canary&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;environment&amp;quot; : &amp;quot;dev&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;environment&amp;quot; : &amp;quot;qa&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;environment&amp;quot; : &amp;quot;production&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;tier&amp;quot; : &amp;quot;frontend&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;tier&amp;quot; : &amp;quot;backend&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;tier&amp;quot; : &amp;quot;cache&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;partition&amp;quot; : &amp;quot;customerA&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;partition&amp;quot; : &amp;quot;customerB&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;track&amp;quot; : &amp;quot;daily&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;track&amp;quot; : &amp;quot;weekly&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;syntax-and-character-set&#34;&gt;语法和字符集&lt;/h2&gt;
&lt;p&gt;标签由 键值对组成。 合法的键可以由两上部分组成， 一个可选的前级加上本身的名称中间用斜线(&lt;code&gt;/&lt;/code&gt;)分隔。
名称部分 不得多于63个字符，必须以字母或数字 ([a-z0-9A-Z])开头和结束，中间部分可以包含中划线(&lt;code&gt;-&lt;/code&gt;)，下划线 &lt;code&gt;(_)&lt;/code&gt;，点 (&lt;code&gt;.&lt;/code&gt;),字母,数字。
如果要使用前缀， 前端必须是一个合法的 DNS 字域名，由多个 DNS 标签组成，中间由点(&lt;code&gt;.&lt;/code&gt;)分隔， 总长度不超过 253 个字符&lt;/p&gt;
&lt;p&gt;如果一个标签键没有前缀则假定它是属于用于私有的，
由系统自动化组件(e.g. &lt;code&gt;kube-scheduler&lt;/code&gt;, &lt;code&gt;kube-controller-manager&lt;/code&gt;, &lt;code&gt;kube-apiserver&lt;/code&gt;, &lt;code&gt;kubectl&lt;/code&gt;, 或其它第三方自动化工具), 在给用户对象加标签时必须加前缀， &lt;code&gt;kubernetes.io/&lt;/code&gt;和 &lt;code&gt;k8s.io/&lt;/code&gt; 为 k8s 核心组件保留前缀&lt;/p&gt;
&lt;p&gt;值 可以为空，不多于63个字符，必须以字母或数字 ([&lt;code&gt;a-z0-9A-Z&lt;/code&gt;])开头和结束，中间可以包含中划线(&lt;code&gt;-&lt;/code&gt;)，下划线 &lt;code&gt;(_)&lt;/code&gt;，点 (.),字母,数字&lt;/p&gt;
&lt;p&gt;以下示例中的 Pod 包含 &lt;code&gt;environment: production&lt;/code&gt; 和 &lt;code&gt;app: nginx&lt;/code&gt; 两个标签:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;label-demo&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;environment&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;production&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx:1.14.2&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;标签选择器&#34;&gt;标签选择器&lt;/h2&gt;
&lt;p&gt;与对象名称和对象UID不同，对象标签在对象之间不需要唯一，并且一般来说，会有多个对象有相同的标签
用户或客户端程序可以选择器可以通过标签选择器选取一个对象集. 标签选择器是 k8s 核心分组r 基础
目前标签选择器支持两种选择方式： 等值选择，集合选择。
在待值选择时， 一个标签选择器可以由多个选择条件组成，每个条件用逗号分隔，表示匹配同时满足这些条件的对象，所以这里逗号相当与逻辑与(&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;)关系
空选择器或不使用选择在不同的情况下，有不同的表现。用户标签的 API 需要要正确和有意义的说明文档
注意: 对于有些 API 类型，比如 &lt;code&gt;ReplicaSets&lt;/code&gt; 同一个命名空间两个不同实例的标签选择器不能有交叉， 否些控制器就会将些认为是冲突，导致不能别副本数是否正确(TODO 这里需要有一个示例，描述不太好理解)&lt;/p&gt;
&lt;p&gt;警告: 在编写选择器条件时需要注意，对于 等值选择，集合选择 都没有逻辑与(&lt;code&gt;||&lt;/code&gt;)操作符&lt;/p&gt;
&lt;h3 id=&#34;等值选择&#34;&gt;等值选择&lt;/h3&gt;
&lt;p&gt;等值选择可以分别对于标签的 键和值的相等与不相等，只要对象的标签含有选择器所有的条件相匹配的标签就会选中，不管该对象是否还有其它标签
操作符&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;=,==, 表示等于; 对象标签的键和值都要一致&lt;/li&gt;
&lt;li&gt;!= 表示不等于; 对象标签有该键，但不是该值
例：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;environment = production # 筛选包含标签键为 environment， 且对应值为 production 的对象
tier != frontend # 筛选包含标签键为 tier，且对应值不为 frontend 的对象
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;筛选&lt;code&gt;production&lt;/code&gt;环境中层级除 &lt;code&gt;frontend&lt;/code&gt; 外的对象可以写成 &lt;code&gt;environment=production,tier!=frontend&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;等值选择标签选择器的一个应用场景为为 Pod 指定 节点选择的条件。以下 Pod 节点选择的条件为 &lt;code&gt;accelerator=nvidia-tesla-p100&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cuda-test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cuda-test&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;k8s.gcr.io/cuda-vector-add:v0.1&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;limits&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;nvidia.com/gpu&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;accelerator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nvidia-tesla-p100&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;集合选择器&#34;&gt;集合选择器&lt;/h3&gt;
&lt;p&gt;标签的集合选择器可以且于多值过虑。 支持三种操作符 &lt;code&gt;in&lt;/code&gt;,&lt;code&gt;notin&lt;/code&gt; 和 &lt;code&gt;exists&lt;/code&gt;(仅用在键上)
示例:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;environment in (production, qa) # 筛选 有键为 environment 且对应值为 production 或 qa 的对象
tier notin (frontend, backend) # 筛选 有键为 tier 且值 不是frontend 也不是 backend 的对象和所有不包含键为 tier 的对象
partition # 筛选 存在键为 tier 不管值是啥的对象
!partition # 筛选 不存在键为 tier 不管值是啥的对象
partition,environment notin (qa) # 筛选 存在键为 partition 且 environment 的值不是 qa
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;多个条件之间的逗号赞同一逻辑与(&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;)
&lt;code&gt;environment=production&lt;/code&gt; 与 &lt;code&gt;environment in (production)&lt;/code&gt; 等同
&lt;code&gt;!=&lt;/code&gt; 与 &lt;code&gt;notin&lt;/code&gt; 单值时等同
两种标签选择方式可以组合使用 例： &lt;code&gt;partition in (customerA, customerB),environment!=qa&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;选择器在-api-上的使用&#34;&gt;选择器在 API 上的使用&lt;/h2&gt;
&lt;h3 id=&#34;list-watch-操作时过虑对象&#34;&gt;LIST/ WATCH 操作时过虑对象&lt;/h3&gt;
&lt;p&gt;在时行LIST/ WATCH 操作时可以通过标签选择筛选需要的对象。 上节提及的两种方式都可以使用， 在URL中请求参数类似如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;等值选择: &lt;code&gt;?labelSelector=environment%3Dproduction,tier%3Dfrontend&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;集合选择: &lt;code&gt;?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;也可以用于 REST 客户端 或 &lt;code&gt;kubectl&lt;/code&gt;
例如 &lt;code&gt;kubectl&lt;/code&gt; 中使用&lt;/p&gt;
&lt;p&gt;等值选择&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get pods -l environment&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;production,tier&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;frontend
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;集合选择&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get pods -l &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;environment in (production),tier in (frontend)&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;集合选择的表达更宽泛，比如这个可以达到或的效果&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get pods -l &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;environment in (production, qa)&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;逻辑否的效果&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get pods -l &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;environment,environment notin (frontend)&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;set-references-in-api-objects-啥意思&#34;&gt;Set references in API objects 啥意思&lt;/h3&gt;
&lt;p&gt;有些 k8s 对象，比如 &lt;a href=&#34;../../../services-networking/service/&#34;&gt;Service&lt;/a&gt; &lt;a href=&#34;../../../03-workloads/01-controllers/01-replicationcontroller/&#34;&gt;ReplicationController&lt;/a&gt; 也是通过标签选择器来限定被其管理的资源对象，比如 &lt;a href=&#34;../../../03-workloads/00-pods/&#34;&gt;Pod&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;service-和-replicationcontroller-对选择器的使用&#34;&gt;&lt;code&gt;Service&lt;/code&gt; 和 &lt;code&gt;ReplicationController&lt;/code&gt; 对选择器的使用&lt;/h3&gt;
&lt;p&gt;Service 通过标签选择器来指定负载均衡的 Pod
ReplicationController 也是通过标签选择器来指定被其管理的 Pod&lt;/p&gt;
&lt;p&gt;只支持等值选择，可以为 &lt;code&gt;yaml&lt;/code&gt; 或 &lt;code&gt;JSON&lt;/code&gt; 格式
示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;component&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis # 相当于 component=redis 或 component in (redis)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;支持集合选择的资源&#34;&gt;支持集合选择的资源&lt;/h3&gt;
&lt;p&gt;较新的资源，如 &lt;code&gt;Job&lt;/code&gt;, &lt;code&gt;Deployment&lt;/code&gt;, &lt;code&gt;ReplicaSet&lt;/code&gt;, &lt;code&gt;DaemonSet&lt;/code&gt; 都支持集合选择&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;component&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
    - {&lt;span style=&#34;color:#f92672&#34;&gt;key: tier, operator: In, values&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;cache]}&lt;/span&gt;
    - {&lt;span style=&#34;color:#f92672&#34;&gt;key: environment, operator: NotIn, values&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;dev]}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;matchLabels&lt;/code&gt; 是一个键值对字典&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;component&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 等同与&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
    - {&lt;span style=&#34;color:#f92672&#34;&gt;key: component, operator: In, values&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;redis]}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;matchExpressions&lt;/code&gt; 是一个条件的集合
其条件中可用的操作符(operator)包括: &lt;code&gt;In&lt;/code&gt;, &lt;code&gt;NotIn&lt;/code&gt;, &lt;code&gt;Exists&lt;/code&gt;, &lt;code&gt;DoesNotExist&lt;/code&gt;
&lt;code&gt;NotIn&lt;/code&gt; 的值必须非空
包括 &lt;code&gt;matchLabels&lt;/code&gt; 和 &lt;code&gt;matchExpressions&lt;/code&gt; 定义的条件,所有条件之间的关系为逻辑与.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;选择节点集合&#34;&gt;选择节点集合&lt;/h3&gt;
&lt;p&gt;标签的另一个应用场景为筛选 Pod 可以调度的 节点。 具体见&lt;a href=&#34;../../../09-scheduling-eviction/02-assign-pod-node/&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 注解 (Annotations)</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/04-annotation/</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/04-annotation/</guid>
      <description>
        
        
        &lt;p&gt;用户可以将任非标识符元素关联到对象上，其它库或工具可以读取这些元数据&lt;/p&gt;
&lt;h2 id=&#34;关联元数据到对象&#34;&gt;关联元数据到对象&lt;/h2&gt;
&lt;p&gt;用户可以通过标签或注意的方式将元数据关联到对象上，标签用于选择或查找符合条件的对象， 而在注解中的元数据则不是用于选择或查找对象的，其中的内容可以很小也可以很大，可以是结构化数据也可以是非结构化数据，还可以使用标签不允许的字符
注解与标签类似，也是键值对形式的字典，例如:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;metadata&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;:&lt;/span&gt; {
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;annotations&amp;#34;&lt;/span&gt;: {
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;key1&amp;#34;&lt;/span&gt; : &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;value1&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;key2&amp;#34;&lt;/span&gt; : &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;value2&amp;#34;&lt;/span&gt;
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以用注解存储的常见数据示例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;由声明式配置层管理的字段. 把这些字段作为注解关联到对象是为能与诸如以下字段做区分: 由客户端或服务端设置的默认值; 由自动扩容系统自动生成的字段&lt;/li&gt;
&lt;li&gt;构建，发布，镜像相关信息如 时间戳，发布编号，git 分支，PR 编号，镜像 hash, 镜像库地址&lt;/li&gt;
&lt;li&gt;日志, 监控, 分析, 审计仓库的信息.&lt;/li&gt;
&lt;li&gt;关于客户端库或工具可用于调试目的的信息 例如：名称, 版本, 构建信息.&lt;/li&gt;
&lt;li&gt;来自用户或 工具/系统 信息, 例如 对象在外部系统中的URLs地址&lt;/li&gt;
&lt;li&gt;轻量级回滚帮助信息， 例如, 配置或检查点.&lt;/li&gt;
&lt;li&gt;对使用对象的用户提供修改指导或非标准特性的使用说明&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当然这些信息可以保存在外部的数据库或系统中，但这样制作部署，管理，自省的工具库就不那么容易了&lt;/p&gt;
&lt;h2 id=&#34;语法和字符集&#34;&gt;语法和字符集&lt;/h2&gt;
&lt;p&gt;标签由 键值对组成。 合法的键可以由两上部分组成， 一个可选的前级加上本身的名称中间用斜线(&lt;code&gt;/&lt;/code&gt;)分隔。
名称部分 不得多于63个字符，必须以字母或数字 ([a-z0-9A-Z])开头和结束，中间部分可以包含中划线(&lt;code&gt;-&lt;/code&gt;)，下划线 &lt;code&gt;(_)&lt;/code&gt;，点 (&lt;code&gt;.&lt;/code&gt;),字母,数字。
如果要使用前缀， 前端必须是一个合法的 DNS 字域名，由多个 DNS 标签组成，中间由点(&lt;code&gt;.&lt;/code&gt;)分隔， 总长度不超过 253 个字符&lt;/p&gt;
&lt;p&gt;如果一个标签键没有前缀则假定它是属于用于私有的，
由系统自动化组件(e.g. &lt;code&gt;kube-scheduler&lt;/code&gt;, &lt;code&gt;kube-controller-manager&lt;/code&gt;, &lt;code&gt;kube-apiserver&lt;/code&gt;, &lt;code&gt;kubectl&lt;/code&gt;, 或其它第三方自动化工具), 在给用户对象加标签时必须加前缀， &lt;code&gt;kubernetes.io/&lt;/code&gt;和 &lt;code&gt;k8s.io/&lt;/code&gt; 为 k8s 核心组件保留前缀&lt;/p&gt;
&lt;p&gt;以下示例一个包含一个注解 &lt;code&gt;imageregistry: https://hub.docker.com/&lt;/code&gt; 的 Pod&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;annotations-demo&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;imageregistry&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://hub.docker.com/&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx:1.14.2&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
      </description>
    </item>
    
    <item>
      <title>Docs: 字段选择器</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/05-field-selectors/</link>
      <pubDate>Fri, 03 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/05-field-selectors/</guid>
      <description>
        
        
        &lt;p&gt;用户可以通过字段选择器的以对象的一个或多个字段的值作为选择条件实现对 &lt;a href=&#34;../00-kubernetes-objects/&#34;&gt;k8s 对象&lt;/a&gt;的选择。示例如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;metadata.name=my-service&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;metadata.namespace!=default&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;status.phase=Pending&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下 &lt;code&gt;kubectl&lt;/code&gt; 命令通过选择器，选择 &lt;code&gt;status.phase&lt;/code&gt; 字段值是 &lt;code&gt;Running&lt;/code&gt; 的对象:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get pods --field-selector status.phase&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Running
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意: 字段选择器是基本的资源选择器。 默认没有设置选择条件。 也就是选择该类型所有的对象。这就让以下两个 &lt;code&gt;kubectl&lt;/code&gt; 命令等效 &lt;code&gt;kubectl get pods&lt;/code&gt;， &lt;code&gt;kubectl get pods --field-selector &amp;quot;&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;支持选择的字段&#34;&gt;支持选择的字段&lt;/h2&gt;
&lt;p&gt;字段选择器支持的字段因 k8s 资源不同而不同。 但所有的资源都支持 &lt;code&gt;metadata.name&lt;/code&gt; 和 &lt;code&gt;metadata.namespace&lt;/code&gt;， 使用不支持的字段会报错。
比如以下示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get ingress --field-selector foo.bar&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;baz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;错误信息如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Error from server (BadRequest): Unable to find &amp;quot;ingresses&amp;quot; that match label selector &amp;quot;&amp;quot;, field selector &amp;quot;foo.bar=baz&amp;quot;: &amp;quot;foo.bar&amp;quot; is not a known field selector: only &amp;quot;metadata.name&amp;quot;, &amp;quot;metadata.namespace&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;支持的操作符&#34;&gt;支持的操作符&lt;/h2&gt;
&lt;p&gt;字段选择器支持的操作符有 &lt;code&gt;=&lt;/code&gt;, &lt;code&gt;==,&lt;/code&gt; 和 &lt;code&gt;!=&lt;/code&gt;， 其中 (&lt;code&gt;=&lt;/code&gt; 和 &lt;code&gt;==&lt;/code&gt; 效果一样)。 例如以下示例表示，选择所有不属于 &lt;code&gt;default&lt;/code&gt; 命名空间的 &lt;code&gt;Service&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get services  --all-namespaces --field-selector metadata.namespace!&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;default
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;多个选择条件&#34;&gt;多个选择条件&lt;/h2&gt;
&lt;p&gt;与标签选择类似， 字段选择和多个条件也可以通过逗号分隔，例如以下示例表示选择所有 &lt;code&gt;status.phase&lt;/code&gt; 不是 &lt;code&gt;Running&lt;/code&gt; 且 &lt;code&gt;spec.restartPolicy&lt;/code&gt; 字段值为 &lt;code&gt;Always&lt;/code&gt; 的 Pod&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get pods --field-selector&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;status.phase!&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Running,spec.restartPolicy&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Always
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;同时筛选多个类型的资源&#34;&gt;同时筛选多个类型的资源&lt;/h2&gt;
&lt;p&gt;字段选择器可以同时对多种类型对象进行筛选， 例如以下示例表示，选择所有不属于 &lt;code&gt;default&lt;/code&gt; 命名空间的 &lt;code&gt;Statefulsets&lt;/code&gt; 和 &lt;code&gt;Services&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;default
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
      </description>
    </item>
    
    <item>
      <title>Docs: 标签设置指导</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/06-common-labels/</link>
      <pubDate>Fri, 03 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/06-common-labels/</guid>
      <description>
        
        
        &lt;p&gt;用户可以使用 kubectl 和 dashboard 外的可视化管理工具。一些通用的描述对象信息的标签配置可以让这些工具更好地工作
为了工具能更好的使用这些标签，建议标签以方便查询的方式来定义对象信息
元数据是围绕应用这个概念来组织的。k8s 并不是一个平台即服务(PaaS),也没有一个对应用有一个强制的格式规范。只是通过元数据来提供应用的描述和信息。关于应用所包含的信息的的规范是相关宽松的
注意: 只能推荐使用这些标签。以方便对应用的管理，这些都不是 k8s 核心工具所必要的
共享标签和注解需要共享一个共同的前缀 &lt;code&gt;app.kubernetes.io&lt;/code&gt;, 没有前缀的标签是用户私有的。在共享标签上使用使用共享前缀是为了保证不会影响到用户私有的标签。&lt;/p&gt;
&lt;h2 id=&#34;标签&#34;&gt;标签&lt;/h2&gt;
&lt;p&gt;为了给读者一个标签使用的整体印象，以下标签可以用在第一个资源对象上。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;标签键&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;th&gt;示例值&lt;/th&gt;
&lt;th&gt;值数据类型&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;app.kubernetes.io/name&lt;/td&gt;
&lt;td&gt;应用名称&lt;/td&gt;
&lt;td&gt;&lt;code&gt;mysql&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;app.kubernetes.io/instance&lt;/td&gt;
&lt;td&gt;用于识别应用实例的唯一名称&lt;/td&gt;
&lt;td&gt;&lt;code&gt;wordpress-abcxzy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;app.kubernetes.io/version&lt;/td&gt;
&lt;td&gt;应用的当前版本 (如 版本号, 版本哈希, 等.)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5.7.21&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;app.kubernetes.io/component&lt;/td&gt;
&lt;td&gt;架构中的结构名&lt;/td&gt;
&lt;td&gt;&lt;code&gt;database&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;app.kubernetes.io/part-of&lt;/td&gt;
&lt;td&gt;这个对象是哪个应用的一部分&lt;/td&gt;
&lt;td&gt;&lt;code&gt;wordpress&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;app.kubernetes.io/managed-by&lt;/td&gt;
&lt;td&gt;用于管理这个对象的工具&lt;/td&gt;
&lt;td&gt;&lt;code&gt;helm&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;以下是一个 &lt;code&gt;StatefulSet&lt;/code&gt; 的实践示例&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StatefulSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress-abcxzy&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/version&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;5.7.21&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/component&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;database&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/part-of&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/managed-by&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;helm&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;应用及应用实例&#34;&gt;应用及应用实例&lt;/h2&gt;
&lt;p&gt;一个应用可能在 k8s 集群的一个命名空间中安装一次或多次。比如 安装多个 wordpress 的(不同)网站
应用名和其实例名都应该作区分， 比如 一个 wordpress 应用为 &lt;code&gt;app.kubernetes.io/name: wordpress&lt;/code&gt;, 其实例可以设置为 &lt;code&gt;app.kubernetes.io/instance: wordpress-abcxzy&lt;/code&gt; 这就应用和实例就比较好识别，当一个应用有多个实例是每个实例的名称都要唯一。&lt;/p&gt;
&lt;h2 id=&#34;示例&#34;&gt;示例&lt;/h2&gt;
&lt;p&gt;以下示例展示这些标签的不同用法&lt;/p&gt;
&lt;h3 id=&#34;一个简单的无状态-service&#34;&gt;一个简单的无状态 Service&lt;/h3&gt;
&lt;p&gt;以下应用场景为 用一个 Deployment 和 Service 对象部署一个简单的无状态服务。 以下为其标签的最简配置
Deployment 用于管理应用运行的 Pod&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myservice&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myservice-abcxzy&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Service 用于应用接入&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myservice&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myservice-abcxzy&lt;/span&gt;
...

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;带数据库的web应用&#34;&gt;带数据库的Web应用&lt;/h3&gt;
&lt;p&gt;稍复杂一点的应用场景: 一个web 应用(WordPress)用到一个数据库(MySQL), 通过 Helm 安装&lt;/p&gt;
&lt;p&gt;WordPress 的 Deployment&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress-abcxzy&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/version&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;4.9.4&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/managed-by&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;helm&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/component&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;server&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/part-of&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;WordPress 的 Service&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress-abcxzy&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/version&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;4.9.4&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/managed-by&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;helm&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/component&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;server&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/part-of&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;MySQL 的 StatefulSet&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StatefulSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql-abcxzy&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/version&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;5.7.21&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/managed-by&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;helm&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/component&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;database&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/part-of&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress&lt;/span&gt;
...

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;MySQL 的 Service&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql-abcxzy&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/version&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;5.7.21&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/managed-by&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;helm&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/component&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;database&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/part-of&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress&lt;/span&gt;
...

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在 StatefulSet 和 Service 可以找到它们自己的信息所属的应用的信息。这些在更大的应用中相当有用。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 初始化容器</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/init-containers/</link>
      <pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/init-containers/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- erictune
title: Init Containers
content_type: concept
weight: 40
---
--&gt;
&lt;!-- overview --&gt;
&lt;!--
This page provides an overview of init containers: specialized containers that run
before app containers in a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;.
Init containers can contain utilities or setup scripts not present in an app image.

You can specify init containers in the Pod specification alongside the `containers`
array (which describes app containers).
  --&gt;
&lt;p&gt;本文主要介绍初始化容器: 一种在 Pod 中在应用容器启动之前运行的专用容器。
初始化容器用于提供应用镜像没有的工具或初始化脚本。
初始化容器定义与应用容器定义是相邻关系&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Understanding init containers

A &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; can have multiple containers
running apps within it, but it can also have one or more init containers, which are run
before the app containers are started.

Init containers are exactly like regular containers, except:

* Init containers always run to completion.
* Each init container must complete successfully before the next one starts.

If a Pod&#39;s init container fails, Kubernetes repeatedly restarts the Pod until the init container
succeeds. However, if the Pod has a `restartPolicy` of Never, Kubernetes does not restart the Pod.

To specify an init container for a Pod, add the `initContainers` field into
the Pod specification, as an array of objects of type
[Container](/docs/reference/generated/kubernetes-api/v1.19/#container-v1-core),
alongside the app `containers` array.
The status of the init containers is returned in `.status.initContainerStatuses`
field as an array of the container statuses (similar to the `.status.containerStatuses`
field).
 --&gt;
&lt;h2 id=&#34;理解初始化容器是做什么的&#34;&gt;理解初始化容器是做什么的&lt;/h2&gt;
&lt;p&gt;一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 中可以饮上多个应用容器，同时还可以
有一个或多个初始化容器，这些初始化容器会在应用容器之前运行并结束。&lt;/p&gt;
&lt;p&gt;初始化容器与普通容器并无太多不同，除了以下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;初始化化容器运行在有限时间结束&lt;/li&gt;
&lt;li&gt;只有在上一初始化容器运行结束后，下一个才能开始运行。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果 Pod 的初始化容器挂了， k8s 会不停重启该 Pod 直至初始化容器成功运行完成。
但如果 Pod 的 &lt;code&gt;restartPolicy&lt;/code&gt; 值为 &lt;code&gt;Never&lt;/code&gt;， 则 k8s 不会重启该 Pod。&lt;/p&gt;
&lt;p&gt;要为一个 Pod 配置初始化容器， 需要在配置中添加 &lt;code&gt;initContainers&lt;/code&gt; 字段， 字段值为一个类型为
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#container-v1-core&#34;&gt;Container&lt;/a&gt;
的对象数组， 与 &lt;code&gt;containers&lt;/code&gt; 字段相邻。
初始化容器返回的状态存放于 &lt;code&gt;.status.initContainerStatuses&lt;/code&gt; 字段，是一个与容器状态字段(&lt;code&gt;.status.containerStatuses&lt;/code&gt;)类似的数组&lt;/p&gt;
&lt;!--
### Differences from regular containers

Init containers support all the fields and features of app containers,
including resource limits, volumes, and security settings. However, the
resource requests and limits for an init container are handled differently,
as documented in [Resources](#resources).

Also, init containers do not support `lifecycle`, `livenessProbe`, `readinessProbe`, or
`startupProbe` because they must run to completion before the Pod can be ready.

If you specify multiple init containers for a Pod, Kubelet runs each init
container sequentially. Each init container must succeed before the next can run.
When all of the init containers have run to completion, Kubelet initializes
the application containers for the Pod and runs them as usual.
 --&gt;
&lt;h3 id=&#34;初始化容器和普通容器有啥区别&#34;&gt;初始化容器和普通容器有啥区别&lt;/h3&gt;
&lt;p&gt;初始化容器动脚应用容器的所有字段和特性， 包括资源限制， 数据卷， 安全设置。
但是对初始化化容器对资源的限制处理方式有所区别，具体见 &lt;a href=&#34;#resources&#34;&gt;资源&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;还有，初始化容器不支持 &lt;code&gt;lifecycle&lt;/code&gt;, &lt;code&gt;livenessProbe&lt;/code&gt;, &lt;code&gt;readinessProbe&lt;/code&gt;, &lt;code&gt;startupProbe&lt;/code&gt;，
因为只有初始化容器执行完 Pod 才可能进入就绪状态。&lt;/p&gt;
&lt;p&gt;如果一个 Pod 中配置了多个初始化容器，则 kubelet 会顺序依次运行。
只有在上一初始化容器运行结束后，下一个才能开始运行。
当所有初始化容器运行完成后， kubelet 才会初始化 Pod 中的应用容器并以常规方式运行&lt;/p&gt;
&lt;!--
## Using init containers

Because init containers have separate images from app containers, they
have some advantages for start-up related code:

* Init containers can contain utilities or custom code for setup that are not present in an app
  image. For example, there is no need to make an image `FROM` another image just to use a tool like
  `sed`, `awk`, `python`, or `dig` during setup.
* The application image builder and deployer roles can work independently without
  the need to jointly build a single app image.
* Init containers can run with a different view of the filesystem than app containers in the
  same Pod. Consequently, they can be given access to
  &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secrets&lt;span class=&#39;tooltip-text&#39;&gt;Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.&lt;/span&gt;
&lt;/a&gt; that app containers cannot access.
* Because init containers run to completion before any app containers start, init containers offer
  a mechanism to block or delay app container startup until a set of preconditions are met. Once
  preconditions are met, all of the app containers in a Pod can start in parallel.
* Init containers can securely run utilities or custom code that would otherwise make an app
  container image less secure. By keeping unnecessary tools separate you can limit the attack
  surface of your app container image.
 --&gt;
&lt;h2 id=&#34;怎么使用初始化容器&#34;&gt;怎么使用初始化容器&lt;/h2&gt;
&lt;p&gt;因为初始化容器与应用容器是使用不同的镜像，所以在执行初始化相关代码有些优势:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;初始化容器可以包含应用容器中没有的工作或自定义代码。
例如， 不需要为了用像 &lt;code&gt;sed&lt;/code&gt;, &lt;code&gt;awk&lt;/code&gt;, &lt;code&gt;python&lt;/code&gt;, &lt;code&gt;dig&lt;/code&gt; 来做初始化而要在应用镜像中加入
&lt;code&gt;FROM&lt;/code&gt; 其它的镜像。&lt;/li&gt;
&lt;li&gt;应用镜像的构建和部署角色可以独立工作，而不需要打在一个应用镜像里&lt;/li&gt;
&lt;li&gt;同一个 Pod 的初始化化容器可以与应用容器运行在不同文件系统视角下， 因而可以让初始化容器可以读取
应用容器不能读取的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secrets&lt;span class=&#39;tooltip-text&#39;&gt;Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.&lt;/span&gt;
&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;因为只能在初始化容器运行完成后应用容器才能启动， 初始化容器就提供了一种机制，可以让容器在
达成一定前置条件之前应用容器会被阻塞或延迟。 当前置条件达成， Pod 中所有的应用容器可以并行启动。&lt;/li&gt;
&lt;li&gt;可以在初始化容器可以安全地运行放在应用容器中可以不那么安全的工具或自定义代码。
通过将不必要的工具从应用镜像中移出(到初始化容器中)可以减少应用容器的攻击面&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Examples
Here are some ideas for how to use init containers:

* Wait for a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; to
  be created, using a shell one-line command like:
  ```shell
  for i in {1..100}; do sleep 1; if dig myservice; then exit 0; fi; done; exit 1
  ```

* Register this Pod with a remote server from the downward API with a command like:
  ```shell
  curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d &#39;instance=$(&lt;POD_NAME&gt;)&amp;ip=$(&lt;POD_IP&gt;)&#39;
  ```

* Wait for some time before starting the app container with a command like
  ```shell
  sleep 60
  ```

* Clone a Git repository into a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;Volume&lt;span class=&#39;tooltip-text&#39;&gt;A directory containing data, accessible to the containers in a pod.&lt;/span&gt;
&lt;/a&gt;

* Place values into a configuration file and run a template tool to dynamically
  generate a configuration file for the main app container. For example,
  place the `POD_IP` value in a configuration and generate the main app
  configuration file using Jinja.
 --&gt;
&lt;h3 id=&#34;示例&#34;&gt;示例&lt;/h3&gt;
&lt;p&gt;以下为几个怎么用初始化容器的点子:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;等待一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 创建完成，
使用以下 shell 命令:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i in &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;1..100&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;; &lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt; sleep 1; &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; dig myservice; &lt;span style=&#34;color:#66d9ef&#34;&gt;then&lt;/span&gt; exit 0; &lt;span style=&#34;color:#66d9ef&#34;&gt;fi&lt;/span&gt;; &lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;; exit &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;通过以下命令将该 Pod 注册到一个远程服务的 WEB API
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;instance=$(&amp;lt;POD_NAME&amp;gt;)&amp;amp;ip=$(&amp;lt;POD_IP&amp;gt;)&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;等待一定时间后再启动 Pod中的应用容器
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sleep &lt;span style=&#34;color:#ae81ff&#34;&gt;60&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;从 Git 仓库中克隆一个库到 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;Volume&lt;span class=&#39;tooltip-text&#39;&gt;A directory containing data, accessible to the containers in a pod.&lt;/span&gt;
&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;将配置值放入配置文件，通过一个模板工具为应用容器动态生成配置文件， 例， 将 &lt;code&gt;POD_IP&lt;/code&gt; 放丰配置文件中
通过如 &lt;code&gt;Jinja&lt;/code&gt; 这样的工具生成主应用的配置文件&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
#### Init containers in use

This example defines a simple Pod that has two init containers.
The first waits for `myservice`, and the second waits for `mydb`. Once both
init containers complete, the Pod runs the app container from its `spec` section.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: [&#39;sh&#39;, &#39;-c&#39;, &#39;echo The app is running! &amp;&amp; sleep 3600&#39;]
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: [&#39;sh&#39;, &#39;-c&#39;, &#34;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done&#34;]
  - name: init-mydb
    image: busybox:1.28
    command: [&#39;sh&#39;, &#39;-c&#39;, &#34;until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done&#34;]
```

You can start this Pod by running:

```shell
kubectl apply -f myapp.yaml
```
```
pod/myapp-pod created
```

And check on its status with:
```shell
kubectl get -f myapp.yaml
```
```
NAME        READY     STATUS     RESTARTS   AGE
myapp-pod   0/1       Init:0/2   0          6m
```

or for more details:
```shell
kubectl describe -f myapp.yaml
```
```
Name:          myapp-pod
Namespace:     default
[...]
Labels:        app=myapp
Status:        Pending
[...]
Init Containers:
  init-myservice:
[...]
    State:         Running
[...]
  init-mydb:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Containers:
  myapp-container:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Events:
  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message
  ---------    --------    -----    ----                      -------------                           --------      ------        -------
  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201
  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image &#34;busybox&#34;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image &#34;busybox&#34;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container with docker id 5ced34a04634; Security:[seccomp=unconfined]
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container with docker id 5ced34a04634
```

To see logs for the init containers in this Pod, run:
```shell
kubectl logs myapp-pod -c init-myservice # Inspect the first init container
kubectl logs myapp-pod -c init-mydb      # Inspect the second init container
```

At this point, those init containers will be waiting to discover Services named
`mydb` and `myservice`.

Here&#39;s a configuration you can use to make those Services appear:

```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
---
apiVersion: v1
kind: Service
metadata:
  name: mydb
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9377
```

To create the `mydb` and `myservice` services:

```shell
kubectl apply -f services.yaml
```
```
service/myservice created
service/mydb created
```

You&#39;ll then see that those init containers complete, and that the `myapp-pod`
Pod moves into the Running state:

```shell
kubectl get -f myapp.yaml
```
```
NAME        READY     STATUS    RESTARTS   AGE
myapp-pod   1/1       Running   0          9m
```

This simple example should provide some inspiration for you to create your own
init containers. [What&#39;s next](#whats-next) contains a link to a more detailed example.
 --&gt;
&lt;h4 id=&#34;初始化容器实践&#34;&gt;初始化容器实践&lt;/h4&gt;
&lt;p&gt;本示例定义一个简单的Pod， 其中包含两个初始化容器。 第一个等待 &lt;code&gt;myservice&lt;/code&gt; 的创建，
第二个等待 &lt;code&gt;mydb&lt;/code&gt; 的创建。 当这两个初始化容器都运行完成，则运行 &lt;code&gt;spec&lt;/code&gt; 配置的应用容器。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myapp-pod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myapp&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myapp-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox:1.28&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sh&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;-c&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;echo The app is running! &amp;amp;&amp;amp; sleep 3600&amp;#39;&lt;/span&gt;]
  &lt;span style=&#34;color:#f92672&#34;&gt;initContainers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;init-myservice&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox:1.28&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sh&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;-c&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done&amp;#34;&lt;/span&gt;]
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;init-mydb&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox:1.28&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sh&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;-c&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done&amp;#34;&lt;/span&gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;通过以下命令启动该 Pod&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl apply -f myapp.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;pod/myapp-pod created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过以下命令查看 Pod 状态&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get -f myapp.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;结果类似如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME        READY     STATUS     RESTARTS   AGE
myapp-pod   0/1       Init:0/2   0          6m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过以下命令可以查看更详情的信息&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl describe -f myapp.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;结果类似如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:          myapp-pod
Namespace:     default
[...]
Labels:        app=myapp
Status:        Pending
[...]
Init Containers:
  init-myservice:
[...]
    State:         Running
[...]
  init-mydb:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Containers:
  myapp-container:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Events:
  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message
  ---------    --------    -----    ----                      -------------                           --------      ------        -------
  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201
  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image &amp;quot;busybox&amp;quot;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image &amp;quot;busybox&amp;quot;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container with docker id 5ced34a04634; Security:[seccomp=unconfined]
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container with docker id 5ced34a04634
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过以下命令查看初始化容器的日志&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl logs myapp-pod -c init-myservice &lt;span style=&#34;color:#75715e&#34;&gt;# 查看第一个初始化容器&lt;/span&gt;
kubectl logs myapp-pod -c init-mydb      &lt;span style=&#34;color:#75715e&#34;&gt;# 查看第二个初始化容器&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;此时，这两个初始化容器都分别在等待 &lt;code&gt;myservice&lt;/code&gt; 和 &lt;code&gt;mydb&lt;/code&gt; 两个
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 被创建
以下为创建所需要 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 的定义文件&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myservice&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mydb&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9377&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;通过以下命令创建 &lt;code&gt;myservice&lt;/code&gt; 和 &lt;code&gt;mydb&lt;/code&gt; 两个
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl apply -f services.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;service/myservice created
service/mydb created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这时候再使用以下命令就会发现初始化容器已经完成， 名叫 myapp-pod 的 Pod 进入运行状态&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get -f myapp.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME        READY     STATUS    RESTARTS   AGE
myapp-pod   1/1       Running   0          9m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个简单示例可以对用户创建自己的初始化容器有所启发。 更多关于初始化容器的示例见 &lt;a href=&#34;#whats-next&#34;&gt;相关资料&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Detailed behavior

During Pod startup, the kubelet delays running init containers until the networking
and storage are ready. Then the kubelet runs the Pod&#39;s init containers in the order
they appear in the Pod&#39;s spec.

Each init container must exit successfully before
the next container starts. If a container fails to start due to the runtime or
exits with failure, it is retried according to the Pod `restartPolicy`. However,
if the Pod `restartPolicy` is set to Always, the init containers use
`restartPolicy` OnFailure.

A Pod cannot be `Ready` until all init containers have succeeded. The ports on an
init container are not aggregated under a Service. A Pod that is initializing
is in the `Pending` state but should have a condition `Initialized` set to true.

If the Pod [restarts](#pod-restart-reasons), or is restarted, all init containers
must execute again.

Changes to the init container spec are limited to the container image field.
Altering an init container image field is equivalent to restarting the Pod.

Because init containers can be restarted, retried, or re-executed, init container
code should be idempotent. In particular, code that writes to files on `EmptyDirs`
should be prepared for the possibility that an output file already exists.

Init containers have all of the fields of an app container. However, Kubernetes
prohibits `readinessProbe` from being used because init containers cannot
define readiness distinct from completion. This is enforced during validation.

Use `activeDeadlineSeconds` on the Pod and `livenessProbe` on the container to
prevent init containers from failing forever. The active deadline includes init
containers.

The name of each app and init container in a Pod must be unique; a
validation error is thrown for any container sharing a name with another.
 --&gt;
&lt;h2 id=&#34;一些细节行为&#34;&gt;一些细节行为&lt;/h2&gt;
&lt;p&gt;在 Pod 的启动过程中， kubelet 会等待网络和存储就绪后才会运行初始化容器。
kubelet 会按照定义配置中的顺序运行初始化容器。&lt;/p&gt;
&lt;p&gt;后一个初始化容器已经在前一个运行且成功退出后才启动。 如果一个容器因为运行环境而挂掉或错误退出，
会根据 Pod 上配置 &lt;code&gt;restartPolicy&lt;/code&gt; 进行重试。 但如果 Pod &lt;code&gt;restartPolicy&lt;/code&gt; 值为 &lt;code&gt;Always&lt;/code&gt;，
初始化容器对应 &lt;code&gt;restartPolicy&lt;/code&gt; 的值为 &lt;code&gt;OnFailure&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;如果 Pod &lt;a href=&#34;#pod-restart-reasons&#34;&gt;重启&lt;/a&gt;，或已经重启，所有的初始化容器都会重新执行。&lt;/p&gt;
&lt;p&gt;初始化容器的定义配置中能修改的字段只有容器的 &lt;code&gt;image&lt;/code&gt; 字段，
如果修改初始化容器的 &lt;code&gt;image&lt;/code&gt; 字段，则表示要重启该 Pod。&lt;/p&gt;
&lt;p&gt;因为初始化容器可以重启，重试，或重新执行， 所以初始化容器的代码必须是幂等的。 特别是，如果代码
需要向 &lt;code&gt;EmptyDirs&lt;/code&gt; 写入文件，需要考虑到输出文件已经存在的情况。&lt;/p&gt;
&lt;p&gt;初始化容器包含应用容器拥有的所有字段。但 kubelet 禁止在初始化容器上使用 &lt;code&gt;readinessProbe&lt;/code&gt;，
因为定义的就绪探针与一个执行完就退出的任务是不适用的。 这会在验证是检查，如果出现则报错。&lt;/p&gt;
&lt;p&gt;在 Pod 上使用 &lt;code&gt;activeDeadlineSeconds&lt;/code&gt; 和容器上使用 &lt;code&gt;livenessProbe&lt;/code&gt; 可以防止初始化容器
一直失败的情况出现。 活跃死线包含初始化容器。&lt;/p&gt;
&lt;p&gt;在 Pod 中的应用容器和初始化容器的名称全局(Pod 作用域内)唯一。 如果有相同的名称在验证时会报错。&lt;/p&gt;
&lt;!--
### Resources

Given the ordering and execution for init containers, the following rules
for resource usage apply:

* The highest of any particular resource request or limit defined on all init
  containers is the *effective init request/limit*
* The Pod&#39;s *effective request/limit* for a resource is the higher of:
  * the sum of all app containers request/limit for a resource
  * the effective init request/limit for a resource
* Scheduling is done based on effective requests/limits, which means
  init containers can reserve resources for initialization that are not used
  during the life of the Pod.
* The QoS (quality of service) tier of the Pod&#39;s *effective QoS tier* is the
  QoS tier for init containers and app containers alike.

Quota and limits are applied based on the effective Pod request and
limit.

Pod level control groups (cgroups) are based on the effective Pod request and
limit, the same as the scheduler.
 --&gt;
&lt;h3 id=&#34;resources&#34;&gt;资源&lt;/h3&gt;
&lt;p&gt;为了让初始化容器能获取到执行所需要的资源，以下为资源申请的规则:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在所有初始化容器中的资源 下限/上限 的单项资源最高值为 &lt;em&gt;有效初始化下限/上限&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Pod 的每项资源的 &lt;em&gt;有效 下限/上限&lt;/em&gt; 是以下中较大的一个:
&lt;ul&gt;
&lt;li&gt;所有应用容器该资源的 下限/上限 的总和&lt;/li&gt;
&lt;li&gt;该资源的 有效初始化下限/上限&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;调度是基于 资源 有效 下限/上限来进行的。也就是初始化容器可以申请用于初始化的资源可能在 Pod
的余生中都用不到了。&lt;/li&gt;
&lt;li&gt;Pod 的 有效 Qos 层中的 QoS (服务质量)层就是初始化容器和应用容器通用的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;配额和限制都是基于有效 Pod 下限/上限执行。&lt;/p&gt;
&lt;p&gt;Pod 级别的控制组(cgroups)基于 有效 Pod 下限/上限， 与调度器一致&lt;/p&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Pod restart reasons {#pod-restart-reasons}

A Pod can restart, causing re-execution of init containers, for the following
reasons:

* A user updates the Pod specification, causing the init container image to change.
  Any changes to the init container image restarts the Pod. App container image
  changes only restart the app container.
* The Pod infrastructure container is restarted. This is uncommon and would
  have to be done by someone with root access to nodes.
* All containers in a Pod are terminated while `restartPolicy` is set to Always,
  forcing a restart, and the init container completion record has been lost due
  to garbage collection.
 --&gt;
&lt;h3 id=&#34;pod-restart-reasons&#34;&gt;引起 Pod 重启的原因&lt;/h3&gt;
&lt;p&gt;可能引起 一个 Pod 重启并导致初始化容器的重新执行的原因如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;某个用户更新的 Pod 定义配置， 导致初始化容器的镜像变更。 任意对初始化容器镜像的修改都会导致 Pod 重启。
应用容器镜像变更只能重启该应用容器&lt;/li&gt;
&lt;li&gt;Pod 基础设施容器被重启，这种情况不常见， 只能由拥有节点 root 权限的用户进行。&lt;/li&gt;
&lt;li&gt;Pod 中所有的容器都被终止，因为&lt;code&gt;restartPolicy&lt;/code&gt; 的值为 &lt;code&gt;Always&lt;/code&gt;，所以强制重启， 此时初始化容器
的完成记录因为垃圾清楚而丢失。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
* Read about [creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container)
* Learn how to [debug init containers](/docs/tasks/debug-application-cluster/debug-init-containers/)
 --&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container&#34;&gt;创建带有初始化容器的 Pod&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/debug-application-cluster/debug-init-containers/&#34;&gt;调试初始化容器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Pod 拓扑分布约束条件</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/pod-topology-spread-constraints/</link>
      <pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/pod-topology-spread-constraints/</guid>
      <description>
        
        
        &lt;!--
---
title: Pod Topology Spread Constraints
content_type: concept
weight: 40
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--  





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [beta]&lt;/code&gt;
&lt;/div&gt;



You can use _topology spread constraints_ to control how &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; are spread across your cluster among failure-domains such as regions, zones, nodes, and other user-defined topology domains. This can help to achieve high availability as well as efficient resource utilization.
--&gt;
&lt;p&gt;




&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [beta]&lt;/code&gt;
&lt;/div&gt;


用户可以通过 &lt;em&gt;拓扑分布约束条件&lt;/em&gt; 来控制 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
在包含集群中故障域 （如 地区，分区，节点和其它用户定义拓扑域）中是怎样分布的。
该功能可以帮助用户在实现高可用的同时充分利用资源。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Prerequisites

### Enable Feature Gate

The `EvenPodsSpread` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
must be enabled for the
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API Server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; **and**
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-scheduler/&#39; target=&#39;_blank&#39;&gt;scheduler&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.&lt;/span&gt;
&lt;/a&gt;.

### Node Labels

Topology spread constraints rely on node labels to identify the topology domain(s) that each Node is in. For example, a Node might have labels: `node=node1,zone=us-east-1a,region=us-east-1`

Suppose you have a 4-node cluster with the following labels:

```
NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &lt;none&gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &lt;none&gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &lt;none&gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &lt;none&gt;   2m43s   v1.16.0   node=node4,zone=zoneB
```

Then the cluster is logically viewed as below:

```
+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
```

Instead of manually applying labels, you can also reuse the [well-known labels](/docs/reference/kubernetes-api/labels-annotations-taints/) that are created and populated automatically on most clusters.
 --&gt;
&lt;h2 id=&#34;准备工作&#34;&gt;准备工作&lt;/h2&gt;
&lt;h3 id=&#34;打开功能开关&#34;&gt;打开功能开关&lt;/h3&gt;
&lt;p&gt;需要打开
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API Server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; &lt;strong&gt;和&lt;/strong&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-scheduler/&#39; target=&#39;_blank&#39;&gt;scheduler&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.&lt;/span&gt;
&lt;/a&gt;
中叫 &lt;code&gt;EvenPodsSpread&lt;/code&gt; 的&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;为节点添加恰当的标签&#34;&gt;为节点添加恰当的标签&lt;/h3&gt;
&lt;p&gt;拓扑分布约束条件信赖于节点标签来区分其所在的拓扑域。 例如， 某节点标签可以为:
&lt;code&gt;node=node1,zone=us-east-1a,region=us-east-1&lt;/code&gt;
假设集群中有4个节点，标签如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &amp;lt;none&amp;gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &amp;lt;none&amp;gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &amp;lt;none&amp;gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &amp;lt;none&amp;gt;   2m43s   v1.16.0   node=node4,zone=zoneB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;那么该集群的逻辑视图如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;相较于手动添加标签，可以重用在大多数集群会自动创建和添加的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/kubernetes-api/labels-annotations-taints/&#34;&gt;常用标签&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Spread Constraints for Pods

### API

The field `pod.spec.topologySpreadConstraints` is introduced in 1.16 as below:

```
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  topologySpreadConstraints:
    - maxSkew: &lt;integer&gt;
      topologyKey: &lt;string&gt;
      whenUnsatisfiable: &lt;string&gt;
      labelSelector: &lt;object&gt;
```

You can define one or multiple `topologySpreadConstraint` to instruct the kube-scheduler how to place each incoming Pod in relation to the existing Pods across your cluster. The fields are:

- **maxSkew** describes the degree to which Pods may be unevenly distributed. It&#39;s the maximum permitted difference between the number of matching Pods in any two topology domains of a given topology type. It must be greater than zero.
- **topologyKey** is the key of node labels. If two Nodes are labelled with this key and have identical values for that label, the scheduler treats both Nodes as being in the same topology. The scheduler tries to place a balanced number of Pods into each topology domain.
- **whenUnsatisfiable** indicates how to deal with a Pod if it doesn&#39;t satisfy the spread constraint:
  - `DoNotSchedule` (default) tells the scheduler not to schedule it.
  - `ScheduleAnyway` tells the scheduler to still schedule it while prioritizing nodes that minimize the skew.
- **labelSelector** is used to find matching Pods. Pods that match this label selector are counted to determine the number of Pods in their corresponding topology domain. See [Label Selectors](/docs/concepts/overview/working-with-objects/labels/#label-selectors) for more details.

You can read more about this field by running `kubectl explain Pod.spec.topologySpreadConstraints`.
 --&gt;
&lt;h2 id=&#34;pod-的扩散约束&#34;&gt;Pod 的扩散约束&lt;/h2&gt;
&lt;h3 id=&#34;api&#34;&gt;API&lt;/h3&gt;
&lt;p&gt;在 &lt;code&gt;1.16&lt;/code&gt; 版本中加入了 &lt;code&gt;pod.spec.topologySpreadConstraints&lt;/code&gt; 字段，如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  topologySpreadConstraints:
    - maxSkew: &amp;lt;integer&amp;gt;
      topologyKey: &amp;lt;string&amp;gt;
      whenUnsatisfiable: &amp;lt;string&amp;gt;
      labelSelector: &amp;lt;object&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用户可以在 Pod 上定义一个或多个 &lt;code&gt;topologySpreadConstraint&lt;/code&gt;， 用于指导 &lt;code&gt;kube-scheduler&lt;/code&gt;
在集群中有与已经存在的 Pod 相关的新的 Pod 时应该怎么放置。有如下字段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;maxSkew&lt;/strong&gt; 该字段描述 Pod 分布不均匀的程度。 在指定拓扑类型的两个拓扑域中特定 Pod 数量相差数允许的最大值，这个值必须大于 0&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;topologyKey&lt;/strong&gt; 该字段使用节点的标签键， 如果有两个节点包含一个键，且该键值也相同，
调度器会将这两个节点认为在同一个拓扑。 调度器会尝试让两个拓扑域中的 Pod 数量平衡。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;whenUnsatisfiable&lt;/strong&gt; 该字段设置怎么处理不满足分布约束的Pod
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;DoNotSchedule&lt;/code&gt; (默认) 让调度器不要调度该 Pod&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ScheduleAnyway&lt;/code&gt; 让调度器仍然调度，但调度到不均匀度(skew)最低的节点&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;labelSelector&lt;/strong&gt; 用于找到匹配的 Pod。 匹配到的 Pod 会作为对应拓扑域的的一员(参与数量统计)
更多标签和选择器见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/#label-selectors&#34;&gt;标签和选择器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更新多关于该字段的信息请查看 &lt;code&gt;kubectl explain Pod.spec.topologySpreadConstraints&lt;/code&gt; 命令结果。&lt;/p&gt;
&lt;!--
### Example: One TopologySpreadConstraint

Suppose you have a 4-node cluster where 3 Pods labeled `foo:bar` are located in node1, node2 and node3 respectively (`P` represents Pod):

```
+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
|   P   |   P   |   P   |       |
+-------+-------+-------+-------+
```

If we want an incoming Pod to be evenly spread with existing Pods across zones, the spec can be given as:



 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintsone-constraintyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/one-constraint.yaml&#34; download=&#34;pods/topology-spread-constraints/one-constraint.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/one-constraint.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintsone-constraintyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/one-constraint.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



`topologyKey: zone` implies the even distribution will only be applied to the nodes which have label pair &#34;zone:&amp;lt;any value&amp;gt;&#34; present. `whenUnsatisfiable: DoNotSchedule` tells the scheduler to let it stay pending if the incoming Pod can’t satisfy the constraint.

If the scheduler placed this incoming Pod into &#34;zoneA&#34;, the Pods distribution would become [3, 1], hence the actual skew is 2 (3 - 1) - which violates `maxSkew: 1`. In this example, the incoming Pod can only be placed onto &#34;zoneB&#34;:

```
+---------------+---------------+      +---------------+---------------+
|     zoneA     |     zoneB     |      |     zoneA     |     zoneB     |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |  OR  | node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
|   P   |   P   |   P   |   P   |      |   P   |   P   |  P P  |       |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
```

You can tweak the Pod spec to meet various kinds of requirements:

- Change `maxSkew` to a bigger value like &#34;2&#34; so that the incoming Pod can be placed onto &#34;zoneA&#34; as well.
- Change `topologyKey` to &#34;node&#34; so as to distribute the Pods evenly across nodes instead of zones. In the above example, if `maxSkew` remains &#34;1&#34;, the incoming Pod can only be placed onto &#34;node4&#34;.
- Change `whenUnsatisfiable: DoNotSchedule` to `whenUnsatisfiable: ScheduleAnyway` to ensure the incoming Pod to be always schedulable (suppose other scheduling APIs are satisfied). However, it’s preferred to be placed onto the topology domain which has fewer matching Pods. (Be aware that this preferability is jointly normalized with other internal scheduling priorities like resource usage ratio, etc.)
 --&gt;
&lt;h3 id=&#34;示例-单个-topologyspreadconstraint&#34;&gt;示例: 单个 TopologySpreadConstraint&lt;/h3&gt;
&lt;p&gt;假设有一个4节点的集群中有三个标签包含标签为 &lt;code&gt;foo:bar&lt;/code&gt;的 Pod， 分别分布在 1，2，3 号节点上(一个 &lt;code&gt;P&lt;/code&gt; 代表一个 Pod)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
|   P   |   P   |   P   |       |
+-------+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果想要新加入的 Pod 与之前的三个节点均匀的分布在不同的区域内， 可以使用如下配置:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintsone-constraintyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/one-constraint.yaml&#34; download=&#34;pods/topology-spread-constraints/one-constraint.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/one-constraint.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintsone-constraintyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/one-constraint.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;配置中 &lt;code&gt;topologyKey: zone&lt;/code&gt; 表示均匀分布只针对包含标签 &lt;code&gt;zone:&amp;lt;any value&amp;gt;&lt;/code&gt;的节点
&lt;code&gt;whenUnsatisfiable: DoNotSchedul&lt;/code&gt; 表示针对不满足约束的的 Pod， 调度器应该让其挂起&lt;/p&gt;
&lt;p&gt;如果调度器将 Pod 分配的 “zoneA”中， 则 Pod 分布就变成 [3,1], 这时偏差(skew)就为 2(3 - 1)
这就与 &lt;code&gt;maxSkew: 1&lt;/code&gt; 相违背。所以在本例中，新加入的 Pod 就只能被分配到  “zoneB”:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+---------------+      +---------------+---------------+
|     zoneA     |     zoneB     |      |     zoneA     |     zoneB     |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |  OR  | node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
|   P   |   P   |   P   |   P   |      |   P   |   P   |  P P  |       |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用户也可以通过调整配置实现不同的需求&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将 &lt;code&gt;maxSkew&lt;/code&gt; 设置为大于 &lt;code&gt;2&lt;/code&gt;， 这样新加入 Pod 也可以分配到 “zoneA”中&lt;/li&gt;
&lt;li&gt;将 &lt;code&gt;topologyKey&lt;/code&gt; 设置为 &amp;ldquo;node&amp;rdquo;, 则 Pod 的均匀分布范围就从区域变为节点&lt;/li&gt;
&lt;li&gt;将 &lt;code&gt;whenUnsatisfiable&lt;/code&gt; 设置为 &lt;code&gt;ScheduleAnyway&lt;/code&gt; 来保证新加入的 Pod 都能被调度(假设满足其它的调度 API)
但是，会被优先调度到匹配 Pod 少的拓扑域中(也要注意这个优先还需要连同其它内部调度优先级如资源使用率等一起考量)。&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Example: Multiple TopologySpreadConstraints

This builds upon the previous example. Suppose you have a 4-node cluster where 3 Pods labeled `foo:bar` are located in node1, node2 and node3 respectively (`P` represents Pod):

```
+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
|   P   |   P   |   P   |       |
+-------+-------+-------+-------+
```

You can use 2 TopologySpreadConstraints to control the Pods spreading on both zone and node:



 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintstwo-constraintsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/two-constraints.yaml&#34; download=&#34;pods/topology-spread-constraints/two-constraints.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/two-constraints.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintstwo-constraintsyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/two-constraints.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;node&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



In this case, to match the first constraint, the incoming Pod can only be placed onto &#34;zoneB&#34;; while in terms of the second constraint, the incoming Pod can only be placed onto &#34;node4&#34;. Then the results of 2 constraints are ANDed, so the only viable option is to place on &#34;node4&#34;.

Multiple constraints can lead to conflicts. Suppose you have a 3-node cluster across 2 zones:

```
+---------------+-------+
|     zoneA     | zoneB |
+-------+-------+-------+
| node1 | node2 | node3 |
+-------+-------+-------+
|  P P  |   P   |  P P  |
+-------+-------+-------+
```

If you apply &#34;two-constraints.yaml&#34; to this cluster, you will notice &#34;mypod&#34; stays in `Pending` state. This is because: to satisfy the first constraint, &#34;mypod&#34; can only be put to &#34;zoneB&#34;; while in terms of the second constraint, &#34;mypod&#34; can only put to &#34;node2&#34;. Then a joint result of &#34;zoneB&#34; and &#34;node2&#34; returns nothing.

To overcome this situation, you can either increase the `maxSkew` or modify one of the constraints to use `whenUnsatisfiable: ScheduleAnyway`.
 --&gt;
&lt;h3 id=&#34;示例-多个-topologyspreadconstraint&#34;&gt;示例: 多个 TopologySpreadConstraint&lt;/h3&gt;
&lt;p&gt;与上一个示例相同， 假设有一个4节点的集群中有三个标签包含标签为 &lt;code&gt;foo:bar&lt;/code&gt;的 Pod，
分别分布在 1，2，3 号节点上 (一个 &lt;code&gt;P&lt;/code&gt; 代表一个 Pod)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
|   P   |   P   |   P   |       |
+-------+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这次用两个 TopologySpreadConstraints， 同时通过 区域 和节点为控制 Pod 的分布&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintstwo-constraintsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/two-constraints.yaml&#34; download=&#34;pods/topology-spread-constraints/two-constraints.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/two-constraints.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintstwo-constraintsyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/two-constraints.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;node&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;在这种情况下， 要符合第一个约束， 新加入的 Pod 就分被分配到 “zoneB”
再要符合第二个约束，新加入的 Pod 就分被分配到  “node4”
而这两个约束之间是逻辑与关系，也就最终可分配的就 “node4”。&lt;/p&gt;
&lt;p&gt;多个约束可能产生冲突。比如集群在两个区域中有三个节点:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+-------+
|     zoneA     | zoneB |
+-------+-------+-------+
| node1 | node2 | node3 |
+-------+-------+-------+
|  P P  |   P   |  P P  |
+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果在这个集群中执行 &lt;code&gt;two-constraints.yaml&lt;/code&gt;， 就会发现名称为 &lt;code&gt;mypod&lt;/code&gt; 的 Pod 状态一直是 &lt;code&gt;Pending&lt;/code&gt;。
这是因为，要符合第一个约束， 就只能分配到 “zoneB”， 同时要符合第二个约束， 就只能分配到 “node2”
而 “zoneB” 与 “node2” 的交集为空集。&lt;/p&gt;
&lt;p&gt;要解决这种情况， 可以通过增加 &lt;code&gt;maxSkew&lt;/code&gt; 的值，
或 修改其中一个约束的 &lt;code&gt;whenUnsatisfiable&lt;/code&gt;值为&lt;code&gt;ScheduleAnyway&lt;/code&gt;&lt;/p&gt;
&lt;!--  
### Conventions

There are some implicit conventions worth noting here:

- Only the Pods holding the same namespace as the incoming Pod can be matching candidates.

- Nodes without `topologySpreadConstraints[*].topologyKey` present will be bypassed. It implies that:

  1. the Pods located on those nodes do not impact `maxSkew` calculation - in the above example, suppose &#34;node1&#34; does not have label &#34;zone&#34;, then the 2 Pods will be disregarded, hence the incoming Pod will be scheduled into &#34;zoneA&#34;.
  2. the incoming Pod has no chances to be scheduled onto this kind of nodes - in the above example, suppose a &#34;node5&#34; carrying label `{zone-typo: zoneC}` joins the cluster, it will be bypassed due to the absence of label key &#34;zone&#34;.

- Be aware of what will happen if the incomingPod’s `topologySpreadConstraints[*].labelSelector` doesn’t match its own labels. In the above example, if we remove the incoming Pod’s labels, it can still be placed onto &#34;zoneB&#34; since the constraints are still satisfied. However, after the placement, the degree of imbalance of the cluster remains unchanged - it’s still zoneA having 2 Pods which hold label {foo:bar}, and zoneB having 1 Pod which holds label {foo:bar}. So if this is not what you expect, we recommend the workload’s `topologySpreadConstraints[*].labelSelector` to match its own labels.

- If the incoming Pod has `spec.nodeSelector` or `spec.affinity.nodeAffinity` defined, nodes not matching them will be bypassed.

    Suppose you have a 5-node cluster ranging from zoneA to zoneC:

    ```
    +---------------+---------------+-------+
    |     zoneA     |     zoneB     | zoneC |
    +-------+-------+-------+-------+-------+
    | node1 | node2 | node3 | node4 | node5 |
    +-------+-------+-------+-------+-------+
    |   P   |   P   |   P   |       |       |
    +-------+-------+-------+-------+-------+
    ```

    and you know that &#34;zoneC&#34; must be excluded. In this case, you can compose the yaml as below, so that &#34;mypod&#34; will be placed onto &#34;zoneB&#34; instead of &#34;zoneC&#34;. Similarly `spec.nodeSelector` is also respected.

    

 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintsone-constraint-with-nodeaffinityyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&#34; download=&#34;pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintsone-constraint-with-nodeaffinityyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;affinity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nodeAffinity&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelectorTerms&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NotIn&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;zoneC&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


--&gt;
&lt;h3 id=&#34;约定&#34;&gt;约定&lt;/h3&gt;
&lt;p&gt;以下为一些值得注意的隐性约定:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;只有在同一个命名空间中的 Pod 才能作为匹配候选者&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;没有 &lt;code&gt;topologySpreadConstraints[*].topologyKey&lt;/code&gt; 的节点会被当作旁路，隐含的意思为:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;这些只点上的 Pod 不会被用于计算 &lt;code&gt;maxSkew&lt;/code&gt;， 在上面的例子中，假设 &lt;code&gt;node1&lt;/code&gt; 上没有标签 &lt;code&gt;zone&lt;/code&gt;，
这时其上的两个 Pod 会被忽略， 这样新加入的 Pod 就会被分配到  “zoneA”&lt;/li&gt;
&lt;li&gt;新加入的 Pod 也不会有机会被分配到此类节点上， 在上面的例子中，
假设集群中加入了一个 “node5” 上面有个标签为 &lt;code&gt;zone-typo: zoneC&lt;/code&gt;
这个节点(区域)会因为没有标签键 &lt;code&gt;zone&lt;/code&gt; 而被忽略&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;还可能发生的一种情况是 新加入的 Pod 上的 &lt;code&gt;topologySpreadConstraints[*].labelSelector&lt;/code&gt;
与自身的标签不匹配。在上面的例子中， 如果删除新加入 Pod 上的标签，该 Pod 也会被分配到 “zoneB”。
因为约束条件是满足的。 但是在 Pod 分配之后，集群的均衡程度并没有改变， 也就是 &lt;code&gt;zoneA&lt;/code&gt; 中
有两个包含标签 &lt;code&gt;foo:bar&lt;/code&gt;的 Pod， &lt;code&gt;zoneB&lt;/code&gt; 中 有一个包含标签 &lt;code&gt;foo:bar&lt;/code&gt;的 Pod。 如果这不是预期的行为，
官方推荐工作负载的 &lt;code&gt;topologySpreadConstraints[*].labelSelector&lt;/code&gt; 需要匹配自身的标签。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果新加入的 Pod 还定义了 &lt;code&gt;spec.nodeSelector&lt;/code&gt; 或 &lt;code&gt;spec.affinity.nodeAffinity&lt;/code&gt;
不匹配的节点也会被忽略。&lt;/p&gt;
&lt;p&gt;假如一个包含5个节点的集群，有A，B，C三个分区:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+---------------+-------+
|     zoneA     |     zoneB     | zoneC |
+-------+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 | node5 |
+-------+-------+-------+-------+-------+
|   P   |   P   |   P   |       |       |
+-------+-------+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果想让 zoneC 被排除， 这时可以使用以下配置，让 &lt;code&gt;mypod&lt;/code&gt; 被分配到 &lt;code&gt;zoneB&lt;/code&gt; 而不是 &lt;code&gt;zoneC&lt;/code&gt;
同样的 spec.nodeSelector 也要考量&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintsone-constraint-with-nodeaffinityyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&#34; download=&#34;pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintsone-constraint-with-nodeaffinityyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;affinity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nodeAffinity&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelectorTerms&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NotIn&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;zoneC&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Cluster-level default constraints






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [alpha]&lt;/code&gt;
&lt;/div&gt;



It is possible to set default topology spread constraints for a cluster. Default
topology spread constraints are applied to a Pod if, and only if:

- It doesn&#39;t define any constraints in its `.spec.topologySpreadConstraints`.
- It belongs to a service, replication controller, replica set or stateful set.

Default constraints can be set as part of the `PodTopologySpread` plugin args
in a [scheduling profile](/docs/reference/scheduling/profiles).
The constraints are specified with the same [API above](#api), except that
`labelSelector` must be empty. The selectors are calculated from the services,
replication controllers, replica sets or stateful sets that the Pod belongs to.

An example configuration might look like follows:

```yaml
apiVersion: kubescheduler.config.k8s.io/v1alpha2
kind: KubeSchedulerConfiguration

profiles:
  pluginConfig:
    - name: PodTopologySpread
      args:
        defaultConstraints:
          - maxSkew: 1
            topologyKey: failure-domain.beta.kubernetes.io/zone
            whenUnsatisfiable: ScheduleAnyway
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The score produced by default scheduling constraints might conflict with the
score produced by the
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/scheduling/profiles/#scheduling-plugins&#34;&gt;&lt;code&gt;DefaultPodTopologySpread&lt;/code&gt; plugin&lt;/a&gt;.
It is recommended that you disable this plugin in the scheduling profile when
using default constraints for &lt;code&gt;PodTopologySpread&lt;/code&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;集群级的默认约束&#34;&gt;集群级的默认约束&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;可以为一个集群设置默认的拓扑分布约束条件，默认拓扑分布约束条件能且仅能适用于:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod 没有在 &lt;code&gt;.spec.topologySpreadConstraints&lt;/code&gt; 中定义任何约束条件&lt;/li&gt;
&lt;li&gt;Pod 属于
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-replication-controller&#39; target=&#39;_blank&#39;&gt;ReplicationController&lt;span class=&#39;tooltip-text&#39;&gt;一个 (废弃的) API 对象用于管理多副本应用&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#39; target=&#39;_blank&#39;&gt;ReplicaSet&lt;span class=&#39;tooltip-text&#39;&gt;ReplicaSet 保证在任意时间点上某个 Pod 有指定数量副本在运行&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/statefulset/&#39; target=&#39;_blank&#39;&gt;StatefulSet&lt;span class=&#39;tooltip-text&#39;&gt;管理一个 Pod 集合的部署与容量伸缩， 这些 Pod 所以使用的存储是持久的(Pod 被替代后，新的 Pod 继承老 Pod 的存储)， Pod 的标识也是持久化的(重建 Pod 后名字不会变)&lt;/span&gt;
&lt;/a&gt; 之一&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;默认的约束条件可以作为 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/scheduling/profiles&#34;&gt;scheduling profile&lt;/a&gt;
中 &lt;code&gt;PodTopologySpread&lt;/code&gt; 插件参数的一部分。 这些约束条件可以能过同 &lt;a href=&#34;#api&#34;&gt;API&lt;/a&gt; 一样设置，
除了 &lt;code&gt;labelSelector&lt;/code&gt; 必须为空。 选择器通过 Pod 所属的&lt;br&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-replication-controller&#39; target=&#39;_blank&#39;&gt;ReplicationController&lt;span class=&#39;tooltip-text&#39;&gt;一个 (废弃的) API 对象用于管理多副本应用&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#39; target=&#39;_blank&#39;&gt;ReplicaSet&lt;span class=&#39;tooltip-text&#39;&gt;ReplicaSet 保证在任意时间点上某个 Pod 有指定数量副本在运行&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/statefulset/&#39; target=&#39;_blank&#39;&gt;StatefulSet&lt;span class=&#39;tooltip-text&#39;&gt;管理一个 Pod 集合的部署与容量伸缩， 这些 Pod 所以使用的存储是持久的(Pod 被替代后，新的 Pod 继承老 Pod 的存储)， Pod 的标识也是持久化的(重建 Pod 后名字不会变)&lt;/span&gt;
&lt;/a&gt;
计算得出。&lt;/p&gt;
&lt;p&gt;以下为示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubescheduler.config.k8s.io/v1alpha2&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;KubeSchedulerConfiguration&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;profiles&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;pluginConfig&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PodTopologySpread&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;defaultConstraints&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;failure-domain.beta.kubernetes.io/zone&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ScheduleAnyway&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 由默认调度约束计算的结果可能与&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/scheduling/profiles/#scheduling-plugins&#34;&gt;&lt;code&gt;DefaultPodTopologySpread&lt;/code&gt; plugin&lt;/a&gt;计算结果相冲突。
建议用户在使用&lt;code&gt;PodTopologySpread&lt;/code&gt;的默认约束时，关掉调度配置中的插件。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Comparison with PodAffinity/PodAntiAffiniaty

In Kubernetes, directives related to &#34;Affinity&#34; control how Pods are
scheduled - more packed or more scattered.

- For `PodAffinity`, you can try to pack any number of Pods into qualifying
  topology domain(s)
- For `PodAntiAffinity`, only one Pod can be scheduled into a
  single topology domain.

The &#34;EvenPodsSpread&#34; feature provides flexible options to distribute Pods evenly across different
topology domains - to achieve high availability or cost-saving. This can also help on rolling update
workloads and scaling out replicas smoothly. See [Motivation](https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation) for more details.
 --&gt;
&lt;h2 id=&#34;约束条件-vs-podaffinitypodantiaffiniaty&#34;&gt;约束条件 vs &lt;code&gt;PodAffinity/PodAntiAffiniaty&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;在 k8s 中， 与 &lt;code&gt;Affinity&lt;/code&gt; 相关用于控制 Pod 怎么调度的指令，或集中或分散&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于 &lt;code&gt;PodAffinity&lt;/code&gt;， 用户可以尝试向有资格的拓扑域中塞进任意数量的 Pod&lt;/li&gt;
&lt;li&gt;对于 &lt;code&gt;PodAntiAffinity&lt;/code&gt;， 一个 Pod 只能被调度到一个拓扑域中&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;EvenPodsSpread&lt;/code&gt; 特性提供了灵活的选项来让 Pod 均匀的分布到不同的拓扑域中，来达到高可用或减少开支的目的。
这也可以让滚动发布和动态扩容变得更平滑。更多信息见
&lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation&#34;&gt;Motivation&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Known Limitations

As of 1.18, at which this feature is Beta, there are some known limitations:

- Scaling down a Deployment may result in imbalanced Pods distribution.
- Pods matched on tainted nodes are respected. See [Issue 80921](https://github.com/kubernetes/kubernetes/issues/80921)
 --&gt;
&lt;h2 id=&#34;已知的限制&#34;&gt;已知的限制&lt;/h2&gt;
&lt;p&gt;到 &lt;code&gt;1.18&lt;/code&gt;，该特性还是 &lt;code&gt;Beta&lt;/code&gt; 状态， 还有以下已知的限制:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;收缩 Deployment 的容量可能导致 Pod 分布的不均匀。&lt;/li&gt;
&lt;li&gt;匹配到有 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/&#39; target=&#39;_blank&#39;&gt;Taint&lt;span class=&#39;tooltip-text&#39;&gt;A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups.&lt;/span&gt;
&lt;/a&gt; 节点也会被计入， 见&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/80921&#34;&gt;Issue 80921&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Pod 预设信息</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/podpreset/</link>
      <pubDate>Tue, 28 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/podpreset/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jessfraz
title: Pod Presets
content_type: concept
weight: 50
---
 --&gt;
&lt;!-- overview --&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.6 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;This page provides an overview of PodPresets, which are objects for injecting
certain information into pods at creation time. The information can include
secrets, volumes, volume mounts, and environment variables.&lt;/p&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.6 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;本文简单介绍 &lt;code&gt;PodPreset&lt;/code&gt;， 一个用于在特定时间向 Pod 中注入特定信息的对象。
可注入的信息包括
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.&lt;/span&gt;
&lt;/a&gt;,
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;Volume&lt;span class=&#39;tooltip-text&#39;&gt;A directory containing data, accessible to the containers in a pod.&lt;/span&gt;
&lt;/a&gt;,
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;Volume&lt;span class=&#39;tooltip-text&#39;&gt;A directory containing data, accessible to the containers in a pod.&lt;/span&gt;
&lt;/a&gt; 挂载,
和环境变量&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Understanding Pod presets

A PodPreset is an API resource for injecting additional runtime requirements
into a Pod at creation time.
You use [label selectors](/docs/concepts/overview/working-with-objects/labels/#label-selectors)
to specify the Pods to which a given PodPreset applies.

Using a PodPreset allows pod template authors to not have to explicitly provide
all information for every pod. This way, authors of pod templates consuming a
specific service do not need to know all the details about that service.
--&gt;
&lt;h2 id=&#34;理解-pod-预设信息&#34;&gt;理解 Pod 预设信息&lt;/h2&gt;
&lt;p&gt;PodPreset 是在Pod 创建时向其中注入的运行环境需要的额外信息的 API 对象资源。
用户可以通过 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/#label-selectors&#34;&gt;label selectors&lt;/a&gt;
来指定哪些 PodPreset 要用到该 Pod 上面。&lt;/p&gt;
&lt;p&gt;PodPreset 可以让 Pod 模板的创建者不必要为每个 Pod 提供都提供所有信息。
通过这种方式，Pod 模板的创建者在消费特定服务时，不需要知道该服务的所有细节&lt;/p&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Enable PodPreset in your cluster {#enable-pod-preset}

In order to use Pod presets in your cluster you must ensure the following:

1. You have enabled the API type `settings.k8s.io/v1alpha1/podpreset`. For
   example, this can be done by including `settings.k8s.io/v1alpha1=true` in
   the `--runtime-config` option for the API server. In minikube add this flag
   `--extra-config=apiserver.runtime-config=settings.k8s.io/v1alpha1=true` while
   starting the cluster.
1. You have enabled the admission controller named `PodPreset`. One way to doing this
   is to include `PodPreset` in the `--enable-admission-plugins` option value specified
   for the API server. For example, if you use Minikube, add this flag:

   ```shell
   --extra-config=apiserver.enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset
   ```

   while starting your cluster.
 --&gt;
&lt;h2 id=&#34;打开集群中的-podpreset&#34;&gt;打开集群中的 &lt;code&gt;PodPreset&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;为了能使用 Pod 预设信息，集群需要保证达到以下条件:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;需要启用 API 类型 &lt;code&gt;settings.k8s.io/v1alpha1/podpreset&lt;/code&gt;. 具体操作是:
在 api-server 中的 &lt;code&gt;--runtime-config&lt;/code&gt; 选项中添加 &lt;code&gt;settings.k8s.io/v1alpha1=true&lt;/code&gt;;
对于 minikube， 需要在集群启动时添加
&lt;code&gt;--extra-config=apiserver.runtime-config=settings.k8s.io/v1alpha1=true&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;需要启用一个叫 &lt;code&gt;PodPreset&lt;/code&gt; 的准入控制器。
一种方式是在 api-server &lt;code&gt;--enable-admission-plugins&lt;/code&gt; 选择值中添加 &lt;code&gt;PodPreset&lt;/code&gt;
对于 minikube, 则在集群启动时添加以下参数:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt; --extra-config&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;apiserver.enable-admission-plugins&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## How it works

Kubernetes provides an admission controller (`PodPreset`) which, when enabled,
applies Pod Presets to incoming pod creation requests.
When a pod creation request occurs, the system does the following:

1. Retrieve all `PodPresets` available for use.
1. Check if the label selectors of any `PodPreset` matches the labels on the
   pod being created.
1. Attempt to merge the various resources defined by the `PodPreset` into the
   Pod being created.
1. On error, throw an event documenting the merge error on the pod, and create
   the pod _without_ any injected resources from the `PodPreset`.
1. Annotate the resulting modified Pod spec to indicate that it has been
   modified by a `PodPreset`. The annotation is of the form
   `podpreset.admission.kubernetes.io/podpreset-&lt;pod-preset name&gt;: &#34;&lt;resource version&gt;&#34;`.

Each Pod can be matched by zero or more PodPresets; and each PodPreset can be
applied to zero or more Pods. When a PodPreset is applied to one or more
Pods, Kubernetes modifies the Pod Spec. For changes to `env`, `envFrom`, and
`volumeMounts`, Kubernetes modifies the container spec for all containers in
the Pod; for changes to `volumes`, Kubernetes modifies the Pod Spec.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;A Pod Preset is capable of modifying the following fields in a Pod spec when appropriate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;.spec.containers&lt;/code&gt; field&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;.spec.initContainers&lt;/code&gt; field&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;工作原理&#34;&gt;工作原理&lt;/h2&gt;
&lt;p&gt;k8s 提供了一个准入控制器(&lt;code&gt;PodPreset&lt;/code&gt;), 当这个控制器打开时，就会向进入的 Pod 创建请求执行。
当一个 Pod 的创建请求发生时， 系统会做以下操作:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;取得所有可用的 &lt;code&gt;PodPresets&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;检查 &lt;code&gt;PodPresets&lt;/code&gt; 标签选择器是否与新创建的 Pod 上的标签匹配&lt;/li&gt;
&lt;li&gt;尝试将 &lt;code&gt;PodPreset&lt;/code&gt; 中定义的各种资源合并到新创建的 Pod&lt;/li&gt;
&lt;li&gt;如果出错， 抛出一个一个事件描述合并出错到 Pod 上， 并在 &lt;em&gt;不&lt;/em&gt; 注意任意 &lt;code&gt;PodPreset&lt;/code&gt; 资源的情况下创建 Pod&lt;/li&gt;
&lt;li&gt;将由 &lt;code&gt;PodPreset&lt;/code&gt; 修改的结果加入到注解备查。注解格式为 &lt;code&gt;podpreset.admission.kubernetes.io/podpreset-&amp;lt;pod-preset name&amp;gt;: &amp;quot;&amp;lt;resource version&amp;gt;&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
### Disable Pod Preset for a specific pod

There may be instances where you wish for a Pod to not be altered by any Pod
preset mutations. In these cases, you can add an annotation in the Pod&#39;s `.spec`
of the form: `podpreset.admission.kubernetes.io/exclude: &#34;true&#34;`.
 --&gt;
&lt;h3 id=&#34;在指定-pod-禁用-podpreset&#34;&gt;在指定 Pod 禁用 &lt;code&gt;PodPreset&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;针对某些实例，用户可能不希望其被 &lt;code&gt;PodPreset&lt;/code&gt; 修改， 这种情况下， 用户可以在 Pod 定义上添加注解，
格式为 &lt;code&gt;podpreset.admission.kubernetes.io/exclude: &amp;quot;true&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
See [Injecting data into a Pod using PodPreset](/docs/tasks/inject-data-application/podpreset/)

For more information about the background, see the [design proposal for PodPreset](https://git.k8s.io/community/contributors/design-proposals/service-catalog/pod-preset.md).
 --&gt;
&lt;p&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/inject-data-application/podpreset/&#34;&gt;使用 PodPreset 向 Pod 注入数据&lt;/a&gt;
更多背景信息， 见&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/service-catalog/pod-preset.md&#34;&gt;design proposal for PodPreset&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Disruptions</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/disruptions/</link>
      <pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/disruptions/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- erictune
- foxish
- davidopp
title: Disruptions
content_type: concept
weight: 60
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This guide is for application owners who want to build
highly available applications, and thus need to understand
what types of disruptions can happen to Pods.

It is also for cluster administrators who want to perform automated
cluster actions, like upgrading and autoscaling clusters.
 --&gt;
&lt;p&gt;本文主要给
那些需要构建高可用应用的应用所属者，需要理解 Pod 可能遇到哪些类型的故障
也适用于那些需要实现自动集群操作，如升级或集群自动扩容的集群管理员&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Voluntary and involuntary disruptions

Pods do not disappear until someone (a person or a controller) destroys them, or
there is an unavoidable hardware or system software error.

We call these unavoidable cases *involuntary disruptions* to
an application.  Examples are:

- a hardware failure of the physical machine backing the node
- cluster administrator deletes VM (instance) by mistake
- cloud provider or hypervisor failure makes VM disappear
- a kernel panic
- the node disappears from the cluster due to cluster network partition
- eviction of a pod due to the node being [out-of-resources](/docs/tasks/administer-cluster/out-of-resource/).

Except for the out-of-resources condition, all these conditions
should be familiar to most users; they are not specific
to Kubernetes.

We call other cases *voluntary disruptions*.  These include both
actions initiated by the application owner and those initiated by a Cluster
Administrator.  Typical application owner actions include:

- deleting the deployment or other controller that manages the pod
- updating a deployment&#39;s pod template causing a restart
- directly deleting a pod (e.g. by accident)

Cluster administrator actions include:

- [Draining a node](/docs/tasks/administer-cluster/safely-drain-node/) for repair or upgrade.
- Draining a node from a cluster to scale the cluster down (learn about
[Cluster Autoscaling](/docs/tasks/administer-cluster/cluster-management/#cluster-autoscaler)
).
- Removing a pod from a node to permit something else to fit on that node.

These actions might be taken directly by the cluster administrator, or by automation
run by the cluster administrator, or by your cluster hosting provider.

Ask your cluster administrator or consult your cloud provider or distribution documentation
to determine if any sources of voluntary disruptions are enabled for your cluster.
If none are enabled, you can skip creating Pod Disruption Budgets.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; Not all voluntary disruptions are constrained by Pod Disruption Budgets. For example,
deleting deployments or pods bypasses Pod Disruption Budgets.&lt;/div&gt;
&lt;/blockquote&gt;


 --&gt;
&lt;h2 id=&#34;计划内和计划外的故障&#34;&gt;计划内和计划外的故障&lt;/h2&gt;
&lt;p&gt;Pod 只会在有人(真人或一个控制器)销毁时或有不可用的硬件或系统软件错误时才会消失。&lt;/p&gt;
&lt;p&gt;我们把这类在应用上出现不可避免出现的情况称为 &lt;em&gt;计划外故障&lt;/em&gt;， 例如:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节点所以的物理机出现硬件故障&lt;/li&gt;
&lt;li&gt;集群管理误操作删除了虚拟机实例&lt;/li&gt;
&lt;li&gt;云提供商或虚拟化软件故障导致虚拟机丢失&lt;/li&gt;
&lt;li&gt;内核故障&lt;/li&gt;
&lt;li&gt;因为网络分区导致节点与集群失联&lt;/li&gt;
&lt;li&gt;因为节点&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/out-of-resource/&#34;&gt;资源爆了&lt;/a&gt;导致 Pod 被驱逐&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除了资源瀑掉的情况，其它的情况对大多数用户来说都是比较熟悉的， 并不是 k8s 独有的。&lt;/p&gt;
&lt;p&gt;其它的情况就被称为 &lt;em&gt;计划内故障&lt;/em&gt;， 包括由应用所有者发起的行为和集群管理员发起的行为。 常见的应用所有者行为有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;删除管理 Pod 的 Deployment 或其它控制器(controller)&lt;/li&gt;
&lt;li&gt;更新 Pod 定义模板引起重启&lt;/li&gt;
&lt;li&gt;直接删除 Pod (如，误删除)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;集群管理发起的行为有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;因修复或升级 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/safely-drain-node/&#34;&gt;节点清场&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;因收缩集群容量而 节点清场
(更多见&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/cluster-management/#cluster-autoscaler&#34;&gt;集群动态容量&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;因要放入更适合该节点的其它东西而删除 Pod&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;询问系统管理或咨询云提供商或查看文档来确定集群是否开启了计划内故障&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 不是所有的计划内故障都包含在 Pod 故障预算中。 例如， 删除 Deployment 或 Pod 绕过了 Pod 故障预算&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--  
## Dealing with disruptions

Here are some ways to mitigate involuntary disruptions:

- Ensure your pod [requests the resources](/docs/tasks/configure-pod-container/assign-memory-resource) it needs.
- Replicate your application if you need higher availability.  (Learn about running replicated
  [stateless](/docs/tasks/run-application/run-stateless-application-deployment/)
  and [stateful](/docs/tasks/run-application/run-replicated-stateful-application/) applications.)
- For even higher availability when running replicated applications,
  spread applications across racks (using
  [anti-affinity](/docs/user-guide/node-selection/#inter-pod-affinity-and-anti-affinity-beta-feature))
  or across zones (if using a
  [multi-zone cluster](/docs/setup/multiple-zones).)

The frequency of voluntary disruptions varies.  On a basic Kubernetes cluster, there are
no voluntary disruptions at all.  However, your cluster administrator or hosting provider
may run some additional services which cause voluntary disruptions. For example,
rolling out node software updates can cause voluntary disruptions. Also, some implementations
of cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes.
Your cluster administrator or hosting provider should have documented what level of voluntary
disruptions, if any, to expect.
--&gt;
&lt;h2 id=&#34;如何应对故障&#34;&gt;如何应对故障&lt;/h2&gt;
&lt;p&gt;可以通过以下方式缓解计划外故障:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;保证 Pod &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/assign-memory-resource&#34;&gt;申请所需要的资源&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;如果需要高可以，部署多个应用副本。(更多关于如何运行
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/run-stateless-application-deployment/&#34;&gt;无状态&lt;/a&gt;
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/run-replicated-stateful-application/&#34;&gt;有状态&lt;/a&gt;
应用
)&lt;/li&gt;
&lt;li&gt;如果多副本应用还需要更高的可用性， 让应用副本分布在不同的机架(
通过 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/user-guide/node-selection/#inter-pod-affinity-and-anti-affinity-beta-feature&#34;&gt;anti-affinity&lt;/a&gt;
) 或 不同的区域(
如果使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/setup/multiple-zones&#34;&gt;多区域集群&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Pod disruption budgets






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.5 [beta]&lt;/code&gt;
&lt;/div&gt;



Kubernetes offers features to help you run highly available applications even when you
introduce frequent voluntary disruptions.

As an application owner, you can create a PodDisruptionBudget (PDB) for each application.
A PDB limits the number of Pods of a replicated application that are down simultaneously from
voluntary disruptions. For example, a quorum-based application would
like to ensure that the number of replicas running is never brought below the
number needed for a quorum. A web front end might want to
ensure that the number of replicas serving load never falls below a certain
percentage of the total.

Cluster managers and hosting providers should use tools which
respect PodDisruptionBudgets by calling the [Eviction API](/docs/tasks/administer-cluster/safely-drain-node/#the-eviction-api)
instead of directly deleting pods or deployments.

For example, the `kubectl drain` subcommand lets you mark a node as going out of
service. When you run `kubectl drain`, the tool tries to evict all of the Pods on
the Node you&#39;re taking out of service. The eviction request that `kubectl` submits on
your behalf may be temporarily rejected, so the tool periodically retries all failed
requests until all Pods on the target node are terminated, or until a configurable timeout
is reached.

A PDB specifies the number of replicas that an application can tolerate having, relative to how
many it is intended to have.  For example, a Deployment which has a `.spec.replicas: 5` is
supposed to have 5 pods at any given time.  If its PDB allows for there to be 4 at a time,
then the Eviction API will allow voluntary disruption of one (but not two) pods at a time.

The group of pods that comprise the application is specified using a label selector, the same
as the one used by the application&#39;s controller (deployment, stateful-set, etc).

The &#34;intended&#34; number of pods is computed from the `.spec.replicas` of the workload resource
that is managing those pods. The control plane discovers the owning workload resource by
examining the `.metadata.ownerReferences` of the Pod.

PDBs cannot prevent [involuntary disruptions](#voluntary-and-involuntary-disruptions) from
occurring, but they do count against the budget.

Pods which are deleted or unavailable due to a rolling upgrade to an application do count
against the disruption budget, but workload resources (such as Deployment and StatefulSet)
are not limited by PDBs when doing rolling upgrades. Instead, the handling of failures
during application updates is configured in the spec for the specific workload resource.

When a pod is evicted using the eviction API, it is gracefully
[terminated](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination), honoring the
`terminationGracePeriodSeconds` setting in its [PodSpec](/docs/reference/generated/kubernetes-api/v1.19/#podspec-v1-core).)
 --&gt;
&lt;h2 id=&#34;pod-故障预算&#34;&gt;Pod 故障预算&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.5 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;k8s 提供了即便在频繁有计划内故障的情况下依然可以运行高可用应用的特性。&lt;/p&gt;
&lt;p&gt;作为一个应用所有者，用户可以为每个应用创建一个 &lt;code&gt;PodDisruptionBudget&lt;/code&gt; (PDB)，
PDB 会限制在计划内故障时应用副本的 Pod 同时挂掉的数量。例如，一个基于选举的应用，就必须要保证
运行的副本数不能少于选举所需要的数量。 一个WEB前端可能需要保证提供服务的副本数量不能少于某个百分比&lt;/p&gt;
&lt;p&gt;集群管理器和主机提供都应该使用工具来调用遵循 &lt;code&gt;PodDisruptionBudget&lt;/code&gt; 的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/safely-drain-node/#the-eviction-api&#34;&gt;Eviction API&lt;/a&gt;
而不是直接删除 Pod 或 Deployment.&lt;/p&gt;
&lt;p&gt;例如，&lt;code&gt;kubectl drain&lt;/code&gt; 命令将节点标签为即将停止服务。 当运行 &lt;code&gt;kubectl drain&lt;/code&gt; 后，
工具将尝试将停止服务的节点上所有的 Pod 驱逐掉。 由 kubelet 发起的驱逐请求可能会临时被拒绝，
所以工具会对失败的请求周期性重试直到节点上所有的 Pod 被终止或最终超时(超时时间可配置)。&lt;/p&gt;
&lt;p&gt;PDB 指定一个应用可以接受的最少同时正常运行的副本数量。 例如，一个 Deployment 上定义为 &lt;code&gt;.spec.replicas: 5&lt;/code&gt;
也就是任意时间都要有5个正常运行的副本。 如果它的 PDB 允许同时至少要有 4 个副本， 则 驱逐 API
允许故同一时间内的计划内障数为一(不是二)&lt;/p&gt;
&lt;p&gt;而应用是通过特定标签选择器匹配到的 Pod 组成的， 与应用的控制器(deployment, stateful-set,等)一样&lt;/p&gt;
&lt;p&gt;Pod 的预期数量是通过管理这些 Pod 的工作负载资源上的 &lt;code&gt;.spec.replicas&lt;/code&gt; 计算等到的。
控制中心通过 Pod 上的 &lt;code&gt;.metadata.ownerReferences&lt;/code&gt; 来查看它的拥有者。&lt;/p&gt;
&lt;p&gt;PDB 不能阻止 &lt;a href=&#34;#voluntary-and-involuntary-disruptions&#34;&gt;计划内故障&lt;/a&gt;的发生，
但可以让它发生在预算控制内。&lt;/p&gt;
&lt;p&gt;由应用滚动更新造成的 Pod 删除或不可用是遵照故障预算的， 但工作负载(如 Deployment 和 StatefulSet)
的滚动更新则不受 PDB 限制。 而是由工作负载中的配置来处理更新失败的。&lt;/p&gt;
&lt;p&gt;当一个 Pod 因使用 驱逐 API而被驱逐， 会平滑地被 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/pod-lifecycle/#pod-termination&#34;&gt;终止&lt;/a&gt;
或  &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#podspec-v1-core&#34;&gt;PodSpec&lt;/a&gt;.)
中配置的 &lt;code&gt;terminationGracePeriodSeconds&lt;/code&gt; 后被杀死。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## PodDisruptionBudget example {#pdb-example}

Consider a cluster with 3 nodes, `node-1` through `node-3`.
The cluster is running several applications.  One of them has 3 replicas initially called
`pod-a`, `pod-b`, and `pod-c`.  Another, unrelated pod without a PDB, called `pod-x`, is also shown.
Initially, the pods are laid out as follows:

|       node-1         |       node-2        |       node-3       |
|:--------------------:|:-------------------:|:------------------:|
| pod-a  *available*   | pod-b *available*   | pod-c *available*  |
| pod-x  *available*   |                     |                    |

All 3 pods are part of a deployment, and they collectively have a PDB which requires
there be at least 2 of the 3 pods to be available at all times.

For example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel.
The cluster administrator first tries to drain `node-1` using the `kubectl drain` command.
That tool tries to evict `pod-a` and `pod-x`.  This succeeds immediately.
Both pods go into the `terminating` state at the same time.
This puts the cluster in this state:

|   node-1 *draining*  |       node-2        |       node-3       |
|:--------------------:|:-------------------:|:------------------:|
| pod-a  *terminating* | pod-b *available*   | pod-c *available*  |
| pod-x  *terminating* |                     |                    |

The deployment notices that one of the pods is terminating, so it creates a replacement
called `pod-d`.  Since `node-1` is cordoned, it lands on another node.  Something has
also created `pod-y` as a replacement for `pod-x`.

(Note: for a StatefulSet, `pod-a`, which would be called something like `pod-0`, would need
to terminate completely before its replacement, which is also called `pod-0` but has a
different UID, could be created.  Otherwise, the example applies to a StatefulSet as well.)

Now the cluster is in this state:

|   node-1 *draining*  |       node-2        |       node-3       |
|:--------------------:|:-------------------:|:------------------:|
| pod-a  *terminating* | pod-b *available*   | pod-c *available*  |
| pod-x  *terminating* | pod-d *starting*    | pod-y              |

At some point, the pods terminate, and the cluster looks like this:

|    node-1 *drained*  |       node-2        |       node-3       |
|:--------------------:|:-------------------:|:------------------:|
|                      | pod-b *available*   | pod-c *available*  |
|                      | pod-d *starting*    | pod-y              |

At this point, if an impatient cluster administrator tries to drain `node-2` or
`node-3`, the drain command will block, because there are only 2 available
pods for the deployment, and its PDB requires at least 2.  After some time passes, `pod-d` becomes available.

The cluster state now looks like this:

|    node-1 *drained*  |       node-2        |       node-3       |
|:--------------------:|:-------------------:|:------------------:|
|                      | pod-b *available*   | pod-c *available*  |
|                      | pod-d *available*   | pod-y              |

Now, the cluster administrator tries to drain `node-2`.
The drain command will try to evict the two pods in some order, say
`pod-b` first and then `pod-d`.  It will succeed at evicting `pod-b`.
But, when it tries to evict `pod-d`, it will be refused because that would leave only
one pod available for the deployment.

The deployment creates a replacement for `pod-b` called `pod-e`.
Because there are not enough resources in the cluster to schedule
`pod-e` the drain will again block.  The cluster may end up in this
state:

|    node-1 *drained*  |       node-2        |       node-3       | *no node*          |
|:--------------------:|:-------------------:|:------------------:|:------------------:|
|                      | pod-b *terminating* | pod-c *available*  | pod-e *pending*    |
|                      | pod-d *available*   | pod-y              |                    |

At this point, the cluster administrator needs to
add a node back to the cluster to proceed with the upgrade.

You can see how Kubernetes varies the rate at which disruptions
can happen, according to:

- how many replicas an application needs
- how long it takes to gracefully shutdown an instance
- how long it takes a new instance to start up
- the type of controller
- the cluster&#39;s resource capacity
 --&gt;
&lt;h2 id=&#34;poddisruptionbudget-使用示例&#34;&gt;PodDisruptionBudget 使用示例&lt;/h2&gt;
&lt;p&gt;假设有一个三个节点的集群，节点名称依次为 &lt;code&gt;node-1&lt;/code&gt; 到 &lt;code&gt;node-3&lt;/code&gt;, 集群中运行了多个应用。
其中一个应用包含三个副本， 初始叫 &lt;code&gt;pod-a&lt;/code&gt;, &lt;code&gt;pod-a&lt;/code&gt;, &lt;code&gt;pod-c&lt;/code&gt;. 还有一个不相关的 Pod 不包含 PDB
叫 pod-x,初始分布如下:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-1&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-2&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-a  &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-b &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-c &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-x  &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;三个Pod 都属于一个 Deployment, 且共同拥有一个PDB，要求3个中至少有2个 Pod 始终可用。&lt;/p&gt;
&lt;p&gt;例如， 假设集群管理员想要更新内核版本以修复一个 bug, 所以需要重启。
集群管理员第一步通过 &lt;code&gt;kubectl drain&lt;/code&gt; 命令 尝试对 node-1 清场。
这时会尝试驱逐 &lt;code&gt;pod-a&lt;/code&gt; 和 &lt;code&gt;pod-x&lt;/code&gt;。 这步操作应该立即能成功。 两个 Pod 都会同时进入 &lt;code&gt;terminating&lt;/code&gt; 状态。
集群状态变更为:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-1 &lt;em&gt;draining&lt;/em&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-2&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-a  &lt;em&gt;terminating&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-b &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-c &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-x  &lt;em&gt;terminating&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Deployment 发现它有一个 Pod 正在被终止， 所以它会创建一个替代 Pod 叫 &lt;code&gt;pod-d&lt;/code&gt;,
因为 node-1 不可用， 所以会被调度到其它节点上。 其它某个控制器或工具也会创建一个 &lt;code&gt;pod-y&lt;/code&gt; 替代 &lt;code&gt;pod-x&lt;/code&gt;&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 对于一个 StatefulSet， pod-a 对应的名称应该是 pod-0, 且需要被替换的 Pod 完全终止后，才会创建替代的 Pod 仍叫 pod-0, 但 UID 不一样。
如此这样，这个示例也对 StatefulSet 适用。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;集群状态再次变更为:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-1 &lt;em&gt;draining&lt;/em&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-2&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-a  &lt;em&gt;terminating&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-b &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-c &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-x  &lt;em&gt;terminating&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-d &lt;em&gt;starting&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-y&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;一段时间后， Pod 被终止，集群状态变更为:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-1 &lt;em&gt;drained&lt;/em&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-2&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-b &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-c &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-d &lt;em&gt;starting&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-y&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这时如果遇到一个急躁的集群管理尝试去对 &lt;code&gt;node-2&lt;/code&gt; 或 &lt;code&gt;node-3&lt;/code&gt; 进行清场， 则清场命令会被阻塞，
因为 Deployment 只有两个可用的 Pod， 而 PDB 要求至少要两个。 一段时间之后， &lt;code&gt;pod-d&lt;/code&gt; 状态变更为可用&lt;/p&gt;
&lt;p&gt;集群状态变更为:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-1 &lt;em&gt;drained&lt;/em&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-2&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-b &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-c &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-d &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-y&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;此时，假设集群管理员清场的是 node-2. 清场命令会以某个顺序驱逐这两个 Pod， 假设先是 pod-b, 然后是 pod-d.
对 pod-b 的驱逐会成功， 但是对 pod-d 的驱逐会被拒绝，因为如果 pod-d 被终止， Deployment 就只剩下一个可用 Pod。&lt;/p&gt;
&lt;p&gt;Deployment 会创建一个叫 pod-e 来替代 pod-b. 因为集群中没有足够的资源来让 pod-e 被调度， 清场命令会再次被阻塞。
最终集群的状态为成为这样:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-1 &lt;em&gt;drained&lt;/em&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-2&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-3&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;em&gt;no node&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-b &lt;em&gt;terminating&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-c &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-e &lt;em&gt;pending&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-d &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-y&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;至些， 集群管理员需要把完成升级的节点加入到集群中。&lt;/p&gt;
&lt;p&gt;k8s 可以接受的各种故障可能发生的频次，由如下因素决定:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用必须要多少个副本&lt;/li&gt;
&lt;li&gt;一个实例平滑关闭所需要的时间&lt;/li&gt;
&lt;li&gt;一个新实例启动需要多少时间&lt;/li&gt;
&lt;li&gt;控制器的类型&lt;/li&gt;
&lt;li&gt;集群资源的容量&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Separating Cluster Owner and Application Owner Roles

Often, it is useful to think of the Cluster Manager
and Application Owner as separate roles with limited knowledge
of each other.   This separation of responsibilities
may make sense in these scenarios:

- when there are many application teams sharing a Kubernetes cluster, and
  there is natural specialization of roles
- when third-party tools or services are used to automate cluster management

Pod Disruption Budgets support this separation of roles by providing an
interface between the roles.

If you do not have such a separation of responsibilities in your organization,
you may not need to use Pod Disruption Budgets.
 --&gt;
&lt;h2 id=&#34;集群管理员和应用管理员的角色划分&#34;&gt;集群管理员和应用管理员的角色划分&lt;/h2&gt;
&lt;p&gt;通常，将集群管理员和应用管理当作不同的角色，是很有用的。
对责任和区分在以下场景很有意义：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当集群中有多个应用团队共同使用该集群， 只预置的角色。&lt;/li&gt;
&lt;li&gt;当第三方工作或服务对集群进行自动化管理&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pod 故障预算通过角色之间的接口来实现对角色区分的支持&lt;/p&gt;
&lt;p&gt;如果用户组织中没有对责任区分的需求，则可能不震要使用 Pod 故障预算&lt;/p&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## How to perform Disruptive Actions on your Cluster

If you are a Cluster Administrator, and you need to perform a disruptive action on all
the nodes in your cluster, such as a node or system software upgrade, here are some options:

- Accept downtime during the upgrade.
- Failover to another complete replica cluster.
   -  No downtime, but may be costly both for the duplicated nodes
     and for human effort to orchestrate the switchover.
- Write disruption tolerant applications and use PDBs.
   - No downtime.
   - Minimal resource duplication.
   - Allows more automation of cluster administration.
   - Writing disruption-tolerant applications is tricky, but the work to tolerate voluntary
     disruptions largely overlaps with work to support autoscaling and tolerating
     involuntary disruptions.
 --&gt;
&lt;h2 id=&#34;怎么在集群中执行干扰操作&#34;&gt;怎么在集群中执行干扰操作&lt;/h2&gt;
&lt;p&gt;如果用户为集群管理员，需要对集群中的所有节点执行干扰操作, 使用对节点或系统软件升级， 以下为一些选项：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以接收升级期间停止服务&lt;/li&gt;
&lt;li&gt;故障转移到另一个完整的副本集群
&lt;ul&gt;
&lt;li&gt;没有宕机时间， 但可能需要双倍的节点和运行人员来实现精细的切换&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;编写可忍受干扰的应用并使用 PDB。
&lt;ul&gt;
&lt;li&gt;没有宕机时间&lt;/li&gt;
&lt;li&gt;最少资源重复&lt;/li&gt;
&lt;li&gt;允许更多集群自动化管理&lt;/li&gt;
&lt;li&gt;编写可忍受干扰的应用是相当难的。但实现对计划内故障的忍受，则能够很大程度上覆盖了支持自动伸缩容量和忍受计划外故障的工作内容&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
* Follow steps to protect your application by [configuring a Pod Disruption Budget](/docs/tasks/run-application/configure-pdb/).

* Learn more about [draining nodes](/docs/tasks/administer-cluster/safely-drain-node/)

* Learn about [updating a deployment](/docs/concepts/workloads/controllers/deployment/#updating-a-deployment)
  including steps to maintain its availability during the rollout.
 --&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/configure-pdb/&#34;&gt;配置 PodDisruptionBudget&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/safely-drain-node/&#34;&gt;节点清场&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;了解 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/#updating-a-deployment&#34;&gt;更新 Deployment&lt;/a&gt;
包括在回滚时用哪些步骤保持其可用性&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 临时容器</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/ephemeral-containers/</link>
      <pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/ephemeral-containers/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- verb
- yujuhong
title: Ephemeral Containers
content_type: concept
weight: 80
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [alpha]&lt;/code&gt;
&lt;/div&gt;



This page provides an overview of ephemeral containers: a special type of container
that runs temporarily in an existing &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; to
accomplish user-initiated actions such as troubleshooting. You use ephemeral
containers to inspect services rather than to build applications.

&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; Ephemeral containers are in early alpha state and are not suitable for production
clusters. In accordance with the &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/using-api/deprecation-policy/&#34;&gt;Kubernetes Deprecation Policy&lt;/a&gt;, this alpha feature could change
significantly in the future or be removed entirely.&lt;/div&gt;
&lt;/blockquote&gt;


 --&gt;
&lt;p&gt;




&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [alpha]&lt;/code&gt;
&lt;/div&gt;


本文主要简述临时容器: 一个特殊类型的容器，用来实现如应用调试而临时运行在一个已经存在的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
中的容器。用户应该仅使用它来调试服务而不是用来构建应用&lt;/p&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; 临时容器目前还是 alpha 状态， 不适合用于生产一并。 并且根据 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/using-api/deprecation-policy/&#34;&gt;k8s 废弃策略&lt;/a&gt;, 这些处于 alpha 版本特性在未来可能被
动大刀子或直接被干掉&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!-- body --&gt;
&lt;!--  
## Understanding ephemeral containers

&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; are the fundamental building
block of Kubernetes applications. Since Pods are intended to be disposable and
replaceable, you cannot add a container to a Pod once it has been created.
Instead, you usually delete and replace Pods in a controlled fashion using
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#39; target=&#39;_blank&#39;&gt;deployments&lt;span class=&#39;tooltip-text&#39;&gt;管理集群中的一个多副本应用&lt;/span&gt;
&lt;/a&gt;.

Sometimes it&#39;s necessary to inspect the state of an existing Pod, however, for
example to troubleshoot a hard-to-reproduce bug. In these cases you can run
an ephemeral container in an existing Pod to inspect its state and run
arbitrary commands.
--&gt;
&lt;h2 id=&#34;临时容器是啥东西&#34;&gt;临时容器是啥东西&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 是 k8s 应用构建的基石. 但在 Pod 的设计理念上它就是一个
一次性的可替换的组件，所以就不能在 Pod 创建后再往里面塞容器了。 如果要做变更通常就是通过
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#39; target=&#39;_blank&#39;&gt;Deployment&lt;span class=&#39;tooltip-text&#39;&gt;管理集群中的一个多副本应用&lt;/span&gt;
&lt;/a&gt; 这种删除替换的套路。&lt;/p&gt;
&lt;p&gt;但是有时候又需要查看已经存在 Pod 内的某些状太，比如找一个很难重现的虫。 在这种情况下就可以在已经
存在的 Pod 里面塞一个临时容器然后在里面使劲折腾了。&lt;/p&gt;
&lt;!--
### What is an ephemeral container?

Ephemeral containers differ from other containers in that they lack guarantees
for resources or execution, and they will never be automatically restarted, so
they are not appropriate for building applications.  Ephemeral containers are
described using the same `ContainerSpec` as regular containers, but many fields
are incompatible and disallowed for ephemeral containers.

- Ephemeral containers may not have ports, so fields such as `ports`,
  `livenessProbe`, `readinessProbe` are disallowed.
- Pod resource allocations are immutable, so setting `resources` is disallowed.
- For a complete list of allowed fields, see the [EphemeralContainer reference
  documentation](/docs/reference/generated/kubernetes-api/v1.19/#ephemeralcontainer-v1-core).

Ephemeral containers are created using a special `ephemeralcontainers` handler
in the API rather than by adding them directly to `pod.spec`, so it&#39;s not
possible to add an ephemeral container using `kubectl edit`.

Like regular containers, you may not change or remove an ephemeral container
after you have added it to a Pod.
 --&gt;
&lt;h3 id=&#34;临时容器是什么样一个东西&#34;&gt;临时容器是什么样一个东西&lt;/h3&gt;
&lt;p&gt;临时容器与其它容器有点不同，它缺乏对资源与执行的保证，它永远不会被重启，所以它也不适合用来构建应用。
临时容器与其它容器一样也是通过 &lt;code&gt;ContainerSpec&lt;/code&gt; 来定义， 但有许多字段在临时容器上是不可用的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;临时容器不能有端口，所以与端口相关的 &lt;code&gt;ports&lt;/code&gt;, &lt;code&gt;livenessProbe&lt;/code&gt;, &lt;code&gt;readinessProbe&lt;/code&gt; 都是不能用的。&lt;/li&gt;
&lt;li&gt;Pod 分配的资源是不可变的，所以也不能设置 &lt;code&gt;resources&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;所以可用字段列表见 &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#ephemeralcontainer-v1-core&#34;&gt;EphemeralContainer 参考文档&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;临时容器是用一个在 API 叫 &lt;code&gt;ephemeralcontainers&lt;/code&gt; 的特殊处理器创建的， 并不能通过直接添加 &lt;code&gt;pod.spec&lt;/code&gt; 来创建。
所以也不能通过 &lt;code&gt;kubectl edit&lt;/code&gt; 来向 Pod 中添加一个临时容器。&lt;/p&gt;
&lt;p&gt;与普通容器一样，临时容器被加到 Pod 中之后就能再修改或删除。&lt;/p&gt;
&lt;!--
## Uses for ephemeral containers

Ephemeral containers are useful for interactive troubleshooting when `kubectl
exec` is insufficient because a container has crashed or a container image
doesn&#39;t include debugging utilities.

In particular, [distroless images](https://github.com/GoogleContainerTools/distroless)
enable you to deploy minimal container images that reduce attack surface
and exposure to bugs and vulnerabilities. Since distroless images do not include a
shell or any debugging utilities, it&#39;s difficult to troubleshoot distroless
images using `kubectl exec` alone.

When using ephemeral containers, it&#39;s helpful to enable [process namespace sharing](/docs/tasks/configure-pod-container/share-process-namespace/) so
you can view processes in other containers.

See [Debugging with Ephemeral Debug Container](/docs/tasks/debug-application-cluster/debug-running-pod/#debugging-with-ephemeral-debug-container)
for examples of troubleshooting using ephemeral containers.
 --&gt;
&lt;h2 id=&#34;临时容器是怎么用的&#34;&gt;临时容器是怎么用的&lt;/h2&gt;
&lt;p&gt;临时容器在遇到需要交互式地找虫， 而 &lt;code&gt;kubectl exec&lt;/code&gt; 又因为容器已经挂了或都容器镜像中没有调试工具时很有用的。&lt;/p&gt;
&lt;p&gt;特别是 &lt;a href=&#34;https://github.com/GoogleContainerTools/distroless&#34;&gt;distroless images&lt;/a&gt;
允许用户能够部署最小化的容器镜像而达到减少攻击面，减少缺陷和漏洞的暴露。 但 &lt;code&gt;distroless&lt;/code&gt; 镜像中是没
shell 或任何其它的调试工具的，所以对 &lt;code&gt;distroless&lt;/code&gt; 镜像要只通过 &lt;code&gt;kubectl exec&lt;/code&gt; 来调试就变得无比困难了。&lt;/p&gt;
&lt;p&gt;在使用临时容器时，建议打开 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/share-process-namespace/&#34;&gt;进程命名空间共享&lt;/a&gt;
这样就可以看到其它容器中的进程了。&lt;/p&gt;
&lt;p&gt;更多使用临时容器调试的例子见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/debug-application-cluster/debug-running-pod/#debugging-with-ephemeral-debug-container&#34;&gt;使用临时容器找虫&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Ephemeral containers API

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The examples in this section require the &lt;code&gt;EphemeralContainers&lt;/code&gt; &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;feature
gate&lt;/a&gt; to be
enabled, and Kubernetes client and server version v1.16 or later.&lt;/div&gt;
&lt;/blockquote&gt;


The examples in this section demonstrate how ephemeral containers appear in
the API. You would normally use `kubectl alpha debug` or another `kubectl`
[plugin](/docs/tasks/extend-kubectl/kubectl-plugins/) to automate these steps
rather than invoking the API directly.

Ephemeral containers are created using the `ephemeralcontainers` subresource
of Pod, which can be demonstrated using `kubectl --raw`. First describe
the ephemeral container to add as an `EphemeralContainers` list:

```json
{
    &#34;apiVersion&#34;: &#34;v1&#34;,
    &#34;kind&#34;: &#34;EphemeralContainers&#34;,
    &#34;metadata&#34;: {
            &#34;name&#34;: &#34;example-pod&#34;
    },
    &#34;ephemeralContainers&#34;: [{
        &#34;command&#34;: [
            &#34;sh&#34;
        ],
        &#34;image&#34;: &#34;busybox&#34;,
        &#34;imagePullPolicy&#34;: &#34;IfNotPresent&#34;,
        &#34;name&#34;: &#34;debugger&#34;,
        &#34;stdin&#34;: true,
        &#34;tty&#34;: true,
        &#34;terminationMessagePolicy&#34;: &#34;File&#34;
    }]
}
```

To update the ephemeral containers of the already running `example-pod`:

```shell
kubectl replace --raw /api/v1/namespaces/default/pods/example-pod/ephemeralcontainers  -f ec.json
```

This will return the new list of ephemeral containers:

```json
{
   &#34;kind&#34;:&#34;EphemeralContainers&#34;,
   &#34;apiVersion&#34;:&#34;v1&#34;,
   &#34;metadata&#34;:{
      &#34;name&#34;:&#34;example-pod&#34;,
      &#34;namespace&#34;:&#34;default&#34;,
      &#34;selfLink&#34;:&#34;/api/v1/namespaces/default/pods/example-pod/ephemeralcontainers&#34;,
      &#34;uid&#34;:&#34;a14a6d9b-62f2-4119-9d8e-e2ed6bc3a47c&#34;,
      &#34;resourceVersion&#34;:&#34;15886&#34;,
      &#34;creationTimestamp&#34;:&#34;2019-08-29T06:41:42Z&#34;
   },
   &#34;ephemeralContainers&#34;:[
      {
         &#34;name&#34;:&#34;debugger&#34;,
         &#34;image&#34;:&#34;busybox&#34;,
         &#34;command&#34;:[
            &#34;sh&#34;
         ],
         &#34;resources&#34;:{

         },
         &#34;terminationMessagePolicy&#34;:&#34;File&#34;,
         &#34;imagePullPolicy&#34;:&#34;IfNotPresent&#34;,
         &#34;stdin&#34;:true,
         &#34;tty&#34;:true
      }
   ]
}
```

You can view the state of the newly created ephemeral container using `kubectl describe`:

```shell
kubectl describe pod example-pod
```

```
...
Ephemeral Containers:
  debugger:
    Container ID:  docker://cf81908f149e7e9213d3c3644eda55c72efaff67652a2685c1146f0ce151e80f
    Image:         busybox
    Image ID:      docker-pullable://busybox@sha256:9f1003c480699be56815db0f8146ad2e22efea85129b5b5983d0e0fb52d9ab70
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      sh
    State:          Running
      Started:      Thu, 29 Aug 2019 06:42:21 +0000
    Ready:          False
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:         &lt;none&gt;
...
```

You can interact with the new ephemeral container in the same way as other
containers using `kubectl attach`, `kubectl exec`, and `kubectl logs`, for
example:

```shell
kubectl attach -it example-pod -c debugger
```
 --&gt;
&lt;h2 id=&#34;临时容器-api&#34;&gt;临时容器 API&lt;/h2&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 以下示例需要打开 &lt;code&gt;EphemeralContainers&lt;/code&gt; &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;
k8s 客户端和服务端版本大于等于 &lt;code&gt;v1.16&lt;/code&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;本节示例通过演示彼把临时容器塞进 API 中的， 用户通常可以使用 &lt;code&gt;kubectl alpha debug&lt;/code&gt;
或另一个 &lt;code&gt;kubectl&lt;/code&gt; &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/extend-kubectl/kubectl-plugins/&#34;&gt;插件&lt;/a&gt; 来自动实现这些步骤而不需要直接调用 API&lt;/p&gt;
&lt;p&gt;临时容器是使用 Pod 的子资源 &lt;code&gt;ephemeralcontainers&lt;/code&gt; 来创建的。 使用 &lt;code&gt;kubectl --raw&lt;/code&gt;
第一步将临时容器作为 &lt;code&gt;EphemeralContainers&lt;/code&gt; 列表添加:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;{
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;apiVersion&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;v1&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;kind&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;EphemeralContainers&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;metadata&amp;#34;&lt;/span&gt;: {
            &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;example-pod&amp;#34;&lt;/span&gt;
    },
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;ephemeralContainers&amp;#34;&lt;/span&gt;: [{
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;command&amp;#34;&lt;/span&gt;: [
            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sh&amp;#34;&lt;/span&gt;
        ],
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;image&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;busybox&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;imagePullPolicy&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;IfNotPresent&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;debugger&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;stdin&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;,
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;tty&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;,
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;terminationMessagePolicy&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;File&amp;#34;&lt;/span&gt;
    }]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;而要更新已经在运行的 &lt;code&gt;example-pod&lt;/code&gt; 中的临时容器：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl replace --raw /api/v1/namespaces/default/pods/example-pod/ephemeralcontainers  -f ec.json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;返回一个新的临时容器列表：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;{
   &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;kind&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;EphemeralContainers&amp;#34;&lt;/span&gt;,
   &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;apiVersion&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;v1&amp;#34;&lt;/span&gt;,
   &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;metadata&amp;#34;&lt;/span&gt;:{
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;example-pod&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;namespace&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;selfLink&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/api/v1/namespaces/default/pods/example-pod/ephemeralcontainers&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;uid&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a14a6d9b-62f2-4119-9d8e-e2ed6bc3a47c&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;resourceVersion&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;15886&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;creationTimestamp&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2019-08-29T06:41:42Z&amp;#34;&lt;/span&gt;
   },
   &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;ephemeralContainers&amp;#34;&lt;/span&gt;:[
      {
         &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;debugger&amp;#34;&lt;/span&gt;,
         &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;image&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;busybox&amp;#34;&lt;/span&gt;,
         &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;command&amp;#34;&lt;/span&gt;:[
            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sh&amp;#34;&lt;/span&gt;
         ],
         &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;resources&amp;#34;&lt;/span&gt;:{

         },
         &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;terminationMessagePolicy&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;File&amp;#34;&lt;/span&gt;,
         &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;imagePullPolicy&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;IfNotPresent&amp;#34;&lt;/span&gt;,
         &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;stdin&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;,
         &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;tty&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
      }
   ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以通过 &lt;code&gt;kubectl describe&lt;/code&gt; 查看新创建临时容器的状态:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe pod example-pod
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;返回类似如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;...
Ephemeral Containers:
  debugger:
    Container ID:  docker://cf81908f149e7e9213d3c3644eda55c72efaff67652a2685c1146f0ce151e80f
    Image:         busybox
    Image ID:      docker-pullable://busybox@sha256:9f1003c480699be56815db0f8146ad2e22efea85129b5b5983d0e0fb52d9ab70
    Port:          &amp;lt;none&amp;gt;
    Host Port:     &amp;lt;none&amp;gt;
    Command:
      sh
    State:          Running
      Started:      Thu, 29 Aug 2019 06:42:21 +0000
    Ready:          False
    Restart Count:  0
    Environment:    &amp;lt;none&amp;gt;
    Mounts:         &amp;lt;none&amp;gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用户也可以与普通容器一样用 &lt;code&gt;kubectl attach&lt;/code&gt;, &lt;code&gt;kubectl exec&lt;/code&gt;, &lt;code&gt;kubectl logs&lt;/code&gt; 与临时容器交互。
例如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl attach -it example-pod -c debugger
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
      </description>
    </item>
    
    <item>
      <title>Docs: Deployment</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/deployment/</link>
      <pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/deployment/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- janetkuo
title: Deployments
feature:
  title: Automated rollouts and rollbacks
  description: &gt;
    Kubernetes progressively rolls out changes to your application or its configuration, while monitoring application health to ensure it doesn&#39;t kill all your instances at the same time. If something goes wrong, Kubernetes will rollback the change for you. Take advantage of a growing ecosystem of deployment solutions.

content_type: concept
weight: 10
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
A _Deployment_ provides declarative updates for &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#39; target=&#39;_blank&#39;&gt;ReplicaSets&lt;span class=&#39;tooltip-text&#39;&gt;ReplicaSet 保证在任意时间点上某个 Pod 有指定数量副本在运行&lt;/span&gt;
&lt;/a&gt;.

You describe a _desired state_ in a Deployment, and the Deployment &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Do not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use case is not covered below.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;p&gt;&lt;em&gt;Deployment&lt;/em&gt; 为 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#39; target=&#39;_blank&#39;&gt;ReplicaSets&lt;span class=&#39;tooltip-text&#39;&gt;ReplicaSet 保证在任意时间点上某个 Pod 有指定数量副本在运行&lt;/span&gt;
&lt;/a&gt; 管理的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
提供了声明式的更新。&lt;/p&gt;
&lt;p&gt;用户在 Deployment 中描述了一个 &lt;em&gt;期望状态&lt;/em&gt;， 然后这个 Deployment &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt;
以控制器的方式变更实际状态为期望状态。 用户可以定义 Deployment 来创建一个新的 &lt;code&gt;ReplicaSet&lt;/code&gt;，
或者删除一个已经存在的 Deployment， 再用一个新的 Deployment 来接管它的所以资源。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 不要管属于 &lt;code&gt;Deployment&lt;/code&gt; 的 &lt;code&gt;ReplicaSet&lt;/code&gt;。 如果以下提到的应用场景都没办法满足你的需求请考虑到 k8s 主仓库提一个问题单&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!-- body --&gt;
&lt;!--
## Use Case

The following are typical use cases for Deployments:

* [Create a Deployment to rollout a ReplicaSet](#creating-a-deployment). The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.
* [Declare the new state of the Pods](#updating-a-deployment) by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.
* [Rollback to an earlier Deployment revision](#rolling-back-a-deployment) if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.
* [Scale up the Deployment to facilitate more load](#scaling-a-deployment).
* [Pause the Deployment](#pausing-and-resuming-a-deployment) to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.
* [Use the status of the Deployment](#deployment-status) as an indicator that a rollout has stuck.
* [Clean up older ReplicaSets](#clean-up-policy) that you don&#39;t need anymore.
 --&gt;
&lt;h2 id=&#34;应用场景&#34;&gt;应用场景&lt;/h2&gt;
&lt;p&gt;以下为 &lt;code&gt;Deployment&lt;/code&gt; 常见应用场景:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#creating-a-deployment&#34;&gt;使用 Deployment 来管理 ReplicaSet&lt;/a&gt;. ReplicaSet 会在后台创建 Pod， 查看它发布的这些发布是否成功&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#updating-a-deployment&#34;&gt;声明 Pod 的新状态&lt;/a&gt; 通过更新 Deployment 的 Pod 模板定义(PodTemplateSpec). 创建一个新的 ReplicaSet 并以一个受控的频率将 Pod 从旧的 ReplicaSet 移到新的 ReplicaSet。每创建一个新的 ReplicaSet 都会更新一次 Deployment 的版本号&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rolling-back-a-deployment&#34;&gt;Deployment 回滚到之前一个版本&lt;/a&gt;， 如果 Deployment 当前部署的应用状态不稳定，每次回滚也会更新 Deployment 版本&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scaling-a-deployment&#34;&gt;让 Deployment 扩容以承载更多的负载&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pausing-and-resuming-a-deployment&#34;&gt;暂停 Deployment&lt;/a&gt; 后一次修改 PodTemplateSpec 的多个地方，然后恢复这个 Deployment 让它开始一个新的发布&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deployment-status&#34;&gt;通过 Deployment 的状态&lt;/a&gt; 来标示一个发布应用的状态&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clean-up-policy&#34;&gt;清除不需要的 ReplicaSet&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Creating a Deployment

The following is an example of a Deployment. It creates a ReplicaSet to bring up three `nginx` Pods:



 













&lt;table class=&#34;includecode&#34; id=&#34;controllersnginx-deploymentyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/nginx-deployment.yaml&#34; download=&#34;controllers/nginx-deployment.yaml&#34;&gt;
                    &lt;code&gt;controllers/nginx-deployment.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersnginx-deploymentyaml&#39;)&#34; title=&#34;Copy controllers/nginx-deployment.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx-deployment&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx:1.14.2&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



In this example:

* A Deployment named `nginx-deployment` is created, indicated by the `.metadata.name` field.
* The Deployment creates three replicated Pods, indicated by the `.spec.replicas` field.
* The `.spec.selector` field defines how the Deployment finds which Pods to manage.
  In this case, you simply select a label that is defined in the Pod template (`app: nginx`).
  However, more sophisticated selection rules are possible,
  as long as the Pod template itself satisfies the rule.

  &lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The &lt;code&gt;.spec.selector.matchLabels&lt;/code&gt; field is a map of {key,value} pairs.
A single {key,value} in the &lt;code&gt;matchLabels&lt;/code&gt; map is equivalent to an element of &lt;code&gt;matchExpressions&lt;/code&gt;,
whose key field is &amp;ldquo;key&amp;rdquo; the operator is &amp;ldquo;In&amp;rdquo;, and the values array contains only &amp;ldquo;value&amp;rdquo;.
All of the requirements, from both &lt;code&gt;matchLabels&lt;/code&gt; and &lt;code&gt;matchExpressions&lt;/code&gt;, must be satisfied in order to match.&lt;/div&gt;
&lt;/blockquote&gt;


* The `template` field contains the following sub-fields:
  * The Pods are labeled `app: nginx`using the `.metadata.labels` field.
  * The Pod template&#39;s specification, or `.template.spec` field, indicates that
  the Pods run one container, `nginx`, which runs the `nginx`
  [Docker Hub](https://hub.docker.com/) image at version 1.14.2.
  * Create one container and name it `nginx` using the `.spec.template.spec.containers[0].name` field.

Before you begin, make sure your Kubernetes cluster is up and running.
Follow the steps given below to create the above Deployment:


1. Create the Deployment by running the following command:

   ```shell
   kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml
   ```

  &lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You can specify the &lt;code&gt;--record&lt;/code&gt; flag to write the command executed in the resource annotation &lt;code&gt;kubernetes.io/change-cause&lt;/code&gt;.
The recorded change is useful for future introspection. For example, to see the commands executed in each Deployment revision.&lt;/div&gt;
&lt;/blockquote&gt;



2. Run `kubectl get deployments` to check if the Deployment was created.

   If the Deployment is still being created, the output is similar to the following:
   ```shell
   NAME               READY   UP-TO-DATE   AVAILABLE   AGE
   nginx-deployment   0/3     0            0           1s
   ```
   When you inspect the Deployments in your cluster, the following fields are displayed:
   * `NAME` lists the names of the Deployments in the namespace.
   * `READY` displays how many replicas of the application are available to your users. It follows the pattern ready/desired.
   * `UP-TO-DATE` displays the number of replicas that have been updated to achieve the desired state.
   * `AVAILABLE` displays how many replicas of the application are available to your users.
   * `AGE` displays the amount of time that the application has been running.

   Notice how the number of desired replicas is 3 according to `.spec.replicas` field.

3. To see the Deployment rollout status, run `kubectl rollout status deployment.v1.apps/nginx-deployment`.

   The output is similar to:
   ```shell
   Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
   deployment.apps/nginx-deployment successfully rolled out
   ```

4. Run the `kubectl get deployments` again a few seconds later.
   The output is similar to this:
   ```shell
   NAME               READY   UP-TO-DATE   AVAILABLE   AGE
   nginx-deployment   3/3     3            3           18s
   ```
   Notice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available.

5. To see the ReplicaSet (`rs`) created by the Deployment, run `kubectl get rs`. The output is similar to this:
   ```shell
   NAME                          DESIRED   CURRENT   READY   AGE
   nginx-deployment-75675f5897   3         3         3       18s
   ```
   ReplicaSet output shows the following fields:

   * `NAME` lists the names of the ReplicaSets in the namespace.
   * `DESIRED` displays the desired number of _replicas_ of the application, which you define when you create the Deployment. This is the _desired state_.
   * `CURRENT` displays how many replicas are currently running.
   * `READY` displays how many replicas of the application are available to your users.
   * `AGE` displays the amount of time that the application has been running.

   Notice that the name of the ReplicaSet is always formatted as `[DEPLOYMENT-NAME]-[RANDOM-STRING]`.
   The random string is randomly generated and uses the `pod-template-hash` as a seed.

6. To see the labels automatically generated for each Pod, run `kubectl get pods --show-labels`.
   The output is similar to:
   ```shell
   NAME                                READY     STATUS    RESTARTS   AGE       LABELS
   nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
   nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
   nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
   ```
   The created ReplicaSet ensures that there are three `nginx` Pods.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;You must specify an appropriate selector and Pod template labels in a Deployment
(in this case, &lt;code&gt;app: nginx&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Do not overlap labels or selectors with other controllers (including other Deployments and StatefulSets).
Kubernetes doesn&amp;rsquo;t stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;创建一个-deployment&#34;&gt;创建一个 Deployment&lt;/h2&gt;
&lt;p&gt;以下为一个 Deployment 的示例。创建一个 ReplicaSet 来启动三个 &lt;code&gt;nginx&lt;/code&gt; 的 Pod：&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;controllersnginx-deploymentyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/nginx-deployment.yaml&#34; download=&#34;controllers/nginx-deployment.yaml&#34;&gt;
                    &lt;code&gt;controllers/nginx-deployment.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersnginx-deploymentyaml&#39;)&#34; title=&#34;Copy controllers/nginx-deployment.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx-deployment&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx:1.14.2&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;在这个示例中:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;创建了一个叫 &lt;code&gt;nginx-deployment&lt;/code&gt; Deployment， 这个名字定义在 &lt;code&gt;.metadata.name&lt;/code&gt; 字段&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这个 Deployment 会创建三个 Pod 为副本， 由 &lt;code&gt;.spec.replicas&lt;/code&gt; 字段定义&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;.spec.selector&lt;/code&gt; 字段定义 Deployment 怎么去找需要它管理的 Pod. 在本例中，只是定义在 Pod 模板中的
标签(&lt;code&gt;app: nginx&lt;/code&gt;), 更复杂的选择规则也是支持的，只要 Pod 模板满足这些规则
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;code&gt;.spec.selector.matchLabels&lt;/code&gt; 是一个键值对 ({key,value}) 字典。
&lt;code&gt;matchLabels&lt;/code&gt; 中的一个键值对相当于 &lt;code&gt;matchExpressions&lt;/code&gt; 中的一个元素
这个元素的键就是值对的键，操作符为 &lt;code&gt;In&lt;/code&gt;， 值为包含该值的数组。
需要同时满足 &lt;code&gt;matchLabels&lt;/code&gt; 和 &lt;code&gt;matchExpressions&lt;/code&gt; 中的条件，才是符合该选择器的目标&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;template&lt;/code&gt; 字段又包含以下子字段:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;.metadata.labels&lt;/code&gt; 字段中的 &lt;code&gt;app: nginx&lt;/code&gt; 是 Pod 的标签&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.template.spec&lt;/code&gt; 字段为 Pod 的模板定义， 表示这个 Pod 运行一个容器， 应用程序为 &lt;code&gt;nginx&lt;/code&gt;&lt;br&gt;
运行的是 &lt;a href=&#34;https://hub.docker.com/&#34;&gt;Docker Hub&lt;/a&gt; 上面的 &lt;code&gt;1.14.2&lt;/code&gt; 版本的 nginx 镜像&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.spec.template.spec.containers[0].name&lt;/code&gt; 字段表示，创建的第一个也只有一个容器，它的名字为 &lt;code&gt;nginx&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在开始之前，需要保证 k8s 集群已经启动并运行。&lt;/p&gt;
&lt;p&gt;以下为创建这个 Deployment 的具体步骤:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通过以下命令创建该 Deployment：&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f nginx-deployment.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 用户可以通过在命令中添加 &lt;code&gt;--record&lt;/code&gt; 来把执行的命令写入到资源的 &lt;code&gt;kubernetes.io/change-cause&lt;/code&gt; 注解 中。
这个变更记录对于将来的回溯是很有用的。比如，查看 Deployment 每一个版本执行了什么命令&lt;/div&gt;
&lt;/blockquote&gt;

&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;运行 &lt;code&gt;kubectl get deployments&lt;/code&gt; 命令，查看 Deployment 是否已经创建&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果 Deployment 还在创建中，则输出结果如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   0/3     0            0           1s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中每个字段的说明如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;NAME&lt;/code&gt; 显示 Deployment 的名称&lt;/li&gt;
&lt;li&gt;&lt;code&gt;READY&lt;/code&gt; 展示应用的副本可用的数量， 格式为 就绪数/期望数&lt;/li&gt;
&lt;li&gt;&lt;code&gt;UP-TO-DATE&lt;/code&gt; 展示已经被更新达到预期状态的副本的数量&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AVAILABLE&lt;/code&gt; 展示已经能为用户提供服务的副本数量&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AGE&lt;/code&gt; 应用运行的时长&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以看到，根据 &lt;code&gt;.spec.replicas&lt;/code&gt; 字段， 期望的副本数量是 &lt;code&gt;3&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;要查看 Deployment 发布状态，可以通过命令 &lt;code&gt;kubectl rollout status deployment.v1.apps/nginx-deployment&lt;/code&gt;
输出类似如下:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
deployment.apps/nginx-deployment successfully rolled out
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;等几秒再执行命令 &lt;code&gt;kubectl get deployments&lt;/code&gt;
输出类似如下:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           18s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这时可以看到 Deployment 已经创建全部的三个副本， 所有副本的达成更新(是通过最新的 Pod 模板创建的)并且可用的&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;通过命令 &lt;code&gt;kubectl get rs&lt;/code&gt; 查看由 Deployment 创建的 ReplicaSet (&lt;code&gt;rs&lt;/code&gt;)
输出类似如下:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-75675f5897   3         3         3       18s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;字段说明如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;NAME&lt;/code&gt; ReplicaSet 的名称&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DESIRED&lt;/code&gt; 应用的 &lt;em&gt;期望&lt;/em&gt; 副本数，创建 Deployment 时指定。 这是期望状态&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CURRENT&lt;/code&gt; 展示当前正在运行的副本数&lt;/li&gt;
&lt;li&gt;&lt;code&gt;READY&lt;/code&gt; 展示当前可以提供服务的副本数&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AGE&lt;/code&gt; 展示应用的运行时长&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以看到 ReplicaSet 的名称格式总是为 &lt;code&gt;[DEPLOYMENT-NAME]-[RANDOM-STRING]&lt;/code&gt;
为个随机字符串是以 &lt;code&gt;pod-template-hash&lt;/code&gt; 为种子生成的&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;要查看每个 Pod 自动创建的标签，可能执行命令 &lt;code&gt;kubectl get pods --show-labels&lt;/code&gt;:
输出类似如下:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;NAME                                READY     STATUS    RESTARTS   AGE       LABELS
nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;ReplicaSet 会确保有三个 &lt;code&gt;nginx&lt;/code&gt; Pod&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 用户应该在 Deployment 中定义恰当的选择器和 Pod 模板标签(本例为 &lt;code&gt;app: nginx&lt;/code&gt;)
一定不要与其它的控制器(包括其它的 Deployment 和 StatefulSet)使用一样的标签或选择器&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Pod-template-hash label

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; Do not change this label.&lt;/div&gt;
&lt;/blockquote&gt;


The `pod-template-hash` label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.

This label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the `PodTemplate` of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels,
and in any existing Pods that the ReplicaSet might have.
 --&gt;
&lt;h3 id=&#34;标签-pod-template-hash&#34;&gt;标签 &lt;code&gt;pod-template-hash&lt;/code&gt;&lt;/h3&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 这个标签一定不要改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;pod-template-hash&lt;/code&gt; 是由 Deployment 的控制器添加到每个由 Deployment 创建或收编的 ReplicaSet 的。
这个标签确保 Deployment 所属的 ReplicaSet 的标签不会重叠。 这个标签的值是对 ReplicaSet 的 &lt;code&gt;PodTemplate&lt;/code&gt; 进行哈希的结果，
并将这个标签添加到 ReplicaSet 的选择器和 Pod 模板的标签中，然后也会添加到已经由 ReplicaSet 管理或将来收编的所以 Pod 中。&lt;/p&gt;
&lt;!--
## Updating a Deployment

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A Deployment&amp;rsquo;s rollout is triggered if and only if the Deployment&amp;rsquo;s Pod template (that is, &lt;code&gt;.spec.template&lt;/code&gt;)
is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.&lt;/div&gt;
&lt;/blockquote&gt;


Follow the steps given below to update your Deployment:

1. Let&#39;s update the nginx Pods to use the `nginx:1.16.1` image instead of the `nginx:1.14.2` image.

    ```shell
    kubectl --record deployment.apps/nginx-deployment set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1
    ```
    or simply use the following command:

    ```shell
    kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 --record
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment image updated
    ```

    Alternatively, you can `edit` the Deployment and change `.spec.template.spec.containers[0].image` from `nginx:1.14.2` to `nginx:1.16.1`:

    ```shell
    kubectl edit deployment.v1.apps/nginx-deployment
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment edited
    ```

2. To see the rollout status, run:

    ```shell
    kubectl rollout status deployment.v1.apps/nginx-deployment
    ```

    The output is similar to this:
    ```
    Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
    ```
    or
    ```
    deployment.apps/nginx-deployment successfully rolled out
    ```

Get more details on your updated Deployment:

* After the rollout succeeds, you can view the Deployment by running `kubectl get deployments`.
    The output is similar to this:
    ```
    NAME               READY   UP-TO-DATE   AVAILABLE   AGE
    nginx-deployment   3/3     3            3           36s
    ```

* Run `kubectl get rs` to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it
up to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.

    ```shell
    kubectl get rs
    ```

    The output is similar to this:
    ```
    NAME                          DESIRED   CURRENT   READY   AGE
    nginx-deployment-1564180365   3         3         3       6s
    nginx-deployment-2035384211   0         0         0       36s
    ```

* Running `get pods` should now show only the new Pods:

    ```shell
    kubectl get pods
    ```

    The output is similar to this:
    ```
    NAME                                READY     STATUS    RESTARTS   AGE
    nginx-deployment-1564180365-khku8   1/1       Running   0          14s
    nginx-deployment-1564180365-nacti   1/1       Running   0          14s
    nginx-deployment-1564180365-z9gth   1/1       Running   0          14s
    ```

    Next time you want to update these Pods, you only need to update the Deployment&#39;s Pod template again.

    Deployment ensures that only a certain number of Pods are down while they are being updated. By default,
    it ensures that at least 75% of the desired number of Pods are up (25% max unavailable).

    Deployment also ensures that only a certain number of Pods are created above the desired number of Pods.
    By default, it ensures that at most 125% of the desired number of Pods are up (25% max surge).

    For example, if you look at the above Deployment closely, you will see that it first created a new Pod,
    then deleted some old Pods, and created new ones. It does not kill old Pods until a sufficient number of
    new Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed.
    It makes sure that at least 2 Pods are available and that at max 4 Pods in total are available.

* Get details of your Deployment:
  ```shell
  kubectl describe deployments
  ```
  The output is similar to this:
  ```
  Name:                   nginx-deployment
  Namespace:              default
  CreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000
  Labels:                 app=nginx
  Annotations:            deployment.kubernetes.io/revision=2
  Selector:               app=nginx
  Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
  StrategyType:           RollingUpdate
  MinReadySeconds:        0
  RollingUpdateStrategy:  25% max unavailable, 25% max surge
  Pod Template:
    Labels:  app=nginx
     Containers:
      nginx:
        Image:        nginx:1.16.1
        Port:         80/TCP
        Environment:  &lt;none&gt;
        Mounts:       &lt;none&gt;
      Volumes:        &lt;none&gt;
    Conditions:
      Type           Status  Reason
      ----           ------  ------
      Available      True    MinimumReplicasAvailable
      Progressing    True    NewReplicaSetAvailable
    OldReplicaSets:  &lt;none&gt;
    NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)
    Events:
      Type    Reason             Age   From                   Message
      ----    ------             ----  ----                   -------
      Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3
      Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1
      Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2
      Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2
      Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1
      Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3
      Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0
    ```
    Here you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211)
    and scaled it up to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet
    (nginx-deployment-1564180365) and scaled it up to 1 and then scaled down the old ReplicaSet to 2, so that at
    least 2 Pods were available and at most 4 Pods were created at all times. It then continued scaling up and down
    the new and the old ReplicaSet, with the same rolling update strategy. Finally, you&#39;ll have 3 available replicas
    in the new ReplicaSet, and the old ReplicaSet is scaled down to 0.
 --&gt;
&lt;h2 id=&#34;更新-deployment&#34;&gt;更新 Deployment&lt;/h2&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Deployment 有且仅有在对 Deployment Pod 模板(也就是 &lt;code&gt;.spec.template&lt;/code&gt;)发生变更后触发发布，
例如，如果模板标签或容器被更新。 其它如扩充 Deployment 副本数不会触发发布。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;以下步骤更新 Deployment：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 Pod 使用的 &lt;code&gt;nginx&lt;/code&gt; 镜像版本从 &lt;code&gt;nginx:1.14.2&lt;/code&gt; 升到 &lt;code&gt;nginx:1.16.1&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl --record deployment.apps/nginx-deployment set image deployment.v1.apps/nginx-deployment nginx&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx:1.16.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;或者简单使用如下命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl set image deployment/nginx-deployment nginx&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx:1.16.1 --record
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment image updated
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;另外还有一个方式，就是修改 Deployment 中 &lt;code&gt;.spec.template.spec.containers[0].image&lt;/code&gt; 的值将其从原来的 &lt;code&gt;nginx:1.14.2&lt;/code&gt; 改成 &lt;code&gt;nginx:1.16.1&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl edit deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment edited
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;查看发布状态，执行如下命令:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout status deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;或&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment successfully rolled out
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看更新后 Deployment 的更多细节:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;当发布执行完成后，查看 Deployment 执行命令  &lt;code&gt;kubectl get deployments&lt;/code&gt;：
输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           36s
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;执行 &lt;code&gt;kubectl get rs&lt;/code&gt; 命令可以看到 Deployment 是通过创建一个新的 ReplicaSet 并将其副本数扩充到 3
同时将旧的 ReplicaSet 的副本数收缩至 0&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       6s
nginx-deployment-2035384211   0         0         0       36s
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这时再执行命令 &lt;code&gt;get pods&lt;/code&gt;， 应该就只会看到新创建的 Pod:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1564180365-khku8   1/1       Running   0          14s
nginx-deployment-1564180365-nacti   1/1       Running   0          14s
nginx-deployment-1564180365-z9gth   1/1       Running   0          14s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当用户下次想更新这些 Pod 时， 只需要再次更新 Deployment 的 Pod 模板即可。&lt;/p&gt;
&lt;p&gt;Deployment 会确保在更新过程中挂掉的 Pod 数不会大于指定的值。 默认情况下，会保证至少有期望数量 75% 数量的 Pod 是正常运行的(最多只有 25% 是不可用的)&lt;/p&gt;
&lt;p&gt;Deployment 还会保证多于期望数量的 Pod 数不会大于指定的值。 默认情况下同时正常运行的 Pod 数不会大于期望数量的 125%(上限为 25%)&lt;/p&gt;
&lt;p&gt;如果仔细观察上面的例子。 就会发现更新过程中，首先会创建一个新的 Pod， 然后删除一些旧的，再创建新的。
在新创建并就绪的 Pod 的数量没有达到特定数量之前不会干掉旧的 Pod， 同时在没干掉特定数量旧的 Pod 之前不会创建新的 Pod。
这个过程中会保证在少的时候至少有 2 个 Pod 可用，在多的时候至多有 4 个 Pod 可用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;继续来看 Deployment 的更多细节：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe deployments
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=2
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
   Containers:
    nginx:
      Image:        nginx:1.16.1
      Port:         80/TCP
      Environment:  &amp;lt;none&amp;gt;
      Mounts:       &amp;lt;none&amp;gt;
    Volumes:        &amp;lt;none&amp;gt;
  Conditions:
    Type           Status  Reason
    ----           ------  ------
    Available      True    MinimumReplicasAvailable
    Progressing    True    NewReplicaSetAvailable
  OldReplicaSets:  &amp;lt;none&amp;gt;
  NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)
  Events:
    Type    Reason             Age   From                   Message
    ----    ------             ----  ----                   -------
    Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3
    Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1
    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2
    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2
    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1
    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3
    Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里可以看到， 在第一次创建 Deployment 的时候， 创建了一个 ReplicaSet (nginx-deployment-2035384211) 并直接将副本数扩充到 3.
当 更新 Deployment 的时候， 又创建了一个新的 ReplicaSet (nginx-deployment-1564180365) 并将副本数扩充到 1 ， 紧接着将旧的 ReplicaSet
的副本数收缩至 2， 因此至始终少有 2 Pod 可用， 至多有 4 个 Pod 可用。 接下来继续使用相同的滚动更新策略扩充新的，收缩旧的 ReplicaSet。到最后就变成新的
ReplicaSet 有 3 个副本可用， 旧的  ReplicaSet 副本数收缩为 0 。&lt;/p&gt;
&lt;!--
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rollover-aka-multiple-updates-in-flight&#34;&gt;Rollover (aka multiple updates in-flight)&lt;/h3&gt;
&lt;p&gt;Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up
the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels
match &lt;code&gt;.spec.selector&lt;/code&gt; but whose template does not match &lt;code&gt;.spec.template&lt;/code&gt; are scaled down. Eventually, the new
ReplicaSet is scaled to &lt;code&gt;.spec.replicas&lt;/code&gt; and all old ReplicaSets is scaled to 0.&lt;/p&gt;
&lt;p&gt;If you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet
as per the update and start scaling that up, and rolls over the ReplicaSet that it was scaling up previously
&amp;ndash; it will add it to its list of old ReplicaSets and start scaling it down.&lt;/p&gt;
&lt;p&gt;For example, suppose you create a Deployment to create 5 replicas of &lt;code&gt;nginx:1.14.2&lt;/code&gt;,
but then update the Deployment to create 5 replicas of &lt;code&gt;nginx:1.16.1&lt;/code&gt;, when only 3
replicas of &lt;code&gt;nginx:1.14.2&lt;/code&gt; had been created. In that case, the Deployment immediately starts
killing the 3 &lt;code&gt;nginx:1.14.2&lt;/code&gt; Pods that it had created, and starts creating
&lt;code&gt;nginx:1.16.1&lt;/code&gt; Pods. It does not wait for the 5 replicas of &lt;code&gt;nginx:1.14.2&lt;/code&gt; to be created
before changing course.
&amp;ndash;&amp;gt;&lt;/p&gt;
&lt;h3 id=&#34;rollover-aka-multiple-updates-in-flight-没想到比较恰当的&#34;&gt;Rollover (aka multiple updates in-flight) 没想到比较恰当的&lt;/h3&gt;
&lt;p&gt;每次 Deployment 控制器发现新创建了一个 Deployment， 就会创建一个新的 ReplicaSet 来创建期望数量的 Pod.
如 Deployment 被更新后， 对于之前存在的 ReplicaSet， 它控制的 Pod 的标签能够匹配 &lt;code&gt;.spec.selector&lt;/code&gt; 但是模板不能匹配 &lt;code&gt;.spec.template&lt;/code&gt;，副本数量就会被收缩。
最后， 新创建的 ReplicaSet 的副本数扩充到 &lt;code&gt;.spec.replicas&lt;/code&gt; 配置的数量。 所以旧的 ReplicaSet 副本收缩为 0。&lt;/p&gt;
&lt;p&gt;如果在前一个发布还在进行时又更新的 Deployment， Deployment 会为被次更新创建一个对应的 ReplicaSet，就开始扩容。
前一个正在扩容的 ReplicaSet 就会被跳过。 它会添加到旧 ReplicaSet 的列表，然后开始收缩容量。&lt;/p&gt;
&lt;p&gt;例如， 假设创建一个包含 5 个 &lt;code&gt;nginx:1.14.2&lt;/code&gt;  副本的 Deployment 这时将 Deployment 改为包含 &lt;code&gt;nginx:1.16.1&lt;/code&gt; 副本， 此时只创建了 3 个 &lt;code&gt;nginx:1.14.2&lt;/code&gt; 副本。
在这种情况下， Deployment 会马上开始干掉 已经被创建的 3 个 &lt;code&gt;nginx:1.14.2&lt;/code&gt; Pod， 并开始创建 &lt;code&gt;nginx:1.16.1&lt;/code&gt; Pod。 并不会等 5 个 &lt;code&gt;nginx:1.14.2&lt;/code&gt; 创建完成后才应用变更引发的任务&lt;/p&gt;
&lt;!--
### Label selector updates

It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front.
In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped
all of the implications.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; In API version &lt;code&gt;apps/v1&lt;/code&gt;, a Deployment&amp;rsquo;s label selector is immutable after it gets created.&lt;/div&gt;
&lt;/blockquote&gt;


* Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too,
otherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does
not select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and
creating a new ReplicaSet.
* Selector updates changes the existing value in a selector key -- result in the same behavior as additions.
* Selector removals removes an existing key from the Deployment selector -- do not require any changes in the
Pod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the
removed label still exists in any existing Pods and ReplicaSets.
 --&gt;
&lt;h3 id=&#34;标签选择器的更新&#34;&gt;标签选择器的更新&lt;/h3&gt;
&lt;p&gt;通常不建议在标签选择器上做修改，而是预先就想好标签选择器。 在任何你想要修改标签器的情况下，一定要小心小心再小心， 确保已经理清楚所有的头绪。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 对于版本为 &lt;code&gt;apps/v1&lt;/code&gt; API， Deployment 在创建后不可变更。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;标签选择器添加条目必须要 Deployment 的 Pod 模板的标签也要添加该条目， 否则会返回一个校验错误。 也不能让这个变更与原来的不重叠，不重叠的意思是新的选择器
不能匹配到旧的选择创建的 ReplicaSet 主 Pod， 导致旧的 ReplicaSet 脱钩 然后创建新的ReplicaSet&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;标签选择器更新的是其中的已经存在的一个键的值， 与添加条目的行为一至。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;标签选择器更新的删除一个存在的键， 不会对 Pod 模板做任何修改。 存在的 ReplicaSet 不会脱钩， 也不会创建新的 ReplicaSet， 但要注意删除的标签键，依然存在于这些
ReplicaSet 和 Pod 中&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Rolling Back a Deployment

Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping.
By default, all of the Deployment&#39;s rollout history is kept in the system so that you can rollback anytime you want
(you can change that by modifying revision history limit).

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A Deployment&amp;rsquo;s revision is created when a Deployment&amp;rsquo;s rollout is triggered. This means that the
new revision is created if and only if the Deployment&amp;rsquo;s Pod template (&lt;code&gt;.spec.template&lt;/code&gt;) is changed,
for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment,
do not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling.
This means that when you roll back to an earlier revision, only the Deployment&amp;rsquo;s Pod template part is
rolled back.&lt;/div&gt;
&lt;/blockquote&gt;


* Suppose that you made a typo while updating the Deployment, by putting the image name as `nginx:1.161` instead of `nginx:1.16.1`:

    ```shell
    kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.161 --record=true
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment image updated
    ```

* The rollout gets stuck. You can verify it by checking the rollout status:

    ```shell
    kubectl rollout status deployment.v1.apps/nginx-deployment
    ```

    The output is similar to this:
    ```
    Waiting for rollout to finish: 1 out of 3 new replicas have been updated...
    ```

* Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts,
[read more here](#deployment-status).

* You see that the number of old replicas (`nginx-deployment-1564180365` and `nginx-deployment-2035384211`) is 2, and new replicas (nginx-deployment-3066724191) is 1.

    ```shell
    kubectl get rs
    ```

    The output is similar to this:
    ```
    NAME                          DESIRED   CURRENT   READY   AGE
    nginx-deployment-1564180365   3         3         3       25s
    nginx-deployment-2035384211   0         0         0       36s
    nginx-deployment-3066724191   1         1         0       6s
    ```

* Looking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop.

    ```shell
    kubectl get pods
    ```

    The output is similar to this:
    ```
    NAME                                READY     STATUS             RESTARTS   AGE
    nginx-deployment-1564180365-70iae   1/1       Running            0          25s
    nginx-deployment-1564180365-jbqqo   1/1       Running            0          25s
    nginx-deployment-1564180365-hysrc   1/1       Running            0          25s
    nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s
    ```

    &lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on the rollingUpdate parameters (&lt;code&gt;maxUnavailable&lt;/code&gt; specifically) that you have specified. Kubernetes by default sets the value to 25%.&lt;/div&gt;
&lt;/blockquote&gt;


* Get the description of the Deployment:
    ```shell
    kubectl describe deployment
    ```

    The output is similar to this:
    ```
    Name:           nginx-deployment
    Namespace:      default
    CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700
    Labels:         app=nginx
    Selector:       app=nginx
    Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable
    StrategyType:       RollingUpdate
    MinReadySeconds:    0
    RollingUpdateStrategy:  25% max unavailable, 25% max surge
    Pod Template:
      Labels:  app=nginx
      Containers:
       nginx:
        Image:        nginx:1.161
        Port:         80/TCP
        Host Port:    0/TCP
        Environment:  &lt;none&gt;
        Mounts:       &lt;none&gt;
      Volumes:        &lt;none&gt;
    Conditions:
      Type           Status  Reason
      ----           ------  ------
      Available      True    MinimumReplicasAvailable
      Progressing    True    ReplicaSetUpdated
    OldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)
    NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)
    Events:
      FirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message
      --------- --------    -----   ----                    -------------   --------    ------              -------
      1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3
      22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1
      22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2
      22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2
      21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1
      21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3
      13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0
      13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1
    ```

  To fix this, you need to rollback to a previous revision of Deployment that is stable.
 --&gt;
&lt;h2 id=&#34;deployment-回滚示例&#34;&gt;Deployment 回滚示例&lt;/h2&gt;
&lt;p&gt;有时候会需要对 Deployment 进行回滚，例如当一个 Deployment 进入像无限崩溃等不稳定情况时。
默认情况下系统会保存 Deployment 所有的发布历史以便用户通过回滚到之前的任意版本(用户可以修改限制保留历史数)&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Deployment 版本在 Deployment 发布(rollout)时触发。 也就是说一个新的版本在且仅在 Deployment 的 Pod 模板 (&lt;code&gt;.spec.template&lt;/code&gt;) 发生变更，
如更新了模板中的标签或容器的镜像。 其它的如扩充 Deployment 副本数是不会创建版本的，这样在手动或自动伸缩容量时不会引起版本的增加。
也就是说当用户回滚到一个之前的版本时，只有 Deployment 的 Pod 模板部署回到了这个版本的状态。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;假设在将 Deployment 的镜像名改为 &lt;code&gt;nginx:1.16.1&lt;/code&gt; 时，手抖写成了 &lt;code&gt;nginx:1.161&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl set image deployment.v1.apps/nginx-deployment nginx&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx:1.161 --record&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;true
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment image updated
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这时这个发布就卡住了， 可以通过以下命令查看发布状态&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout status deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Waiting for rollout to finish: 1 out of 3 new replicas have been updated...
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Ctrl-C&lt;/code&gt; 退出发布状态监控， 要查看卡住的 Deployment, 见&lt;a href=&#34;#deployment-status&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这时候可以看到旧的副本数 (&lt;code&gt;nginx-deployment-1564180365&lt;/code&gt; 和 &lt;code&gt;nginx-deployment-2035384211&lt;/code&gt;) 是 2, 新的副本数 (&lt;code&gt;nginx-deployment-3066724191&lt;/code&gt;) 是 1.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       25s
nginx-deployment-2035384211   0         0         0       36s
nginx-deployment-3066724191   1         1         0       6s
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;再来看创建的 Pod， 这时候看到由新的  ReplicaSet 因为拉取镜像出问题卡在那了&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                                READY     STATUS             RESTARTS   AGE
nginx-deployment-1564180365-70iae   1/1       Running            0          25s
nginx-deployment-1564180365-jbqqo   1/1       Running            0          25s
nginx-deployment-1564180365-hysrc   1/1       Running            0          25s
nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Deployment 控制器会自动停止错误的发布， 也会停止扩充新 ReplicaSet 的副本数。 这个行为信赖于配置的更新参数 (也就是&lt;code&gt;maxUnavailable&lt;/code&gt;)，
k8s 中默认设置为&lt;/div&gt;
&lt;/blockquote&gt;
25%。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看 Deployment 的描述信息&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:           nginx-deployment
Namespace:      default
CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700
Labels:         app=nginx
Selector:       app=nginx
Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable
StrategyType:       RollingUpdate
MinReadySeconds:    0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.161
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &amp;lt;none&amp;gt;
    Mounts:       &amp;lt;none&amp;gt;
  Volumes:        &amp;lt;none&amp;gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)
NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)
Events:
  FirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message
  --------- --------    -----   ----                    -------------   --------    ------              -------
  1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;而要修复这个问题，就需要将 Deployment 回滚到上一个稳定版本&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Checking Rollout History of a Deployment

Follow the steps given below to check the rollout history:

1. First, check the revisions of this Deployment:
    ```shell
    kubectl rollout history deployment.v1.apps/nginx-deployment
    ```
    The output is similar to this:
    ```
    deployments &#34;nginx-deployment&#34;
    REVISION    CHANGE-CAUSE
    1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml --record=true
    2           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 --record=true
    3           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.161 --record=true
    ```

    `CHANGE-CAUSE` is copied from the Deployment annotation `kubernetes.io/change-cause` to its revisions upon creation. You can specify the`CHANGE-CAUSE` message by:

    * Annotating the Deployment with `kubectl annotate deployment.v1.apps/nginx-deployment kubernetes.io/change-cause=&#34;image updated to 1.16.1&#34;`
    * Append the `--record` flag to save the `kubectl` command that is making changes to the resource.
    * Manually editing the manifest of the resource.

2. To see the details of each revision, run:
    ```shell
    kubectl rollout history deployment.v1.apps/nginx-deployment --revision=2
    ```

    The output is similar to this:
    ```
    deployments &#34;nginx-deployment&#34; revision 2
      Labels:       app=nginx
              pod-template-hash=1159050644
      Annotations:  kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 --record=true
      Containers:
       nginx:
        Image:      nginx:1.16.1
        Port:       80/TCP
         QoS Tier:
            cpu:      BestEffort
            memory:   BestEffort
        Environment Variables:      &lt;none&gt;
      No volumes.
    ```
 --&gt;
&lt;h3 id=&#34;查看-deployment-的历史版本&#34;&gt;查看 Deployment 的历史版本&lt;/h3&gt;
&lt;p&gt;以下步骤可以查看布历史:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;第一步， 查看 Deployment 历史版本列表:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout history deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployments &amp;quot;nginx-deployment&amp;quot;
REVISION    CHANGE-CAUSE
1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml --record=true
2           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 --record=true
3           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.161 --record=true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;CHANGE-CAUSE&lt;/code&gt; 是从 Deployment 中 &lt;code&gt;kubernetes.io/change-cause&lt;/code&gt; 注解中记录的信息。 可能通过以下方式指定 &lt;code&gt;CHANGE-CAUSE&lt;/code&gt; 中的消息&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 Deployment 上打注解 &lt;code&gt;kubectl annotate deployment.v1.apps/nginx-deployment kubernetes.io/change-cause=&amp;quot;image updated to 1.16.1&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;在修改该资源的 &lt;code&gt;kubectl&lt;/code&gt; 命令中添加 &lt;code&gt;--record&lt;/code&gt; 标志&lt;/li&gt;
&lt;li&gt;人工修改资源配置文件&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;要查看每个版本的详细信息，执行如下命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout history deployment.v1.apps/nginx-deployment --revision&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployments &amp;quot;nginx-deployment&amp;quot; revision 2
  Labels:       app=nginx
          pod-template-hash=1159050644
  Annotations:  kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 --record=true
  Containers:
   nginx:
    Image:      nginx:1.16.1
    Port:       80/TCP
     QoS Tier:
        cpu:      BestEffort
        memory:   BestEffort
    Environment Variables:      &amp;lt;none&amp;gt;
  No volumes.
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
### Rolling Back to a Previous Revision
Follow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2.

1. Now you&#39;ve decided to undo the current rollout and rollback to the previous revision:
    ```shell
    kubectl rollout undo deployment.v1.apps/nginx-deployment
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment rolled back
    ```
    Alternatively, you can rollback to a specific revision by specifying it with `--to-revision`:

    ```shell
    kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision=2
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment rolled back
    ```

    For more details about rollout related commands, read [`kubectl rollout`](/docs/reference/generated/kubectl/kubectl-commands#rollout).

    The Deployment is now rolled back to a previous stable revision. As you can see, a `DeploymentRollback` event
    for rolling back to revision 2 is generated from Deployment controller.

2. Check if the rollback was successful and the Deployment is running as expected, run:
    ```shell
    kubectl get deployment nginx-deployment
    ```

    The output is similar to this:
    ```
    NAME               READY   UP-TO-DATE   AVAILABLE   AGE
    nginx-deployment   3/3     3            3           30m
    ```
3. Get the description of the Deployment:
    ```shell
    kubectl describe deployment nginx-deployment
    ```
    The output is similar to this:
    ```
    Name:                   nginx-deployment
    Namespace:              default
    CreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500
    Labels:                 app=nginx
    Annotations:            deployment.kubernetes.io/revision=4
                            kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 --record=true
    Selector:               app=nginx
    Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
    StrategyType:           RollingUpdate
    MinReadySeconds:        0
    RollingUpdateStrategy:  25% max unavailable, 25% max surge
    Pod Template:
      Labels:  app=nginx
      Containers:
       nginx:
        Image:        nginx:1.16.1
        Port:         80/TCP
        Host Port:    0/TCP
        Environment:  &lt;none&gt;
        Mounts:       &lt;none&gt;
      Volumes:        &lt;none&gt;
    Conditions:
      Type           Status  Reason
      ----           ------  ------
      Available      True    MinimumReplicasAvailable
      Progressing    True    NewReplicaSetAvailable
    OldReplicaSets:  &lt;none&gt;
    NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)
    Events:
      Type    Reason              Age   From                   Message
      ----    ------              ----  ----                   -------
      Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3
      Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1
      Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2
      Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2
      Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1
      Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3
      Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0
      Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1
      Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment &#34;nginx-deployment&#34; to revision 2
      Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0
    ```

 --&gt;
&lt;h3 id=&#34;回滚到之前一个版本&#34;&gt;回滚到之前一个版本&lt;/h3&gt;
&lt;p&gt;以下步骤给出把 Deployment 版本从当前版本回滚到之前的版本， 这里是编号为2的版本&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;将当前版本回滚到之前一个版本:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout undo deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment rolled back
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;另外还有一个方式， 可以通过 &lt;code&gt;--to-revision&lt;/code&gt; 参数指定回滚到指定版本:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment rolled back
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;更多发布相关的命令，见&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#rollout&#34;&gt;&lt;code&gt;kubectl rollout&lt;/code&gt;&lt;/a&gt;.
这时 Deployment 就已经回滚到之前的稳定版本。如你所见， 一个用来将 Deployment 回滚到版本 2 的 &lt;code&gt;DeploymentRollback&lt;/code&gt; 事件由 Deployment 控制器创建&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;检查回滚是否成功， Deployment 是否达到预期， 执行发中下命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get deployment nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           30m
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看 Deployment 描述信息:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe deployment nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=4
                        kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 --record=true
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.16.1
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &amp;lt;none&amp;gt;
    Mounts:       &amp;lt;none&amp;gt;
  Volumes:        &amp;lt;none&amp;gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &amp;lt;none&amp;gt;
NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)
Events:
  Type    Reason              Age   From                   Message
  ----    ------              ----  ----                   -------
  Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1
  Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment &amp;quot;nginx-deployment&amp;quot; to revision 2
  Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
## Scaling a Deployment

You can scale a Deployment by using the following command:

```shell
kubectl scale deployment.v1.apps/nginx-deployment --replicas=10
```
The output is similar to this:
```
deployment.apps/nginx-deployment scaled
```

Assuming [horizontal Pod autoscaling](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/) is enabled
in your cluster, you can setup an autoscaler for your Deployment and choose the minimum and maximum number of
Pods you want to run based on the CPU utilization of your existing Pods.

```shell
kubectl autoscale deployment.v1.apps/nginx-deployment --min=10 --max=15 --cpu-percent=80
```
The output is similar to this:
```
deployment.apps/nginx-deployment scaled
```
 --&gt;
&lt;h2 id=&#34;deployment-副本数管理&#34;&gt;Deployment 副本数管理&lt;/h2&gt;
&lt;p&gt;可以通过以下命令修改 Deployment 副本数:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl scale deployment.v1.apps/nginx-deployment --replicas&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment scaled
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;假设集群启用了 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/horizontal-pod-autoscale-walkthrough/&#34;&gt;Pod 水平扩展&lt;/a&gt;， 用户可以为 Deployment 设置
一个自动扩展器，可以根据 Pod 的 CPU 使用率选择 Pod 数量的下限和上限。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl autoscale deployment.v1.apps/nginx-deployment --min&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt; --max&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt; --cpu-percent&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment scaled
&lt;/code&gt;&lt;/pre&gt;&lt;!--
### Proportional scaling

RollingUpdate Deployments support running multiple versions of an application at the same time. When you
or an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress
or paused), the Deployment controller balances the additional replicas in the existing active
ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called *proportional scaling*.

For example, you are running a Deployment with 10 replicas, [maxSurge](#max-surge)=3, and [maxUnavailable](#max-unavailable)=2.

* Ensure that the 10 replicas in your Deployment are running.
  ```shell
  kubectl get deploy
  ```
  The output is similar to this:

  ```
  NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
  nginx-deployment     10        10        10           10          50s
  ```

* You update to a new image which happens to be unresolvable from inside the cluster.
    ```shell
    kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:sometag
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment image updated
    ```

* The image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it&#39;s blocked due to the
`maxUnavailable` requirement that you mentioned above. Check out the rollout status:
    ```shell
    kubectl get rs
    ```
      The output is similar to this:
    ```
    NAME                          DESIRED   CURRENT   READY     AGE
    nginx-deployment-1989198191   5         5         0         9s
    nginx-deployment-618515232    8         8         8         1m
    ```

* Then a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas
to 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren&#39;t using
proportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you
spread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the
most replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the
ReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.

In our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the
new ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assuming
the new replicas become healthy. To confirm this, run:

```shell
kubectl get deploy
```

The output is similar to this:
```
NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     15        18        7            8           7m
```
The rollout status confirms how the replicas were added to each ReplicaSet.
```shell
kubectl get rs
```

The output is similar to this:
```
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   7         7         0         7m
nginx-deployment-618515232    11        11        11        7m
```
 --&gt;
&lt;h3 id=&#34;同比例扩容&#34;&gt;同比例扩容&lt;/h3&gt;
&lt;p&gt;更新策略为 RollingUpdate 的 Deployment 支持一个应用同时运行多个版本。 当用户或自动容量管量器在发布过程中(正在进行或暂停)进行容量伸缩，
Deployment 控制器会新增的副本按已经存在的活跃 ReplicaSet (包含 Pod 的 ReplicaSet)中副本数的比例分散到各个ReplicaSet中与以降低风险。
这就被称为 &lt;em&gt;同比例扩容&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;例如， 用户运行一个有 10 个副本的 Deployment， 它的 &lt;a href=&#34;#max-surge&#34;&gt;maxSurge&lt;/a&gt;=3, &lt;a href=&#34;#max-unavailable&#34;&gt;maxUnavailable&lt;/a&gt;=2.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;确保 Deployment 的 10 个副本正常运行&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get deploy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     10        10        10           10          50s
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;更新了一个集群中不可用的镜像&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl set image deployment.v1.apps/nginx-deployment nginx&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx:sometag
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment image updated
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;更新镜像创建， 开始创建 ReplicaSet &lt;code&gt;nginx-deployment-1989198191&lt;/code&gt;， 但因为上面提到的 &lt;code&gt;maxUnavailable&lt;/code&gt;， 所以会阻塞，通过以下命令查看发布状态：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   5         5         0         9s
nginx-deployment-618515232    8         8         8         1m
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这时候 Deployment 又来了一个扩容请求。 自动伸缩管理器将 Deployment 的副本数加到 15.
Deployment 控制器需要决定这增加的5个副本应该加到哪里。 根据 proportional scaling ， 会将新增的副本分散到所有的 ReplicaSet。
副本占比多的 ReplicaSet 新增的副本也多，副本占比少的新增副本就少。 余数再分给副本最多的 ReplicaSet。 副本数为 0 ReplicaSet 的则不会扩容。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在上面的例子中， 3 个副本被添加到旧的 ReplicaSet 中， 两个副本被添加到新的 ReplicaSet。 发布的继续推进最终会将所有的副本都移到新的 ReplicaSet
保证所以新的副本都是正常的。 执行以下命令验证:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get deploy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     15        18        7            8           7m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;发布状态也确认了副本是怎么加到每个 ReplicaSet 上的&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   7         7         0         7m
nginx-deployment-618515232    11        11        11        7m
&lt;/code&gt;&lt;/pre&gt;&lt;!--
## Pausing and Resuming a Deployment

You can pause a Deployment before triggering one or more updates and then resume it. This allows you to
apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.

* For example, with a Deployment that was just created:
  Get the Deployment details:
  ```shell
  kubectl get deploy
  ```
  The output is similar to this:
  ```
  NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
  nginx     3         3         3            3           1m
  ```
  Get the rollout status:
  ```shell
  kubectl get rs
  ```
  The output is similar to this:
  ```
  NAME               DESIRED   CURRENT   READY     AGE
  nginx-2142116321   3         3         3         1m
  ```

* Pause by running the following command:
    ```shell
    kubectl rollout pause deployment.v1.apps/nginx-deployment
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment paused
    ```

* Then update the image of the Deployment:
    ```shell
    kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment image updated
    ```

* Notice that no new rollout started:
    ```shell
    kubectl rollout history deployment.v1.apps/nginx-deployment
    ```

    The output is similar to this:
    ```
    deployments &#34;nginx&#34;
    REVISION  CHANGE-CAUSE
    1   &lt;none&gt;
    ```
* Get the rollout status to ensure that the Deployment is updates successfully:
    ```shell
    kubectl get rs
    ```

    The output is similar to this:
    ```
    NAME               DESIRED   CURRENT   READY     AGE
    nginx-2142116321   3         3         3         2m
    ```

* You can make as many updates as you wish, for example, update the resources that will be used:
    ```shell
    kubectl set resources deployment.v1.apps/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment resource requirements updated
    ```

    The initial state of the Deployment prior to pausing it will continue its function, but new updates to
    the Deployment will not have any effect as long as the Deployment is paused.

* Eventually, resume the Deployment and observe a new ReplicaSet coming up with all the new updates:
    ```shell
    kubectl rollout resume deployment.v1.apps/nginx-deployment
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment resumed
    ```
* Watch the status of the rollout until it&#39;s done.
    ```shell
    kubectl get rs -w
    ```

    The output is similar to this:
    ```
    NAME               DESIRED   CURRENT   READY     AGE
    nginx-2142116321   2         2         2         2m
    nginx-3926361531   2         2         0         6s
    nginx-3926361531   2         2         1         18s
    nginx-2142116321   1         2         2         2m
    nginx-2142116321   1         2         2         2m
    nginx-3926361531   3         2         1         18s
    nginx-3926361531   3         2         1         18s
    nginx-2142116321   1         1         1         2m
    nginx-3926361531   3         3         1         18s
    nginx-3926361531   3         3         2         19s
    nginx-2142116321   0         1         1         2m
    nginx-2142116321   0         1         1         2m
    nginx-2142116321   0         0         0         2m
    nginx-3926361531   3         3         3         20s
    ```
* Get the status of the latest rollout:
    ```shell
    kubectl get rs
    ```

    The output is similar to this:
    ```
    NAME               DESIRED   CURRENT   READY     AGE
    nginx-2142116321   0         0         0         2m
    nginx-3926361531   3         3         3         28s
    ```
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You cannot rollback a paused Deployment until you resume it.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;deployment-的暂停与恢复&#34;&gt;Deployment 的暂停与恢复&lt;/h2&gt;
&lt;p&gt;用户可以一个或多个更新触发之前暂停 Deployment 然后再恢复它。 通过这种方式用户可以在暂停后进行多次修改然后恢复，这样在多次
修改的过程中就不会触发不必要的发布。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;例如以下为一个刚创建的 Deployment:
查看 Deployment 信息:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get deploy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     3         3         3            3           1m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看发布状态:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         1m
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;执行以下命令暂停 Deployment:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout pause deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment paused
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这时先修改 Deployment 的镜像:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl set image deployment.v1.apps/nginx-deployment nginx&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx:1.16.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment image updated
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可以看到现在没有触发新的发布:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout history deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployments &amp;quot;nginx&amp;quot;
REVISION  CHANGE-CAUSE
1   &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;查看发布状态，确保 Deployment 的更新的成功的：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         2m
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这时候用户就可以进行任意多次修改，例如，以下为修改资源占用:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl set resources deployment.v1.apps/nginx-deployment -c&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx --limits&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;cpu&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;200m,memory&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;512Mi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment resource requirements updated
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Deployment 会继续以暂停之前的状态提供功能， 但在恢复的在的所有修改都不会生效&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;最后，恢复 Deployment 观察新建的 ReplicaSet 会包含暂停过程中的所以的更新&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout resume deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment resumed
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;Watch the status of the rollout until it&amp;rsquo;s done.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;观察发布状态直至任务完成&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs -w
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   2         2         2         2m
nginx-3926361531   2         2         0         6s
nginx-3926361531   2         2         1         18s
nginx-2142116321   1         2         2         2m
nginx-2142116321   1         2         2         2m
nginx-3926361531   3         2         1         18s
nginx-3926361531   3         2         1         18s
nginx-2142116321   1         1         1         2m
nginx-3926361531   3         3         1         18s
nginx-3926361531   3         3         2         19s
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         20s
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;查看最终的发布状态:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         28s
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Deployment 在暂停后直至恢复，中间不能回滚&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Deployment status

A Deployment enters various states during its lifecycle. It can be [progressing](#progressing-deployment) while
rolling out a new ReplicaSet, it can be [complete](#complete-deployment), or it can [fail to progress](#failed-deployment).
 --&gt;
&lt;h2 id=&#34;deployment-status&#34;&gt;Deployment 的状态&lt;/h2&gt;
&lt;p&gt;一个 Deployment 在整个生命周期中会进入许多种状态。更新到一个新的 ReplicaSet 时， 状态可以是 &lt;a href=&#34;#progressing-deployment&#34;&gt;进行中&lt;/a&gt;
也可能是 &lt;a href=&#34;#complete-deployment&#34;&gt;完成&lt;/a&gt;，还可以是 &lt;a href=&#34;#failed-deployment&#34;&gt;失败&lt;/a&gt;&lt;/p&gt;
&lt;!--
### Progressing Deployment

Kubernetes marks a Deployment as _progressing_ when one of the following tasks is performed:

* The Deployment creates a new ReplicaSet.
* The Deployment is scaling up its newest ReplicaSet.
* The Deployment is scaling down its older ReplicaSet(s).
* New Pods become ready or available (ready for at least [MinReadySeconds](#min-ready-seconds)).
 --&gt;
&lt;h3 id=&#34;progressing-deployment&#34;&gt;Deployment 进行中&lt;/h3&gt;
&lt;p&gt;当 Deployment 在进行以下任一任务时，标记为 &lt;em&gt;进行中&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deployment 创建一个新的 ReplicaSet&lt;/li&gt;
&lt;li&gt;Deployment 在对最新的 ReplicaSet 扩容&lt;/li&gt;
&lt;li&gt;Deployment 在对旧的 ReplicaSet 缩减容量&lt;/li&gt;
&lt;li&gt;新创建的 Pod 正在进入就绪或可用状态(就绪最小时间&lt;a href=&#34;#min-ready-seconds&#34;&gt;MinReadySeconds&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;complete-deployment&#34;&gt;Complete Deployment&lt;/h3&gt;
&lt;p&gt;Kubernetes marks a Deployment as &lt;em&gt;complete&lt;/em&gt; when it has the following characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All of the replicas associated with the Deployment have been updated to the latest version you&amp;rsquo;ve specified, meaning any
updates you&amp;rsquo;ve requested have been completed.&lt;/li&gt;
&lt;li&gt;All of the replicas associated with the Deployment are available.&lt;/li&gt;
&lt;li&gt;No old replicas for the Deployment are running.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can check if a Deployment has completed by using &lt;code&gt;kubectl rollout status&lt;/code&gt;. If the rollout completed
successfully, &lt;code&gt;kubectl rollout status&lt;/code&gt; returns a zero exit code.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout status deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The output is similar to this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Waiting for rollout to finish: 2 of 3 updated replicas are available...
deployment.apps/nginx-deployment successfully rolled out
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and the exit status from &lt;code&gt;kubectl rollout&lt;/code&gt; is 0 (success):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;echo $?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;0
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;deployment-完成&#34;&gt;Deployment 完成&lt;/h3&gt;
&lt;p&gt;当 Deployment 拥有以下特征时被 k8s 标记为 完成&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有与 Deployment 关联的副本都已经使用了最新的配置，也就是说所以的更新请求都已经完成。&lt;/li&gt;
&lt;li&gt;所有与 Deployment 关联的副本都已经可用&lt;/li&gt;
&lt;li&gt;Deployment 没有旧的副本在运行&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用户可以通过命令 &lt;code&gt;kubectl rollout status&lt;/code&gt; 检测 Deployment 是否完成。 如果发布已经完成，
&lt;code&gt;kubectl rollout status&lt;/code&gt; 命令的退出码为 0&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout status deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Waiting for rollout to finish: 2 of 3 updated replicas are available...
deployment.apps/nginx-deployment successfully rolled out
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl rollout&lt;/code&gt; 的退出码为 0 (成功):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;echo $?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;0
&lt;/code&gt;&lt;/pre&gt;&lt;!--
### Failed Deployment

Your Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur
due to some of the following factors:

* Insufficient quota
* Readiness probe failures
* Image pull errors
* Insufficient permissions
* Limit ranges
* Application runtime misconfiguration

One way you can detect this condition is to specify a deadline parameter in your Deployment spec:
([`.spec.progressDeadlineSeconds`](#progress-deadline-seconds)). `.spec.progressDeadlineSeconds` denotes the
number of seconds the Deployment controller waits before indicating (in the Deployment status) that the
Deployment progress has stalled.

The following `kubectl` command sets the spec with `progressDeadlineSeconds` to make the controller report
lack of progress for a Deployment after 10 minutes:

```shell
kubectl patch deployment.v1.apps/nginx-deployment -p &#39;{&#34;spec&#34;:{&#34;progressDeadlineSeconds&#34;:600}}&#39;
```
The output is similar to this:
```
deployment.apps/nginx-deployment patched
```
Once the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following
attributes to the Deployment&#39;s `.status.conditions`:

* Type=Progressing
* Status=False
* Reason=ProgressDeadlineExceeded

See the [Kubernetes API conventions](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties) for more information on status conditions.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Kubernetes takes no action on a stalled Deployment other than to report a status condition with
&lt;code&gt;Reason=ProgressDeadlineExceeded&lt;/code&gt;. Higher level orchestrators can take advantage of it and act accordingly, for
example, rollback the Deployment to its previous version.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If you pause a Deployment, Kubernetes does not check progress against your specified deadline.
You can safely pause a Deployment in the middle of a rollout and resume without triggering
the condition for exceeding the deadline.&lt;/div&gt;
&lt;/blockquote&gt;


You may experience transient errors with your Deployments, either due to a low timeout that you have set or
due to any other kind of error that can be treated as transient. For example, let&#39;s suppose you have
insufficient quota. If you describe the Deployment you will notice the following section:

```shell
kubectl describe deployment nginx-deployment
```
The output is similar to this:
```
&lt;...&gt;
Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     True    ReplicaSetUpdated
  ReplicaFailure  True    FailedCreate
&lt;...&gt;
```

If you run `kubectl get deployment nginx-deployment -o yaml`, the Deployment status is similar to this:

```
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: Replica set &#34;nginx-deployment-4262182780&#34; is progressing.
    reason: ReplicaSetUpdated
    status: &#34;True&#34;
    type: Progressing
  - lastTransitionTime: 2016-10-04T12:25:42Z
    lastUpdateTime: 2016-10-04T12:25:42Z
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: &#34;True&#34;
    type: Available
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: &#39;Error creating: pods &#34;nginx-deployment-4262182780-&#34; is forbidden: exceeded quota:
      object-counts, requested: pods=1, used: pods=3, limited: pods=2&#39;
    reason: FailedCreate
    status: &#34;True&#34;
    type: ReplicaFailure
  observedGeneration: 3
  replicas: 2
  unavailableReplicas: 2
```

Eventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the
reason for the Progressing condition:

```
Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     False   ProgressDeadlineExceeded
  ReplicaFailure  True    FailedCreate
```

You can address an issue of insufficient quota by scaling down your Deployment, by scaling down other
controllers you may be running, or by increasing quota in your namespace. If you satisfy the quota
conditions and the Deployment controller then completes the Deployment rollout, you&#39;ll see the
Deployment&#39;s status update with a successful condition (`Status=True` and `Reason=NewReplicaSetAvailable`).

```
Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
```

`Type=Available` with `Status=True` means that your Deployment has minimum availability. Minimum availability is dictated
by the parameters specified in the deployment strategy. `Type=Progressing` with `Status=True` means that your Deployment
is either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum
required new replicas are available (see the Reason of the condition for the particulars - in our case
`Reason=NewReplicaSetAvailable` means that the Deployment is complete).

You can check if a Deployment has failed to progress by using `kubectl rollout status`. `kubectl rollout status`
returns a non-zero exit code if the Deployment has exceeded the progression deadline.

```shell
kubectl rollout status deployment.v1.apps/nginx-deployment
```
The output is similar to this:
```
Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
error: deployment &#34;nginx&#34; exceeded its progress deadline
```
and the exit status from `kubectl rollout` is 1 (indicating an error):
```shell
echo $?
```
```
1
```
 --&gt;
&lt;h3 id=&#34;deployment-失败&#34;&gt;Deployment 失败&lt;/h3&gt;
&lt;p&gt;Deployment 可能在尝试部署最新的 ReplicaSet 的时候被卡住然后再也完不成。造成这种情况的因素可能有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;配额不足&lt;/li&gt;
&lt;li&gt;就绪探针检测结果为失败&lt;/li&gt;
&lt;li&gt;镜像拉取出错&lt;/li&gt;
&lt;li&gt;权限不足&lt;/li&gt;
&lt;li&gt;应用运行环境配置错误&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;检测到这种情况的一种方法为在 Deployment 的配置中加入一个死线参数: (&lt;a href=&#34;#progress-deadline-seconds&#34;&gt;&lt;code&gt;.spec.progressDeadlineSeconds&lt;/code&gt;&lt;/a&gt;).
&lt;code&gt;.spec.progressDeadlineSeconds&lt;/code&gt; 表示 Deployment 控制器在(Deployment 状态上)显示 Deployment 停止之前等待的时间(单位秒)
以下 &lt;code&gt;kubectl&lt;/code&gt; 添加/修改 &lt;code&gt;progressDeadlineSeconds&lt;/code&gt;， 让控制器在 10 分钟还在进行中的发布标记为失败&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl patch deployment.v1.apps/nginx-deployment -p &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;progressDeadlineSeconds&amp;#34;:600}}&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment patched
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当超过设定时限后， Deployment 控制器会向 Deployment 添加一个拥有如下属性的 DeploymentCondition 到 &lt;code&gt;.status.conditions&lt;/code&gt; 。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Type=Progressing&lt;/li&gt;
&lt;li&gt;Status=False&lt;/li&gt;
&lt;li&gt;Reason=ProgressDeadlineExceeded&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更新关于状态条件的信息见 &lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties&#34;&gt;Kubernetes API conventions&lt;/a&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 对于停止的 Deployment k8s 除了添加一个 &lt;code&gt;Reason=ProgressDeadlineExceeded&lt;/code&gt; 条件之前不会做任何其它操作。
层级更高的编排者会根据这个条件进行相应的操作，例如，将 Deployment 回滚到前一个稳定版本&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果 Deployment 被暂停，则 k8s 不会受该时限影响。 用户可以发布乾中安全的暂停并恢复 Deployment 而不需要担心触发这个
时限条件。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;用户可能会遇到 Deployment 短时的错误，可能是因为用户设置一个较低的超时时间或其它任意可以被认作瞬时错误的。
例如， 假设配额不足。 这时时候获取 Deployment 描述信息：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe deployment nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;...&amp;gt;
Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     True    ReplicaSetUpdated
  ReplicaFailure  True    FailedCreate
&amp;lt;...&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;执行命令 &lt;code&gt;kubectl get deployment nginx-deployment -o yaml&lt;/code&gt;, Deployment 状态类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: Replica set &amp;quot;nginx-deployment-4262182780&amp;quot; is progressing.
    reason: ReplicaSetUpdated
    status: &amp;quot;True&amp;quot;
    type: Progressing
  - lastTransitionTime: 2016-10-04T12:25:42Z
    lastUpdateTime: 2016-10-04T12:25:42Z
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: &amp;quot;True&amp;quot;
    type: Available
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: &#39;Error creating: pods &amp;quot;nginx-deployment-4262182780-&amp;quot; is forbidden: exceeded quota:
      object-counts, requested: pods=1, used: pods=3, limited: pods=2&#39;
    reason: FailedCreate
    status: &amp;quot;True&amp;quot;
    type: ReplicaFailure
  observedGeneration: 3
  replicas: 2
  unavailableReplicas: 2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;最终， 当超出 Deployment 的处理时限时， k8s 更新状态和添加处理条件的原因&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     False   ProgressDeadlineExceeded
  ReplicaFailure  True    FailedCreate
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在遇到配额不足的情况，用户可以通过缩减 Deployment 副本数来解决， 也可以缩减用户其它控制器的副本数，还可以增加用户命名空间的配额。
如果达到配额条件， Deployment 这时候就会过成 Deployment 的发布， 这时候就会看到 Deployment 状态被更新为一个成功条件
(&lt;code&gt;Status=True&lt;/code&gt; 和 &lt;code&gt;Reason=NewReplicaSetAvailable&lt;/code&gt;)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当 &lt;code&gt;Type=Available&lt;/code&gt; 的 &lt;code&gt;Status=True&lt;/code&gt; 时意味着 Deployment 达到了最小可用状。 最小可用状态受 deployment 策略中的参数控制。
&lt;code&gt;Type=Progressing&lt;/code&gt; 的 &lt;code&gt;Status=True&lt;/code&gt; 意味着 Deployment 的发布还在进行或已经处理成功完成，最小需求的副本数已经可用(见具体条件的原因 - 就本例来说
&lt;code&gt;Reason=NewReplicaSetAvailable&lt;/code&gt; 表示 Deployment 已经完成)&lt;/p&gt;
&lt;p&gt;用户可以通过执行命令 &lt;code&gt;kubectl rollout status&lt;/code&gt; 来检测 Deployment 是处理失败。如果 Deployment 已经超出处理时限
&lt;code&gt;kubectl rollout status&lt;/code&gt; 返回码非 0&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout status deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
error: deployment &amp;quot;nginx&amp;quot; exceeded its progress deadline
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl rollout&lt;/code&gt; 返回码为 1 (表示出错):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;echo $?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;1
&lt;/code&gt;&lt;/pre&gt;&lt;!--
### Operating on a failed deployment

All actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back
to a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template.
 --&gt;
&lt;h3 id=&#34;在失败状态的-deployment-上可以执行的操作&#34;&gt;在失败状态的 Deployment 上可以执行的操作&lt;/h3&gt;
&lt;p&gt;所有在完成的 Deployment 上可以执行的操作都可以在失败的 Deployment 执行. 用户可以扩充/缩减容量, 回滚到之前的一个版本, 甚至如果用户需要
多次修改 Deployment 的 Pod 模板可以将其暂停.&lt;/p&gt;
&lt;!--
## Clean up Policy

You can set `.spec.revisionHistoryLimit` field in a Deployment to specify how many old ReplicaSets for
this Deployment you want to retain. The rest will be garbage-collected in the background. By default,
it is 10.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Explicitly setting this field to 0, will result in cleaning up all the history of your Deployment
thus that Deployment will not be able to roll back.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;清理策略&#34;&gt;清理策略&lt;/h2&gt;
&lt;p&gt;用户可以通过设置 Deployment 中的 &lt;code&gt;.spec.revisionHistoryLimit&lt;/code&gt; 字段, 指定保留多少份旧的 ReplicaSets 的记录.
超出的部分会被后台垃圾回收. 默认是 10&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果将该字段设置为 0, 则会导致所以的历史都会被删除, Deployment 不能被回滚.&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Canary Deployment

If you want to roll out releases to a subset of users or servers using the Deployment, you
can create multiple Deployments, one for each release, following the canary pattern described in
[managing resources](/docs/concepts/cluster-administration/manage-deployment/#canary-deployments).
 --&gt;
&lt;h2 id=&#34;金丝雀-deployment&#34;&gt;金丝雀 Deployment&lt;/h2&gt;
&lt;p&gt;如查用户想让一部分用户或服务使用这个 Deployment 发布的版本, 可以创建多个 Deployment, 每一个发布一个版本,
依据&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/cluster-administration/manage-deployment/#canary-deployments&#34;&gt;资源管理&lt;/a&gt;中描述的金丝雀规则。&lt;/p&gt;
&lt;!--
## Writing a Deployment Spec

As with all other Kubernetes configs, a Deployment needs `.apiVersion`, `.kind`, and `.metadata` fields.
For general information about working with config files, see
[deploying applications](/docs/tasks/run-application/run-stateless-application-deployment/),
configuring containers, and [using kubectl to manage resources](/docs/concepts/overview/working-with-objects/object-management/) documents.
The name of a Deployment object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

A Deployment also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).
 --&gt;
&lt;h2 id=&#34;编写-deployment-spec&#34;&gt;编写 Deployment Spec&lt;/h2&gt;
&lt;p&gt;As with all other Kubernetes configs, a Deployment needs &lt;code&gt;.apiVersion&lt;/code&gt;, &lt;code&gt;.kind&lt;/code&gt;, and &lt;code&gt;.metadata&lt;/code&gt; fields.
For general information about working with config files, see
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/run-application/run-stateless-application-deployment/&#34;&gt;deploying applications&lt;/a&gt;,
configuring containers, and &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/object-management/&#34;&gt;using kubectl to manage resources&lt;/a&gt; documents.
The name of a Deployment object must be a valid
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS subdomain name&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A Deployment also needs a &lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;&lt;code&gt;.spec&lt;/code&gt; section&lt;/a&gt;.
与所以其它的 k8s 配置一样，Deployment 必须要有  &lt;code&gt;.apiVersion&lt;/code&gt;, &lt;code&gt;.kind&lt;/code&gt;, &lt;code&gt;.metadata&lt;/code&gt; 字段。
配置文件的通用信息见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/run-stateless-application-deployment/&#34;&gt;部署应用&lt;/a&gt;,
配置容器， &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/object-management/&#34;&gt;使用 kubect 管理资源&lt;/a&gt;.
Deployment 对象的名称必须是一个有效的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Deployment 还需要有一个 &lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;&lt;code&gt;.spec&lt;/code&gt; 定义区&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### Pod Template

The `.spec.template` and `.spec.selector` are the only required field of the `.spec`.

The `.spec.template` is a [Pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;, except it is nested and does not have an `apiVersion` or `kind`.

In addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate
labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See [selector](#selector)).

Only a [`.spec.template.spec.restartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Always` is
allowed, which is the default if not specified.
 --&gt;
&lt;h3 id=&#34;pod-模板&#34;&gt;Pod 模板&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec&lt;/code&gt; 的必要字段仅有 &lt;code&gt;.spec.template&lt;/code&gt; 和 &lt;code&gt;.spec.selector&lt;/code&gt;两个。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;.spec.template&lt;/code&gt; 字段就是一个 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/#pod-templates&#34;&gt;Pod 模板&lt;/a&gt;对象。定义规范与 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
完全一样， 除了因为它嵌套的 Deployment 中所以没 &lt;code&gt;apiVersion&lt;/code&gt; 或 &lt;code&gt;kind&lt;/code&gt; 字段。&lt;/p&gt;
&lt;p&gt;相对裸 Pod 所以需要的字段， Deployment 中的 Pod 模板需要指定适当的标签和重启策略。 对于标签定义，需要注意不要与其它的控制器重叠了。 见 &lt;a href=&#34;#selector&#34;&gt;selector&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/pod-lifecycle/#restart-policy&#34;&gt;&lt;code&gt;.spec.template.spec.restartPolicy&lt;/code&gt;&lt;/a&gt; 的值只能是 &lt;code&gt;Always&lt;/code&gt;， 这也是默认值。&lt;/p&gt;
&lt;!--
### Replicas

`.spec.replicas` is an optional field that specifies the number of desired Pods. It defaults to 1.
 --&gt;
&lt;h3 id=&#34;副本数&#34;&gt;副本数&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.replicas&lt;/code&gt; 是一个可选字段，表示期望的 Pod 的数量。 默认为 1&lt;/p&gt;
&lt;!--
### Selector

`.spec.selector` is a required field that specifies a [label selector](/docs/concepts/overview/working-with-objects/labels/)
for the Pods targeted by this Deployment.

`.spec.selector` must match `.spec.template.metadata.labels`, or it will be rejected by the API.

In API version `apps/v1`, `.spec.selector` and `.metadata.labels` do not default to `.spec.template.metadata.labels` if not set. So they must be set explicitly. Also note that `.spec.selector` is immutable after creation of the Deployment in `apps/v1`.

A Deployment may terminate Pods whose labels match the selector if their template is different
from `.spec.template` or if the total number of such Pods exceeds `.spec.replicas`. It brings up new
Pods with `.spec.template` if the number of Pods is less than the desired number.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You should not create other Pods whose labels match this selector, either directly, by creating
another Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you
do so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this.&lt;/div&gt;
&lt;/blockquote&gt;


If you have multiple controllers that have overlapping selectors, the controllers will fight with each
other and won&#39;t behave correctly.
 --&gt;
&lt;h3 id=&#34;选择器&#34;&gt;选择器&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.selector&lt;/code&gt; 是一个必要字段，用于定义 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/&#34;&gt;标签选择器&lt;/a&gt;, 标签选择器用于选择 Deployment 下属的 Pod&lt;/p&gt;
&lt;p&gt;&lt;code&gt;.spec.selector&lt;/code&gt; 必须与 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt;配置, 否则 API 会拒绝.&lt;/p&gt;
&lt;p&gt;API 版本 &lt;code&gt;apps/v1&lt;/code&gt; 中， &lt;code&gt;.spec.selector&lt;/code&gt; 和 &lt;code&gt;.metadata.labels&lt;/code&gt; 如果没有设置不会默认设置为 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt;。 所以这两个字段必须要显示的设置。&lt;/p&gt;
&lt;p&gt;Deployment 终结标签匹配选择器，但模板与  &lt;code&gt;.spec.template&lt;/code&gt; 不同的 Pod。
如果标签选择器匹配的 Pod 数量大于 &lt;code&gt;.spec.replicas&lt;/code&gt; 的数量，则会终结多于的 Pod。
如果标签选择器匹配的 Pod 数量小于 &lt;code&gt;.spec.replicas&lt;/code&gt; 的数量， 会使用&lt;code&gt;.spec.template&lt;/code&gt;创建新的 Pod， 使总数与期望数相同。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 用户需要注意不能创建标签匹配该选择的的 Pod， 不管理是直接创建还是通过 Deployment 创建，或者通过其它的如 ReplicaSet 或 ReplicationController
控制器来创建。 如果这样做了，第一个 Deployment 这些 Pod 也是它创建的。k8s 不会阻止用户这样做&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果有多个控制器使用了一样的选择器，这些选择器就会打架，然后不能正常工作。&lt;/p&gt;
&lt;!--
### Strategy

`.spec.strategy` specifies the strategy used to replace old Pods by new ones.
`.spec.strategy.type` can be &#34;Recreate&#34; or &#34;RollingUpdate&#34;. &#34;RollingUpdate&#34; is
the default value.
 --&gt;
&lt;h3 id=&#34;策略&#34;&gt;策略&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.strategy&lt;/code&gt; 指定新 Pod 代替旧 Pod 使用的策略
&lt;code&gt;.spec.strategy.type&lt;/code&gt; 的值可以是 &lt;code&gt;Recreate&lt;/code&gt; 或 &lt;code&gt;RollingUpdate&lt;/code&gt;。 默认为 &lt;code&gt;RollingUpdate&lt;/code&gt;&lt;/p&gt;
&lt;!--
#### Recreate Deployment

All existing Pods are killed before new ones are created when `.spec.strategy.type==Recreate`.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; This will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods
of the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new
revision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the
replacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an
&amp;ldquo;at most&amp;rdquo; guarantee for your Pods, you should consider using a
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/statefulset/&#34;&gt;StatefulSet&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h4 id=&#34;deployment-重建策略&#34;&gt;Deployment 重建策略&lt;/h4&gt;
&lt;p&gt;当设置 &lt;code&gt;.spec.strategy.type==Recreate&lt;/code&gt; 时，会先将所以旧的 Pod 删除再创建新的&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 这个策略只能保证终止旧 Pod 的操作在创建新 Pod 的操作之前。 如果用户更新的一个 Deployment， 所以旧的 Pod 就会马上被标记为终止产状。
成功删除则要等到有任意一个新版本的 Pod 创建成功之后。 如果在这之前，用户手动删除了一个 Pod， 这时它的生命周期还受 ReplicaSet 控制，
所以会立马又创建一个旧的(尽管旧的 Pod 依然处于终止中的状态)。 如果用户需要 “最大的” 保证， 则建议考虑使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/statefulset/&#34;&gt;StatefulSet&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
#### Rolling Update Deployment

The Deployment updates Pods in a rolling update
fashion when `.spec.strategy.type==RollingUpdate`. You can specify `maxUnavailable` and `maxSurge` to control
the rolling update process.
 --&gt;
&lt;h4 id=&#34;deployment-滚动更新策略&#34;&gt;Deployment 滚动更新策略&lt;/h4&gt;
&lt;p&gt;当 &lt;code&gt;.spec.strategy.type==RollingUpdate&lt;/code&gt; 时 Deployment 以滚动方式更新 Pod。
用户可以通过 &lt;code&gt;maxUnavailable&lt;/code&gt; 和 &lt;code&gt;maxSurge&lt;/code&gt; 来控制更新进程&lt;/p&gt;
&lt;!--
##### Max Unavailable

`.spec.strategy.rollingUpdate.maxUnavailable` is an optional field that specifies the maximum number
of Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5)
or a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by
rounding down. The value cannot be 0 if `.spec.strategy.rollingUpdate.maxSurge` is 0. The default value is 25%.

For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired
Pods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled
down further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available
at all times during the update is at least 70% of the desired Pods.
 --&gt;
&lt;h5 id=&#34;最大不可用数&#34;&gt;最大不可用数&lt;/h5&gt;
&lt;p&gt;&lt;code&gt;.spec.strategy.rollingUpdate.maxUnavailable&lt;/code&gt; 是一个可选字段， 用于设置在更新过程最大不可用 Pod 的数量。 字段值可以是一个数字(如， 5)
也可以是期望副本数的百分比(如，10%)。 数字为百分比向下取整。 如果 &lt;code&gt;.spec.strategy.rollingUpdate.maxSurge&lt;/code&gt; 的值是 0 则该字段值不能是 0.
默认值为 &lt;code&gt;25%&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;例如， 当这个值设置为 30%, 旧的 ReplicaSet 在滚动更新开始时就能缩减容量是期望数量的 70%， 旧的 ReplicaSet 会在 新的 ReplicaSet 扩容后
相应的收缩自己的容量，以确保在整个更新过程中可用的 Pod 数不低于期望数量的 70%。&lt;/p&gt;
&lt;!--
##### Max Surge

`.spec.strategy.rollingUpdate.maxSurge` is an optional field that specifies the maximum number of Pods
that can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a
percentage of desired Pods (for example, 10%). The value cannot be 0 if `MaxUnavailable` is 0. The absolute number
is calculated from the percentage by rounding up. The default value is 25%.

For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the
rolling update starts, such that the total number of old and new Pods does not exceed 130% of desired
Pods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the
total number of Pods running at any time during the update is at most 130% of desired Pods.
 --&gt;
&lt;h5 id=&#34;最大可超预期数&#34;&gt;最大可超预期数&lt;/h5&gt;
&lt;p&gt;&lt;code&gt;.spec.strategy.rollingUpdate.maxSurge&lt;/code&gt; 是一个可选字段， 用于设置在更新过程最大可超过期望数的数量。 这个值可以是一个数量(如， 5)
也可以是期望副本数的百分比(如，10%)。如果 &lt;code&gt;MaxUnavailable&lt;/code&gt; 值为 0， 则该字段值不能是 0. 数字为百分比向上取整。默认值为 25%。&lt;/p&gt;
&lt;p&gt;例如， 当这个值设置为 30%, 新的 ReplicaSet 会在滚动更新开始后马上扩容。 只要新旧 Pod 总数不超过期望数量的 130%。
当旧的 Pod 被干掉， 新的 ReplicaSet 就能继续扩容， 只要始终确保总的 Pod 不超过期望数量的 130%。&lt;/p&gt;
&lt;!--
### Progress Deadline Seconds

`.spec.progressDeadlineSeconds` is an optional field that specifies the number of seconds you want
to wait for your Deployment to progress before the system reports back that the Deployment has
[failed progressing](#failed-deployment) - surfaced as a condition with `Type=Progressing`, `Status=False`.
and `Reason=ProgressDeadlineExceeded` in the status of the resource. The Deployment controller will keep
retrying the Deployment. This defaults to 600. In the future, once automatic rollback will be implemented, the Deployment
controller will roll back a Deployment as soon as it observes such a condition.

If specified, this field needs to be greater than `.spec.minReadySeconds`.
 --&gt;
&lt;h3 id=&#34;更新处理时限&#34;&gt;更新处理时限&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.progressDeadlineSeconds&lt;/code&gt; 是一个可选字段， 用于设置 Deployment 进行更新处理的时限(单位秒)， 如果超过这个时限
这次更新还没有完成则系统就会报告 Deployment &lt;a href=&#34;#failed-deployment&#34;&gt;处理失败&lt;/a&gt;，报告为向状态资源添加一个包含以下字段的条件
&lt;code&gt;Type=Progressing&lt;/code&gt;, &lt;code&gt;Status=False&lt;/code&gt;, &lt;code&gt;Reason=ProgressDeadlineExceeded&lt;/code&gt;. Deployment 控制器会持续重试这个 Deployment.
这个字段默认值为 600. 在将来，当自动回滚实现以后，当发现这个超时条件就会自动回滚这个 Deployment。&lt;/p&gt;
&lt;p&gt;如果为设置该字段，该字段的值需要大于 &lt;code&gt;.spec.minReadySeconds&lt;/code&gt;&lt;/p&gt;
&lt;!--
### Min Ready Seconds

`.spec.minReadySeconds` is an optional field that specifies the minimum number of seconds for which a newly
created Pod should be ready without any of its containers crashing, for it to be considered available.
This defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when
a Pod is considered ready, see [Container Probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).
 --&gt;
&lt;h3 id=&#34;最小就绪时间&#34;&gt;最小就绪时间&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.minReadySeconds&lt;/code&gt; 是一个可选字段， 用于设置一个新创建的 Pod 在没有任何容器崩掉的情况下达成就绪到被认为可用的最小时限(单位秒)
默认是 0 (Pod 在就绪后就认为可用)。 了解更多关于 Pod 被认为就绪的信息，见&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/pod-lifecycle/#container-probes&#34;&gt;容器探针&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### Revision History Limit

A Deployment&#39;s revision history is stored in the ReplicaSets it controls.

`.spec.revisionHistoryLimit` is an optional field that specifies the number of old ReplicaSets to retain
to allow rollback. These old ReplicaSets consume resources in `etcd` and crowd the output of `kubectl get rs`. The configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the frequency and stability of new Deployments.

More specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up.
In this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up.
 --&gt;
&lt;h3 id=&#34;历史版本记录限制&#34;&gt;历史版本记录限制&lt;/h3&gt;
&lt;p&gt;Deployment 的版本历史存在受它控制的 ReplicaSet 中。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;.spec.revisionHistoryLimit&lt;/code&gt; 是一个可选字段。 用户设置允许回滚而保留的旧的 ReplicaSet 的数量。
这个旧的 ReplicaSet 会被存储在 &lt;code&gt;etcd&lt;/code&gt; 中， 可以通过命令 &lt;code&gt;kubectl get rs&lt;/code&gt; 查看。 Deployment 每个版本的配置被保存在它的 ReplicaSet 中，
因此，当一个旧的 ReplicaSet 被删除，就会的会回滚到该版本的能力。 默认会保留 10 个旧的 ReplicaSet， 但是合理的值基于新发 Deployment 的频次和稳定性。&lt;/p&gt;
&lt;p&gt;更准确的说， 如果把该字段值设置为0， 意味着所以副本数为 0 的旧 ReplicaSet 都会被删除， 在这种情况下， 新的 Deployment 就没办法回滚了，因为历史版本都清空了。&lt;/p&gt;
&lt;!--
### Paused

`.spec.paused` is an optional boolean field for pausing and resuming a Deployment. The only difference between
a paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused
Deployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default when
it is created.
 --&gt;
&lt;h3 id=&#34;暂停&#34;&gt;暂停&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.paused&lt;/code&gt; 是一个可选布尔字段，用于暂停和恢复 Deployment。 Deployment 有没有暂停的区别是对 PodTemplateSpec 变更， 暂停的 Deployment
在被恢复之前不会触发更新。 Deployment 创建时默认没有暂停。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: ReplicaSet</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/replicaset/</link>
      <pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/replicaset/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- Kashomon
- bprashanth
- madhusudancs
title: ReplicaSet
content_type: concept
weight: 20
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
A ReplicaSet&#39;s purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often
used to guarantee the availability of a specified number of identical Pods.
 --&gt;
&lt;p&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 的作用是在任意时间内维持一个稳定的 Pod 副本集。因此它经常被用来保证特定 Pod 在指定的数量以提供可用性。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## How a ReplicaSet works

A ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can acquire, a number
of replicas indicating how many Pods it should be maintaining, and a pod template specifying the data of new Pods
it should create to meet the number of replicas criteria. A ReplicaSet then fulfills its purpose by creating
and deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Pod
template.

A ReplicaSet is linked to its Pods via the Pods&#39; [metadata.ownerReferences](/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents)
field, which specifies what resource the current object is owned by. All Pods acquired by a ReplicaSet have their owning
ReplicaSet&#39;s identifying information within their ownerReferences field. It&#39;s through this link that the ReplicaSet
knows of the state of the Pods it is maintaining and plans accordingly.

A ReplicaSet identifies new Pods to acquire by using its selector. If there is a Pod that has no OwnerReference or the
OwnerReference is not a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; and it matches a ReplicaSet&#39;s selector, it will be immediately acquired by said
ReplicaSet.
 --&gt;
&lt;h2 id=&#34;how-a-replicaset-works&#34;&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 是怎么工作的&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 通过以下字段定义，
&lt;code&gt;selector&lt;/code&gt; 标签选择器，决定集合中的 Pod，
&lt;code&gt;replicas&lt;/code&gt; 副本数量， 需要维持多少个副本，
&lt;code&gt;template&lt;/code&gt; Pod 的定义模板，
&lt;code&gt;ReplicaSet&lt;/code&gt; 会创建 Pod 或删除 Pod 让 Pod 的数量达到预期的数量。
&lt;code&gt;ReplicaSet&lt;/code&gt; 在需要创建新的 Pod 时会依照 Pod 的定义模来创建 Pod。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 通过 Pod 上的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents&#34;&gt;metadata.ownerReferences&lt;/a&gt;
字段实现它们的所以关系。 所以属于该 &lt;code&gt;ReplicaSet&lt;/code&gt; 的每一个 Pod 上面都有自己的 ownerReferences 字段来存储它们自己的 &lt;code&gt;ReplicaSet&lt;/code&gt; 信息&lt;/p&gt;
&lt;p&gt;通过这个链接信息 &lt;code&gt;ReplicaSet&lt;/code&gt; 可以知道对应的 Pod 的状态，并根据状态进行相应的维护与计划&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 通过选择器识别新的 Pod。 如果一个 Pod 上没有 &lt;code&gt;OwnerReference&lt;/code&gt; 或 &lt;code&gt;OwnerReference&lt;/code&gt;
不是一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; 然后它又匹配该 &lt;code&gt;ReplicaSet&lt;/code&gt; 的选择器，
那么它马上就会被该 &lt;code&gt;ReplicaSet&lt;/code&gt; 捕获。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## When to use a ReplicaSet

A ReplicaSet ensures that a specified number of pod replicas are running at any given
time. However, a Deployment is a higher-level concept that manages ReplicaSets and
provides declarative updates to Pods along with a lot of other useful features.
Therefore, we recommend using Deployments instead of directly using ReplicaSets, unless
you require custom update orchestration or don&#39;t require updates at all.

This actually means that you may never need to manipulate ReplicaSet objects:
use a Deployment instead, and define your application in the spec section.
 --&gt;
&lt;h2 id=&#34;什么时候应该用-replicaset&#34;&gt;什么时候应该用 &lt;code&gt;ReplicaSet&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 保证在任意时间点上某个 Pod 的拥有指定的副本数。
但是 &lt;code&gt;Deployment&lt;/code&gt; 是一个更高层级的抽象概念，实现对 &lt;code&gt;ReplicaSet&lt;/code&gt; 的管理并提供了对 Pod 的声明式更新及其它一系列有用的特性。
所以，我们建议使用 &lt;code&gt;Deployment&lt;/code&gt; 而不是直接使用 &lt;code&gt;ReplicaSet&lt;/code&gt;
除非用户需要自定义的更新编排甚至干脆不需要更新。&lt;/p&gt;
&lt;p&gt;也就是说一般用户永远都不需要操作 &lt;code&gt;ReplicaSet&lt;/code&gt;： 使用 &lt;code&gt;Deployment&lt;/code&gt; 更佳，只需要在 &lt;code&gt;spec&lt;/code&gt; 区域定义应用就行&lt;/p&gt;
&lt;!--
## Example



 













&lt;table class=&#34;includecode&#34; id=&#34;controllersfrontendyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/frontend.yaml&#34; download=&#34;controllers/frontend.yaml&#34;&gt;
                    &lt;code&gt;controllers/frontend.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersfrontendyaml&#39;)&#34; title=&#34;Copy controllers/frontend.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicaSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;guestbook&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#75715e&#34;&gt;# modify replicas according to your case&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;php-redis&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/gb-frontend:v3-google_samples&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



Saving this manifest into `frontend.yaml` and submitting it to a Kubernetes cluster will
create the defined ReplicaSet and the Pods that it manages.

```shell
kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
```

You can then get the current ReplicaSets deployed:

```shell
kubectl get rs
```

And see the frontend one you created:

```shell
NAME       DESIRED   CURRENT   READY   AGE
frontend   3         3         3       6s
```

You can also check on the state of the ReplicaSet:

```shell
kubectl describe rs/frontend
```

And you will see output similar to:

```shell
Name:         frontend
Namespace:    default
Selector:     tier=frontend
Labels:       app=guestbook
              tier=frontend
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {&#34;apiVersion&#34;:&#34;apps/v1&#34;,&#34;kind&#34;:&#34;ReplicaSet&#34;,&#34;metadata&#34;:{&#34;annotations&#34;:{},&#34;labels&#34;:{&#34;app&#34;:&#34;guestbook&#34;,&#34;tier&#34;:&#34;frontend&#34;},&#34;name&#34;:&#34;frontend&#34;,...
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  tier=frontend
  Containers:
   php-redis:
    Image:        gcr.io/google_samples/gb-frontend:v3
    Port:         &lt;none&gt;
    Host Port:    &lt;none&gt;
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  117s  replicaset-controller  Created pod: frontend-wtsmm
  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-b2zdv
  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-vcmts
```

And lastly you can check for the Pods brought up:

```shell
kubectl get pods
```

You should see Pod information similar to:

```shell
NAME             READY   STATUS    RESTARTS   AGE
frontend-b2zdv   1/1     Running   0          6m36s
frontend-vcmts   1/1     Running   0          6m36s
frontend-wtsmm   1/1     Running   0          6m36s
```

You can also verify that the owner reference of these pods is set to the frontend ReplicaSet.
To do this, get the yaml of one of the Pods running:

```shell
kubectl get pods frontend-b2zdv -o yaml
```

The output will look similar to this, with the frontend ReplicaSet&#39;s info set in the metadata&#39;s ownerReferences field:

```shell
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: &#34;2020-02-12T07:06:16Z&#34;
  generateName: frontend-
  labels:
    tier: frontend
  name: frontend-b2zdv
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: frontend
    uid: f391f6db-bb9b-4c09-ae74-6a1f77f3d5cf
...
```
 --&gt;
&lt;h2 id=&#34;示例&#34;&gt;示例&lt;/h2&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;controllersfrontendyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/frontend.yaml&#34; download=&#34;controllers/frontend.yaml&#34;&gt;
                    &lt;code&gt;controllers/frontend.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersfrontendyaml&#39;)&#34; title=&#34;Copy controllers/frontend.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicaSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;guestbook&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#75715e&#34;&gt;# modify replicas according to your case&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;php-redis&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/gb-frontend:v3-google_samples&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;将以上配置保存到 &lt;code&gt;frontend.yaml&lt;/code&gt;， 然后发布到 k8s 集群，就会在集群中创建一个 ReplicaSet 和对应 Pod。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f frontend.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以通过以下命令查看当前部署的 &lt;code&gt;ReplicaSet&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果中有类似如下如果:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;NAME       DESIRED   CURRENT   READY   AGE
frontend   &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;         &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;         &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;       6s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;也可以通过以下命令查看 &lt;code&gt;ReplicaSet&lt;/code&gt; 状态:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe rs/frontend
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;Name:         frontend
Namespace:    default
Selector:     tier&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;frontend
Labels:       app&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;guestbook
              tier&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;frontend
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;apiVersion&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;apps/v1&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kind&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ReplicaSet&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;metadata&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;annotations&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#f92672&#34;&gt;{}&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;labels&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;app&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;guestbook&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tier&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;frontend&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;frontend&amp;#34;&lt;/span&gt;,...
Replicas:     &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; current / &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; desired
Pods Status:  &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; Running / &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; Waiting / &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; Succeeded / &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; Failed
Pod Template:
  Labels:  tier&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;frontend
  Containers:
   php-redis:
    Image:        gcr.io/google_samples/gb-frontend:v3
    Port:         &amp;lt;none&amp;gt;
    Host Port:    &amp;lt;none&amp;gt;
    Environment:  &amp;lt;none&amp;gt;
    Mounts:       &amp;lt;none&amp;gt;
  Volumes:        &amp;lt;none&amp;gt;
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  117s  replicaset-controller  Created pod: frontend-wtsmm
  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-b2zdv
  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-vcmts
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;最后再用以下命令查看创建的 Pod:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;NAME             READY   STATUS    RESTARTS   AGE
frontend-b2zdv   1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          6m36s
frontend-vcmts   1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          6m36s
frontend-wtsmm   1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          6m36s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;通过以正下命令输出的 yaml 可以验证这些 Pod 所属 &lt;code&gt;ReplicaSet&lt;/code&gt;：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods frontend-b2zdv -o yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下，可以看到 &lt;code&gt;metadata.ownerReferences&lt;/code&gt; 字段中就是这个叫 &lt;code&gt;frontend&lt;/code&gt; 的 &lt;code&gt;ReplicaSet&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2020-02-12T07:06:16Z&amp;#34;&lt;/span&gt;
  generateName: frontend-
  labels:
    tier: frontend
  name: frontend-b2zdv
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: frontend
    uid: f391f6db-bb9b-4c09-ae74-6a1f77f3d5cf
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Non-Template Pod acquisitions

While you can create bare Pods with no problems, it is strongly recommended to make sure that the bare Pods do not have
labels which match the selector of one of your ReplicaSets. The reason for this is because a ReplicaSet is not limited
to owning Pods specified by its template-- it can acquire other Pods in the manner specified in the previous sections.

Take the previous frontend ReplicaSet example, and the Pods specified in the  following manifest:



 













&lt;table class=&#34;includecode&#34; id=&#34;podspod-rsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/pod-rs.yaml&#34; download=&#34;pods/pod-rs.yaml&#34;&gt;
                    &lt;code&gt;pods/pod-rs.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podspod-rsyaml&#39;)&#34; title=&#34;Copy pods/pod-rs.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hello1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/hello-app:2.0-google-samples&lt;/span&gt;

---

&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod2&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hello2&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/hello-app:1.0-google-samples&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



As those Pods do not have a Controller (or any object) as their owner reference and match the selector of the frontend
ReplicaSet, they will immediately be acquired by it.

Suppose you create the Pods after the frontend ReplicaSet has been deployed and has set up its initial Pod replicas to
fulfill its replica count requirement:

```shell
kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
```

The new Pods will be acquired by the ReplicaSet, and then immediately terminated as the ReplicaSet would be over
its desired count.

Fetching the Pods:

```shell
kubectl get pods
```

The output shows that the new Pods are either already terminated, or in the process of being terminated:

```shell
NAME             READY   STATUS        RESTARTS   AGE
frontend-b2zdv   1/1     Running       0          10m
frontend-vcmts   1/1     Running       0          10m
frontend-wtsmm   1/1     Running       0          10m
pod1             0/1     Terminating   0          1s
pod2             0/1     Terminating   0          1s
```

If you create the Pods first:

```shell
kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
```

And then create the ReplicaSet however:

```shell
kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
```

You shall see that the ReplicaSet has acquired the Pods and has only created new ones according to its spec until the
number of its new Pods and the original matches its desired count. As fetching the Pods:

```shell
kubectl get pods
```

Will reveal in its output:
```shell
NAME             READY   STATUS    RESTARTS   AGE
frontend-hmmj2   1/1     Running   0          9s
pod1             1/1     Running   0          36s
pod2             1/1     Running   0          36s
```

In this manner, a ReplicaSet can own a non-homogenous set of Pods
 --&gt;
&lt;h2 id=&#34;不是自己模板定义的-pod-的捕获&#34;&gt;不是自己模板定义的 Pod 的捕获&lt;/h2&gt;
&lt;p&gt;用户可以自由的创建裸 Pod， 但是推荐在创建裸 Pod 时不要在上面打上已经存在的 &lt;code&gt;ReplicaSet&lt;/code&gt; 选择器对应的标签。
不这么做的原因是 &lt;code&gt;ReplicaSet&lt;/code&gt; 并不仅限于自身模板创建的 Pod， 也可以捕获其它拥有对应标签的 Pod(上一节介绍过)&lt;/p&gt;
&lt;p&gt;继续上一个例子中的 &lt;code&gt;frontend&lt;/code&gt; &lt;code&gt;ReplicaSet&lt;/code&gt;， 再加下如下定义的 Pod&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;podspod-rsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/pod-rs.yaml&#34; download=&#34;pods/pod-rs.yaml&#34;&gt;
                    &lt;code&gt;pods/pod-rs.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podspod-rsyaml&#39;)&#34; title=&#34;Copy pods/pod-rs.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hello1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/hello-app:2.0-google-samples&lt;/span&gt;

---

&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod2&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hello2&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/hello-app:1.0-google-samples&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;这些 Pod 没并有在所属者引用上有控制器(或其它对象)， 但是它上面的标签又被 &lt;code&gt;frontend&lt;/code&gt; &lt;code&gt;ReplicaSet&lt;/code&gt;
的选择器所匹配，它们马上就会被这个 &lt;code&gt;ReplicaSet&lt;/code&gt; 捕获。&lt;/p&gt;
&lt;p&gt;假设 Pod 的创建时间是在 &lt;code&gt;frontend&lt;/code&gt; &lt;code&gt;ReplicaSet&lt;/code&gt; 部署之后，并且其创建的 Pod 已经达到了所期望的数量：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f pod-rs.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;新创建的 Pod 会被 &lt;code&gt;ReplicaSet&lt;/code&gt; 捕获，然后发现 &lt;code&gt;ReplicaSet&lt;/code&gt; 副本数已经超过预期数量，所以就会终止这些多余的 Pod&lt;/p&gt;
&lt;p&gt;查看 Pod 状态:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出类似如下，会发现新创建的 Pod 正在被干掉或已经被干掉:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;NAME             READY   STATUS        RESTARTS   AGE
frontend-b2zdv   1/1     Running       &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          10m
frontend-vcmts   1/1     Running       &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          10m
frontend-wtsmm   1/1     Running       &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          10m
pod1             0/1     Terminating   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          1s
pod2             0/1     Terminating   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          1s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果创建的顺序调换一下，先创建这些 Pod:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f pod-rs.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;然后再创建 &lt;code&gt;ReplicaSet&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f frontend.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这次就会发现 &lt;code&gt;ReplicaSet&lt;/code&gt; 会捕获已经创建的两个 Pod，然后根据预期的状态中需要3个，所以就再创建一个新的，就完工了。
再次查看 Pod 状态&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果就会变得不一样，具体如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;NAME             READY   STATUS    RESTARTS   AGE
frontend-hmmj2   1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          9s
pod1             1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          36s
pod2             1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          36s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;也就是， &lt;code&gt;ReplicaSet&lt;/code&gt; 下属的 Pod 可以是不是通过本身模板创建的 Pod&lt;/p&gt;
&lt;!--  
## Writing a ReplicaSet manifest

As with all other Kubernetes API objects, a ReplicaSet needs the `apiVersion`, `kind`, and `metadata` fields.
For ReplicaSets, the kind is always just ReplicaSet.
In Kubernetes 1.9 the API version `apps/v1` on the ReplicaSet kind is the current version and is enabled by default. The API version `apps/v1beta2` is deprecated.
Refer to the first lines of the `frontend.yaml` example for guidance.

The name of a ReplicaSet object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

A ReplicaSet also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).
--&gt;
&lt;h2 id=&#34;编写-replicaset-定义清单&#34;&gt;编写 &lt;code&gt;ReplicaSet&lt;/code&gt; 定义清单&lt;/h2&gt;
&lt;p&gt;与其它所以其它 k8s API 对象一样， &lt;code&gt;ReplicaSet&lt;/code&gt; 需要有 &lt;code&gt;apiVersion&lt;/code&gt;, &lt;code&gt;kind&lt;/code&gt;, &lt;code&gt;metadata&lt;/code&gt; 字段.
对于 &lt;code&gt;ReplicaSet&lt;/code&gt;， &lt;code&gt;kind&lt;/code&gt; 字段就一直是 &lt;code&gt;ReplicaSet&lt;/code&gt;。
在 k8s v1.9+ &lt;code&gt;apiVersion&lt;/code&gt; 字段的值为 &lt;code&gt;apps/v1&lt;/code&gt;，默认开启。 API 版本 &lt;code&gt;apps/v1beta2&lt;/code&gt; 被废弃。
具体可以看之前示例中 &lt;code&gt;frontend.yaml&lt;/code&gt; 文件内容。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 名称必须是一个有效的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.
&lt;code&gt;ReplicaSet&lt;/code&gt; 还必须要有一个
&lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;&lt;code&gt;.spec&lt;/code&gt; 字段&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### Pod Template

The `.spec.template` is a [pod template](/docs/concepts/workloads/Pods/pod-overview/#pod-templates) which is also
required to have labels in place. In our `frontend.yaml` example we had one label: `tier: frontend`.
Be careful not to overlap with the selectors of other controllers, lest they try to adopt this Pod.

For the template&#39;s [restart policy](/docs/concepts/workloads/Pods/pod-lifecycle/#restart-policy) field,
`.spec.template.spec.restartPolicy`, the only allowed value is `Always`, which is the default.
 --&gt;
&lt;h3 id=&#34;pod-模板&#34;&gt;Pod 模板&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.template&lt;/code&gt; 字段就是 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/Pods/pod-overview/#pod-templates&#34;&gt;pod template&lt;/a&gt; 而且其中还必须要定义标签.
在示例中的 &lt;code&gt;frontend.yaml&lt;/code&gt; 打的标签是: &lt;code&gt;tier: frontend&lt;/code&gt;.
注意不能与其它控制器的选择器相重叠，以免它们会争抢这个 Pod&lt;/p&gt;
&lt;p&gt;模板中的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/Pods/pod-lifecycle/#restart-policy&#34;&gt;重启策略&lt;/a&gt; 字段,
&lt;code&gt;.spec.template.spec.restartPolicy&lt;/code&gt;, 允许的值只能是 &lt;code&gt;Always&lt;/code&gt;, 默认也是  &lt;code&gt;Always&lt;/code&gt;.&lt;/p&gt;
&lt;!--
### Pod Selector

The `.spec.selector` field is a [label selector](/docs/concepts/overview/working-with-objects/labels/). As discussed
[earlier](#how-a-replicaset-works) these are the labels used to identify potential Pods to acquire. In our
`frontend.yaml` example, the selector was:
```shell
matchLabels:
	tier: frontend
```

In the ReplicaSet, `.spec.template.metadata.labels` must match `spec.selector`, or it will
be rejected by the API.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; For 2 ReplicaSets specifying the same &lt;code&gt;.spec.selector&lt;/code&gt; but different &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt; and &lt;code&gt;.spec.template.spec&lt;/code&gt; fields, each ReplicaSet ignores the Pods created by the other ReplicaSet.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;pod-选择器&#34;&gt;Pod 选择器&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.selector&lt;/code&gt; 字段是一个 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/&#34;&gt;标签选择器&lt;/a&gt;
&lt;a href=&#34;#how-a-replicaset-works&#34;&gt;之前&lt;/a&gt;讨论过，这些标签用于识别潜在的捕获对象。 在之前示例中 &lt;code&gt;frontend.yaml&lt;/code&gt;
定义的选择器如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在 &lt;code&gt;ReplicaSet&lt;/code&gt; 中， &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt; 必须要与 &lt;code&gt;spec.selector&lt;/code&gt; 匹配，否则会被 API 拒绝。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果两个 &lt;code&gt;ReplicaSet&lt;/code&gt; 拥有相同的 &lt;code&gt;.spec.selector&lt;/code&gt;， 但 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt; 和 &lt;code&gt;.spec.template.spec&lt;/code&gt; 字段不同
则相互之间会忽略对方创建的 Pod&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;HOW？&lt;/p&gt;
&lt;!--
### Replicas

You can specify how many Pods should run concurrently by setting `.spec.replicas`. The ReplicaSet will create/delete
its Pods to match this number.

If you do not specify `.spec.replicas`, then it defaults to 1.
 --&gt;
&lt;h3 id=&#34;副本数&#34;&gt;副本数&lt;/h3&gt;
&lt;p&gt;用户可以通过 &lt;code&gt;.spec.replicas&lt;/code&gt; 设置需要同时运行 Pod 的数量。 &lt;code&gt;ReplicaSet&lt;/code&gt; 会创建/删除 它管理的 Pod 以达成该数量。
如果在配置中没有 &lt;code&gt;.spec.replicas&lt;/code&gt;， 则默认为 1.&lt;/p&gt;
&lt;!--
## Working with ReplicaSets

### Deleting a ReplicaSet and its Pods

To delete a ReplicaSet and all of its Pods, use [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete). The [Garbage collector](/docs/concepts/workloads/controllers/garbage-collection/) automatically deletes all of the dependent Pods by default.

When using the REST API or the `client-go` library, you must set `propagationPolicy` to `Background` or `Foreground` in
the -d option.
For example:
```shell
kubectl proxy --port=8080
curl -X DELETE  &#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend&#39; \
&gt; -d &#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Foreground&#34;}&#39; \
&gt; -H &#34;Content-Type: application/json&#34;
```
 --&gt;
&lt;h2 id=&#34;replicaset-的使用&#34;&gt;ReplicaSet 的使用&lt;/h2&gt;
&lt;h3 id=&#34;删除一个-replicaset-及其-pod&#34;&gt;删除一个 &lt;code&gt;ReplicaSet&lt;/code&gt; 及其 Pod&lt;/h3&gt;
&lt;p&gt;要删除一个 &lt;code&gt;ReplicaSet&lt;/code&gt; 及其所属的全部 Pod， 可以使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubectl/kubectl-commands#delete&#34;&gt;&lt;code&gt;kubectl delete&lt;/code&gt;&lt;/a&gt;
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/garbage-collection/&#34;&gt;垃圾回收&lt;/a&gt; 默认会自动删除它所属的全部 Pod。&lt;/p&gt;
&lt;p&gt;当使用  &lt;code&gt;REST API&lt;/code&gt; 或 &lt;code&gt;client-go&lt;/code&gt; 库， 必须将 &lt;code&gt;propagationPolicy&lt;/code&gt; 设置为 &lt;code&gt;Background&lt;/code&gt; 或 &lt;code&gt;Foreground&lt;/code&gt;
例如:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl proxy --port&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
curl -X DELETE  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;&amp;gt; -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#34;kind&amp;#34;:&amp;#34;DeleteOptions&amp;#34;,&amp;#34;apiVersion&amp;#34;:&amp;#34;v1&amp;#34;,&amp;#34;propagationPolicy&amp;#34;:&amp;#34;Foreground&amp;#34;}&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;&amp;gt; -H &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Content-Type: application/json&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Deleting just a ReplicaSet

You can delete a ReplicaSet without affecting any of its Pods using [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete) with the `--cascade=false` option.
When using the REST API or the `client-go` library, you must set `propagationPolicy` to `Orphan`.
For example:
```shell
kubectl proxy --port=8080
curl -X DELETE  &#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend&#39; \
&gt; -d &#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Orphan&#34;}&#39; \
&gt; -H &#34;Content-Type: application/json&#34;
```

Once the original is deleted, you can create a new ReplicaSet to replace it.  As long
as the old and new `.spec.selector` are the same, then the new one will adopt the old Pods.
However, it will not make any effort to make existing Pods match a new, different pod template.
To update Pods to a new spec in a controlled way, use a
[Deployment](/docs/concepts/workloads/controllers/deployment/#creating-a-deployment), as ReplicaSets do not support a rolling update directly.
 --&gt;
&lt;h3 id=&#34;仅删除-replicaset&#34;&gt;仅删除 &lt;code&gt;ReplicaSet&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;用户可以通过 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubectl/kubectl-commands#delete&#34;&gt;&lt;code&gt;kubectl delete&lt;/code&gt;&lt;/a&gt;
加 &lt;code&gt;--cascade=false&lt;/code&gt; 选项，实现仅删除 &lt;code&gt;ReplicaSet&lt;/code&gt; 对象，而不删除其所属的 Pod。
当使用  &lt;code&gt;REST API&lt;/code&gt; 或 &lt;code&gt;client-go&lt;/code&gt; 库， 必须将 &lt;code&gt;propagationPolicy&lt;/code&gt; 设置为 &lt;code&gt;Orphan&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;例如:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl proxy --port&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
curl -X DELETE  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;&amp;gt; -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#34;kind&amp;#34;:&amp;#34;DeleteOptions&amp;#34;,&amp;#34;apiVersion&amp;#34;:&amp;#34;v1&amp;#34;,&amp;#34;propagationPolicy&amp;#34;:&amp;#34;Orphan&amp;#34;}&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;&amp;gt; -H &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Content-Type: application/json&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;当原来的 &lt;code&gt;ReplicaSet&lt;/code&gt; 就可以创建一个新的来替代它。 新创建的 ReplicaSet 的 &lt;code&gt;.spec.selector&lt;/code&gt; 需要与原来的一样， 这样它就能接管这些 Pod。
但是它不会让旧的 Pod 使用新的模板。
而想要以控制器方式更新 Pod 的模板，需要使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/#creating-a-deployment&#34;&gt;Deployment&lt;/a&gt;
因为 &lt;code&gt;ReplicaSet&lt;/code&gt; 不支持直接的滚动更新。&lt;/p&gt;
&lt;!--
### Isolating Pods from a ReplicaSet

You can remove Pods from a ReplicaSet by changing their labels. This technique may be used to remove Pods
from service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (
assuming that the number of replicas is not also changed).
 --&gt;
&lt;h3 id=&#34;将-pod-从-replicaset-剥离出来&#34;&gt;将 Pod 从 &lt;code&gt;ReplicaSet&lt;/code&gt; 剥离出来&lt;/h3&gt;
&lt;p&gt;用户可以通过修改标签的方式将 Pod 从 &lt;code&gt;ReplicaSet&lt;/code&gt; 中移出， 也可以通过这种方式将 Pod 从其它的服务中移出后用作调度，数据恢复等。
用这种方式移出的 Pod 会自动被替代(如果副本数没有同步修改)&lt;/p&gt;
&lt;!--
### Scaling a ReplicaSet

A ReplicaSet can be easily scaled up or down by simply updating the `.spec.replicas` field. The ReplicaSet controller
ensures that a desired number of Pods with a matching label selector are available and operational.
 --&gt;
&lt;h3 id=&#34;replicaset-容量伸缩&#34;&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 容量伸缩&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 可以通过修改 &lt;code&gt;.spec.replicas&lt;/code&gt; 字段来实现容量的扩大或缩小。 &lt;code&gt;ReplicaSet&lt;/code&gt; 控制器会保证 所属的 Pod 的数量与预期的相同&lt;/p&gt;
&lt;!--
### ReplicaSet as a Horizontal Pod Autoscaler Target

A ReplicaSet can also be a target for
[Horizontal Pod Autoscalers (HPA)](/docs/tasks/run-application/horizontal-pod-autoscale/). That is,
a ReplicaSet can be auto-scaled by an HPA. Here is an example HPA targeting
the ReplicaSet we created in the previous example.



 













&lt;table class=&#34;includecode&#34; id=&#34;controllershpa-rsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/hpa-rs.yaml&#34; download=&#34;controllers/hpa-rs.yaml&#34;&gt;
                    &lt;code&gt;controllers/hpa-rs.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllershpa-rsyaml&#39;)&#34; title=&#34;Copy controllers/hpa-rs.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;autoscaling/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;HorizontalPodAutoscaler&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend-scaler&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;scaleTargetRef&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicaSet&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;minReplicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;maxReplicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;targetCPUUtilizationPercentage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



Saving this manifest into `hpa-rs.yaml` and submitting it to a Kubernetes cluster should
create the defined HPA that autoscales the target ReplicaSet depending on the CPU usage
of the replicated Pods.

```shell
kubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml
```

Alternatively, you can use the `kubectl autoscale` command to accomplish the same
(and it&#39;s easier!)

```shell
kubectl autoscale rs frontend --max=10 --min=3 --cpu-percent=50
```
 --&gt;
&lt;h3 id=&#34;replicaset-作为-pod-水平自动扩展目标&#34;&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 作为 Pod 水平自动扩展目标&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 也可作为 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/horizontal-pod-autoscale/&#34;&gt;Horizontal Pod Autoscalers (HPA)&lt;/a&gt;
的目标。 也就是 &lt;code&gt;ReplicaSet&lt;/code&gt; 可以通过一个 HPA 来自动伸缩。 以下为一个 &lt;code&gt;ReplicaSet&lt;/code&gt; 为目标的 HPA 的示例:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;controllershpa-rsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/hpa-rs.yaml&#34; download=&#34;controllers/hpa-rs.yaml&#34;&gt;
                    &lt;code&gt;controllers/hpa-rs.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllershpa-rsyaml&#39;)&#34; title=&#34;Copy controllers/hpa-rs.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;autoscaling/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;HorizontalPodAutoscaler&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend-scaler&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;scaleTargetRef&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicaSet&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;minReplicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;maxReplicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;targetCPUUtilizationPercentage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;将配置文件内容存入 &lt;code&gt;hpa-rs.yaml&lt;/code&gt; 并提交到 k8s 集群， 会创建一个 HPA，它会根据所属 Pod CPU使用自动伸缩目标 &lt;code&gt;ReplicaSet&lt;/code&gt; 的容量&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f hpa-rs.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;相应的，也可能通过 &lt;code&gt;kubectl autoscale&lt;/code&gt; 达到相同的效果(更容易)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl autoscale rs frontend --max&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt; --min&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; --cpu-percent&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Alternatives to ReplicaSet

### Deployment (recommended)

[`Deployment`](/docs/concepts/workloads/controllers/deployment/) is an object which can own ReplicaSets and update
them and their Pods via declarative, server-side rolling updates.
While ReplicaSets can be used independently, today they&#39;re  mainly used by Deployments as a mechanism to orchestrate Pod
creation, deletion and updates. When you use Deployments you don’t have to worry about managing the ReplicaSets that
they create. Deployments own and manage their ReplicaSets.
As such, it is recommended to use Deployments when you want ReplicaSets.
 --&gt;
&lt;h2 id=&#34;replicaset-替代方案&#34;&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 替代方案&lt;/h2&gt;
&lt;h3 id=&#34;deployment-推荐&#34;&gt;Deployment (推荐)&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#34;&gt;&lt;code&gt;Deployment&lt;/code&gt;&lt;/a&gt; 对象可以包含 &lt;code&gt;ReplicaSet&lt;/code&gt; 并且可以通过声明方式更新自身及所属的 Pod，
服务端滚动更新。 虽然 ReplicaSet 可以独立使用，但是现在主要是使用 Deployment 作为组织 Pod 创建，删除，更新的方式。
当用户使用 &lt;code&gt;Deployment&lt;/code&gt; 不需要关心它所创建的 &lt;code&gt;ReplicaSet&lt;/code&gt;， &lt;code&gt;Deployment&lt;/code&gt; 会管理所属的 &lt;code&gt;ReplicaSet&lt;/code&gt;
因此用户在想要使用 &lt;code&gt;ReplicaSet&lt;/code&gt; 时推荐使用 &lt;code&gt;Deployment&lt;/code&gt; 代替。&lt;/p&gt;
&lt;!--
### Bare Pods

Unlike the case where a user directly created Pods, a ReplicaSet replaces Pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicaSet even if your application requires only a single Pod. Think of it similarly to a process supervisor, only it supervises multiple Pods across multiple nodes instead of individual processes on a single node. A ReplicaSet delegates local container restarts to some agent on the node (for example, Kubelet or Docker).
 --&gt;
&lt;h3 id=&#34;让-pod-裸奔&#34;&gt;让 Pod 裸奔&lt;/h3&gt;
&lt;p&gt;与用户直接创建 Pod 不同， &lt;code&gt;ReplicaSet&lt;/code&gt; 在 Pod 因为某些原因被删除或终止时会创建替代的 Pod， 这些原因可能是 节点挂了，节点因维护如升级内格而引起的计划内故障。
对于这些原因，我们建议用户使用 &lt;code&gt;ReplicaSet&lt;/code&gt; 即便这个应用只需要一个 Pod。 可以把它认为是一个进程监控， 只是它可以监控多个节点上的多个 Pod，而不是一个节点上的一个进程。
&lt;code&gt;ReplicaSet&lt;/code&gt; 会委托节点上的代理(如 kubelet 或 docker)来实现对本地容器的重启。&lt;/p&gt;
&lt;!--
### Job

Use a [`Job`](/docs/concepts/jobs/run-to-completion-finite-workloads/) instead of a ReplicaSet for Pods that are expected to terminate on their own
(that is, batch jobs).
 --&gt;
&lt;h3 id=&#34;job&#34;&gt;Job&lt;/h3&gt;
&lt;p&gt;在运行那会在有限时间内自己运行结束后自动终止的 Pod(批处理任务)，请使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/jobs/run-to-completion-finite-workloads/&#34;&gt;&lt;code&gt;Job&lt;/code&gt;&lt;/a&gt;,就不要用 &lt;code&gt;ReplicaSet&lt;/code&gt;了&lt;/p&gt;
&lt;!--
### DaemonSet

Use a [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) instead of a ReplicaSet for Pods that provide a
machine-level function, such as machine monitoring or machine logging.  These Pods have a lifetime that is tied
to a machine lifetime: the Pod needs to be running on the machine before other Pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.
 --&gt;
&lt;h3 id=&#34;daemonset&#34;&gt;DaemonSet&lt;/h3&gt;
&lt;p&gt;当 Pod 需要使用到机器级别的功能，如机器监控或机器日志时，使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/daemonset/&#34;&gt;&lt;code&gt;DaemonSet&lt;/code&gt;&lt;/a&gt;
因为这些 Pod 的生存期会与对应机器的生存期绑定在一起: 这些 Pod 需要在机器上其它 Pod 运行之前就在节点上运行， 只有在机器准备重启或关机时才能
安全的终止。&lt;/p&gt;
&lt;!--
### ReplicationController

ReplicaSets are the successors to [_ReplicationControllers_](/docs/concepts/workloads/controllers/replicationcontroller/).
The two serve the same purpose, and behave similarly, except that a ReplicationController does not support set-based
selector requirements as described in the [labels user guide](/docs/concepts/overview/working-with-objects/labels/#label-selectors).
As such, ReplicaSets are preferred over ReplicationControllers
 --&gt;
&lt;h3 id=&#34;replicationcontroller&#34;&gt;ReplicationController&lt;/h3&gt;
&lt;p&gt;ReplicaSet 是  &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicationcontroller/&#34;&gt;&lt;em&gt;ReplicationControllers&lt;/em&gt;&lt;/a&gt; 的继任者。
它们两个可以达成相同的目的，而且行为方式也相似， 除了 &lt;code&gt;ReplicationController&lt;/code&gt; 不支持像
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/#label-selectors&#34;&gt;标签使用&lt;/a&gt; 中介绍的基于集合的选择器。
因此相对于 ReplicationControllers 推荐使用 &lt;code&gt;ReplicaSet&lt;/code&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: StatefulSet</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/statefulset/</link>
      <pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/statefulset/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- enisoc
- erictune
- foxish
- janetkuo
- kow3ns
- smarterclayton
title: StatefulSets
content_type: concept
weight: 30
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
StatefulSet is the workload API object used to manage stateful applications.

&lt;p&gt;管理一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 集合的部署与容量伸缩，&lt;em&gt;并提供了顺序的一致性和唯一性&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;与 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#39; target=&#39;_blank&#39;&gt;Deployment&lt;span class=&#39;tooltip-text&#39;&gt;管理集群中的一个多副本应用&lt;/span&gt;
&lt;/a&gt; 类似， StatefulSet 也是基于相同的容器定义来管理 Pod 的。
与 Deployment 不同的是 StatefulSet 维护了其所管理的每个 Pod 的唯一身份。 这些 Pod 是使用现一份定义创建的，但它们之间是不可互换的:
每个 Pod 在所有的重新调度过程中都会被维护使其拥有唯一的持久化标识。&lt;/p&gt;
&lt;p&gt;如果用户需要在工作负载中使用存储卷来提供持久化，可以使用 StatefulSet 作为解决方式的一部分。 尽管 StatefulSet 中每个独立的 Pod 是容易挂掉的，
但是拥有持久化身份标识的 Pod 能够很容易地将已经存在的数据卷与新创建用于替换挂掉的 Pod 重新绑定&lt;/p&gt;
 --&gt;
&lt;p&gt;StatefulSet 是一种用于运行有状态应用的工作负载 API 对象
&lt;p&gt;管理一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 集合的部署与容量伸缩，&lt;em&gt;并提供了顺序的一致性和唯一性&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;与 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#39; target=&#39;_blank&#39;&gt;Deployment&lt;span class=&#39;tooltip-text&#39;&gt;管理集群中的一个多副本应用&lt;/span&gt;
&lt;/a&gt; 类似， StatefulSet 也是基于相同的容器定义来管理 Pod 的。
与 Deployment 不同的是 StatefulSet 维护了其所管理的每个 Pod 的唯一身份。 这些 Pod 是使用现一份定义创建的，但它们之间是不可互换的:
每个 Pod 在所有的重新调度过程中都会被维护使其拥有唯一的持久化标识。&lt;/p&gt;
&lt;p&gt;如果用户需要在工作负载中使用存储卷来提供持久化，可以使用 StatefulSet 作为解决方式的一部分。 尽管 StatefulSet 中每个独立的 Pod 是容易挂掉的，
但是拥有持久化身份标识的 Pod 能够很容易地将已经存在的数据卷与新创建用于替换挂掉的 Pod 重新绑定&lt;/p&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Using StatefulSets


StatefulSets are valuable for applications that require one or more of the
following.

* Stable, unique network identifiers.
* Stable, persistent storage.
* Ordered, graceful deployment and scaling.
* Ordered, automated rolling updates.

In the above, stable is synonymous with persistence across Pod (re)scheduling.
If an application doesn&#39;t require any stable identifiers or ordered deployment,
deletion, or scaling, you should deploy your application using a workload object
that provides a set of stateless replicas.
[Deployment](/docs/concepts/workloads/controllers/deployment/) or
[ReplicaSet](/docs/concepts/workloads/controllers/replicaset/) may be better suited to your stateless needs.
 --&gt;
&lt;h2 id=&#34;什么时候用-statefulset&#34;&gt;什么时候用 StatefulSet&lt;/h2&gt;
&lt;p&gt;StatefulSet 适用于那些满足以下一个或多个条件的应用.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;需要拥有稳定且唯一的网络标识的&lt;/li&gt;
&lt;li&gt;需要拥有稳定的持久化存储的&lt;/li&gt;
&lt;li&gt;需要按顺序进行部署或进行容量伸缩的&lt;/li&gt;
&lt;li&gt;需要按顺序进行自动滚动更新的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在上面的条件中，稳定是指在 Pod 调度(或重新调度)后依然不变(功能上).
如果一个应用不需要任何稳定的标识或顺序的部署，删除，伸缩容量，用户应该使用那些无状态的工作负载对象。
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#34;&gt;Deployment&lt;/a&gt; 或
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#34;&gt;ReplicaSet&lt;/a&gt; 可能就更适合用于无状态的需求。&lt;/p&gt;
&lt;!--
## Limitations

* The storage for a given Pod must either be provisioned by a [PersistentVolume Provisioner](https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md) based on the requested `storage class`, or pre-provisioned by an admin.
* Deleting and/or scaling a StatefulSet down will *not* delete the volumes associated with the StatefulSet. This is done to ensure data safety, which is generally more valuable than an automatic purge of all related StatefulSet resources.
* StatefulSets currently require a [Headless Service](/docs/concepts/services-networking/service/#headless-services) to be responsible for the network identity of the Pods. You are responsible for creating this Service.
* StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is possible to scale the StatefulSet down to 0 prior to deletion.
* When using [Rolling Updates](#rolling-updates) with the default
  [Pod Management Policy](#pod-management-policies) (`OrderedReady`),
  it&#39;s possible to get into a broken state that requires
  [manual intervention to repair](#forced-rollback).
--&gt;
&lt;h2 id=&#34;limitations&#34;&gt;限制&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;给予某个指定 Pod 的存储必须要由 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md&#34;&gt;PersistentVolume Provisioner&lt;/a&gt; 基于 &lt;code&gt;storage class&lt;/code&gt; 的请求或由管理员预先创建好。&lt;/li&gt;
&lt;li&gt;删除或收缩 StatefulSet 的副本数 &lt;em&gt;不会&lt;/em&gt; 删除与这个 StatefulSet 相关联的卷。 这么做是为了保证数据这险，相对与自动删除所有 StatefulSet 相关资源，这样保留下来通常更有意义。&lt;/li&gt;
&lt;li&gt;StatefulSet 目前还需要一个 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/#headless-services&#34;&gt;Headless Service&lt;/a&gt;  来作为这些 Pod 的网络标识。
需要用户负责创建。&lt;/li&gt;
&lt;li&gt;当删除一个 StatefulSet 时，并不能对 Pod 的终结提供任何保障。 而要达成有序和平滑的终止，将 StatefulSet 的副本设置为 0 比直接删除更有保障。&lt;/li&gt;
&lt;li&gt;在使用基于 &lt;a href=&#34;#pod-management-policies&#34;&gt;Pod 管理策略&lt;/a&gt; (&lt;code&gt;OrderedReady&lt;/code&gt;) 的 &lt;a href=&#34;#rolling-updates&#34;&gt;Rolling Updates&lt;/a&gt;时，
可能会出错且需要 &lt;a href=&#34;#forced-rollback&#34;&gt;人工介入进行修复&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Components

The example below demonstrates the components of a StatefulSet.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: &#34;nginx&#34;
  replicas: 3 # by default is 1
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ &#34;ReadWriteOnce&#34; ]
      storageClassName: &#34;my-storage-class&#34;
      resources:
        requests:
          storage: 1Gi
```

In the above example:

* A Headless Service, named `nginx`, is used to control the network domain.
* The StatefulSet, named `web`, has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.
* The `volumeClaimTemplates` will provide stable storage using [PersistentVolumes](/docs/concepts/storage/persistent-volumes/) provisioned by a PersistentVolume Provisioner.

The name of a StatefulSet object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
 --&gt;
&lt;h2 id=&#34;组件&#34;&gt;组件&lt;/h2&gt;
&lt;p&gt;以下示例演示了 StatefulSet 的组件。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;web&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;clusterIP&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;None&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StatefulSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;web&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx # 与要匹配 .spec.template.metadata.labels&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;serviceName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nginx&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# 默认为 1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx # 与要匹配 .spec.selector.matchLabels&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;terminationGracePeriodSeconds&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/nginx-slim:0.8&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;web&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/usr/share/nginx/html&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeClaimTemplates&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ReadWriteOnce&amp;#34;&lt;/span&gt; ]
      &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;my-storage-class&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# 需要替换成实际的&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1Gi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在上面的例子中:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个 Headless Service， 名称为 &lt;code&gt;nginx&lt;/code&gt;， 用来控制网络域&lt;/li&gt;
&lt;li&gt;StatefulSet， 名称为 &lt;code&gt;web&lt;/code&gt;， 在定义中指定有 3 个副本的 nginx 的容器启动在不同的 Pod 中&lt;/li&gt;
&lt;li&gt;&lt;code&gt;volumeClaimTemplates&lt;/code&gt; 通过 PersistentVolume 提供者提供的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/storage/persistent-volumes/&#34;&gt;PersistentVolumes&lt;/a&gt;
提供稳定存储&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;StatefulSet 的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Pod Selector

You must set the `.spec.selector` field of a StatefulSet to match the labels of its `.spec.template.metadata.labels`. Prior to Kubernetes 1.8, the `.spec.selector` field was defaulted when omitted. In 1.8 and later versions, failing to specify a matching Pod Selector will result in a validation error during StatefulSet creation.
 --&gt;
&lt;h2 id=&#34;pod-选择器&#34;&gt;Pod 选择器&lt;/h2&gt;
&lt;p&gt;用户必须让 StatefulSet 的 &lt;code&gt;.spec.selector&lt;/code&gt; 字段与其 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt; 配置的标签相匹配。
在 k8s &lt;code&gt;1.8&lt;/code&gt; 之前  &lt;code&gt;.spec.selector&lt;/code&gt; 不设置会被添加默认。 &lt;code&gt;1.8&lt;/code&gt; 及之后的版本，如果标签与选择器不匹配，StatefulSet 在创建时就会验证不过。&lt;/p&gt;
&lt;!--
## Pod Identity

StatefulSet Pods have a unique identity that is comprised of an ordinal, a
stable network identity, and stable storage. The identity sticks to the Pod,
regardless of which node it&#39;s (re)scheduled on.
 --&gt;
&lt;h2 id=&#34;pod-标识&#34;&gt;Pod 标识&lt;/h2&gt;
&lt;p&gt;StatefulSet 的 Pod 都有一个顺序的唯一的标识， 一个稳定的网络标识，和稳定的存储。
标识与 Pod 绑定，不受节点之间的调度影响&lt;/p&gt;
&lt;!--
### Ordinal Index

For a StatefulSet with N replicas, each Pod in the StatefulSet will be
assigned an integer ordinal, from 0 up through N-1, that is unique over the Set.
 --&gt;
&lt;h3 id=&#34;有序索引&#34;&gt;有序索引&lt;/h3&gt;
&lt;p&gt;对于一个有 N 个副本的 StatefulSet， StatefulSet 中的每个 Pod 都会被分配一个从 0 到 N - 1 的整数序号，这些序号在这个集合中唯一&lt;/p&gt;
&lt;!--
### Stable Network ID

Each Pod in a StatefulSet derives its hostname from the name of the StatefulSet
and the ordinal of the Pod. The pattern for the constructed hostname
is `$(statefulset name)-$(ordinal)`. The example above will create three Pods
named `web-0,web-1,web-2`.
A StatefulSet can use a [Headless Service](/docs/concepts/services-networking/service/#headless-services)
to control the domain of its Pods. The domain managed by this Service takes the form:
`$(service name).$(namespace).svc.cluster.local`, where &#34;cluster.local&#34; is the
cluster domain.
As each Pod is created, it gets a matching DNS subdomain, taking the form:
`$(podname).$(governing service domain)`, where the governing service is defined
by the `serviceName` field on the StatefulSet.

Depending on how DNS is configured in your cluster, you may not be able to look up the DNS
name for a newly-run Pod immediately. This behavior can occur when other clients in the
cluster have already sent queries for the hostname of the Pod before it was created.
Negative caching (normal in DNS) means that the results of previous failed lookups are
remembered and reused, even after the Pod is running, for at least a few seconds.

If you need to discover Pods promptly after they are created, you have a few options:

- Query the Kubernetes API directly (for example, using a watch) rather than relying on DNS lookups.
- Decrease the time of caching in your Kubernetes DNS provider (tpyically this means editing the config map for CoreDNS, which currently caches for 30 seconds).


As mentioned in the [limitations](#limitations) section, you are responsible for
creating the [Headless Service](/docs/concepts/services-networking/service/#headless-services)
responsible for the network identity of the pods.

Here are some examples of choices for Cluster Domain, Service name,
StatefulSet name, and how that affects the DNS names for the StatefulSet&#39;s Pods.

Cluster Domain | Service (ns/name) | StatefulSet (ns/name)  | StatefulSet Domain  | Pod DNS | Pod Hostname |
-------------- | ----------------- | ----------------- | -------------- | ------- | ------------ |
 cluster.local | default/nginx     | default/web       | nginx.default.svc.cluster.local | web-{0..N-1}.nginx.default.svc.cluster.local | web-{0..N-1} |
 cluster.local | foo/nginx         | foo/web           | nginx.foo.svc.cluster.local     | web-{0..N-1}.nginx.foo.svc.cluster.local     | web-{0..N-1} |
 kube.local    | foo/nginx         | foo/web           | nginx.foo.svc.kube.local        | web-{0..N-1}.nginx.foo.svc.kube.local        | web-{0..N-1} |

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Cluster Domain will be set to &lt;code&gt;cluster.local&lt;/code&gt; unless
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/dns-pod-service/&#34;&gt;otherwise configured&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;稳定的网络-id&#34;&gt;稳定的网络 ID&lt;/h3&gt;
&lt;p&gt;StatefulSet 中 Pod 的主机名由 StatefulSet 名称和 Pod 的序号组成。 格式为 &lt;code&gt;$(statefulset name)-$(ordinal)&lt;/code&gt;
如果 Pod 数量为 3 则名称依次为 &lt;code&gt;web-0,web-1,web-2&lt;/code&gt;.
StatefulSet 可以使用一个 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/#headless-services&#34;&gt;Headless Service&lt;/a&gt;
来控制其 Pod 的网络域。 Service 的域名格式为 &lt;code&gt;$(service name).$(namespace).svc.cluster.local&lt;/code&gt;
&lt;code&gt;cluster.local&lt;/code&gt; 为集群域。
当每个 Pod 被创建后，都会获得一个相应的 DNS 子域名，格式为 &lt;code&gt;$(podname).$(governing service domain)&lt;/code&gt;
其中 &lt;code&gt;governing service&lt;/code&gt; 就是 StatefulSet 定义中 &lt;code&gt;serviceName&lt;/code&gt; 字段&lt;/p&gt;
&lt;p&gt;基于集群中的 DNS 配置方式， 用户可能不能在 Pod 创建后，马上就能查询到对应的 DNS 名称。 当这个 Pod 在创建之前，
有其它的客户端对该主机名查询就可能出现这种情况。 因为 DNS 服务中会缓存之前失败的的查询记录，即便在 Pod 运行之后
至少几秒种内依然可能会有这种情况出现。&lt;/p&gt;
&lt;p&gt;如果想要在 Pod 创建后就能查询有，有以下几个选择:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;直接查询 k8s API(例如通过 watch), 而不是信赖 DNS 查询&lt;/li&gt;
&lt;li&gt;减少 k8s DNS 提供者(通常是修改 CoreDNS 的配置， 当前配置的缓存时间是 30s)的缓存时间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 &lt;a href=&#34;#limitations&#34;&gt;限制&lt;/a&gt; 一节种提到，需要用户在负责创建用于 Pod 网络标识的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/#headless-services&#34;&gt;Headless Service&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;以下为怎么设置 集群域，Service 命名， StatefulSet 命名，以及这命名对 StatefulSet 的 Pod 的 DNS 记录的影响:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Cluster Domain&lt;/th&gt;
&lt;th&gt;Service (ns/name)&lt;/th&gt;
&lt;th&gt;StatefulSet (ns/name)&lt;/th&gt;
&lt;th&gt;StatefulSet Domain&lt;/th&gt;
&lt;th&gt;Pod DNS&lt;/th&gt;
&lt;th&gt;Pod Hostname&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;cluster.local&lt;/td&gt;
&lt;td&gt;default/nginx&lt;/td&gt;
&lt;td&gt;default/web&lt;/td&gt;
&lt;td&gt;nginx.default.svc.cluster.local&lt;/td&gt;
&lt;td&gt;web-{0..N-1}.nginx.default.svc.cluster.local&lt;/td&gt;
&lt;td&gt;web-{0..N-1}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cluster.local&lt;/td&gt;
&lt;td&gt;foo/nginx&lt;/td&gt;
&lt;td&gt;foo/web&lt;/td&gt;
&lt;td&gt;nginx.foo.svc.cluster.local&lt;/td&gt;
&lt;td&gt;web-{0..N-1}.nginx.foo.svc.cluster.local&lt;/td&gt;
&lt;td&gt;web-{0..N-1}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube.local&lt;/td&gt;
&lt;td&gt;foo/nginx&lt;/td&gt;
&lt;td&gt;foo/web&lt;/td&gt;
&lt;td&gt;nginx.foo.svc.kube.local&lt;/td&gt;
&lt;td&gt;web-{0..N-1}.nginx.foo.svc.kube.local&lt;/td&gt;
&lt;td&gt;web-{0..N-1}&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果没有 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/dns-pod-service/&#34;&gt;其它的配置&lt;/a&gt; 集群域会是 &lt;code&gt;cluster.local&lt;/code&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Stable Storage

Kubernetes creates one [PersistentVolume](/docs/concepts/storage/persistent-volumes/) for each
VolumeClaimTemplate. In the nginx example above, each Pod will receive a single PersistentVolume
with a StorageClass of `my-storage-class` and 1 Gib of provisioned storage. If no StorageClass
is specified, then the default StorageClass will be used. When a Pod is (re)scheduled
onto a node, its `volumeMounts` mount the PersistentVolumes associated with its
PersistentVolume Claims. Note that, the PersistentVolumes associated with the
Pods&#39; PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted.
This must be done manually.
 --&gt;
&lt;h3 id=&#34;稳定的存储&#34;&gt;稳定的存储&lt;/h3&gt;
&lt;p&gt;k8s 会依照 VolumeClaimTemplate 为每个 Pod 创建一个 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/storage/persistent-volumes/&#34;&gt;PersistentVolume&lt;/a&gt;
在上面的示例中， 每个 Pod 都会收到一个 StorageClass 为 &lt;code&gt;my-storage-class&lt;/code&gt; 容量为 1G 的 PersistentVolume。
如果没有配置 StorageClass， 则会使用默认的 StorageClass。 当一个 Pod 被(重新)调度到一个节点时，它的 &lt;code&gt;volumeMounts&lt;/code&gt;
挂载 PersistentVolumeClaim 中对应的 PersistentVolume。 要注意与 PersistentVolumeClaim 关联的 PersistentVolume
在 Pod 或 StatefulSet 删除时不会被删除。想要删除必须要手动删除才行。&lt;/p&gt;
&lt;!--
### Pod Name Label

When the StatefulSet &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; creates a Pod,
it adds a label, `statefulset.kubernetes.io/pod-name`, that is set to the name of
the Pod. This label allows you to attach a Service to a specific Pod in
the StatefulSet.
 --&gt;
&lt;h3 id=&#34;pod-名称的标签&#34;&gt;Pod 名称的标签&lt;/h3&gt;
&lt;p&gt;当 StatefulSet &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; 创建一个 Pod 进会向它塞一个标签
标签的键为 &lt;code&gt;statefulset.kubernetes.io/pod-name&lt;/code&gt; 值为 Pod 的名称。这个标签可以让用户配置
Service 指向 StatefulSet 的特定 Pod。&lt;/p&gt;
&lt;!--
## Deployment and Scaling Guarantees

* For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.
* When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.
* Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.
* Before a Pod is terminated, all of its successors must be completely shutdown.

The StatefulSet should not specify a `pod.Spec.TerminationGracePeriodSeconds` of 0. This practice is unsafe and strongly discouraged. For further explanation, please refer to [force deleting StatefulSet Pods](/docs/tasks/run-application/force-delete-stateful-set-pod/).

When the nginx example above is created, three Pods will be deployed in the order
web-0, web-1, web-2. web-1 will not be deployed before web-0 is
[Running and Ready](/docs/concepts/workloads/pods/pod-lifecycle/), and web-2 will not be deployed until
web-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before
web-2 is launched, web-2 will not be launched until web-0 is successfully relaunched and
becomes Running and Ready.

If a user were to scale the deployed example by patching the StatefulSet such that
`replicas=1`, web-2 would be terminated first. web-1 would not be terminated until web-2
is fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated and
is completely shutdown, but prior to web-1&#39;s termination, web-1 would not be terminated
until web-0 is Running and Ready.
 --&gt;
&lt;h2 id=&#34;deployment-and-scaling-guarantees&#34;&gt;部署和容量伸缩保证&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;对于一个有 N 个副本的 StatefulSet， 当 Pod 被部署时，是以 {0..N-1} 的顺序依次创建的&lt;/li&gt;
&lt;li&gt;当 Pod 被删除除时，则是以 {N-1..0} 与创建相反的顺序终止&lt;/li&gt;
&lt;li&gt;在一个扩容操作之前，它的前辈 Pod 必须是运行和就绪的&lt;/li&gt;
&lt;li&gt;在一个 Pod 终止之前，它的后辈必须全部完全关闭&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;StatefulSet 不应该配置 &lt;code&gt;pod.Spec.TerminationGracePeriodSeconds&lt;/code&gt; 为 0. 这个操作是不安全的，强烈建议不要这样搞。
更多解决请见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/force-delete-stateful-set-pod/&#34;&gt;强制删除 StatefulSet 的 Pod&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在上面的例子中， 三个 Pod 会以 web-0, web-1, web-2 的顺序创建， web-1 不会在 web-0 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/pod-lifecycle/&#34;&gt;运行并就绪&lt;/a&gt;
之前部署。 如果 web-0 在 web-1 运行并就绪后挂了，则 web-2 在 web-0 重新成功启动然后运行并就绪之前不会启动。&lt;/p&gt;
&lt;p&gt;在上面的例子中，三个副本成功运行后，用户设置 &lt;code&gt;replicas=1&lt;/code&gt;。 web-2 会先终止。 web-1 会在 web-2 关闭并删除后
才会开始终止。 如果 web-0 刚好在 web-2 完全关闭，但 web-1 还没终止之前挂了， 则 web-1 会在 web-0 重新运行并就绪后才会终止。&lt;/p&gt;
&lt;!--
### Pod Management Policies
In Kubernetes 1.7 and later, StatefulSet allows you to relax its ordering guarantees while
preserving its uniqueness and identity guarantees via its `.spec.podManagementPolicy` field.

#### OrderedReady Pod Management

`OrderedReady` pod management is the default for StatefulSets. It implements the behavior
described [above](#deployment-and-scaling-guarantees).

#### Parallel Pod Management

`Parallel` pod management tells the StatefulSet controller to launch or
terminate all Pods in parallel, and to not wait for Pods to become Running
and Ready or completely terminated prior to launching or terminating another
Pod. This option only affects the behavior for scaling operations. Updates are not
affected.
 --&gt;
&lt;h3 id=&#34;pod-management-policies&#34;&gt;Pod 管理策略&lt;/h3&gt;
&lt;p&gt;在 k8s 1.7+, StatefulSet 允许使用宽松的序号保证，通过 &lt;code&gt;.spec.podManagementPolicy&lt;/code&gt; 字段来保证唯一性和标识保证。&lt;/p&gt;
&lt;h4 id=&#34;orderedready-pod-管理&#34;&gt;OrderedReady Pod 管理&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;OrderedReady&lt;/code&gt; Pod 管理是 StatefulSet 的默认方式。它实现的行为有 &lt;a href=&#34;#deployment-and-scaling-guarantees&#34;&gt;上面&lt;/a&gt;讲过.&lt;/p&gt;
&lt;h4 id=&#34;parallel-pod-管理&#34;&gt;Parallel Pod 管理&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;Parallel&lt;/code&gt; Pod 管理，这种策略让 StatefulSet 控制器在启动或终止 Pod 并行进行， 不需要在启动的时候等待前辈运行并就绪，
也不需要在终止的时候等待后台完成终止并删除。 这个选项只影响容量伸缩行为。更新行为不受影响&lt;/p&gt;
&lt;!--
## Update Strategies

In Kubernetes 1.7 and later, StatefulSet&#39;s `.spec.updateStrategy` field allows you to configure
and disable automated rolling updates for containers, labels, resource request/limits, and
annotations for the Pods in a StatefulSet.
 --&gt;
&lt;h2 id=&#34;更新策略&#34;&gt;更新策略&lt;/h2&gt;
&lt;p&gt;在 k8s 1.7+, StatefulSet &lt;code&gt;.spec.updateStrategy&lt;/code&gt; 允许用户配置和禁用在对容器，标签，资源需求/限制，和标签的更新触发的自动滚动更新&lt;/p&gt;
&lt;!--
### On Delete

The `OnDelete` update strategy implements the legacy (1.6 and prior) behavior. When a StatefulSet&#39;s
`.spec.updateStrategy.type` is set to `OnDelete`, the StatefulSet controller will not automatically
update the Pods in a StatefulSet. Users must manually delete Pods to cause the controller to
create new Pods that reflect modifications made to a StatefulSet&#39;s `.spec.template`.
 --&gt;
&lt;h3 id=&#34;ondelete&#34;&gt;&lt;code&gt;OnDelete&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;OnDelete&lt;/code&gt; 更新策略实现的是经典(&lt;code&gt;v1.6-&lt;/code&gt;)行为. 当 StatefulSet 的 &lt;code&gt;.spec.updateStrategy.type&lt;/code&gt; 设置为 &lt;code&gt;OnDelete&lt;/code&gt;，
StatefulSet 就不会自动的更新 StatefulSet 中的 Pod。只有用户手动删除 Pod 后 控制器才会以 StatefulSet 中 &lt;code&gt;.spec.template&lt;/code&gt;
创建应用修改后的新 Pod。&lt;/p&gt;
&lt;!--
### Rolling Updates

The `RollingUpdate` update strategy implements automated, rolling update for the Pods in a
StatefulSet. It is the default strategy when `.spec.updateStrategy` is left unspecified. When a StatefulSet&#39;s `.spec.updateStrategy.type` is set to `RollingUpdate`, the
StatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed
in the same order as Pod termination (from the largest ordinal to the smallest), updating
each Pod one at a time. It will wait until an updated Pod is Running and Ready prior to
updating its predecessor.
 --&gt;
&lt;h3 id=&#34;rolling-updates&#34;&gt;&lt;code&gt;RollingUpdate&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;RollingUpdate&lt;/code&gt; 更新策略实现了自动，滚动更新 StatefulSet 中的 Pod。如果 &lt;code&gt;.spec.updateStrategy&lt;/code&gt; 没有指定这，默认使用该策略。
当 StatefulSet 的 &lt;code&gt;.spec.updateStrategy.type&lt;/code&gt; 设置为 &lt;code&gt;RollingUpdate&lt;/code&gt;， StatefulSet 控制器会删除并重建 StatefulSet 中的每一个 Pod。
处理顺序与终止顺序相同(序号从大到小)，每次更新一个 Pod。 会等待上一个更新的 Pod 运行并就绪后，才会进行下一个。&lt;/p&gt;
&lt;!--
#### Partitions

The `RollingUpdate` update strategy can be partitioned, by specifying a
`.spec.updateStrategy.rollingUpdate.partition`. If a partition is specified, all Pods with an
ordinal that is greater than or equal to the partition will be updated when the StatefulSet&#39;s
`.spec.template` is updated. All Pods with an ordinal that is less than the partition will not
be updated, and, even if they are deleted, they will be recreated at the previous version. If a
StatefulSet&#39;s `.spec.updateStrategy.rollingUpdate.partition` is greater than its `.spec.replicas`,
updates to its `.spec.template` will not be propagated to its Pods.
In most cases you will not need to use a partition, but they are useful if you want to stage an
update, roll out a canary, or perform a phased roll out.
 --&gt;
&lt;h4 id=&#34;分区&#34;&gt;分区&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;RollingUpdate&lt;/code&gt; 更新策略可以分区， 通过 &lt;code&gt;.spec.updateStrategy.rollingUpdate.partition&lt;/code&gt;的方式。
如果分区指定， 所有序号大于等于分区的 Pod 会在 StatefulSet &lt;code&gt;.spec.template&lt;/code&gt; 更新时更新。
所有序号小于分区的 Pod 都不更新， 即便被删除，也会以之前的版本重建。 如果一个 StatefulSet 的 &lt;code&gt;.spec.updateStrategy.rollingUpdate.partition&lt;/code&gt;
的值比它的 &lt;code&gt;.spec.replicas&lt;/code&gt; 还大，则所有对  &lt;code&gt;.spec.template&lt;/code&gt; 的修改都不会应用到 Pod 上。
大多数情况下，用户不需要使用到分区。但当用户想要分步更新，或金丝雀发布，或分阶段发布(phased roll out)时都相当有用&lt;/p&gt;
&lt;!--
#### Forced Rollback

When using [Rolling Updates](#rolling-updates) with the default
[Pod Management Policy](#pod-management-policies) (`OrderedReady`),
it&#39;s possible to get into a broken state that requires manual intervention to repair.

If you update the Pod template to a configuration that never becomes Running and
Ready (for example, due to a bad binary or application-level configuration error),
StatefulSet will stop the rollout and wait.

In this state, it&#39;s not enough to revert the Pod template to a good configuration.
Due to a [known issue](https://github.com/kubernetes/kubernetes/issues/67250),
StatefulSet will continue to wait for the broken Pod to become Ready
(which never happens) before it will attempt to revert it back to the working
configuration.

After reverting the template, you must also delete any Pods that StatefulSet had
already attempted to run with the bad configuration.
StatefulSet will then begin to recreate the Pods using the reverted template.
 --&gt;
&lt;h4 id=&#34;forced-rollback&#34;&gt;强制回退&lt;/h4&gt;
&lt;p&gt;当使用 &lt;a href=&#34;#rolling-updates&#34;&gt;滚动更新&lt;/a&gt; 配合 &lt;a href=&#34;#pod-management-policies&#34;&gt;Pod 管理策略&lt;/a&gt; (&lt;code&gt;OrderedReady&lt;/code&gt;) 可能会造成故障，需要人工介入修复。&lt;/p&gt;
&lt;p&gt;如果用户更新的 Pod 模板永远都不会进入运行和就绪状态(例如，因为二进制文件损坏或应用级配置错误)。 StatefulSet 就会停止发布并等待。&lt;/p&gt;
&lt;p&gt;在这种状态下。就不能回退 Pod 模板到正常的配置。 因为一个 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/67250&#34;&gt;已知的问题单&lt;/a&gt;
StatefulSet 在尝试回滚到正常的配置之前会一直等着那个故障的 Pod 变成就绪状态(但永远不会)&lt;/p&gt;
&lt;p&gt;在回退模板后，用户还需要删除所以 StatefulSet 使用错误的配置已经启动的 Pod 。StatefulSet 这时候才会以回退后的模板重新创建 Pod。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tutorials/stateful-application/basic-stateful-set/&#34;&gt;部署一个有状态的应用&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tutorials/stateful-application/cassandra/&#34;&gt;用StatefulSet 部署  Cassandra &lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/run-replicated-stateful-application/&#34;&gt;部署一个多副本的 有状态的应用&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: DaemonSet</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/daemonset/</link>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/daemonset/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- enisoc
- erictune
- foxish
- janetkuo
- kow3ns
title: DaemonSet
content_type: concept
weight: 40
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
A _DaemonSet_ ensures that all (or some) Nodes run a copy of a Pod.  As nodes are added to the
cluster, Pods are added to them.  As nodes are removed from the cluster, those Pods are garbage
collected.  Deleting a DaemonSet will clean up the Pods it created.

Some typical uses of a DaemonSet are:

- running a cluster storage daemon on every node
- running a logs collection daemon on every node
- running a node monitoring daemon on every node

In a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.
A more complex setup might use multiple DaemonSets for a single type of daemon, but with
different flags and/or different memory and cpu requests for different hardware types.
 --&gt;
&lt;p&gt;&lt;em&gt;DaemonSet&lt;/em&gt; 确保所以(或部分)节点上都会运行一个副本的 Pod。 当有节点加入到集群时，这个 Pod 也会自动加到该节点上。
当节点从集群移除时， 这些 Pod 也会被垃圾清理掉。 删除一个 DaemonSet 也会清除其所创建的 Pod。&lt;/p&gt;
&lt;p&gt;DaemonSet 常见使用场景:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在每个节点上运行集群存储守护进程&lt;/li&gt;
&lt;li&gt;在每个节点上运行日志收集守护进程&lt;/li&gt;
&lt;li&gt;在每个节点上运行监控守护进程&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个简单的用法是对于每个类型的守护进程使用一个 DaemonSet 运行在每一个节点。
复杂点的配置就是对一个类型的守护进程使用多个 DaemonSet， 针对不同的硬件类型使用 不同的标志， 内存， CPU 需求的 DaemonSet&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Writing a DaemonSet Spec

### Create a DaemonSet

You can describe a DaemonSet in a YAML file. For example, the `daemonset.yaml` file below describes a DaemonSet that runs the fluentd-elasticsearch Docker image:



 













&lt;table class=&#34;includecode&#34; id=&#34;controllersdaemonsetyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/daemonset.yaml&#34; download=&#34;controllers/daemonset.yaml&#34;&gt;
                    &lt;code&gt;controllers/daemonset.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersdaemonsetyaml&#39;)&#34; title=&#34;Copy controllers/daemonset.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DaemonSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-elasticsearch&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube-system&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;k8s-app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-logging&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-elasticsearch&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-elasticsearch&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:
      &lt;span style=&#34;color:#75715e&#34;&gt;# this toleration is to have the daemonset runnable on master nodes&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# remove it if your masters can&amp;#39;t run pods&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# 该耐受表示在主控节点上运行 DaemonSet,如果用户的主控节点不能运行 Pod，请移除&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;node-role.kubernetes.io/master&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NoSchedule&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-elasticsearch&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# quay.io/fluentd_elasticsearch/fluentd:v2.5.2&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/fluentd:v2.5.2-fluentd_elasticsearch&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;limits&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;memory&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;200Mi&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;cpu&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;100m&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;memory&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;200Mi&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;varlog&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/log&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;varlibdockercontainers&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/lib/docker/containers&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;terminationGracePeriodSeconds&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;varlog&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/log&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;varlibdockercontainers&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/lib/docker/containers&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



Create a DaemonSet based on the YAML file:

```
kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml
```
 --&gt;
&lt;h2 id=&#34;编写-daemonset-配制&#34;&gt;编写 DaemonSet 配制&lt;/h2&gt;
&lt;h3 id=&#34;创建-daemonset&#34;&gt;创建 DaemonSet&lt;/h3&gt;
&lt;p&gt;用户可以通过一个 YAML 文件在描述一个 DaemonSet。 例如， 下面的 &lt;code&gt;daemonset.yaml&lt;/code&gt; 中就描述了一个运行 &lt;code&gt;fluentd-elasticsearch&lt;/code&gt; Docker 镜像的 DaemonSet:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;controllersdaemonsetyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/daemonset.yaml&#34; download=&#34;controllers/daemonset.yaml&#34;&gt;
                    &lt;code&gt;controllers/daemonset.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersdaemonsetyaml&#39;)&#34; title=&#34;Copy controllers/daemonset.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DaemonSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-elasticsearch&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube-system&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;k8s-app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-logging&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-elasticsearch&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-elasticsearch&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:
      &lt;span style=&#34;color:#75715e&#34;&gt;# this toleration is to have the daemonset runnable on master nodes&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# remove it if your masters can&amp;#39;t run pods&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# 该耐受表示在主控节点上运行 DaemonSet,如果用户的主控节点不能运行 Pod，请移除&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;node-role.kubernetes.io/master&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NoSchedule&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-elasticsearch&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# quay.io/fluentd_elasticsearch/fluentd:v2.5.2&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/fluentd:v2.5.2-fluentd_elasticsearch&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;limits&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;memory&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;200Mi&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;cpu&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;100m&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;memory&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;200Mi&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;varlog&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/log&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;varlibdockercontainers&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/lib/docker/containers&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;terminationGracePeriodSeconds&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;varlog&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/log&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;varlibdockercontainers&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/lib/docker/containers&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;基于 YAML 文件创建一个　DaemonSet&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl apply -f daemonset.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;!--
### Required Fields

As with all other Kubernetes config, a DaemonSet needs `apiVersion`, `kind`, and `metadata` fields.  For
general information about working with config files, see
[running stateless applications](/docs/tasks/run-application/run-stateless-application-deployment/),
[configuring containers](/docs/tasks/), and [object management using kubectl](/docs/concepts/overview/working-with-objects/object-management/) documents.

The name of a DaemonSet object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

A DaemonSet also needs a [`.spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status) section.
 --&gt;
&lt;h3 id=&#34;必要字段&#34;&gt;必要字段&lt;/h3&gt;
&lt;p&gt;与其它所有其它的 k8s 配置一样， DaemonSet 必须有 &lt;code&gt;apiVersion&lt;/code&gt;, &lt;code&gt;kind&lt;/code&gt;, &lt;code&gt;metadata&lt;/code&gt; 字段，
关于配置文件的通用信息见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/run-stateless-application-deployment/&#34;&gt;运行无状态应用&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/&#34;&gt;配置容器&lt;/a&gt;, &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/object-management/&#34;&gt;使用 kubectl 管理对象&lt;/a&gt;
DaemonSet 的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;DaemonSet 也是必须要有一个 &lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;&lt;code&gt;.spec&lt;/code&gt;&lt;/a&gt; 配置区.&lt;/p&gt;
&lt;!--
### Pod Template

The `.spec.template` is one of the required fields in `.spec`.

The `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;, except it is nested and does not have an `apiVersion` or `kind`.

In addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriate
labels (see [pod selector](#pod-selector)).

A Pod Template in a DaemonSet must have a [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)
 equal to `Always`, or be unspecified, which defaults to `Always`.
 --&gt;
&lt;h3 id=&#34;pod-模板&#34;&gt;Pod 模板&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.template&lt;/code&gt; 是 &lt;code&gt;.spec&lt;/code&gt; 中的一个必要字段.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;.spec.template&lt;/code&gt; 是一个 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/#pod-templates&#34;&gt;pod 模板&lt;/a&gt;.
除了因为嵌套没有 &lt;code&gt;apiVersion&lt;/code&gt; 或 &lt;code&gt;kind&lt;/code&gt;字段外，与 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;的定义完全相同,&lt;/p&gt;
&lt;p&gt;相较与裸 Pod 增加的字段还有 DaemonSet 需要设置恰当的标签 (见 &lt;a href=&#34;#pod-selector&#34;&gt;Pod 选择器&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;DaemonSet 中的 Pod 模板必须要有 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/pod-lifecycle/#restart-policy&#34;&gt;&lt;code&gt;RestartPolicy&lt;/code&gt;&lt;/a&gt;
且值为 &lt;code&gt;Always&lt;/code&gt; 或 留空，然后默认为 &lt;code&gt;Always&lt;/code&gt;&lt;/p&gt;
&lt;!--
### Pod Selector

The `.spec.selector` field is a pod selector.  It works the same as the `.spec.selector` of
a [Job](/docs/concepts/workloads/controllers/job/).

As of Kubernetes 1.8, you must specify a pod selector that matches the labels of the
`.spec.template`. The pod selector will no longer be defaulted when left empty. Selector
defaulting was not compatible with `kubectl apply`. Also, once a DaemonSet is created,
its `.spec.selector` can not be mutated. Mutating the pod selector can lead to the
unintentional orphaning of Pods, and it was found to be confusing to users.

The `.spec.selector` is an object consisting of two fields:

* `matchLabels` - works the same as the `.spec.selector` of a [ReplicationController](/docs/concepts/workloads/controllers/replicationcontroller/).
* `matchExpressions` - allows to build more sophisticated selectors by specifying key,
  list of values and an operator that relates the key and values.

When the two are specified the result is ANDed.

If the `.spec.selector` is specified, it must match the `.spec.template.metadata.labels`. Config with these not matching will be rejected by the API.

Also you should not normally create any Pods whose labels match this selector, either directly, via
another DaemonSet, or via another workload resource such as ReplicaSet.  Otherwise, the DaemonSet
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; will think that those Pods were created by it.
Kubernetes will not stop you from doing this. One case where you might want to do this is manually
create a Pod with a different value on a node for testing.
 --&gt;
&lt;h3 id=&#34;pod-selector&#34;&gt;Pod 选择器&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.selector&lt;/code&gt; 字段是一个 Pod 选择器.  与 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/job/&#34;&gt;Job&lt;/a&gt; 的 &lt;code&gt;.spec.selector&lt;/code&gt; 是一样的。&lt;/p&gt;
&lt;p&gt;从 k8s 1.8 开始，用户必须要设置一个与 &lt;code&gt;.spec.template&lt;/code&gt; 中标签相匹配的标签选择器。不再是留空会添加默认值。
选择器默认添加的行为与 &lt;code&gt;kubectl apply&lt;/code&gt; 不兼容。 并且，当一个 DaemonSet 创建后，&lt;code&gt;.spec.selector&lt;/code&gt; 字段将不可变。
对标签选择器的修改可能无意间导致产生孤儿 Pod， 这样会让用户费解。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;.spec.selector&lt;/code&gt; 对象由以下两个字段组成:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;matchLabels&lt;/code&gt; - 与 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicationcontroller/&#34;&gt;ReplicationController&lt;/a&gt; 中的 &lt;code&gt;.spec.selector&lt;/code&gt; 作用一样。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;matchExpressions&lt;/code&gt; - 能通过键与对应的值列表，操作符组成更复杂的选择器
如果以上两个字段都有设置，它们之间是逻辑与关系&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果 &lt;code&gt;.spec.selector&lt;/code&gt; 设置了值(对应 1.8 之前的版本), 则必须与 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt;匹配. 如果配置不匹配会被 api-server 拒绝.&lt;/p&gt;
&lt;p&gt;在此之外用户不应该再直接或间接(如其它的 DaemonSet 或如 ReplicaSet 之类的工作负载)创建与该选择器匹配的 Pod，
否则 DaemonSet &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; 会认为这些 Pod 也是它创建的。
k8s 不会阻止用户这么干。 能这么干的和种场景是手动创建与这个不一样配置的 Pod 用于测试&lt;/p&gt;
&lt;!--
### Running Pods on select Nodes

If you specify a `.spec.template.spec.nodeSelector`, then the DaemonSet controller will
create Pods on nodes which match that [node
selector](/docs/concepts/scheduling-eviction/assign-pod-node/). Likewise if you specify a `.spec.template.spec.affinity`,
then DaemonSet controller will create Pods on nodes which match that [node affinity](/docs/concepts/scheduling-eviction/assign-pod-node/).
If you do not specify either, then the DaemonSet controller will create Pods on all nodes.
 --&gt;
&lt;h3 id=&#34;只在选定的节点上运行-pod&#34;&gt;只在选定的节点上运行 Pod&lt;/h3&gt;
&lt;p&gt;如果指定了 &lt;code&gt;.spec.template.spec.nodeSelector&lt;/code&gt;， 则 DaemonSet 控制器只会在那些匹配 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/scheduling-eviction/assign-pod-node/&#34;&gt;节点选择器&lt;/a&gt;
节点上创建 Pod。 类似地，如果指定了 &lt;code&gt;.spec.template.spec.affinity&lt;/code&gt; 控制器只会在那些匹配 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/scheduling-eviction/assign-pod-node/&#34;&gt;node affinity&lt;/a&gt;
节点上创建 Pod。如果一个都没指定，则 DaemonSet 控制器会在所有的节点上创建 Pod。&lt;/p&gt;
&lt;!--
## How Daemon Pods are scheduled

### Scheduled by default scheduler






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [stable]&lt;/code&gt;
&lt;/div&gt;



A DaemonSet ensures that all eligible nodes run a copy of a Pod. Normally, the
node that a Pod runs on is selected by the Kubernetes scheduler. However,
DaemonSet pods are created and scheduled by the DaemonSet controller instead.
That introduces the following issues:

 * Inconsistent Pod behavior: Normal Pods waiting to be scheduled are created
   and in `Pending` state, but DaemonSet pods are not created in `Pending`
   state. This is confusing to the user.
 * [Pod preemption](/docs/concepts/configuration/pod-priority-preemption/)
   is handled by default scheduler. When preemption is enabled, the DaemonSet controller
   will make scheduling decisions without considering pod priority and preemption.

`ScheduleDaemonSetPods` allows you to schedule DaemonSets using the default
scheduler instead of the DaemonSet controller, by adding the `NodeAffinity` term
to the DaemonSet pods, instead of the `.spec.nodeName` term. The default
scheduler is then used to bind the pod to the target host. If node affinity of
the DaemonSet pod already exists, it is replaced (the original node affinity was taken into account before selecting the target host). The DaemonSet controller only
performs these operations when creating or modifying DaemonSet pods, and no
changes are made to the `spec.template` of the DaemonSet.

```yaml
nodeAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
    nodeSelectorTerms:
    - matchFields:
      - key: metadata.name
        operator: In
        values:
        - target-host-name
```

In addition, `node.kubernetes.io/unschedulable:NoSchedule` toleration is added
automatically to DaemonSet Pods. The default scheduler ignores
`unschedulable` Nodes when scheduling DaemonSet Pods.
 --&gt;
&lt;h2 id=&#34;daemonset-的-pod-是怎么调度的&#34;&gt;DaemonSet 的 Pod 是怎么调度的&lt;/h2&gt;
&lt;h3 id=&#34;由默认调度器调度&#34;&gt;由默认调度器调度&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;DaemonSet 会确保所有合适的节点者会运行一个 Pod 的副本。 通常 Pod 应该运行在哪个节点上是由 k8s 调度器来选的。
但是 DaemonSet 的 Pod 也可以由 DaemonSet 控制器来创建和调度。
由此也引出了一些问题:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod 的行为不一致: 普通的 Pod 在创建和等待调度时的状态是 &lt;code&gt;Pending&lt;/code&gt;，但 DaemonSet 的 Pod 创建时不是 &lt;code&gt;Pending&lt;/code&gt; 状态，这对用户来说比较费解。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/configuration/pod-priority-preemption/&#34;&gt;Pod preemption&lt;/a&gt; 是由默认调度器处理的。 当 优先权被开启时， DaemonSet
控制器在进行调度决策时不会考虑 Pod 的优先级(priority)和优先权(preemption)(这俩有啥区别？)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;ScheduleDaemonSetPods&lt;/code&gt; 允许用户可以让默认调度器来调度 DaemonSet 的 Pod，而不是 DaemonSet 的控制来调度，
只需要向 DaemonSet 的 Pod 模板添加 &lt;code&gt;NodeAffinity&lt;/code&gt; 项， 而不是添加 &lt;code&gt;.spec.nodeName&lt;/code&gt; 项就可以。
这时默认调度就与 Pod 的目标主机绑定了。 如果 DaemonSet Pod 已经存在了节点亲和性，会被替换(原本的节点亲和性在选择目标主机之间也会被考量)
DaemonSet 控制器只会在创建或修改 DaemonSet Pod 时执行这些操作， 不会对 DaemonSet 的 &lt;code&gt;spec.template&lt;/code&gt; 作任何修改。&lt;/p&gt;
&lt;p&gt;{{ &lt;todo-optimize&gt; }}&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;nodeAffinity&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelectorTerms&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;matchFields&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;metadata.name&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
        - &lt;span style=&#34;color:#ae81ff&#34;&gt;target-host-name&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In addition, &lt;code&gt;node.kubernetes.io/unschedulable:NoSchedule&lt;/code&gt; toleration is added
automatically to DaemonSet Pods. The default scheduler ignores
&lt;code&gt;unschedulable&lt;/code&gt; Nodes when scheduling DaemonSet Pods.
另外 &lt;code&gt;node.kubernetes.io/unschedulable:NoSchedule&lt;/code&gt; 耐受会默认添加到 DaemonSet 的 Pod 上。
默认调度器在调度 DaemonSet 的 Pod时会忽略 &lt;code&gt;unschedulable&lt;/code&gt; 节点&lt;/p&gt;
&lt;!--
### Taints and Tolerations

Although Daemon Pods respect
[taints and tolerations](/docs/concepts/scheduling-eviction/taint-and-toleration/),
the following tolerations are added to DaemonSet Pods automatically according to
the related features.

| Toleration Key                           | Effect     | Version | Description |
| ---------------------------------------- | ---------- | ------- | ----------- |
| `node.kubernetes.io/not-ready`           | NoExecute  | 1.13+   | DaemonSet pods will not be evicted when there are node problems such as a network partition. |
| `node.kubernetes.io/unreachable`         | NoExecute  | 1.13+   | DaemonSet pods will not be evicted when there are node problems such as a network partition. |
| `node.kubernetes.io/disk-pressure`       | NoSchedule | 1.8+    | |
| `node.kubernetes.io/memory-pressure`     | NoSchedule | 1.8+    | |
| `node.kubernetes.io/unschedulable`       | NoSchedule | 1.12+   | DaemonSet pods tolerate unschedulable attributes by default scheduler. |
| `node.kubernetes.io/network-unavailable` | NoSchedule | 1.12+   | DaemonSet pods, who uses host network, tolerate network-unavailable attributes by default scheduler. |
 --&gt;
&lt;h3 id=&#34;毒点taint与耐受toleration&#34;&gt;毒点(Taint)与耐受(Toleration)&lt;/h3&gt;
&lt;p&gt;尽管 DaemonSet 的 Pod 是遵守 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/scheduling-eviction/taint-and-toleration/&#34;&gt;毒点与耐受&lt;/a&gt;的,
但以下耐受会根据相关的特性自动添加到 DaemonSet 的 Pod 上。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Toleration Key&lt;/th&gt;
&lt;th&gt;Effect&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node.kubernetes.io/not-ready&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;NoExecute&lt;/td&gt;
&lt;td&gt;1.13+&lt;/td&gt;
&lt;td&gt;DaemonSet 的 Pod 在遇到如网络分区的节点问题时不会衩上踢出去&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node.kubernetes.io/unreachable&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;NoExecute&lt;/td&gt;
&lt;td&gt;1.13+&lt;/td&gt;
&lt;td&gt;DaemonSet 的 Pod 在遇到如网络分区的节点问题时不会衩上踢出去&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node.kubernetes.io/disk-pressure&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;NoSchedule&lt;/td&gt;
&lt;td&gt;1.8+&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node.kubernetes.io/memory-pressure&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;NoSchedule&lt;/td&gt;
&lt;td&gt;1.8+&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node.kubernetes.io/unschedulable&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;NoSchedule&lt;/td&gt;
&lt;td&gt;1.12+&lt;/td&gt;
&lt;td&gt;DaemonSet 的 Pod 由默认调度器调度时耐受 &lt;code&gt;unschedulable&lt;/code&gt; 属性&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node.kubernetes.io/network-unavailable&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;NoSchedule&lt;/td&gt;
&lt;td&gt;1.12+&lt;/td&gt;
&lt;td&gt;DaemonSet 使用主机网络的 Pod 由默认调度器调度时耐受 &lt;code&gt;network-unavailable&lt;/code&gt; 属性&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
## Communicating with Daemon Pods

Some possible patterns for communicating with Pods in a DaemonSet are:

- **Push**: Pods in the DaemonSet are configured to send updates to another service, such
  as a stats database.  They do not have clients.
- **NodeIP and Known Port**: Pods in the DaemonSet can use a `hostPort`, so that the pods are reachable via the node IPs.  Clients know the list of node IPs somehow, and know the port by convention.
- **DNS**: Create a [headless service](/docs/concepts/services-networking/service/#headless-services) with the same pod selector,
  and then discover DaemonSets using the `endpoints` resource or retrieve multiple A records from
  DNS.
- **Service**: Create a service with the same Pod selector, and use the service to reach a
  daemon on a random node. (No way to reach specific node.)
 --&gt;
&lt;h2 id=&#34;与-daemonset-的-pod-通信&#34;&gt;与 DaemonSet 的 Pod 通信&lt;/h2&gt;
&lt;p&gt;与 DaemonSet 的 Pod 通信 可行模式有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Push&lt;/strong&gt;: DaemonSet 中的 Pod 配置为向另一个服务发送更新， 如 一个状态数据。 它们没有客户端。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NodeIP and Known Port&lt;/strong&gt;: DaemonSet 中的 Pod 可以使用 &lt;code&gt;hostPort&lt;/code&gt;， 这样就可以通过节点IP来访问这些 Pod。 客户端可以通过某些方便的方式获得节点的IP 和端口。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DNS&lt;/strong&gt;: 创建一个有相同 Pod 选择器的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/#headless-services&#34;&gt;headless Service&lt;/a&gt;, 这样就可以通过 &lt;code&gt;endpoints&lt;/code&gt; 找到 DaemonSet 或 通过 DNS 找到一个 A 记录列表&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service&lt;/strong&gt;: 创建个拥有相同 Pod 选择器的 Service , 通过 Service 随机访问一个节点上的 Pod(但没办法直接访问指定节点)&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Updating a DaemonSet

If node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete
Pods from newly not-matching nodes.

You can modify the Pods that a DaemonSet creates.  However, Pods do not allow all
fields to be updated.  Also, the DaemonSet controller will use the original template the next
time a node (even with the same name) is created.

You can delete a DaemonSet.  If you specify `--cascade=false` with `kubectl`, then the Pods
will be left on the nodes.  If you subsequently create a new DaemonSet with the same selector,
the new DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replaces
them according to its `updateStrategy`.

You can [perform a rolling update](/docs/tasks/manage-daemon/update-daemon-set/) on a DaemonSet.
 --&gt;
&lt;h2 id=&#34;更新-daemonset&#34;&gt;更新 DaemonSet&lt;/h2&gt;
&lt;p&gt;如果节点的标签发生变更，则 DaemonSet 及时时在新匹配的节点上添加 Pod，将不匹配的节点上的 Pod 删掉。&lt;/p&gt;
&lt;p&gt;用户可以修改由 DaemonSet 创建的 Pod， 但不是所以的 Pod 字段都是可以更新的。 同时 DaemonSet 会使用原版的模板来创建新加入的节点(即使节点名是一样的)&lt;/p&gt;
&lt;p&gt;用户可以删除一个 DaemonSet。 如果在删除时 &lt;code&gt;kubectl&lt;/code&gt; 添加了参数 &lt;code&gt;--cascade=false&lt;/code&gt;， 则对应的 Pod 会被保留在节点上。
如果用户接着又以相同的选择器创建了新的 DaemonSet， 这个新的 DaemonSet 会接管这些 Pod。 如果有 Pod 需要被替换则会根据 &lt;code&gt;updateStrategy&lt;/code&gt; 进行替换&lt;/p&gt;
&lt;p&gt;用户也可以在 DaemonSet 上&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/manage-daemon/update-daemon-set/&#34;&gt;执行滚动更新&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Alternatives to DaemonSet

### Init scripts

It is certainly possible to run daemon processes by directly starting them on a node (e.g. using
`init`, `upstartd`, or `systemd`).  This is perfectly fine.  However, there are several advantages to
running such processes via a DaemonSet:

- Ability to monitor and manage logs for daemons in the same way as applications.
- Same config language and tools (e.g. Pod templates, `kubectl`) for daemons and applications.
- Running daemons in containers with resource limits increases isolation between daemons from app
  containers.  However, this can also be accomplished by running the daemons in a container but not in a Pod
  (e.g. start directly via Docker).
 --&gt;
&lt;h2 id=&#34;daemonset-的替代方案&#34;&gt;DaemonSet 的替代方案&lt;/h2&gt;
&lt;h3 id=&#34;初始化脚本&#34;&gt;初始化脚本&lt;/h3&gt;
&lt;p&gt;有时可以直接在节点上(通过如 &lt;code&gt;init&lt;/code&gt;, &lt;code&gt;upstartd&lt;/code&gt;, &lt;code&gt;systemd&lt;/code&gt;)的方式直接运行守护进程。 这样也是很好的方案。
但是使用 DaemonSet 运行这些进程有如下优势:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以与应用相同的方式来实现这些进程的监控和日志管理&lt;/li&gt;
&lt;li&gt;守护进程与应用使用同样的配置语言与工具(如， Pod 模板 &lt;code&gt;kubectl&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;在容器中运行带资源限制的空城计进程可以提与应用容器之间的隔离级别。 当然，这也可以通过在容器中运行守护进程而不是 Pod(如直接通过 Docker 启动)来运行守护进程&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Bare Pods

It is possible to create Pods directly which specify a particular node to run on.  However,
a DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of
node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you should
use a DaemonSet rather than creating individual Pods.
 --&gt;
&lt;h3 id=&#34;裸-pod&#34;&gt;裸 Pod&lt;/h3&gt;
&lt;p&gt;可以直接在指定节点上直接创建 Pod。 但这些 Pod 可能因为某些如节点挂掉或节点维护(升级内核)引起的故障原因被删除或终止。
因此应该使用 DaemonSet，而不是单独使用 Pod。&lt;/p&gt;
&lt;!--
### Static Pods

It is possible to create Pods by writing a file to a certain directory watched by Kubelet.  These
are called [static pods](/docs/tasks/configure-pod-container/static-pod/).
Unlike DaemonSet, static Pods cannot be managed with kubectl
or other Kubernetes API clients.  Static Pods do not depend on the apiserver, making them useful
in cluster bootstrapping cases.  Also, static Pods may be deprecated in the future.
--&gt;
&lt;h3 id=&#34;静态-pod&#34;&gt;静态 Pod&lt;/h3&gt;
&lt;p&gt;可以直接在 kubelet 监控的目录中创建 Pod 配置文件. 这些被称为 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/static-pod/&#34;&gt;静态 Pod&lt;/a&gt;.
与 DaemonSet 不现, 静态 Pod 不能通过 kubectl 或其它 k8s API 客户端管理. 静态 Pod 也不信赖于 api-server, 让它在
集群初始化的时候很有用. 并且,静态 Pod 可以在未来的版本中被废弃.&lt;/p&gt;
&lt;!--
### Deployments

DaemonSets are similar to [Deployments](/docs/concepts/workloads/controllers/deployment/) in that
they both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers,
storage servers).

Use a Deployment for stateless services, like frontends, where scaling up and down the
number of replicas and rolling out updates are more important than controlling exactly which host
the Pod runs on.  Use a DaemonSet when it is important that a copy of a Pod always run on
all or certain hosts, and when it needs to start before other Pods.
 --&gt;
&lt;h3 id=&#34;deployment&#34;&gt;Deployment&lt;/h3&gt;
&lt;p&gt;DaemonSet 与 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#34;&gt;Deployment&lt;/a&gt; 类似, 它们都可以创建 Pod
并且这些 Pod 运行的都是不应该被终止的进程(如, web 服务器,存储服务器)&lt;/p&gt;
&lt;p&gt;Deployment 用于无状态的服务, 如前端,可以通过副本数来扩容或缩减容量, 发布更新比控制 Pod 在哪个主机上运行更重要.
在 Pod 需要始终在所有或特定主机上运行进更重要些时,并且需要它们在其它 Pod 之前启动时,应该使用 DaemonSet&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Job</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/job/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/job/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- erictune
- soltysh
title: Jobs
content_type: concept
feature:
  title: Batch execution
  description: &gt;
    In addition to services, Kubernetes can manage your batch and CI workloads, replacing containers that fail, if desired.
weight: 50
---
 --&gt;
&lt;!-- overview --&gt;
&lt;p&gt;一个 Job 会创建一个或多个 Pod 并保证这些 Pod 中指定数量的最终执行完成并终止。 当一个 Pod 成功完成时， Job 跟踪这些成功完成的状态。
当成功完成的数量达到指定的数量时， 这个任务(Job)就完成了。 删除一个 Job 会清除它所创建的 Pod。&lt;/p&gt;
&lt;p&gt;使用 Pod 的一个简单场景为创建一个 Job 对象以保证一个 Pod 可靠地完成。 Job 对象会在第一个 Pod 挂掉或被删除(如因为节点硬件故障或节点重启)后创建一个新的 Pod。&lt;/p&gt;
&lt;p&gt;用户也可以使用 Job 并行地运行多个 Pod。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--  
## Running an example Job

Here is an example Job config.  It computes π to 2000 places and prints it out.
It takes around 10s to complete.



 













&lt;table class=&#34;includecode&#34; id=&#34;controllersjobyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/job.yaml&#34; download=&#34;controllers/job.yaml&#34;&gt;
                    &lt;code&gt;controllers/job.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersjobyaml&#39;)&#34; title=&#34;Copy controllers/job.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;batch/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Job&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pi&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pi&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;perl&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;perl&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-Mbignum=bpi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-wle&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;print bpi(2000)&amp;#34;&lt;/span&gt;]
      &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;backoffLimit&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



You can run the example with this command:

```shell
kubectl apply -f https://kubernetes.io/examples/controllers/job.yaml
```
```
job.batch/pi created
```

Check on the status of the Job with `kubectl`:

```shell
kubectl describe jobs/pi
```
```
Name:           pi
Namespace:      default
Selector:       controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
Labels:         controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
                job-name=pi
Annotations:    kubectl.kubernetes.io/last-applied-configuration:
                  {&#34;apiVersion&#34;:&#34;batch/v1&#34;,&#34;kind&#34;:&#34;Job&#34;,&#34;metadata&#34;:{&#34;annotations&#34;:{},&#34;name&#34;:&#34;pi&#34;,&#34;namespace&#34;:&#34;default&#34;},&#34;spec&#34;:{&#34;backoffLimit&#34;:4,&#34;template&#34;:...
Parallelism:    1
Completions:    1
Start Time:     Mon, 02 Dec 2019 15:20:11 +0200
Completed At:   Mon, 02 Dec 2019 15:21:16 +0200
Duration:       65s
Pods Statuses:  0 Running / 1 Succeeded / 0 Failed
Pod Template:
  Labels:  controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
           job-name=pi
  Containers:
   pi:
    Image:      perl
    Port:       &lt;none&gt;
    Host Port:  &lt;none&gt;
    Command:
      perl
      -Mbignum=bpi
      -wle
      print bpi(2000)
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  14m   job-controller  Created pod: pi-5rwd7
```

To view completed Pods of a Job, use `kubectl get pods`.

To list all the Pods that belong to a Job in a machine readable form, you can use a command like this:

```shell
pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath=&#39;{.items[*].metadata.name}&#39;)
echo $pods
```
```
pi-5rwd7
```

Here, the selector is the same as the selector for the Job.  The `--output=jsonpath` option specifies an expression
that just gets the name from each Pod in the returned list.

View the standard output of one of the pods:

```shell
kubectl logs $pods
```
The output is similar to this:
```shell
3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
```
--&gt;
&lt;h2 id=&#34;运行一个示例-job&#34;&gt;运行一个示例 Job&lt;/h2&gt;
&lt;p&gt;以下为一个示例 Job 的配置。 它会计算圆周率(π)后 2000 位并打印出来. 大概用时为 10 秒。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;controllersjobyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/job.yaml&#34; download=&#34;controllers/job.yaml&#34;&gt;
                    &lt;code&gt;controllers/job.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersjobyaml&#39;)&#34; title=&#34;Copy controllers/job.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;batch/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Job&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pi&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pi&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;perl&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;perl&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-Mbignum=bpi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-wle&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;print bpi(2000)&amp;#34;&lt;/span&gt;]
      &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;backoffLimit&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;通过以下命令运行该示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f job.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;job.batch/pi created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过 &lt;code&gt;kubectl&lt;/code&gt; 命令查看 Job 状态:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe jobs/pi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Name:           pi
Namespace:      default
Selector:       controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
Labels:         controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
                job-name=pi
Annotations:    kubectl.kubernetes.io/last-applied-configuration:
                  {&amp;quot;apiVersion&amp;quot;:&amp;quot;batch/v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;Job&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;pi&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;},&amp;quot;spec&amp;quot;:{&amp;quot;backoffLimit&amp;quot;:4,&amp;quot;template&amp;quot;:...
Parallelism:    1
Completions:    1
Start Time:     Mon, 02 Dec 2019 15:20:11 +0200
Completed At:   Mon, 02 Dec 2019 15:21:16 +0200
Duration:       65s
Pods Statuses:  0 Running / 1 Succeeded / 0 Failed
Pod Template:
  Labels:  controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
           job-name=pi
  Containers:
   pi:
    Image:      perl
    Port:       &amp;lt;none&amp;gt;
    Host Port:  &amp;lt;none&amp;gt;
    Command:
      perl
      -Mbignum=bpi
      -wle
      print bpi(2000)
    Environment:  &amp;lt;none&amp;gt;
    Mounts:       &amp;lt;none&amp;gt;
  Volumes:        &amp;lt;none&amp;gt;
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  14m   job-controller  Created pod: pi-5rwd7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用过命令 &lt;code&gt;kubectl get pods&lt;/code&gt; 查看 Job 中执行完成的 Pod。
以机器可读的模式列举属于 Job 的所有 Pod， 执行以下命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;pods&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;kubectl get pods --selector&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;job-name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;pi --output&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;jsonpath&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{.items[*].metadata.name}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;
echo $pods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;pi-5rwd7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;命令里面的选择器与 Job 的选择器是一样的。 &lt;code&gt;--output=jsonpath&lt;/code&gt; 选项中的表达式用于指定返回列表中只有 Pod 的名称&lt;/p&gt;
&lt;p&gt;查看 Pod 的标准输出:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl logs $pods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出内容类似如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Writing a Job spec

As with all other Kubernetes config, a Job needs `apiVersion`, `kind`, and `metadata` fields.
Its name must be a valid [DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

A Job also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).
 --&gt;
&lt;h2 id=&#34;编写-job-配置&#34;&gt;编写 Job 配置&lt;/h2&gt;
&lt;p&gt;与其它所有 k8s 配置一样， Job 的必要字段有 &lt;code&gt;apiVersion&lt;/code&gt;, &lt;code&gt;kind&lt;/code&gt;, &lt;code&gt;metadata&lt;/code&gt;。
它的名字必须是一个有效的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Job 还是需要有一个 &lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;&lt;code&gt;.spec&lt;/code&gt;&lt;/a&gt; 配置区域&lt;/p&gt;
&lt;!--
### Pod Template

The `.spec.template` is the only required field of the `.spec`.


The `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;, except it is nested and does not have an `apiVersion` or `kind`.

In addition to required fields for a Pod, a pod template in a Job must specify appropriate
labels (see [pod selector](#pod-selector)) and an appropriate restart policy.

Only a [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Never` or `OnFailure` is allowed.
 --&gt;
&lt;h3 id=&#34;pod-template&#34;&gt;Pod Template&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.template&lt;/code&gt; 是 &lt;code&gt;.spec&lt;/code&gt; 唯一必要字段。
&lt;code&gt;.spec.template&lt;/code&gt; 是一个 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/#pod-templates&#34;&gt;Pod 模板&lt;/a&gt;
除了因为嵌套没有 &lt;code&gt;apiVersion&lt;/code&gt; 或 &lt;code&gt;kind&lt;/code&gt; 字段外， 与 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 的配置格式完全一样。&lt;/p&gt;
&lt;p&gt;相对于 Pod 新增的字段是 Job 中的 Pod 模板需要有恰当的标签 (见 &lt;a href=&#34;#pod-selector&#34;&gt;Pod 选择器&lt;/a&gt;) 和重启策略。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/pod-lifecycle/#restart-policy&#34;&gt;&lt;code&gt;RestartPolicy&lt;/code&gt;&lt;/a&gt; 的值只能是 &lt;code&gt;Never&lt;/code&gt; 或 &lt;code&gt;OnFailure&lt;/code&gt;&lt;/p&gt;
&lt;!--  
### Pod selector

The `.spec.selector` field is optional.  In almost all cases you should not specify it.
See section [specifying your own pod selector](#specifying-your-own-pod-selector).
--&gt;
&lt;h3 id=&#34;pod-selector&#34;&gt;Pod 选择器&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.selector&lt;/code&gt; 字段为可选，几乎所有的情景中用户都不应该指定。见 &lt;a href=&#34;#specifying-your-own-pod-selector&#34;&gt;设置自己的 Pod 选择器&lt;/a&gt;&lt;/p&gt;
&lt;!--
### Parallel execution for Jobs

There are three main types of task suitable to run as a Job:

1. Non-parallel Jobs
   - normally, only one Pod is started, unless the Pod fails.
   - the Job is complete as soon as its Pod terminates successfully.
1. Parallel Jobs with a *fixed completion count*:
   - specify a non-zero positive value for `.spec.completions`.
   - the Job represents the overall task, and is complete when there is one successful Pod for each value in the range 1 to `.spec.completions`.
   - **not implemented yet:** Each Pod is passed a different index in the range 1 to `.spec.completions`.
1. Parallel Jobs with a *work queue*:
   - do not specify `.spec.completions`, default to `.spec.parallelism`.
   - the Pods must coordinate amongst themselves or an external service to determine what each should work on. For example, a Pod might fetch a batch of up to N items from the work queue.
   - each Pod is independently capable of determining whether or not all its peers are done, and thus that the entire Job is done.
   - when _any_ Pod from the Job terminates with success, no new Pods are created.
   - once at least one Pod has terminated with success and all Pods are terminated, then the Job is completed with success.
   - once any Pod has exited with success, no other Pod should still be doing any work for this task or writing any output.  They should all be in the process of exiting.

For a _non-parallel_ Job, you can leave both `.spec.completions` and `.spec.parallelism` unset.  When both are
unset, both are defaulted to 1.

For a _fixed completion count_ Job, you should set `.spec.completions` to the number of completions needed.
You can set `.spec.parallelism`, or leave it unset and it will default to 1.

For a _work queue_ Job, you must leave `.spec.completions` unset, and set `.spec.parallelism` to
a non-negative integer.

For more information about how to make use of the different types of job, see the [job patterns](#job-patterns) section.
 --&gt;
&lt;h3 id=&#34;parallel-jobs&#34;&gt;并行执行的&lt;/h3&gt;
&lt;p&gt;以下为三种适合用 Job 跑的任务:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;非并行 Job&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;通常除非挂掉否则只启动一个 Pod&lt;/li&gt;
&lt;li&gt;当 Pod 以成功状态终结则 Job 也同时完成&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;带 &lt;em&gt;固定完成数&lt;/em&gt; 的并行 Job&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;.spec.completions&lt;/code&gt; 值是一个非零正数&lt;/li&gt;
&lt;li&gt;这个 Job 代表任务的总包，每个 Pod 执行成功则在完成则相当于领到 1 到 &lt;code&gt;.spec.completions&lt;/code&gt; 之间的一个号&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;还没有实现的功能:&lt;/strong&gt; 每个 Pod 完成可以传递邮不是 1 到 &lt;code&gt;.spec.completions&lt;/code&gt; 之间的索引&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;带 &lt;em&gt;工作队列&lt;/em&gt; 的并行 Job&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;不要设置 &lt;code&gt;.spec.completions&lt;/code&gt;， 默认使用 &lt;code&gt;.spec.parallelism&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;这些 Pod 需要相互协作或由外部服务来决定每个应该做什么。 比如一个 Pod 可以从工作队列中获取一批 N 个条目。&lt;/li&gt;
&lt;li&gt;每个 Pod 都能独立决定它的协作者是否干完，如果都干完了整个 Job 就完成。&lt;/li&gt;
&lt;li&gt;当 Job 中的 &lt;em&gt;任意&lt;/em&gt; Pod 成功完成并终止，就不会再创建新的 Pod。&lt;/li&gt;
&lt;li&gt;当至少有一个 Pod 成功完成并终止且所以 Pod 被终止，则 Job 成功完成&lt;/li&gt;
&lt;li&gt;当任意 Pod 成功完成并退出， 其它的 Pod 就不能再干任何工作或写任何输出。它们都应该在退出的过程中。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于 &lt;em&gt;非并行&lt;/em&gt; Job， 可以不设置 &lt;code&gt;.spec.completions&lt;/code&gt; 和 &lt;code&gt;.spec.parallelism&lt;/code&gt;， 它们默认都是 1&lt;/p&gt;
&lt;p&gt;对于 &lt;em&gt;固定完成数&lt;/em&gt; 的并行 Job， 用户应该通过设置 &lt;code&gt;.spec.completions&lt;/code&gt; 来指定需要完成的数量。
可以设置 &lt;code&gt;.spec.parallelism&lt;/code&gt;，也可以不设置，让它使用默认的 1&lt;/p&gt;
&lt;p&gt;对于 &lt;em&gt;工作队列&lt;/em&gt; 的并行 Job， 用户必须不要设置 &lt;code&gt;.spec.completions&lt;/code&gt;， 并将  &lt;code&gt;.spec.parallelism&lt;/code&gt; 设置为非负数(0怎么说？)&lt;/p&gt;
&lt;p&gt;For more information about how to make use of the different types of job, see the &lt;a href=&#34;#job-patterns&#34;&gt;job patterns&lt;/a&gt; section.&lt;/p&gt;
&lt;p&gt;更多关于怎么用不同类型的 Job 的信息见 &lt;a href=&#34;#job-patterns&#34;&gt;Job 模式&lt;/a&gt; 小节&lt;/p&gt;
&lt;!--
#### Controlling parallelism

The requested parallelism (`.spec.parallelism`) can be set to any non-negative value.
If it is unspecified, it defaults to 1.
If it is specified as 0, then the Job is effectively paused until it is increased.

Actual parallelism (number of pods running at any instant) may be more or less than requested
parallelism, for a variety of reasons:

- For _fixed completion count_ Jobs, the actual number of pods running in parallel will not exceed the number of
  remaining completions.   Higher values of `.spec.parallelism` are effectively ignored.
- For _work queue_ Jobs, no new Pods are started after any Pod has succeeded -- remaining Pods are allowed to complete, however.
- If the Job &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; has not had time to react.
- If the Job controller failed to create Pods for any reason (lack of `ResourceQuota`, lack of permission, etc.),
  then there may be fewer pods than requested.
- The Job controller may throttle new Pod creation due to excessive previous pod failures in the same Job.
- When a Pod is gracefully shut down, it takes time to stop.
 --&gt;
&lt;h4 id=&#34;并行控制&#34;&gt;并行控制&lt;/h4&gt;
&lt;p&gt;申请并行数量 (&lt;code&gt;.spec.parallelism&lt;/code&gt;) 可以被设置为任意非零的数。
如果没有设置则默认为 1.
如果设置为 0， 则这个 Job 实际上是被暂停，想要恢复需要改大这个值。&lt;/p&gt;
&lt;p&gt;实际运行数量(任意时刻运行的 Pod 数量)可能会比请求的数量或多或少差一些， 可能的原因有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于 &lt;em&gt;固定完成数&lt;/em&gt; Job， 实际并行运行的 Pod 数量不会超出未完成的数量。 设置更大的 &lt;code&gt;.spec.parallelism&lt;/code&gt; 值实际是被忽略的。&lt;/li&gt;
&lt;li&gt;对于 &lt;em&gt;工作队列&lt;/em&gt; Job， 在任意 Pod 成功后就不会再创建新的 Pod &amp;ndash; 但是，剩下的 Pod 还是允许继续完成&lt;/li&gt;
&lt;li&gt;如果 Job &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; 没有反应过来。&lt;/li&gt;
&lt;li&gt;如果 Job 控制器因为某些原因(&lt;code&gt;ResourceQuota&lt;/code&gt; 资源配额不足，权限不足，等)，可以会比请求数量要少。&lt;/li&gt;
&lt;li&gt;Job 控制器可能会因为同一个 Job 之前的 Pod 的故障情况来限制新 Pod 的创建。&lt;/li&gt;
&lt;li&gt;当一个 Pod 是平滑关闭的时候，需要时间来停止。&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Handling Pod and container failures

A container in a Pod may fail for a number of reasons, such as because the process in it exited with
a non-zero exit code, or the container was killed for exceeding a memory limit, etc.  If this
happens, and the `.spec.template.spec.restartPolicy = &#34;OnFailure&#34;`, then the Pod stays
on the node, but the container is re-run.  Therefore, your program needs to handle the case when it is
restarted locally, or else specify `.spec.template.spec.restartPolicy = &#34;Never&#34;`.
See [pod lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/#example-states) for more information on `restartPolicy`.

An entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the node
(node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the
`.spec.template.spec.restartPolicy = &#34;Never&#34;`.  When a Pod fails, then the Job controller
starts a new Pod.  This means that your application needs to handle the case when it is restarted in a new
pod.  In particular, it needs to handle temporary files, locks, incomplete output and the like
caused by previous runs.

Note that even if you specify `.spec.parallelism = 1` and `.spec.completions = 1` and
`.spec.template.spec.restartPolicy = &#34;Never&#34;`, the same program may
sometimes be started twice.

If you do specify `.spec.parallelism` and `.spec.completions` both greater than 1, then there may be
multiple pods running at once.  Therefore, your pods must also be tolerant of concurrency.
 --&gt;
&lt;h2 id=&#34;如何应对-pod-和容器的故障&#34;&gt;如何应对 Pod 和容器的故障&lt;/h2&gt;
&lt;p&gt;Pod 中的容器可能因为几种原因失败， 例如因为容器内的进程以非 0 的返回码退出， 或容器因为超出内存限制而退出，等。
如果发生了这些情况，并且 &lt;code&gt;.spec.template.spec.restartPolicy = &amp;quot;OnFailure&amp;quot;&lt;/code&gt;， 这个 Pod 会留在原有的节点上，但容器会重启。
因此，用户的应用程序需要通过应对这种在本地的重新启动，或都设置 &lt;code&gt;.spec.template.spec.restartPolicy = &amp;quot;Never&amp;quot;&lt;/code&gt;
更多关于重启策略(&lt;code&gt;restartPolicy&lt;/code&gt;)的信息见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/pod-lifecycle/#example-states&#34;&gt;Pod 生命周期&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;整个 Pod 也可能整个挂掉，也可能有几种原因， 例如当 Pod 被从节点上踢出(节点在更新，重启，删除，等)， 或者 Pod 中的容器挂了而
&lt;code&gt;.spec.template.spec.restartPolicy = &amp;quot;Never&amp;quot;&lt;/code&gt;。 当一个 Pod 挂掉，Job 控制器会重启一个新的 Pod。 也就是说用户的应用能够
处理这种在新 Pod 重启的情况。特别是要能够处理临时文件，锁，未完成的输出和与前一次运行相似的原因(这意思不太明白?)&lt;/p&gt;
&lt;p&gt;还要注意即便设置了  &lt;code&gt;.spec.parallelism = 1&lt;/code&gt; 和 &lt;code&gt;.spec.completions = 1&lt;/code&gt; 且
&lt;code&gt;.spec.template.spec.restartPolicy = &amp;quot;Never&amp;quot;&lt;/code&gt;， 同一个程序也有时候可能会启动两次。&lt;/p&gt;
&lt;p&gt;如果用户设置 &lt;code&gt;.spec.parallelism&lt;/code&gt; 和 &lt;code&gt;.spec.completions&lt;/code&gt; 的值都大于 1，那么就可能同时启动并运行多个 Pod，
因此要保证这些 Pod 是能够处理并发的。&lt;/p&gt;
&lt;!--
### Pod backoff failure policy

There are situations where you want to fail a Job after some amount of retries
due to a logical error in configuration etc.
To do so, set `.spec.backoffLimit` to specify the number of retries before
considering a Job as failed. The back-off limit is set by default to 6. Failed
Pods associated with the Job are recreated by the Job controller with an
exponential back-off delay (10s, 20s, 40s ...) capped at six minutes. The
back-off count is reset when a Job&#39;s Pod is deleted or successful without any
other Pods for the Job failing around that time.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If your job has &lt;code&gt;restartPolicy = &amp;quot;OnFailure&amp;quot;&lt;/code&gt;, keep in mind that your container running the Job
will be terminated once the job backoff limit has been reached. This can make debugging the Job&amp;rsquo;s executable more difficult. We suggest setting
&lt;code&gt;restartPolicy = &amp;quot;Never&amp;quot;&lt;/code&gt; when debugging the Job or using a logging system to ensure output
from failed Jobs is not lost inadvertently.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;pod-失效补尝策略&#34;&gt;Pod 失效补尝策略&lt;/h3&gt;
&lt;p&gt;在有些情况下，用户期望因为配置中的一些逻辑错误在进行一定次数的重试后，让一个 Job 失败。
要达到这个目的，将 &lt;code&gt;.spec.backoffLimit&lt;/code&gt; 设置为认为失败前重试的次数。 这个补尝数默认为 6.
Job 相关的 Pod 在挂掉以后， Job 控制器会以指数延迟(10s, 20s, 40s &amp;hellip;)最长至六分钟来尝试重建 Pod.
补尝计数在 Job 的 Pod 被删除或该 Job 的其它 Pod 都没有出毛病时被重置&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果 Job 配置有 &lt;code&gt;restartPolicy = &amp;quot;OnFailure&amp;quot;&lt;/code&gt;， 就要注意到这个 Job 中运行的容器会在达到补尝上限后终止。
这会使得调度 Job 内部的程序更麻烦。 我们建议在调度 Job 使用配置 &lt;code&gt;restartPolicy = &amp;quot;Never&amp;quot;&lt;/code&gt; 或者使用日志系统
以保证失败的 Job 的日志不会无意间丢失。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Job termination and cleanup

When a Job completes, no more Pods are created, but the Pods are not deleted either.  Keeping them around
allows you to still view the logs of completed pods to check for errors, warnings, or other diagnostic output.
The job object also remains after it is completed so that you can view its status.  It is up to the user to delete
old jobs after noting their status.  Delete the job with `kubectl` (e.g. `kubectl delete jobs/pi` or `kubectl delete -f ./job.yaml`). When you delete the job using `kubectl`, all the pods it created are deleted too.

By default, a Job will run uninterrupted unless a Pod fails (`restartPolicy=Never`) or a Container exits in error (`restartPolicy=OnFailure`), at which point the Job defers to the
`.spec.backoffLimit` described above. Once `.spec.backoffLimit` has been reached the Job will be marked as failed and any running Pods will be terminated.

Another way to terminate a Job is by setting an active deadline.
Do this by setting the `.spec.activeDeadlineSeconds` field of the Job to a number of seconds.
The `activeDeadlineSeconds` applies to the duration of the job, no matter how many Pods are created.
Once a Job reaches `activeDeadlineSeconds`, all of its running Pods are terminated and the Job status will become `type: Failed` with `reason: DeadlineExceeded`.

Note that a Job&#39;s `.spec.activeDeadlineSeconds` takes precedence over its `.spec.backoffLimit`. Therefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once it reaches the time limit specified by `activeDeadlineSeconds`, even if the `backoffLimit` is not yet reached.

Example:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi-with-timeout
spec:
  backoffLimit: 5
  activeDeadlineSeconds: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: [&#34;perl&#34;,  &#34;-Mbignum=bpi&#34;, &#34;-wle&#34;, &#34;print bpi(2000)&#34;]
      restartPolicy: Never
```

Note that both the Job spec and the [Pod template spec](/docs/concepts/workloads/pods/init-containers/#detailed-behavior) within the Job have an `activeDeadlineSeconds` field. Ensure that you set this field at the proper level.

Keep in mind that the `restartPolicy` applies to the Pod, and not to the Job itself: there is no automatic Job restart once the Job status is `type: Failed`.
That is, the Job termination mechanisms activated with `.spec.activeDeadlineSeconds` and `.spec.backoffLimit` result in a permanent Job failure that requires manual intervention to resolve.
 --&gt;
&lt;h2 id=&#34;job-终止与清理&#34;&gt;Job 终止与清理&lt;/h2&gt;
&lt;p&gt;当一个 Job 完成后，就不会再创建新的 Pod， 但现有的 Pod 也不会删除。 保留这个 Pod 的目的是为了让用户能够
在 Job 过成后还能够查看 Pod 的日志，以方便检查错误，警告或其它诊断输出。 Job 对象本身也会在完成后继续保底以方便用户能够查看它的状态。
由用户决定在查看状态后是否删除这些 Job。 可以通过 &lt;code&gt;kubectl&lt;/code&gt; (如. &lt;code&gt;kubectl delete jobs/pi&lt;/code&gt; 或 &lt;code&gt;kubectl delete -f ./job.yaml&lt;/code&gt;) 删除 Job。
当用户使用 &lt;code&gt;kubectl&lt;/code&gt; 删除 Job 后， 它所创建的所有的 Pod 也一起被删除了。&lt;/p&gt;
&lt;p&gt;默认情况下， 一个 Job 是连续运行的， 但当一个 Pod 挂了(&lt;code&gt;restartPolicy=Never&lt;/code&gt;) 或一个容器存在错误(&lt;code&gt;restartPolicy=OnFailure&lt;/code&gt;)
在这些情况下 Job 就会进行上面介经的补尝 &lt;code&gt;.spec.backoffLimit&lt;/code&gt; 。 当 &lt;code&gt;.spec.backoffLimit&lt;/code&gt; 上限达到后， Job 就会被标记为失败，其它仍在运行的 Pod 也会被终止。&lt;/p&gt;
&lt;p&gt;另一个终止 Job 的办法是活跃死线(active deadline), 通过 Job 的 &lt;code&gt;.spec.activeDeadlineSeconds&lt;/code&gt; 设置秒数。
&lt;code&gt;activeDeadlineSeconds&lt;/code&gt; 表示 Job 的持续时间， 无论创建了多少个 Pod。当 Job 达到 &lt;code&gt;activeDeadlineSeconds&lt;/code&gt; 时，
它所以正在运行的 Pod都会被终止，Job 的状态会进入 &lt;code&gt;type: Failed&lt;/code&gt;， &lt;code&gt;reason: DeadlineExceeded&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;注意 Job 的 &lt;code&gt;.spec.activeDeadlineSeconds&lt;/code&gt; 优先级比它的 &lt;code&gt;.spec.backoffLimit&lt;/code&gt; 高。 因此，如果 Job 在 Pod 失败后尝试一两次后
如果达到 &lt;code&gt;activeDeadlineSeconds&lt;/code&gt; 设置的时间就会再部署额外的 Pod 了， 即便 &lt;code&gt;backoffLimit&lt;/code&gt; 还没有达到。&lt;/p&gt;
&lt;p&gt;示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;batch/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Job&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pi-with-timeout&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;backoffLimit&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;activeDeadlineSeconds&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pi&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;perl&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;perl&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-Mbignum=bpi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-wle&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;print bpi(2000)&amp;#34;&lt;/span&gt;]
      &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意 Job 本身的配置和 Job 内的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/init-containers/#detailed-behavior&#34;&gt;Pod 模板配置&lt;/a&gt;
都有 &lt;code&gt;activeDeadlineSeconds&lt;/code&gt; 字段， 要注意设置的是哪个。&lt;/p&gt;
&lt;p&gt;还要注意重启策略(&lt;code&gt;restartPolicy&lt;/code&gt;) 是应用在 Pod 上的，而不是在 Job 上面: 当 Job 状态是 &lt;code&gt;type: Failed&lt;/code&gt; 时是没有怎么重启的。
也就是当 Job 被 &lt;code&gt;.spec.activeDeadlineSeconds&lt;/code&gt; 和 &lt;code&gt;.spec.backoffLimit&lt;/code&gt; 机制触发了之后， Job 就会进行永久的失败状态，需要人工介入才能解决。&lt;/p&gt;
&lt;!--
## Clean up finished jobs automatically

Finished Jobs are usually no longer needed in the system. Keeping them around in
the system will put pressure on the API server. If the Jobs are managed directly
by a higher level controller, such as
[CronJobs](/docs/concepts/workloads/controllers/cron-jobs/), the Jobs can be
cleaned up by CronJobs based on the specified capacity-based cleanup policy.
 --&gt;
&lt;h2 id=&#34;自动清理已经完成的-job&#34;&gt;自动清理已经完成的 Job&lt;/h2&gt;
&lt;p&gt;完成的 Job 通常不需要再留在系统中。 把它们留在系统中会增加 api-server 的压力。 如果 Job 是由
更高级的控制器，如 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/cron-jobs/&#34;&gt;CronJobs&lt;/a&gt;,
Job 可以被 CronJob 基于指定容量的清理策略来清除。&lt;/p&gt;
&lt;!--
### TTL mechanism for finished Jobs






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.12 [alpha]&lt;/code&gt;
&lt;/div&gt;



Another way to clean up finished Jobs (either `Complete` or `Failed`)
automatically is to use a TTL mechanism provided by a
[TTL controller](/docs/concepts/workloads/controllers/ttlafterfinished/) for
finished resources, by specifying the `.spec.ttlSecondsAfterFinished` field of
the Job.

When the TTL controller cleans up the Job, it will delete the Job cascadingly,
i.e. delete its dependent objects, such as Pods, together with the Job. Note
that when the Job is deleted, its lifecycle guarantees, such as finalizers, will
be honored.

For example:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi-with-ttl
spec:
  ttlSecondsAfterFinished: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: [&#34;perl&#34;,  &#34;-Mbignum=bpi&#34;, &#34;-wle&#34;, &#34;print bpi(2000)&#34;]
      restartPolicy: Never
```

The Job `pi-with-ttl` will be eligible to be automatically deleted, `100`
seconds after it finishes.

If the field is set to `0`, the Job will be eligible to be automatically deleted
immediately after it finishes. If the field is unset, this Job won&#39;t be cleaned
up by the TTL controller after it finishes.

Note that this TTL mechanism is alpha, with feature gate `TTLAfterFinished`. For
more information, see the documentation for
[TTL controller](/docs/concepts/workloads/controllers/ttlafterfinished/) for
finished resources.
 --&gt;
&lt;h3 id=&#34;对于已完成的-job-的-ttl-机制&#34;&gt;对于已完成的 Job 的 TTL 机制&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.12 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;另一种自动清理完成的 Job (无论是 完成(&lt;code&gt;Complete&lt;/code&gt;) 或失败(&lt;code&gt;Failed&lt;/code&gt;) 的)方式是使用 TTL 机制，
它由 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/ttlafterfinished/&#34;&gt;TTL 控制器&lt;/a&gt;提供用于
清理完成的资源，通过在 Job 上配置 &lt;code&gt;.spec.ttlSecondsAfterFinished&lt;/code&gt; 字段实现。&lt;/p&gt;
&lt;p&gt;当 TTL 控制器清理 Job 时，它会级联地删除 Job, 例如， 它依赖的对象，如 Pod 会与 Job 一起删除
注意当 Job 被删除后，它的生命周期保证对象如解构器也会被触发。(猜的)
示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;batch/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Job&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pi-with-ttl&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ttlSecondsAfterFinished&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pi&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;perl&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;perl&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-Mbignum=bpi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-wle&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;print bpi(2000)&amp;#34;&lt;/span&gt;]
      &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个叫 &lt;code&gt;pi-with-ttl&lt;/code&gt; Job 当它结束 &lt;code&gt;100&lt;/code&gt; 秒后被自动删除。&lt;/p&gt;
&lt;p&gt;这个这个字段的值设置为 &lt;code&gt;0&lt;/code&gt;， 则这个 Job 会在执行完成之后马上就被自动删除了。 如果没有设置这个字段
则这个 Job 在结束后不会被 TTL 控制器清除。&lt;/p&gt;
&lt;p&gt;要注意 这个 TTL 机器还在 &lt;code&gt;alpha&lt;/code&gt; 状态， 需要使用 &lt;code&gt;TTLAfterFinished&lt;/code&gt; 功能阀启用。
更多相关信息见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/ttlafterfinished/&#34;&gt;TTL 控制器&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Job patterns

The Job object can be used to support reliable parallel execution of Pods.  The Job object is not
designed to support closely-communicating parallel processes, as commonly found in scientific
computing.  It does support parallel processing of a set of independent but related *work items*.
These might be emails to be sent, frames to be rendered, files to be transcoded, ranges of keys in a
NoSQL database to scan, and so on.

In a complex system, there may be multiple different sets of work items.  Here we are just
considering one set of work items that the user wants to manage together &amp;mdash; a *batch job*.

There are several different patterns for parallel computation, each with strengths and weaknesses.
The tradeoffs are:

- One Job object for each work item, vs. a single Job object for all work items.  The latter is
  better for large numbers of work items.  The former creates some overhead for the user and for the
  system to manage large numbers of Job objects.
- Number of pods created equals number of work items, vs. each Pod can process multiple work items.
  The former typically requires less modification to existing code and containers.  The latter
  is better for large numbers of work items, for similar reasons to the previous bullet.
- Several approaches use a work queue.  This requires running a queue service,
  and modifications to the existing program or container to make it use the work queue.
  Other approaches are easier to adapt to an existing containerised application.


The tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs.
The pattern names are also links to examples and more detailed description.

|                            Pattern                                   | Single Job object | Fewer pods than work items? | Use app unmodified? |  Works in Kube 1.1? |
| -------------------------------------------------------------------- |:-----------------:|:---------------------------:|:-------------------:|:-------------------:|
| [Job Template Expansion](/docs/tasks/job/parallel-processing-expansion/)            |                   |                             |          ✓          |          ✓          |
| [Queue with Pod Per Work Item](/docs/tasks/job/coarse-parallel-processing-work-queue/)   |         ✓         |                             |      sometimes      |          ✓          |
| [Queue with Variable Pod Count](/docs/tasks/job/fine-parallel-processing-work-queue/)  |         ✓         |             ✓               |                     |          ✓          |
| Single Job with Static Work Assignment                               |         ✓         |                             |          ✓          |                     |

When you specify completions with `.spec.completions`, each Pod created by the Job controller
has an identical [`spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).  This means that
all pods for a task will have the same command line and the same
image, the same volumes, and (almost) the same environment variables.  These patterns
are different ways to arrange for pods to work on different things.

This table shows the required settings for `.spec.parallelism` and `.spec.completions` for each of the patterns.
Here, `W` is the number of work items.

|                             Pattern                                  | `.spec.completions` |  `.spec.parallelism` |
| -------------------------------------------------------------------- |:-------------------:|:--------------------:|
| [Job Template Expansion](/docs/tasks/job/parallel-processing-expansion/)           |          1          |     should be 1      |
| [Queue with Pod Per Work Item](/docs/tasks/job/coarse-parallel-processing-work-queue/)   |          W          |        any           |
| [Queue with Variable Pod Count](/docs/tasks/job/fine-parallel-processing-work-queue/)  |          1          |        any           |
| Single Job with Static Work Assignment                               |          W          |        any           |
 --&gt;
&lt;h2 id=&#34;job-的形式&#34;&gt;Job 的形式&lt;/h2&gt;
&lt;p&gt;Job 对象可用于支持可靠的并行 Pod。 Job 对象在设计上不是用来支持紧密通信的并行处理，这种场景常见于科学计算。
Job 支持的是并行处理一组相互独立但有关系的 &lt;em&gt;工作内容&lt;/em&gt;。 它们可能是发送电子邮件，需要渲染的帧，文件转码，NoSQL 数据库中需要被扫描的键，等等。&lt;/p&gt;
&lt;p&gt;在一个复杂的系统中，可能会有多个不同的工作项集合。 这里我们只考虑一个用户想要一起管理的工作项集群 — 一个 &lt;em&gt;批量任务&lt;/em&gt;，
对于并行计算有几种不同的模式，每一种有其优势和劣势。需要做出以下权衡:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;一个 Job 对象对一个工作项， vs. 一个 Job 对象对所有的工作项。 后一种更适合于工作项比较多的情况。
前一种对于用户来说有更多额外的创建工作和对系统来说要管理大量的 Job 对象。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建 Pod 的数量与工作项的数量相同， vs. 每个 Pod 可以处理多个工作项。
前者通常需要对现有代码和容器做出少量更改。后台适用于数量很大的工作项，与上一个条目的原因类似&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;以不同的方式来使用队列。 需要运行一个队列服务，还需要修改现有代码或容器使其能够使用工作队列。
前面介绍的方式更容易用在已经存在的容器化应用上。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上因素权衡总结如下， 包含上面提到的 4 种情况，分 2 列。
以下模式的名称有对象实例的连接地址，其中有更多详细说明。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Pattern&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Single Job object&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Fewer pods than work items?&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Use app unmodified?&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Works in Kube 1.1?&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/job/parallel-processing-expansion/&#34;&gt;Job Template Expansion&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/job/coarse-parallel-processing-work-queue/&#34;&gt;有队列，每个工作项一个 Pod&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;有时候&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/job/fine-parallel-processing-work-queue/&#34;&gt;Queue with Variable Pod Count&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;带静态任务分配的单个 Job&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;当用户设置完成数(&lt;code&gt;.spec.completions&lt;/code&gt;)时， Job 控制器创建的每一个 Pod 都拥有完全相同的 &lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;&lt;code&gt;spec&lt;/code&gt;&lt;/a&gt;。 也就是说这个任务所有的 Pod 都拥有相同的命令，相同的镜像，
相同的数据卷， (几乎)相同的环境变量。 这些模式是以不同的方式组织 Pod 以应对不同的工作内容。&lt;/p&gt;
&lt;p&gt;以下表格展示每种模式所需要设置的 &lt;code&gt;.spec.parallelism&lt;/code&gt; 和 &lt;code&gt;.spec.completions&lt;/code&gt; 字段的值。
其中， &lt;code&gt;W&lt;/code&gt; 是工作项的数量&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Pattern&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;code&gt;.spec.completions&lt;/code&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;code&gt;.spec.parallelism&lt;/code&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/job/parallel-processing-expansion/&#34;&gt;Job Template Expansion&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;should be 1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/job/coarse-parallel-processing-work-queue/&#34;&gt;有队列，每个工作项一个 Pod&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;W&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;any&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/job/fine-parallel-processing-work-queue/&#34;&gt;Queue with Variable Pod Count&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;any&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;带静态任务分配的单个 Job&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;W&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;any&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
## Advanced usage

### Specifying your own Pod selector

Normally, when you create a Job object, you do not specify `.spec.selector`.
The system defaulting logic adds this field when the Job is created.
It picks a selector value that will not overlap with any other jobs.

However, in some cases, you might need to override this automatically set selector.
To do this, you can specify the `.spec.selector` of the Job.

Be very careful when doing this.  If you specify a label selector which is not
unique to the pods of that Job, and which matches unrelated Pods, then pods of the unrelated
job may be deleted, or this Job may count other Pods as completing it, or one or both
Jobs may refuse to create Pods or run to completion.  If a non-unique selector is
chosen, then other controllers (e.g. ReplicationController) and their Pods may behave
in unpredictable ways too.  Kubernetes will not stop you from making a mistake when
specifying `.spec.selector`.

Here is an example of a case when you might want to use this feature.

Say Job `old` is already running.  You want existing Pods
to keep running, but you want the rest of the Pods it creates
to use a different pod template and for the Job to have a new name.
You cannot update the Job because these fields are not updatable.
Therefore, you delete Job `old` but _leave its pods
running_, using `kubectl delete jobs/old --cascade=false`.
Before deleting it, you make a note of what selector it uses:

```
kubectl get job old -o yaml
```
```
kind: Job
metadata:
  name: old
  ...
spec:
  selector:
    matchLabels:
      controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
```

Then you create a new Job with name `new` and you explicitly specify the same selector.
Since the existing Pods have label `controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002`,
they are controlled by Job `new` as well.

You need to specify `manualSelector: true` in the new Job since you are not using
the selector that the system normally generates for you automatically.

```
kind: Job
metadata:
  name: new
  ...
spec:
  manualSelector: true
  selector:
    matchLabels:
      controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
```

The new Job itself will have a different uid from `a8f3d00d-c6d2-11e5-9f87-42010af00002`.  Setting
`manualSelector: true` tells the system to that you know what you are doing and to allow this
mismatch.
 --&gt;
&lt;h2 id=&#34;高级用法&#34;&gt;高级用法&lt;/h2&gt;
&lt;h3 id=&#34;specifying-your-own-pod-selector&#34;&gt;设置自定义 Pod 选择器&lt;/h3&gt;
&lt;p&gt;通常，当用户创始 Job 对象时，不需要配置 &lt;code&gt;.spec.selector&lt;/code&gt;. 系统默认逻辑会在 Job 创建时添加该字段。
它会选择一个与其它 Job 对象不重叠的值设置在上面。&lt;/p&gt;
&lt;p&gt;但是，有些时候，可能需要用户手动指定 &lt;code&gt;.spec.selector&lt;/code&gt; 来覆盖掉这种默认设置的选择器。&lt;/p&gt;
&lt;p&gt;这么做的时候要千万小心。 如果用户设置的选择器不止匹配到当前 Job 的 Pod，这些不相关的 Pod 可能就会被删除,
或者这个 Job 就会批别个 Job 完成的 Pod 认为是自己的，甚至其中一个或两个 Job 都不能创建 Pod 或 成功运行完成。
如果使用了一个非唯一的选择器，其它的控制器(如 ReplicationController) 和它们的 Pod 也可以出现不可预知的行为。
在设置 &lt;code&gt;.spec.selector&lt;/code&gt; 时， k8s 不会阻止你去犯错误。&lt;/p&gt;
&lt;p&gt;以下是一个用到该性场景的示例。&lt;/p&gt;
&lt;p&gt;假设有一个已经在运行的 Job 名字叫 &lt;code&gt;old&lt;/code&gt;。 用户想要让现有的 Pod 继续运行， 但余下的 Pod 需要以新的 Job 名称和新的 Pod 模板创建。
但是 Job 不能更新，因为这些字段不能更新。因此用户需要删除叫 &lt;code&gt;old&lt;/code&gt; 的 Job, 但是要 &lt;em&gt;保留它在运行的 Pod&lt;/em&gt;.
使用 &lt;code&gt;kubectl delete jobs/old --cascade=false&lt;/code&gt; 命令删除 Job前，需要先看看它使用的选择器:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get job old -o yaml
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;kind: Job
metadata:
  name: old
  ...
spec:
  selector:
    matchLabels:
      controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这时候创建一个叫 &lt;code&gt;new&lt;/code&gt; 的新 Job 并显示的设置相同的选择器。
因为已经存在的 Pod 都有标签 &lt;code&gt;controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002&lt;/code&gt;，
所以它们会被 &lt;code&gt;new&lt;/code&gt; Job 管理&lt;/p&gt;
&lt;p&gt;要使用自定义的选择器而不是系统自动创建的，需要在新 Job 上配置 &lt;code&gt;manualSelector: true&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kind: Job
metadata:
  name: new
  ...
spec:
  manualSelector: true
  selector:
    matchLabels:
      controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个新的 Job 会拥有一个与 &lt;code&gt;a8f3d00d-c6d2-11e5-9f87-42010af00002&lt;/code&gt; 不同的 UID。
设置 &lt;code&gt;manualSelector: true&lt;/code&gt; 就是告诉系统你知道自己在做什么，这个不匹配是允许的。&lt;/p&gt;
&lt;!--
## Alternatives

### Bare Pods

When the node that a Pod is running on reboots or fails, the pod is terminated
and will not be restarted.  However, a Job will create new Pods to replace terminated ones.
For this reason, we recommend that you use a Job rather than a bare Pod, even if your application
requires only a single Pod.

### Replication Controller

Jobs are complementary to [Replication Controllers](/docs/concepts/workloads/controllers/replicationcontroller/).
A Replication Controller manages Pods which are not expected to terminate (e.g. web servers), and a Job
manages Pods that are expected to terminate (e.g. batch tasks).

As discussed in [Pod Lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/), `Job` is *only* appropriate
for pods with `RestartPolicy` equal to `OnFailure` or `Never`.
(Note: If `RestartPolicy` is not set, the default value is `Always`.)

### Single Job starts controller Pod

Another pattern is for a single Job to create a Pod which then creates other Pods, acting as a sort
of custom controller for those Pods.  This allows the most flexibility, but may be somewhat
complicated to get started with and offers less integration with Kubernetes.

One example of this pattern would be a Job which starts a Pod which runs a script that in turn
starts a Spark master controller (see [spark example](https://github.com/kubernetes/examples/tree/master/staging/spark/README.md)), runs a spark
driver, and then cleans up.

An advantage of this approach is that the overall process gets the completion guarantee of a Job
object, but maintains complete control over what Pods are created and how work is assigned to them.
 --&gt;
&lt;h2 id=&#34;替代方案&#34;&gt;替代方案&lt;/h2&gt;
&lt;h3 id=&#34;裸-pod&#34;&gt;裸 Pod&lt;/h3&gt;
&lt;p&gt;当 Pod 所在的节点重启或挂掉时， Pod 就会终止并不会被重启。 但 Job 会创建新的 Job 来代替被终止掉的。
因为这个原因， 我们推荐用户使用 Job 而不是裸 Pod， 即使该应用只需要一个 Pod。&lt;/p&gt;
&lt;h3 id=&#34;replicationcontroller&#34;&gt;&lt;code&gt;ReplicationController&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Job 与 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicationcontroller/&#34;&gt;ReplicationController&lt;/a&gt;是互补的.
ReplicationController 管理那些不期望被终止的 Pod (如 web 服务)，
Job 管理那些预期要终止的 Pod (如 批量任务)&lt;/p&gt;
&lt;p&gt;就如在 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/pod-lifecycle/&#34;&gt;Pod 生命周期&lt;/a&gt;中讨论的，
&lt;code&gt;Job&lt;/code&gt; 是唯一适合将 Pod 重启策略(&lt;code&gt;RestartPolicy&lt;/code&gt;) 设置为 在失败时(&lt;code&gt;OnFailure&lt;/code&gt;) 或 永不(&lt;code&gt;Never&lt;/code&gt;) (重启)的。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果 &lt;code&gt;RestartPolicy&lt;/code&gt; 没有设置，则默认值为 总是(&lt;code&gt;Always&lt;/code&gt;)重启&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;h3 id=&#34;单个-job-启动作为控制器的-pod&#34;&gt;单个 Job 启动作为控制器的 Pod&lt;/h3&gt;
&lt;p&gt;另一种模式为单个 Job 创建一个 Pod， 这个 Pod 再创建其它的 Pod，这个 Pod 对于其它的 Pod 来说就表现为一个自定义控制器。
这种方式允许最大的灵活性， 但可能入门有点复杂并且与k8s 集成比较少。&lt;/p&gt;
&lt;p&gt;使用这个模式的一个示例为一个 Job 创建了一个 Pod， 这个 Pod 内运行了一个脚本启动一个 Spark 主控制器
(见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/spark/README.md&#34;&gt;spark 示例&lt;/a&gt;)
运行一个 spark driver， 然后清理现场。&lt;/p&gt;
&lt;p&gt;这种方式是的一个好处是由一个 Job 对象来保证所以进程的完成， 只需要维护完成控制不震怒近 哪些 Pod 要创建，怎么给它们分配工作&lt;/p&gt;
&lt;h2 id=&#34;cron-jobs&#34;&gt;&lt;code&gt;CronJob&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;用户可以使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/cron-jobs/&#34;&gt;&lt;code&gt;CronJob&lt;/code&gt;&lt;/a&gt; 来创建一个在指定 时间/日期 运行的 Job,
与 Unix 的 &lt;code&gt;cron&lt;/code&gt; 类似。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 垃圾回收</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/garbage-collection/</link>
      <pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/garbage-collection/</guid>
      <description>
        
        
        &lt;!--  
---
title: Garbage Collection
content_type: concept
weight: 60
---
--&gt;
&lt;!-- overview --&gt;
&lt;p&gt;在 k8s 中垃圾回收的作用就是删除那些曾经有所有者，现在没有的对象。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Owners and dependents

Some Kubernetes objects are owners of other objects. For example, a ReplicaSet
is the owner of a set of Pods. The owned objects are called *dependents* of the
owner object. Every dependent object has a `metadata.ownerReferences` field that
points to the owning object.

Sometimes, Kubernetes sets the value of `ownerReference` automatically. For
example, when you create a ReplicaSet, Kubernetes automatically sets the
`ownerReference` field of each Pod in the ReplicaSet. In 1.8, Kubernetes
automatically sets the value of `ownerReference` for objects created or adopted
by ReplicationController, ReplicaSet, StatefulSet, DaemonSet, Deployment, Job
and CronJob.

You can also specify relationships between owners and dependents by manually
setting the `ownerReference` field.

Here&#39;s a configuration file for a ReplicaSet that has three Pods:



 













&lt;table class=&#34;includecode&#34; id=&#34;controllersreplicasetyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/replicaset.yaml&#34; download=&#34;controllers/replicaset.yaml&#34;&gt;
                    &lt;code&gt;controllers/replicaset.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersreplicasetyaml&#39;)&#34; title=&#34;Copy controllers/replicaset.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicaSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-repset&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;pod-is-for&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;garbage-collection-example&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;pod-is-for&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;garbage-collection-example&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



If you create the ReplicaSet and then view the Pod metadata, you can see
OwnerReferences field:

```shell
kubectl apply -f https://k8s.io/examples/controllers/replicaset.yaml
kubectl get pods --output=yaml
```

The output shows that the Pod owner is a ReplicaSet named `my-repset`:

```yaml
apiVersion: v1
kind: Pod
metadata:
  ...
  ownerReferences:
  - apiVersion: apps/v1
    controller: true
    blockOwnerDeletion: true
    kind: ReplicaSet
    name: my-repset
    uid: d9607e19-f88f-11e6-a518-42010a800195
  ...
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;Cross-namespace owner references are disallowed by design. This means:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Namespace-scoped dependents can only specify owners in the same namespace,
and owners that are cluster-scoped.&lt;/li&gt;
&lt;li&gt;Cluster-scoped dependents can only specify cluster-scoped owners, but not
namespace-scoped owners.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;所有者和从属者&#34;&gt;所有者和从属者&lt;/h2&gt;
&lt;p&gt;有些 k8s 对象是其它对象的所有者。 如，一个 ReplicaSet 就是一个 Pod 集合的所有者。
那些被拥有的对象就叫所有者的 &lt;em&gt;从属者(dependents)&lt;/em&gt;， 每个从属者都有一个 &lt;code&gt;metadata.ownerReferences&lt;/code&gt;
字段指向它的所有者。&lt;/p&gt;
&lt;p&gt;有时候， k8s 会自动添加 &lt;code&gt;ownerReference&lt;/code&gt; 的值。 如，当创建一个 ReplicaSet 时， k8s 自动
为这个 ReplicaSet 所属的 Pod 设置 &lt;code&gt;ownerReference&lt;/code&gt;。 在 &lt;code&gt;1.8&lt;/code&gt; 中， k8s 为创建或捕获对象
自动设置 &lt;code&gt;ownerReference&lt;/code&gt;值的对象有 ReplicationController, ReplicaSet, StatefulSet,
DaemonSet, Deployment, Job, CronJob.&lt;/p&gt;
&lt;p&gt;用户可以通过设置 &lt;code&gt;ownerReference&lt;/code&gt; 字段来手动指定所有者和从属者之间的关系
以下为一个包含 3 个 Pod 的 ReplicaSet 的配置文件:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;controllersreplicasetyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/replicaset.yaml&#34; download=&#34;controllers/replicaset.yaml&#34;&gt;
                    &lt;code&gt;controllers/replicaset.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersreplicasetyaml&#39;)&#34; title=&#34;Copy controllers/replicaset.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicaSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-repset&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;pod-is-for&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;garbage-collection-example&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;pod-is-for&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;garbage-collection-example&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;如果用户创建了 ReplicaSet 然后查看所属 Pod 的元数据(metadata), 就可以看到 &lt;code&gt;OwnerReferences&lt;/code&gt; 字段&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f replicaset.yaml
kubectl get pods --output&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从输出结果可以看到 Pod 的所有者是一个叫 &lt;code&gt;my-repset&lt;/code&gt; 的 ReplicaSet&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ownerReferences&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;controller&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;blockOwnerDeletion&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicaSet&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-repset&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;uid&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;d9607e19-f88f-11e6-a518-42010a800195&lt;/span&gt;
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;在设计上跨命名空间的所属关系是不允许的。 这就是说:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;命名空间作用域内从属者只能指定同一个命名空间的对象为其所有者和集群作用域的所有者&lt;/li&gt;
&lt;li&gt;集群作用哉的从属者只能指定集群作用域的所有者，不能指定命名空间作用域的所有者&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Controlling how the garbage collector deletes dependents

When you delete an object, you can specify whether the object&#39;s dependents are
also deleted automatically. Deleting dependents automatically is called *cascading
deletion*.  There are two modes of *cascading deletion*: *background* and *foreground*.

If you delete an object without deleting its dependents
automatically, the dependents are said to be *orphaned*.
 --&gt;
&lt;h2 id=&#34;控制垃圾回收器怎么删除从属者&#34;&gt;控制垃圾回收器怎么删除从属者&lt;/h2&gt;
&lt;p&gt;当用户删除一个对象时，可能指定是否同时自动删除它的从属者。 自动删除从属都的行为叫做 &lt;em&gt;级联删除&lt;/em&gt;。
级联删除又有两种模式 &lt;em&gt;后台&lt;/em&gt; 和 &lt;em&gt;前台&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;如果用户在删除一个对象是没有自动删除它的所属者，这些从属者就认为是 &lt;em&gt;孤儿&lt;/em&gt;&lt;/p&gt;
&lt;!--
### Foreground cascading deletion

In *foreground cascading deletion*, the root object first
enters a &#34;deletion in progress&#34; state. In the &#34;deletion in progress&#34; state,
the following things are true:

 * The object is still visible via the REST API
 * The object&#39;s `deletionTimestamp` is set
 * The object&#39;s `metadata.finalizers` contains the value &#34;foregroundDeletion&#34;.

Once the &#34;deletion in progress&#34; state is set, the garbage
collector deletes the object&#39;s dependents. Once the garbage collector has deleted all
&#34;blocking&#34; dependents (objects with `ownerReference.blockOwnerDeletion=true`), it deletes
the owner object.

Note that in the &#34;foregroundDeletion&#34;, only dependents with
`ownerReference.blockOwnerDeletion=true` block the deletion of the owner object.
Kubernetes version 1.7 added an [admission controller](/docs/reference/access-authn-authz/admission-controllers/#ownerreferencespermissionenforcement) that controls user access to set
`blockOwnerDeletion` to true based on delete permissions on the owner object, so that
unauthorized dependents cannot delay deletion of an owner object.

If an object&#39;s `ownerReferences` field is set by a controller (such as Deployment or ReplicaSet),
blockOwnerDeletion is set automatically and you do not need to manually modify this field.
 --&gt;
&lt;h3 id=&#34;前台级联删除&#34;&gt;前台级联删除&lt;/h3&gt;
&lt;p&gt;In &lt;em&gt;foreground cascading deletion&lt;/em&gt;, the root object first
enters a &amp;ldquo;deletion in progress&amp;rdquo; state. In the &amp;ldquo;deletion in progress&amp;rdquo; state,
the following things are true:&lt;/p&gt;
&lt;p&gt;在使用 &lt;em&gt;前台级联删除&lt;/em&gt; 时， 根对象先进入 &amp;ldquo;删除中&amp;rdquo; 状态。 在 &amp;ldquo;删除中&amp;rdquo; 状态时, 以下状态为真:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;依然可以通过 REST API 访问该对象&lt;/li&gt;
&lt;li&gt;该对象上已经设置了 &lt;code&gt;deletionTimestamp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;该对象上的 &lt;code&gt;metadata.finalizers&lt;/code&gt; 包含值 &amp;ldquo;foregroundDeletion&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当 &amp;ldquo;删除中&amp;rdquo; 被设置后， 垃圾回收器就会删除它的从属对象。 当垃圾回收器删除所有 &amp;ldquo;阻塞&amp;rdquo; 的对象(包含 &lt;code&gt;ownerReference.blockOwnerDeletion=true&lt;/code&gt; 的对象)后，
就会删除对象本身。&lt;/p&gt;
&lt;p&gt;要注意到 &lt;em&gt;前台级联删除&lt;/em&gt; 只会被那些包含 &lt;code&gt;ownerReference.blockOwnerDeletion=true&lt;/code&gt; 对象的删除所阻塞。
在 k8s &lt;code&gt;v1.7&lt;/code&gt; 添加了 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/access-authn-authz/admission-controllers/#ownerreferencespermissionenforcement&#34;&gt;admission controller&lt;/a&gt;
其上添加了基于所有者对象上的删除权限设置用户对 &lt;code&gt;blockOwnerDeletion&lt;/code&gt; 设置为 true 的权限。
因此对于未授权的从属对象不会延迟其所有者对象的删除。&lt;/p&gt;
&lt;p&gt;如果一个对象的 &lt;code&gt;ownerReferences&lt;/code&gt; 字段是由控制器(如 Deployment 或 ReplicaSet) 设置的，
&lt;code&gt;blockOwnerDeletion&lt;/code&gt; 也会自动被设置，用户不需要手动修改该字段。&lt;/p&gt;
&lt;!--
### Background cascading deletion

In *background cascading deletion*, Kubernetes deletes the owner object
immediately and the garbage collector then deletes the dependents in
the background.
 --&gt;
&lt;h3 id=&#34;后台级联删除&#34;&gt;后台级联删除&lt;/h3&gt;
&lt;p&gt;在使用 &lt;em&gt;后台级联删除&lt;/em&gt;, k8s 立马删除所有者对象，然后垃圾回器后台删除其从属对象&lt;/p&gt;
&lt;!--
### Setting the cascading deletion policy

To control the cascading deletion policy, set the `propagationPolicy`
field on the `deleteOptions` argument when deleting an Object. Possible values include &#34;Orphan&#34;,
&#34;Foreground&#34;, or &#34;Background&#34;.

Here&#39;s an example that deletes dependents in background:

```shell
kubectl proxy --port=8080
curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset \
  -d &#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Background&#34;}&#39; \
  -H &#34;Content-Type: application/json&#34;
```

Here&#39;s an example that deletes dependents in foreground:

```shell
kubectl proxy --port=8080
curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset \
  -d &#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Foreground&#34;}&#39; \
  -H &#34;Content-Type: application/json&#34;
```

Here&#39;s an example that orphans dependents:

```shell
kubectl proxy --port=8080
curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset \
  -d &#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Orphan&#34;}&#39; \
  -H &#34;Content-Type: application/json&#34;
```

kubectl also supports cascading deletion.
To delete dependents automatically using kubectl, set `--cascade` to true.  To
orphan dependents, set `--cascade` to false. The default value for `--cascade`
is true.

Here&#39;s an example that orphans the dependents of a ReplicaSet:

```shell
kubectl delete replicaset my-repset --cascade=false
```
 --&gt;
&lt;h3 id=&#34;设置级联删除策略&#34;&gt;设置级联删除策略&lt;/h3&gt;
&lt;p&gt;想要控制级联删除策略, 可以在删除对象时设置 &lt;code&gt;deleteOptions&lt;/code&gt; 参数的 &lt;code&gt;propagationPolicy&lt;/code&gt; 字段，
可选的值有 &amp;ldquo;Orphan&amp;rdquo;, &amp;ldquo;Foreground&amp;rdquo;, &amp;ldquo;Background&amp;rdquo;.
以下为一个后台删除从属对象的示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl proxy --port&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#34;kind&amp;#34;:&amp;#34;DeleteOptions&amp;#34;,&amp;#34;apiVersion&amp;#34;:&amp;#34;v1&amp;#34;,&amp;#34;propagationPolicy&amp;#34;:&amp;#34;Background&amp;#34;}&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -H &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Content-Type: application/json&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;以下为一个前台删除从属对象的示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl proxy --port&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#34;kind&amp;#34;:&amp;#34;DeleteOptions&amp;#34;,&amp;#34;apiVersion&amp;#34;:&amp;#34;v1&amp;#34;,&amp;#34;propagationPolicy&amp;#34;:&amp;#34;Foreground&amp;#34;}&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -H &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Content-Type: application/json&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;以下为一个将从属对象设置为 孤儿的示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl proxy --port&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#34;kind&amp;#34;:&amp;#34;DeleteOptions&amp;#34;,&amp;#34;apiVersion&amp;#34;:&amp;#34;v1&amp;#34;,&amp;#34;propagationPolicy&amp;#34;:&amp;#34;Orphan&amp;#34;}&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -H &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Content-Type: application/json&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;kubectl 也支持级联删除。要使用 kubectl 自动删除从属对象， 设置 &lt;code&gt;--cascade&lt;/code&gt; 为 &lt;code&gt;true&lt;/code&gt;
将从属对象设置为 孤儿 设置 &lt;code&gt;--cascade&lt;/code&gt; 为 &lt;code&gt;false&lt;/code&gt;. &lt;code&gt;--cascade&lt;/code&gt; 默认值为 &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;以下为一个将 ReplicaSet 从属对象设置为 孤儿的示例&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl delete replicaset my-repset --cascade&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Additional note on Deployments

Prior to 1.7, When using cascading deletes with Deployments you *must* use `propagationPolicy: Foreground`
to delete not only the ReplicaSets created, but also their Pods. If this type of _propagationPolicy_
is not used, only the ReplicaSets will be deleted, and the Pods will be orphaned.
See [kubeadm/#149](https://github.com/kubernetes/kubeadm/issues/149#issuecomment-284766613) for more information.
 --&gt;
&lt;h3 id=&#34;deployment-额外需要注意的地方&#34;&gt;Deployment 额外需要注意的地方&lt;/h3&gt;
&lt;p&gt;Prior to 1.7, When using cascading deletes with Deployments you &lt;em&gt;must&lt;/em&gt; use &lt;code&gt;propagationPolicy: Foreground&lt;/code&gt;
to delete not only the ReplicaSets created, but also their Pods. If this type of &lt;em&gt;propagationPolicy&lt;/em&gt;
is not used, only the ReplicaSets will be deleted, and the Pods will be orphaned.
See &lt;a href=&#34;https://github.com/kubernetes/kubeadm/issues/149#issuecomment-284766613&#34;&gt;kubeadm/#149&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;在 &lt;code&gt;1.7&lt;/code&gt; 版本之前， 当使用级联删除 Deployment 时，&lt;em&gt;必须&lt;/em&gt; 要使用 &lt;code&gt;propagationPolicy: Foreground&lt;/code&gt;
来确定不止删除创建的  ReplicaSet，还要删除对应的 Pod。如果没有使用该类型的 &lt;em&gt;propagationPolicy&lt;/em&gt;，
只有 ReplicaSet 会被删除，Pod 会被设置人孤儿。
更多信息见 &lt;a href=&#34;https://github.com/kubernetes/kubeadm/issues/149#issuecomment-284766613&#34;&gt;kubeadm/#149&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;已知问题&#34;&gt;已知问题&lt;/h2&gt;
&lt;p&gt;见问题单 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/26120&#34;&gt;#26120&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/api-machinery/garbage-collection.md&#34;&gt;设计文稿 1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/api-machinery/synchronous-garbage-collection.md&#34;&gt;设计文稿 2&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 用于已完成资源的 TTL 控制器</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/ttlafterfinished/</link>
      <pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/ttlafterfinished/</guid>
      <description>
        
        
        &lt;!--  
---
reviewers:
- janetkuo
title: TTL Controller for Finished Resources
content_type: concept
weight: 70
---
--&gt;
&lt;!-- overview --&gt;
&lt;!--





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.12 [alpha]&lt;/code&gt;
&lt;/div&gt;



The TTL controller provides a TTL (time to live) mechanism to limit the lifetime of resource
objects that have finished execution. TTL controller only handles
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/job/&#39; target=&#39;_blank&#39;&gt;Jobs&lt;span class=&#39;tooltip-text&#39;&gt;一个运行后即结束的有限或批量任务&lt;/span&gt;
&lt;/a&gt; for now,
and may be expanded to handle other resources that will finish execution,
such as Pods and custom resources.

Alpha Disclaimer: this feature is currently alpha, and can be enabled with both kube-apiserver and kube-controller-manager
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
`TTLAfterFinished`.
 --&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.12 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;TTL 控制器提供了一个限制那些已经执行完成的资源对象生存期的 TTL (存活时间)机制。
目前 TTL 控制器只能控制 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/job/&#39; target=&#39;_blank&#39;&gt;Job&lt;span class=&#39;tooltip-text&#39;&gt;一个运行后即结束的有限或批量任务&lt;/span&gt;
&lt;/a&gt;， 可能会扩展到通解处理其它完成执行的资源
如 Pod 和 自定义资源。
Alpha Disclaimer: this feature is currently alpha, and can be enabled with both kube-apiserver and kube-controller-manager
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;feature gate&lt;/a&gt;
&lt;code&gt;TTLAfterFinished&lt;/code&gt;.
Alpha 版本的免责声明: 这个特性现在还是 alpha 版本， 可以在 kube-apiserver 中 kube-controller-manager
通过 &lt;code&gt;TTLAfterFinished&lt;/code&gt; &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt; 开启&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## TTL Controller

The TTL controller only supports Jobs for now. A cluster operator can use this feature to clean
up finished Jobs (either `Complete` or `Failed`) automatically by specifying the
`.spec.ttlSecondsAfterFinished` field of a Job, as in this
[example](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically).
The TTL controller will assume that a resource is eligible to be cleaned up
TTL seconds after the resource has finished, in other words, when the TTL has expired. When the
TTL controller cleans up a resource, it will delete it cascadingly, that is to say it will delete
its dependent objects together with it. Note that when the resource is deleted,
its lifecycle guarantees, such as finalizers, will be honored.

The TTL seconds can be set at any time. Here are some examples for setting the
`.spec.ttlSecondsAfterFinished` field of a Job:

* Specify this field in the resource manifest, so that a Job can be cleaned up
  automatically some time after it finishes.
* Set this field of existing, already finished resources, to adopt this new
  feature.
* Use a
  [mutating admission webhook](/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks)
  to set this field dynamically at resource creation time. Cluster administrators can
  use this to enforce a TTL policy for finished resources.
* Use a
  [mutating admission webhook](/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks)
  to set this field dynamically after the resource has finished, and choose
  different TTL values based on resource status, labels, etc.
 --&gt;
&lt;h2 id=&#34;ttl-控制器&#34;&gt;TTL 控制器&lt;/h2&gt;
&lt;p&gt;目前 TTL 控制器只支持 Job。 集群管理员可以通过该特性来自动清理已完成的 Job(无论 &lt;code&gt;Complete&lt;/code&gt; 或 &lt;code&gt;Failed&lt;/code&gt;)，
只需要在 Job 对象上设置 &lt;code&gt;.spec.ttlSecondsAfterFinished&lt;/code&gt;， 见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically&#34;&gt;示例&lt;/a&gt;.
TTL 控制器假定一个资源在完成后 TTL 秒之后就是能够被回收的，换句话来说，就是当 TTL 过期的时候。
当 TTL 控制器清理一个资源时，会级联地删除，也就是说会连同它的从属对象一起删除。 要注意当一个资源被删除时，
它的生命周期保证，如析构器，就会被触发。
TTL 时间可以在任意时刻设置。 以下为在 Job 上设置 &lt;code&gt;.spec.ttlSecondsAfterFinished&lt;/code&gt; 的一些示例:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在资源的配置清单中设置该字段，因而 Job 可以在完成后的某个时间点被自动清理。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在已经存在，已经完成的资源上设置该字段，然后享受该特性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks&#34;&gt;mutating admission webhook&lt;/a&gt;
来在资源创建时动态添加该字段。集群管理员可以使用它来给已经完成的资源加持一个 TTL 策略&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks&#34;&gt;mutating admission webhook&lt;/a&gt;
在资源完成时动态添加该字段。 通过不同的资源状态，标签，等来设置不同的 TTL 值&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Caveat

### Updating TTL Seconds

Note that the TTL period, e.g. `.spec.ttlSecondsAfterFinished` field of Jobs,
can be modified after the resource is created or has finished. However, once the
Job becomes eligible to be deleted (when the TTL has expired), the system won&#39;t
guarantee that the Jobs will be kept, even if an update to extend the TTL
returns a successful API response.

### Time Skew

Because TTL controller uses timestamps stored in the Kubernetes resources to
determine whether the TTL has expired or not, this feature is sensitive to time
skew in the cluster, which may cause TTL controller to clean up resource objects
at the wrong time.

In Kubernetes, it&#39;s required to run NTP on all nodes
(see [#6159](https://github.com/kubernetes/kubernetes/issues/6159#issuecomment-93844058))
to avoid time skew. Clocks aren&#39;t always correct, but the difference should be
very small. Please be aware of this risk when setting a non-zero TTL.
 --&gt;
&lt;h2 id=&#34;附加说明&#34;&gt;附加说明&lt;/h2&gt;
&lt;h3 id=&#34;更新-ttl-时间&#34;&gt;更新 TTL 时间&lt;/h3&gt;
&lt;p&gt;要注意 TTL 的时间， 例如 Job 的 &lt;code&gt;.spec.ttlSecondsAfterFinished&lt;/code&gt; 字段可以在资源创建或完成后进行修改。
但是，当 Job 变得可以被删除时(当 TTL 过期后)， 系统就不保证 Job 对象会继续保留，即便增加 TTL 的请求 API 请求响应成功的。&lt;/p&gt;
&lt;h3 id=&#34;时间偏差&#34;&gt;时间偏差&lt;/h3&gt;
&lt;p&gt;因为 TTL 控制器使用存放于 k8s 资源中的时间戳来决定 TTL 是否已经过期， 这个特性对集群中的时间偏差很敏感。
可能会导致 TTL 控制器在错误的时间清理资源对象。&lt;/p&gt;
&lt;p&gt;在 k8s 系统中，需要在所有的节点上运行 NTP (见问题单 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/6159#issuecomment-93844058&#34;&gt;#6159&lt;/a&gt;)
来避免引志时间偏差。 时钟不是始终正确的，但差异必须要特别小。 在设置非零 TTL 时一定要注意这个风险。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically&#34;&gt;自动清理 Job&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/0026-ttl-after-finish.md&#34;&gt;设置文稿&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: CronJob</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/cron-jobs/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/cron-jobs/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- erictune
- soltysh
- janetkuo
title: CronJob
content_type: concept
weight: 80
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.8 [beta]&lt;/code&gt;
&lt;/div&gt;



A _CronJob_ creates &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/job/&#39; target=&#39;_blank&#39;&gt;Jobs&lt;span class=&#39;tooltip-text&#39;&gt;一个运行后即结束的有限或批量任务&lt;/span&gt;
&lt;/a&gt; on a repeating schedule.

One CronJob object is like one line of a _crontab_ (cron table) file. It runs a job periodically
on a given schedule, written in [Cron](https://en.wikipedia.org/wiki/Cron) format.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; &lt;p&gt;All &lt;strong&gt;CronJob&lt;/strong&gt; &lt;code&gt;schedule:&lt;/code&gt; times are based on the timezone of the
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/kube-controller-manager/&#39; target=&#39;_blank&#39;&gt;kube-controller-manager&lt;span class=&#39;tooltip-text&#39;&gt;Control Plane component that runs controller processes.&lt;/span&gt;
&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If your control plane runs the kube-controller-manager in Pods or bare
containers, the timezone set for the kube-controller-manager container determines the timezone
that the cron job controller uses.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;


When creating the manifest for a CronJob resource, make sure the name you provide
is a valid [DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
The name must be no longer than 52 characters. This is because the CronJob controller will automatically
append 11 characters to the job name provided and there is a constraint that the
maximum length of a Job name is no more than 63 characters.
 --&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.8 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;em&gt;CronJob&lt;/em&gt; 可以定时重复地创建 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/job/&#39; target=&#39;_blank&#39;&gt;Job&lt;span class=&#39;tooltip-text&#39;&gt;一个运行后即结束的有限或批量任务&lt;/span&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一个 CronJob 对象看起来像是 &lt;em&gt;crontab&lt;/em&gt; (cron table) 文件的一行。
它会按照以 &lt;a href=&#34;https://en.wikipedia.org/wiki/Cron&#34;&gt;Cron&lt;/a&gt; 编写的计划定期的运行 Job&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; &lt;p&gt;所有 &lt;strong&gt;CronJob&lt;/strong&gt; &lt;code&gt;时间表:&lt;/code&gt;的时间都是基于
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/kube-controller-manager/&#39; target=&#39;_blank&#39;&gt;kube-controller-manager&lt;span class=&#39;tooltip-text&#39;&gt;Control Plane component that runs controller processes.&lt;/span&gt;
&lt;/a&gt;
的时间的。&lt;/p&gt;
&lt;p&gt;如果控制中心把 kube-controller-manager 运行在 Pod 中或裸容器中， kube-controller-manager 所在
容器的时区就决定了 CronJob 控制器所使用的时区&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;在创建 CronJob 资源的配置定义时，要确保其名称是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.
名称不能多于 52 个字符。 因为 CronJob 会自动在它创建的 Job 的名称是自身的名称再加 11 个字符，
这样 Job 的名称就不会超过 63 个字符的限制。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## CronJob

CronJobs are useful for creating periodic and recurring tasks, like running backups or
sending emails. CronJobs can also schedule individual tasks for a specific time, such as
scheduling a Job for when your cluster is likely to be idle.
--&gt;
&lt;h2 id=&#34;cronjob&#34;&gt;CronJob&lt;/h2&gt;
&lt;p&gt;CronJob 对于创建定期重复的任务是很有用的，例如运行备份任务或发送邮件。
CronJob 也可以在指定时间调度单个应用，例如当集群变得空闲时调度 Job 任务&lt;/p&gt;
&lt;!--
### Example

This example CronJob manifest prints the current time and a hello message every minute:



 













&lt;table class=&#34;includecode&#34; id=&#34;applicationjobcronjobyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/application/job/cronjob.yaml&#34; download=&#34;application/job/cronjob.yaml&#34;&gt;
                    &lt;code&gt;application/job/cronjob.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;applicationjobcronjobyaml&#39;)&#34; title=&#34;Copy application/job/cronjob.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;batch/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;CronJob&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hello&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;schedule&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*/1 * * * *&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;jobTemplate&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hello&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;:
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;/bin/sh&lt;/span&gt;
            - -&lt;span style=&#34;color:#ae81ff&#34;&gt;c&lt;/span&gt;
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;date; echo Hello from the Kubernetes cluster&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;OnFailure&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



([Running Automated Tasks with a CronJob](/docs/tasks/job/automated-tasks-with-cron-jobs/)
takes you through this example in more detail).
 --&gt;
&lt;h3 id=&#34;示例&#34;&gt;示例&lt;/h3&gt;
&lt;p&gt;This example CronJob manifest prints the current time and a hello message every minute:
这个示例中 CronJob 定义的任务是每分钟打印当前时间和一个问候信息:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;applicationjobcronjobyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/application/job/cronjob.yaml&#34; download=&#34;application/job/cronjob.yaml&#34;&gt;
                    &lt;code&gt;application/job/cronjob.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;applicationjobcronjobyaml&#39;)&#34; title=&#34;Copy application/job/cronjob.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;batch/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;CronJob&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hello&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;schedule&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*/1 * * * *&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;jobTemplate&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hello&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;:
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;/bin/sh&lt;/span&gt;
            - -&lt;span style=&#34;color:#ae81ff&#34;&gt;c&lt;/span&gt;
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;date; echo Hello from the Kubernetes cluster&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;OnFailure&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;(&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/job/automated-tasks-with-cron-jobs/&#34;&gt;使用 CronJob 运行自动化任务&lt;/a&gt;
中的示例有更多详细的说明).&lt;/p&gt;
&lt;!--
## CronJob limitations {#cron-job-limitations}

A cron job creates a job object _about_ once per execution time of its schedule. We say &#34;about&#34; because there
are certain circumstances where two jobs might be created, or no job might be created. We attempt to make these rare,
but do not completely prevent them. Therefore, jobs should be _idempotent_.

If `startingDeadlineSeconds` is set to a large value or left unset (the default)
and if `concurrencyPolicy` is set to `Allow`, the jobs will always run
at least once.

For every CronJob, the CronJob &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the job and logs the error

````
Cannot determine if job needs to be started. Too many missed start time (&gt; 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.
````

It is important to note that if the `startingDeadlineSeconds` field is set (not `nil`), the controller counts how many missed jobs occurred from the value of `startingDeadlineSeconds` until now rather than from the last scheduled time until now. For example, if `startingDeadlineSeconds` is `200`, the controller counts how many missed jobs occurred in the last 200 seconds.

A CronJob is counted as missed if it has failed to be created at its scheduled time. For example, If `concurrencyPolicy` is set to `Forbid` and a CronJob was attempted to be scheduled when there was a previous schedule still running, then it would count as missed.

For example, suppose a CronJob is set to schedule a new Job every one minute beginning at `08:30:00`, and its
`startingDeadlineSeconds` field is not set. If the CronJob controller happens to
be down from `08:29:00` to `10:21:00`, the job will not start as the number of missed jobs which missed their schedule is greater than 100.

To illustrate this concept further, suppose a CronJob is set to schedule a new Job every one minute beginning at `08:30:00`, and its
`startingDeadlineSeconds` is set to 200 seconds. If the CronJob controller happens to
be down for the same period as the previous example (`08:29:00` to `10:21:00`,) the Job will still start at 10:22:00. This happens as the controller now checks how many missed schedules happened in the last 200 seconds (ie, 3 missed schedules), rather than from the last scheduled time until now.

The CronJob is only responsible for creating Jobs that match its schedule, and
the Job in turn is responsible for the management of the Pods it represents.
 --&gt;
&lt;h2 id=&#34;cron-job-limitations&#34;&gt;CronJob 局限&lt;/h2&gt;
&lt;p&gt;CronJob 按照计划在每个执行时刻创建 &lt;em&gt;大约&lt;/em&gt; 一个 Job 对象。 这里说 &amp;ldquo;大约&amp;rdquo; 是因为在某些特定的情况下可能会创建两个，或者一个都没有创建。
我们尽量避免这些情况的发生，但是不能实现完全不发生。 因此 Job 应该是 &lt;em&gt;幂等&lt;/em&gt; 的。&lt;/p&gt;
&lt;p&gt;如果 &lt;code&gt;startingDeadlineSeconds&lt;/code&gt; 设置得足够大或不设置(默认)并且 如果 &lt;code&gt;concurrencyPolicy&lt;/code&gt;  设置为 &lt;code&gt;Allow&lt;/code&gt;
这时候 Job 始终至少能运行一次&lt;/p&gt;
&lt;p&gt;如果错过的调度次数大于 100， 则不再启动这个 Job 并向日志输出如下错误信息。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Cannot determine if job needs to be started. Too many missed start time (&amp;gt; 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;一个重要的信息是如果 &lt;code&gt;startingDeadlineSeconds&lt;/code&gt; 字段被设置(不是 &lt;code&gt;nil&lt;/code&gt;)，控制器在计数错过的 Job 的时间是距离当前 &lt;code&gt;startingDeadlineSeconds&lt;/code&gt;和时间而不是上次调度到当前的时间。
例如， 如果 &lt;code&gt;startingDeadlineSeconds&lt;/code&gt; 的值为 &lt;code&gt;200&lt;/code&gt;， 控制器会计数过去 200 秒内错过的次数。&lt;/p&gt;
&lt;p&gt;CronJob 在调度时间创建失败被认为是一个错过计数。 例如， 如果 &lt;code&gt;concurrencyPolicy&lt;/code&gt; 设置为 &lt;code&gt;Forbid&lt;/code&gt;
并且 CronJob 在之前的调度还没有执行完的时间尝试新的调度，就会被认为是错过。&lt;/p&gt;
&lt;p&gt;例如， 假定，一个 CronJob 设置为 从 &lt;code&gt;08:30:00&lt;/code&gt; 开始，每分钟调度一个新的 Job，并且它的
&lt;code&gt;startingDeadlineSeconds&lt;/code&gt; 字段没有设置。 如果 CronJob 控制器恰好在 &lt;code&gt;08:29:00&lt;/code&gt; 到 &lt;code&gt;10:21:00&lt;/code&gt; 挂了，
Job 就会因为没有成功启动而引起没有成功调度的次数就会大于 100.
再假设， 如果一个 CronJob 设置为 从 &lt;code&gt;08:30:00&lt;/code&gt; 开始，每分钟调度一个新的 Job，
&lt;code&gt;startingDeadlineSeconds&lt;/code&gt; 设置为 200 秒。 CronJob 控制器挂掉的时间也是一样 (&lt;code&gt;08:29:00&lt;/code&gt; 到 &lt;code&gt;10:21:00&lt;/code&gt;)，
Job 仍然会在 &lt;code&gt;10:22:00&lt;/code&gt; 启动， 这是因为控制器计算的是过去 200 秒(也就是 3 个错过的调度)发生错过的调度数，而不是从最后一个调度时间到当前的时间。&lt;/p&gt;
&lt;p&gt;CronJob 只负责创建符合它时间表的 Job，然后 Job 负责管理它代表的 Pod。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cron&#34;&gt;Cron 表达式格式&lt;/a&gt;
CronJob &lt;code&gt;schedule&lt;/code&gt; 字段的详细文档.&lt;/p&gt;
&lt;p&gt;更多 CronJob 创建和管理，以及示例，见  &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/job/automated-tasks-with-cron-jobs&#34;&gt;使用 CronJob 运行自动化任务&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: ReplicationController</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/replicationcontroller/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/replicationcontroller/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- bprashanth
- janetkuo
title: ReplicationController
feature:
  title: Self-healing
  anchor: How a ReplicationController Works
  description: &gt;
    Restarts containers that fail, replaces and reschedules containers when nodes die, kills containers that don&#39;t respond to your user-defined health check, and doesn&#39;t advertise them to clients until they are ready to serve.

content_type: concept
weight: 90
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--  
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/deployment/&#34;&gt;&lt;code&gt;Deployment&lt;/code&gt;&lt;/a&gt; that configures a &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/replicaset/&#34;&gt;&lt;code&gt;ReplicaSet&lt;/code&gt;&lt;/a&gt; is now the recommended way to set up replication.&lt;/div&gt;
&lt;/blockquote&gt;


A _ReplicationController_ ensures that a specified number of pod replicas are running at any one
time. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is
always up and available.
--&gt;
&lt;p&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#34;&gt;&lt;code&gt;Deployment&lt;/code&gt;&lt;/a&gt; 管理
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#34;&gt;&lt;code&gt;ReplicaSet&lt;/code&gt;&lt;/a&gt;
是目前推荐的运行多副本的方式。&lt;/div&gt;
&lt;/blockquote&gt;

``
&lt;em&gt;ReplicationController&lt;/em&gt; 确保在任意时刻都有指定数量的 Pod 副本在运行， 换句话来说
就是 &lt;code&gt;ReplicationController&lt;/code&gt; 保证一个 Pod 或 一组同质 Pod 的集群始终运行并可用&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## How a ReplicationController Works

If there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the
ReplicationController starts more pods. Unlike manually created pods, the pods maintained by a
ReplicationController are automatically replaced if they fail, are deleted, or are terminated.
For example, your pods are re-created on a node after disruptive maintenance such as a kernel upgrade.
For this reason, you should use a ReplicationController even if your application requires
only a single pod. A ReplicationController is similar to a process supervisor,
but instead of supervising individual processes on a single node, the ReplicationController supervises multiple pods
across multiple nodes.

ReplicationController is often abbreviated to &#34;rc&#34; in discussion, and as a shortcut in
kubectl commands.

A simple case is to create one ReplicationController object to reliably run one instance of
a Pod indefinitely.  A more complex use case is to run several identical replicas of a replicated
service, such as web servers.
 --&gt;
&lt;h2 id=&#34;replicationcontroller-是怎么工作的&#34;&gt;ReplicationController 是怎么工作的&lt;/h2&gt;
&lt;p&gt;如果同时运行的 Pod 太多了， ReplicationController 终止多余的 Pod。
如果同时运行的 Pod 太少了， ReplicationController 启动更多的 Pod。
与手动创建 Pod 不同， 由 ReplicationController 维护的 Pod 在挂掉，被删除，或被终止都会自动被替换。
例如， Pod 因为所在的节点升级内核引起维护故障而被重新创建。因为这些原因，即便应用只需要一单个 Pod 也应该使用 ReplicationController。
ReplicationController 与进程监督类似，但是与只监督一个节点上的单个线程不同，
ReplicationController 监督多个节点上的多个 Pod。&lt;/p&gt;
&lt;p&gt;简单的一个场景为创建一个 ReplicationController 对象来运行一个不限期运行的单实例 Pod。
复杂的一个应用场景为运行一个多副本应用的多个副本，如 web 服务。&lt;/p&gt;
&lt;!--
## Running an example ReplicationController

This example ReplicationController config runs three copies of the nginx web server.



 













&lt;table class=&#34;includecode&#34; id=&#34;controllersreplicationyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/replication.yaml&#34; download=&#34;controllers/replication.yaml&#34;&gt;
                    &lt;code&gt;controllers/replication.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersreplicationyaml&#39;)&#34; title=&#34;Copy controllers/replication.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicationController&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



Run the example job by downloading the example file and then running this command:

```shell
kubectl apply -f https://k8s.io/examples/controllers/replication.yaml
```
```
replicationcontroller/nginx created
```

Check on the status of the ReplicationController using this command:

```shell
kubectl describe replicationcontrollers/nginx
```
```
Name:        nginx
Namespace:   default
Selector:    app=nginx
Labels:      app=nginx
Annotations:    &lt;none&gt;
Replicas:    3 current / 3 desired
Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       app=nginx
  Containers:
   nginx:
    Image:              nginx
    Port:               80/TCP
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  FirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message
  ---------       --------     -----    ----                        -------------    ----      ------              -------
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v
```

Here, three pods are created, but none is running yet, perhaps because the image is being pulled.
A little later, the same command may show:

```shell
Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed
```

To list all the pods that belong to the ReplicationController in a machine readable form, you can use a command like this:

```shell
pods=$(kubectl get pods --selector=app=nginx --output=jsonpath={.items..metadata.name})
echo $pods
```
```
nginx-3ntk0 nginx-4ok8v nginx-qrm3m
```

Here, the selector is the same as the selector for the ReplicationController (seen in the
`kubectl describe` output), and in a different form in `replication.yaml`.  The `--output=jsonpath` option
specifies an expression that just gets the name from each pod in the returned list.
 --&gt;
&lt;h2 id=&#34;一个-replicationcontroller-的示例&#34;&gt;一个 ReplicationController 的示例&lt;/h2&gt;
&lt;p&gt;这是一个运行三个 nginx web 服务副本的 ReplicationController 配置：&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;controllersreplicationyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/replication.yaml&#34; download=&#34;controllers/replication.yaml&#34;&gt;
                    &lt;code&gt;controllers/replication.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersreplicationyaml&#39;)&#34; title=&#34;Copy controllers/replication.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicationController&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;下载这个示例配置文件，并运行以下命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f replication.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果为:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;replicationcontroller/nginx created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;使用以下命令查看 ReplicationController 的状态:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe replicationcontrollers/nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:        nginx
Namespace:   default
Selector:    app=nginx
Labels:      app=nginx
Annotations:    &amp;lt;none&amp;gt;
Replicas:    3 current / 3 desired
Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       app=nginx
  Containers:
   nginx:
    Image:              nginx
    Port:               80/TCP
    Environment:        &amp;lt;none&amp;gt;
    Mounts:             &amp;lt;none&amp;gt;
  Volumes:              &amp;lt;none&amp;gt;
Events:
  FirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message
  ---------       --------     -----    ----                        -------------    ----      ------              -------
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这时，三个 Pod 都已经衩上创建， 但还没有一个在运行，可能是因为还在拉镜像。
再过一会，同一条命令的输出结果可能如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以机器可读的横笔列举所以属于 ReplicationController Pod，可运行以下命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;pods&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;kubectl get pods --selector&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;app&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx --output&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;jsonpath&lt;span style=&#34;color:#f92672&#34;&gt;={&lt;/span&gt;.items..metadata.name&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;
echo $pods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nginx-3ntk0 nginx-4ok8v nginx-qrm3m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里使用的选择器与 ReplicationController 的选择器相同(可以通过 &lt;code&gt;kubectl describe&lt;/code&gt; 输出验证)，
与 &lt;code&gt;replication.yaml&lt;/code&gt; 的格式不一样。&lt;code&gt;--output=jsonpath&lt;/code&gt; 指定了一个表达式， 表示只返回每个 Pod 的名称为一个列表。&lt;/p&gt;
&lt;!--
## Writing a ReplicationController Spec

As with all other Kubernetes config, a ReplicationController needs `apiVersion`, `kind`, and `metadata` fields.
The name of a ReplicationController object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
For general information about working with config files, see [object management ](/docs/concepts/overview/working-with-objects/object-management/).

A ReplicationController also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).
 --&gt;
&lt;h2 id=&#34;编写-replicationcontroller-spec&#34;&gt;编写 ReplicationController Spec&lt;/h2&gt;
&lt;p&gt;与所以其它的 k8s 配置一样， ReplicationController 必要字段有 &lt;code&gt;apiVersion&lt;/code&gt;, &lt;code&gt;kind&lt;/code&gt;, &lt;code&gt;metadata&lt;/code&gt;。
ReplicationController 对象的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.
更多编写配置文件所需要的信息，见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/object-management/&#34;&gt;对象管理 &lt;/a&gt;.
ReplicationController 还需要一个 &lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;&lt;code&gt;.spec&lt;/code&gt; 配置区&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### Pod Template

The `.spec.template` is the only required field of the `.spec`.

The `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;, except it is nested and does not have an `apiVersion` or `kind`.

In addition to required fields for a Pod, a pod template in a ReplicationController must specify appropriate
labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See [pod selector](#pod-selector).

Only a [`.spec.template.spec.restartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Always` is allowed, which is the default if not specified.

For local container restarts, ReplicationControllers delegate to an agent on the node,
for example the [Kubelet](/docs/reference/command-line-tools-reference/kubelet/) or Docker.
 --&gt;
&lt;h3 id=&#34;pod-模板&#34;&gt;Pod 模板&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.template&lt;/code&gt; 是 &lt;code&gt;.spec&lt;/code&gt; 的唯一必要字段。&lt;/p&gt;
&lt;p&gt;与 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;， 除了因为嵌套没有 &lt;code&gt;apiVersion&lt;/code&gt; 或 &lt;code&gt;kind&lt;/code&gt;外完全相同的结构。&lt;/p&gt;
&lt;p&gt;写 Pod 相比额外的必要字段是 ReplicationController 的 Pod 模板必须要指定恰当的标签和一个恰当的重启策略。
对于标签，需要确保不能与其他控制器重叠。 见 &lt;a href=&#34;#pod-selector&#34;&gt;pod 选择器&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy&#34;&gt;&lt;code&gt;.spec.template.spec.restartPolicy&lt;/code&gt;&lt;/a&gt;
的值只能是 &lt;code&gt;Always&lt;/code&gt;， 也是不指定时的默认值。&lt;/p&gt;
&lt;p&gt;ReplicationControllers 委任节点上的代理，如，
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/kubelet/&#34;&gt;Kubelet&lt;/a&gt;
或 Docker 来实现对于本地容器的重启。&lt;/p&gt;
&lt;!--
### Labels on the ReplicationController

The ReplicationController can itself have labels (`.metadata.labels`).  Typically, you
would set these the same as the `.spec.template.metadata.labels`; if `.metadata.labels` is not specified
then it defaults to  `.spec.template.metadata.labels`.  However, they are allowed to be
different, and the `.metadata.labels` do not affect the behavior of the ReplicationController.
 --&gt;
&lt;h3 id=&#34;replicationcontroller-的标签&#34;&gt;ReplicationController 的标签&lt;/h3&gt;
&lt;p&gt;ReplicationController 本身可以有标签(&lt;code&gt;.metadata.labels&lt;/code&gt;)。 通常与 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt; 设置相同的内容。
如果 &lt;code&gt;.metadata.labels&lt;/code&gt; 没有设置，则默认会设置与 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt; 相同。
但是，它们之间的内容可以不同， &lt;code&gt;.metadata.labels&lt;/code&gt; 不会影响 ReplicationController 的行为。&lt;/p&gt;
&lt;!--
### Pod Selector

The `.spec.selector` field is a [label selector](/docs/concepts/overview/working-with-objects/labels/#label-selectors). A ReplicationController
manages all the pods with labels that match the selector. It does not distinguish
between pods that it created or deleted and pods that another person or process created or
deleted. This allows the ReplicationController to be replaced without affecting the running pods.

If specified, the `.spec.template.metadata.labels` must be equal to the `.spec.selector`, or it will
be rejected by the API.  If `.spec.selector` is unspecified, it will be defaulted to
`.spec.template.metadata.labels`.

Also you should not normally create any pods whose labels match this selector, either directly, with
another ReplicationController, or with another controller such as Job. If you do so, the
ReplicationController thinks that it created the other pods.  Kubernetes does not stop you
from doing this.

If you do end up with multiple controllers that have overlapping selectors, you
will have to manage the deletion yourself (see [below](#working-with-replicationcontrollers)).
 --&gt;
&lt;h3 id=&#34;pod-selector&#34;&gt;Pod 选择器&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.selector&lt;/code&gt; 字段是一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/#label-selectors&#34;&gt;标签选择器&lt;/a&gt;.
ReplicationController 会管理所有匹配它的选择器的 Pod。 不会区分这些 Pod 是由它自己创建或删除
还是由别的人或进行创建或删除的 Pod。 这让 ReplicationController 可以在不影响运行 Pod 的情况下被替换。&lt;/p&gt;
&lt;p&gt;如果设置 &lt;code&gt;.spec.selector&lt;/code&gt;，则它的内容必须要与 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt; 相同，
否则会被 API 拒绝。 如果没有设置 &lt;code&gt;.spec.selector&lt;/code&gt;， 则会被默认设置为 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt; 的内容。&lt;/p&gt;
&lt;p&gt;用户通常不应该再创建能被这个选择匹配标签的 Pod， 无论是直接创建还是通过另一个 ReplicationController，
或通过其它的诸如 Job 的控制器。 如果这样做， ReplicationController 也会把这些 Pod 认为是它的。
k8s 不会阻止用户这样做。&lt;/p&gt;
&lt;p&gt;如果最终有多个控制器拥有重叠的选择器，用户需要自己来管理删除(见 &lt;a href=&#34;#working-with-replicationcontrollers&#34;&gt;下面&lt;/a&gt;)&lt;/p&gt;
&lt;!--
### Multiple Replicas

You can specify how many pods should run concurrently by setting `.spec.replicas` to the number
of pods you would like to have running concurrently.  The number running at any time may be higher
or lower, such as if the replicas were just increased or decreased, or if a pod is gracefully
shutdown, and a replacement starts early.

If you do not specify `.spec.replicas`, then it defaults to 1.
 --&gt;
&lt;h3 id=&#34;多副本&#34;&gt;多副本&lt;/h3&gt;
&lt;p&gt;用户可以通过设置 &lt;code&gt;.spec.replicas&lt;/code&gt; 来指定同时运行的 Pod 的数量。 在任意时间内运行的 Pod 的数量
可能比这个值大也可能小， 例如，刚好的增加或减少副本数量， 或一个 Pod 正在被平滑地关闭，而它的替代者先启动起来了。&lt;/p&gt;
&lt;p&gt;如果没有指定 &lt;code&gt;.spec.replicas&lt;/code&gt;， 则默认为 1&lt;/p&gt;
&lt;!--
## Working with ReplicationControllers

### Deleting a ReplicationController and its Pods

To delete a ReplicationController and all its pods, use [`kubectl
delete`](/docs/reference/generated/kubectl/kubectl-commands#delete).  Kubectl will scale the ReplicationController to zero and wait
for it to delete each pod before deleting the ReplicationController itself.  If this kubectl
command is interrupted, it can be restarted.

When using the REST API or go client library, you need to do the steps explicitly (scale replicas to
0, wait for pod deletions, then delete the ReplicationController).

### Deleting just a ReplicationController

You can delete a ReplicationController without affecting any of its pods.

Using kubectl, specify the `--cascade=false` option to [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete).

When using the REST API or go client library, simply delete the ReplicationController object.

Once the original is deleted, you can create a new ReplicationController to replace it.  As long
as the old and new `.spec.selector` are the same, then the new one will adopt the old pods.
However, it will not make any effort to make existing pods match a new, different pod template.
To update pods to a new spec in a controlled way, use a [rolling update](#rolling-updates).

### Isolating pods from a ReplicationController

Pods may be removed from a ReplicationController&#39;s target set by changing their labels. This technique may be used to remove pods from service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (assuming that the number of replicas is not also changed).
 --&gt;
&lt;h2 id=&#34;working-with-replicationcontrollers&#34;&gt;ReplicationController 的使用&lt;/h2&gt;
&lt;h3 id=&#34;删除-replicationcontroller-及其-pod&#34;&gt;删除 ReplicationController 及其 Pod&lt;/h3&gt;
&lt;p&gt;要删除一个 ReplicationController 及其所有的 Pod 可以通过命令
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#delete&#34;&gt;&lt;code&gt;kubectl delete&lt;/code&gt;&lt;/a&gt;.
kubectl 会将 ReplicationController 的副本数缩减为 0， 然后等待删除掉它的每一个 Pod，最后删除
ReplicationController 本身。 如果这个 kubectl 命令被打断，可以被重新启动。&lt;/p&gt;
&lt;p&gt;当使用 REST API 或 go 客户端库时， 用户需要显示地进行这两个步骤(将副本数缩减为0，然后等待 Pod 被删除后，
再删除 ReplicationController)。&lt;/p&gt;
&lt;h3 id=&#34;仅删除-replicationcontroller-对象&#34;&gt;仅删除 ReplicationController 对象&lt;/h3&gt;
&lt;p&gt;用户可以只删除 ReplicationController 而不影响任何它的 Pod。&lt;/p&gt;
&lt;p&gt;在使用 kubectl 时，添加 &lt;code&gt;--cascade=false&lt;/code&gt; 选项到  &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#delete&#34;&gt;&lt;code&gt;kubectl delete&lt;/code&gt;&lt;/a&gt;.
在使用 REST API 或 go 客户端库， 直接删除 ReplicationController 对象。
当原来的对象被删除后，可以再创建一个新的 ReplicationController 来替代它。只要原来的对象和新创建的对象
拥有相同的 &lt;code&gt;.spec.selector&lt;/code&gt;， 新的对象就会接管旧的 Pod。 但是，它不会尝试对旧的 Pod 做任何修改以适应新的不同的 Pod 模板。
要受控地更新 Pod 的配置，需要使用 &lt;a href=&#34;#rolling-updates&#34;&gt;滚动更新&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;从-replicationcontroller-剥离-pod&#34;&gt;从 ReplicationController 剥离 Pod&lt;/h3&gt;
&lt;p&gt;可以通过修改 Pod 标签的方式将 Pod 从 ReplicationController 中移出来。 通过这种方式可以将
Pod 从服务从移出来用作测试，数据恢复等。 通过这种方式移除的 Pod 会自动地被替代(假设副本数没有同步修改)&lt;/p&gt;
&lt;!--
## Common usage patterns

### Rescheduling

As mentioned above, whether you have 1 pod you want to keep running, or 1000, a ReplicationController will ensure that the specified number of pods exists, even in the event of node failure or pod termination (for example, due to an action by another control agent).

### Scaling

The ReplicationController makes it easy to scale the number of replicas up or down, either manually or by an auto-scaling control agent, by simply updating the `replicas` field.
 --&gt;
&lt;h2 id=&#34;常见使用方式&#34;&gt;常见使用方式&lt;/h2&gt;
&lt;h3 id=&#34;重新调度&#34;&gt;重新调度&lt;/h3&gt;
&lt;p&gt;就是上面掉到的，无认是想运行 1 个 Pod 还是 1000 个， ReplicationController 就会确保指定数量
的 Pod 存在。 即便某个节点挂了或 Pod被终止(例如，由于其它控制代理的操作)&lt;/p&gt;
&lt;h3 id=&#34;容量变更&#34;&gt;容量变更&lt;/h3&gt;
&lt;p&gt;ReplicationController 使得对副本数量的增加或减少变得很容易， 只需要手动或自动容量控制程序来更新一个 &lt;code&gt;replicas&lt;/code&gt; 字段。&lt;/p&gt;
&lt;!--
### Rolling updates

The ReplicationController is designed to facilitate rolling updates to a service by replacing pods one-by-one.

As explained in [#1353](https://issue.k8s.io/1353), the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.

Ideally, the rolling update controller would take application readiness into account, and would ensure that a sufficient number of pods were productively serving at any given time.

The two ReplicationControllers would need to create pods with at least one differentiating label, such as the image tag of the primary container of the pod, since it is typically image updates that motivate rolling updates.
 --&gt;
&lt;h3 id=&#34;rolling-updates&#34;&gt;滚动更新&lt;/h3&gt;
&lt;p&gt;ReplicationController 设计的初衷便是通过一个一个替换 服务的 Pod 的方式实现滚动更新。&lt;/p&gt;
&lt;p&gt;就如 &lt;a href=&#34;https://issue.k8s.io/1353&#34;&gt;#1353&lt;/a&gt; 解释的那样。推荐的方是创建一个新的包含一个副本的 ReplicationController。
控制器一次一次地将 新的(+1) 然后 旧的 (-1)， 当旧的副本数降为 0 后再把它本身删除。 通过这种可预期的方式更新那认为是非预期的故障。&lt;/p&gt;
&lt;p&gt;理想情况下，滚动更新控制会考虑应用的就绪探针， 会确保有足够数量的 Pod 在任何时间都能有效的提供服务。&lt;/p&gt;
&lt;p&gt;这两个 ReplicationController 创建的 Pod 至少有一个标签是不同的， 例如 Pod 主要容器的镜像标签
因为通过滚动更新的动力就是更新镜像的版本。&lt;/p&gt;
&lt;!--
### Multiple release tracks

In addition to running multiple releases of an application while a rolling update is in progress, it&#39;s common to run multiple releases for an extended period of time, or even continuously, using multiple release tracks. The tracks would be differentiated by labels.

For instance, a service might target all pods with `tier in (frontend), environment in (prod)`.  Now say you have 10 replicated pods that make up this tier.  But you want to be able to &#39;canary&#39; a new version of this component.  You could set up a ReplicationController with `replicas` set to 9 for the bulk of the replicas, with labels `tier=frontend, environment=prod, track=stable`, and another ReplicationController with `replicas` set to 1 for the canary, with labels `tier=frontend, environment=prod, track=canary`.  Now the service is covering both the canary and non-canary pods.  But you can mess with the ReplicationControllers separately to test things out, monitor the results, etc.
 --&gt;
&lt;h3 id=&#34;多版本发布&#34;&gt;多版本发布&lt;/h3&gt;
&lt;p&gt;在滚动更新过程中运行多个版本的基础上，通常也可能运行多个版本一段时间，或长久的运行多个版本。 这些版本之间通过标签来区分。&lt;/p&gt;
&lt;p&gt;比如一个 Service 的标签选择器为 &lt;code&gt;tier in (frontend), environment in (prod)&lt;/code&gt;。 这时候假设这一层由 10 个副本的 Pod 组成。
此时想要在该组件发布一个&amp;quot;金丝雀&amp;quot;的新版本. 这时可以配置一个 9 副本的 ReplicationController 打标签为
&lt;code&gt;tier=frontend, environment=prod, track=stable&lt;/code&gt;， 再配置一个 1 副本的 ReplicationController
打标签为 &lt;code&gt;tier=frontend, environment=prod, track=canary&lt;/code&gt;。 此时 Service 能匹配所以
金丝雀版本和稳定版本的 Pod。 但却可以分不同的 ReplicationController 进行测试和查看监控结果等。&lt;/p&gt;
&lt;!--
### Using ReplicationControllers with Services

Multiple ReplicationControllers can sit behind a single service, so that, for example, some traffic
goes to the old version, and some goes to the new version.

A ReplicationController will never terminate on its own, but it isn&#39;t expected to be as long-lived as services. Services may be composed of pods controlled by multiple ReplicationControllers, and it is expected that many ReplicationControllers may be created and destroyed over the lifetime of a service (for instance, to perform an update of pods that run the service). Both services themselves and their clients should remain oblivious to the ReplicationControllers that maintain the pods of the services.
 --&gt;
&lt;h3 id=&#34;replicationcontroller-配合-service-使用&#34;&gt;ReplicationController 配合 Service 使用&lt;/h3&gt;
&lt;p&gt;Multiple ReplicationControllers can sit behind a single service, so that, for example, some traffic
goes to the old version, and some goes to the new version.
多个 ReplicationController 可以挂在一个 Service 下面， 这样可以让有些流量到新的版本，有的流量到旧的版本。&lt;/p&gt;
&lt;p&gt;ReplicationController 永远都不会把自己干掉， 但也没有期望它可以和 Service 活得一样久。
Service 可能由多个 ReplicationController 控制的 Pod 组成。
在 Service 的生存期内或心预料有许多 ReplicationController 被创建另一些被删除
(例如， 更新 Service 运行的 Pod)。 Service 和它的客户端都不会注意到这些维护 Service Pod 的
ReplicationController。&lt;/p&gt;
&lt;!--
## Writing programs for Replication

Pods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the [RabbitMQ work queues](https://www.rabbitmq.com/tutorials/tutorial-two-python.html), as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for example, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself.
 --&gt;
&lt;!--
## Writing programs for Replication

Pods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the [RabbitMQ work queues](https://www.rabbitmq.com/tutorials/tutorial-two-python.html), as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for example, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself.
  --&gt;
&lt;h2 id=&#34;编写多副本程序&#34;&gt;编写多副本程序&lt;/h2&gt;
&lt;p&gt;ReplicationController 创建的 Pod 在设计上是能够替换的和语义相同的， 尽管经过时间的推移它们的配置可能变得不同。
这很明显适合于无状态多副本服务， 但 ReplicationControllers 也可以用于维护 主选举，分片，
工作池应用的可用性。 这些应用应该有动态的工作分配机制， 例如
&lt;a href=&#34;https://www.rabbitmq.com/tutorials/tutorial-two-python.html&#34;&gt;RabbitMQ 工作队列&lt;/a&gt;,
这与静态的一次性对每个 Pod 的自定义配置相反， 被认为是反范式。
任意对 Pod 的自定义操作，如资源(如，CPU或RAM)资源的垂直自动容量调整， 应该通过另一个
在线控制器程序来执行， 而不是 ReplicationController 本身。
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;!--
## Responsibilities of the ReplicationController

The ReplicationController simply ensures that the desired number of pods matches its label selector and are operational. Currently, only terminated pods are excluded from its count. In the future, [readiness](https://issue.k8s.io/620) and other information available from the system may be taken into account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external clients to implement arbitrarily sophisticated replacement and/or scale-down policies.

The ReplicationController is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in [#492](https://issue.k8s.io/492)), which would change its `replicas` field. We will not add scheduling policies (for example, [spreading](https://issue.k8s.io/367#issuecomment-48428019)) to the ReplicationController. Nor should it verify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated processes. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere. We even plan to factor out the mechanism for bulk pod creation ([#170](https://issue.k8s.io/170)).

The ReplicationController is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The &#34;macro&#34; operations currently supported by kubectl (run, scale) are proof-of-concept examples of this. For instance, we could imagine something like [Asgard](https://techblog.netflix.com/2012/06/asgard-web-based-cloud-management-and.html) managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.
 --&gt;
&lt;h2 id=&#34;replicationcontroller-的职责&#34;&gt;ReplicationController 的职责&lt;/h2&gt;
&lt;p&gt;ReplicationController 只是确保匹配其标签选择器的 Pod 的数量与期望值相同，并且可用。
目前，只有被终止的 Pod 才不会被计算。 在将来，&lt;a href=&#34;https://issue.k8s.io/620&#34;&gt;就绪探针&lt;/a&gt;
和其它来自系统的可用性信息也会被计入， 我们还可能添加由通过替代策略的控制，
我们还计划发出可以用于外部客户端实现任意广泛代替和缩减容量策略的事件。&lt;/p&gt;
&lt;p&gt;ReplicationController 永远被限制在这个小的责任范围内。 它本身也不会执行就绪探针或存活探针。
不执行自动容量控制， 在设计上就由外部自动容量控制器来控制(就如 &lt;a href=&#34;https://issue.k8s.io/492&#34;&gt;#492&lt;/a&gt; 中讨论的一样)
来修改它的 &lt;code&gt;replicas&lt;/code&gt; 字段。 我们不会添加调度策略(例如，&lt;a href=&#34;https://issue.k8s.io/367#issuecomment-48428019&#34;&gt;spreading&lt;/a&gt;)
到 ReplicationController， 也不会让它来验证它控制的 Pod 是否与当前的 Pod 模板一致，因为这会
妨碍自动容量这得和其它的自动化管理。 类似的 完成死线，顺序信赖，配置扩展和属于其它地方的其它鹅。
我们甚至计划把批量创建 Pod 的机制拆出来。 (&lt;a href=&#34;https://issue.k8s.io/170&#34;&gt;#170&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;ReplicationController 设计上就是一个可组合的构建基础模块。我个预期的是在未来使用更高级的 API 或
工具基于它和其它的互补基础来为用户提供便利。 目前由 kubectl (run, scale) 的宏操作就是证明。
目前我们能想到的一些时，如 &lt;a href=&#34;https://techblog.netflix.com/2012/06/asgard-web-based-cloud-management-and.html&#34;&gt;Asgard&lt;/a&gt;
管理 ReplicationControllers，自动容量管理器， Service, 调度策略，金丝雀等。&lt;/p&gt;
&lt;!--
## API Object

Replication controller is a top-level resource in the Kubernetes REST API. More details about the
API object can be found at:
[ReplicationController API object](/docs/reference/generated/kubernetes-api/v1.19/#replicationcontroller-v1-core).
 --&gt;
&lt;h2 id=&#34;api-对象&#34;&gt;API 对象&lt;/h2&gt;
&lt;p&gt;ReplicationController 是一个在 k8s REST API 中的顶级资源。更多关于 API 对象的信息见:
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#replicationcontroller-v1-core&#34;&gt;ReplicationController API 对象&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Alternatives to ReplicationController

### ReplicaSet

[`ReplicaSet`](/docs/concepts/workloads/controllers/replicaset/) is the next-generation ReplicationController that supports the new [set-based label selector](/docs/concepts/overview/working-with-objects/labels/#set-based-requirement).
It&#39;s mainly used by [Deployment](/docs/concepts/workloads/controllers/deployment/) as a mechanism to orchestrate pod creation, deletion and updates.
Note that we recommend using Deployments instead of directly using Replica Sets, unless you require custom update orchestration or don&#39;t require updates at all.


### Deployment (Recommended)

[`Deployment`](/docs/concepts/workloads/controllers/deployment/) is a higher-level API object that updates its underlying Replica Sets and their Pods. Deployments are recommended if you want this rolling update functionality because, they are declarative, server-side, and have additional features.

### Bare Pods

Unlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node.  A ReplicationController delegates local container restarts to some agent on the node (for example, Kubelet or Docker).

### Job

Use a [`Job`](/docs/concepts/workloads/controllers/job/) instead of a ReplicationController for pods that are expected to terminate on their own
(that is, batch jobs).

### DaemonSet

Use a [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) instead of a ReplicationController for pods that provide a
machine-level function, such as machine monitoring or machine logging.  These pods have a lifetime that is tied
to a machine lifetime: the pod needs to be running on the machine before other pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.
 --&gt;
&lt;h2 id=&#34;replicationcontroller-替代方案&#34;&gt;ReplicationController 替代方案&lt;/h2&gt;
&lt;h3 id=&#34;replicaset&#34;&gt;ReplicaSet&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#34;&gt;&lt;code&gt;ReplicaSet&lt;/code&gt;&lt;/a&gt;&lt;br&gt;
是下一代的 ReplicationController， 它支持新的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/#set-based-requirement&#34;&gt;基于集合的标签选择器&lt;/a&gt;.
它主要被 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#34;&gt;Deployment&lt;/a&gt; 用于编排 Pod 创建，删除，更新的一个机制。
还要注意的是我们推荐使用 Deployment 而不是直接使用 ReplicaSet，除非你需要自定义的更新编排或者完全不需要更新。&lt;/p&gt;
&lt;h3 id=&#34;deployment-推荐&#34;&gt;Deployment (推荐)&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#34;&gt;&lt;code&gt;Deployment&lt;/code&gt;&lt;/a&gt;
是一个更高级别的 API 对象，用于更新它下层的 ReplicaSet 和它们的 Pod。
如果想要使用滚动更新推荐使用 Deployment， 因为它们是声明式的，服务端的，还有其它额外的特性。&lt;/p&gt;
&lt;h3 id=&#34;裸-pod&#34;&gt;裸 Pod&lt;/h3&gt;
&lt;p&gt;与直接创建 Pod 的情况不同， ReplicationController 会替换那些因任意原为被删除或终止的 Pod，
例如节点掉挂，或因为升级内格而维护。 因此我们推荐即便应用只需要一个 Pod 也使用 ReplicationController。
可以认为它是一个进程监督者，只是它监督的是多个节点上的多个 Pod 而不是一个节点上的一个进程。
ReplicationController 委托节点上的一些代理(如 kubelet 或 Docker)来重启本地容器。&lt;/p&gt;
&lt;h3 id=&#34;job&#34;&gt;Job&lt;/h3&gt;
&lt;p&gt;如果 Pod 计划中会自己终止(如，批量任务)就应该使用&lt;br&gt;
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/job/&#34;&gt;&lt;code&gt;Job&lt;/code&gt;&lt;/a&gt;
而不是 ReplicationController&lt;/p&gt;
&lt;h3 id=&#34;daemonset&#34;&gt;DaemonSet&lt;/h3&gt;
&lt;p&gt;Use a &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/daemonset/&#34;&gt;&lt;code&gt;DaemonSet&lt;/code&gt;&lt;/a&gt; instead of a ReplicationController for pods that provide a
machine-level function, such as machine monitoring or machine logging.  These pods have a lifetime that is tied
to a machine lifetime: the pod needs to be running on the machine before other pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.&lt;/p&gt;
&lt;p&gt;如果 Pod 提供的是机器级别的功能，如机器监控，机器日志。就应该使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/daemonset/&#34;&gt;&lt;code&gt;DaemonSet&lt;/code&gt;&lt;/a&gt;
而不是 ReplicationController。 这些 Pod 的生命期与机器绑定: 这些 Pod 需要先于其它的 Pod
在机器上运行， 且它们只有在机器准备重启或关机时才是终止的时候。&lt;/p&gt;
&lt;h2 id=&#34;更多信息&#34;&gt;更多信息&lt;/h2&gt;
&lt;p&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/run-stateless-application-deployment/&#34;&gt;使用 Deployment 运行无状态应用&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
