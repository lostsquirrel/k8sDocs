<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes – Documentation</title>
    <link>https://lostsquirrel.github.io/k8sDocs/docs/</link>
    <description>Recent content in Documentation on Kubernetes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    
	  <atom:link href="https://lostsquirrel.github.io/k8sDocs/docs/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Pod 生命周期</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/</guid>
      <description>
        
        
        &lt;!--
---
title: Pod Lifecycle
content_type: concept
weight: 30
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This page describes the lifecycle of a Pod. Pods follow a defined lifecycle, starting
in the `Pending` [phase](#pod-phase), moving through `Running` if at least one
of its primary containers starts OK, and then through either the `Succeeded` or
`Failed` phases depending on whether any container in the Pod terminated in failure.

Whilst a Pod is running, the kubelet is able to restart containers to handle some
kind of faults. Within a Pod, Kubernetes tracks different container
[states](#container-states) and handles

In the Kubernetes API, Pods have both a specification and an actual status. The
status for a Pod object consists of a set of [Pod conditions](#pod-conditions).
You can also inject [custom readiness information](#pod-readiness-gate) into the
condition data for a Pod, if that is useful to your application.

Pods are only [scheduled](/k8sDocs/docs/concepts/scheduling-eviction/) once in their lifetime.
Once a Pod is scheduled (assigned) to a Node, the Pod runs on that Node until it stops
or is [terminated](#pod-termination).


 --&gt;
&lt;p&gt;本文介经 Pod 的生命周期。 Pod 有一个既定的生命周期，开启后进行 &lt;code&gt;Pending&lt;/code&gt; &lt;a href=&#34;#pod-phase&#34;&gt;阶段&lt;/a&gt;，
当其中至少有一个主要容器正常启动后变更为 &lt;code&gt;Running&lt;/code&gt; 阶段， 如果所有容器全部正常启动则进入 &lt;code&gt;Succeeded&lt;/code&gt; 阶段，
如果有任意容器启动失败则进行 &lt;code&gt;Failed&lt;/code&gt; 阶段。&lt;/p&gt;
&lt;p&gt;当一个 Pod 在运行中， kubelet 可以在某些情况下容器挂掉后将其重启。 在 Pod 中， k8s 会跟踪和处理容器的 &lt;a href=&#34;#container-states&#34;&gt;状态&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在 k8s 的 API 对象中， Pod 对象拥有定义明细和实时状态。 Pod 对象的状态上包含一系列 &lt;a href=&#34;#pod-conditions&#34;&gt;Pod 条件&lt;/a&gt;
如果应用有需要，可以向 Pod 中加入 &lt;a href=&#34;#pod-readiness-gate&#34;&gt;自定义就绪信息&lt;/a&gt; 到条件子对象。&lt;/p&gt;
&lt;p&gt;在 Pod 的整个生命周期中只会被&lt;a href=&#34;../../../scheduling-eviction/&#34;&gt;调度&lt;/a&gt;一次，
当一个 Pod 被调度(分配)到一个节点后，就会一直运行在这个节点上，直接被停止或被&lt;a href=&#34;#pod-termination&#34;&gt;终止&lt;/a&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Pod lifetime

Like individual application containers, Pods are considered to be relatively
ephemeral (rather than durable) entities. Pods are created, assigned a unique
ID ([UID](/k8sDocs/docs/concepts/overview/working-with-objects/names/#uids)), and scheduled
to nodes where they remain until termination (according to restart policy) or
deletion.  
If a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;节点&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; dies, the Pods scheduled to that node
are [scheduled for deletion](#pod-garbage-collection) after a timeout period.

Pods do not, by themselves, self-heal. If a Pod is scheduled to a
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;node&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; that then fails,
or if the scheduling operation itself fails, the Pod is deleted; likewise, a Pod won&#39;t
survive an eviction due to a lack of resources or Node maintenance. Kubernetes uses a
higher-level abstraction, called a
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;controller&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt;, that handles the work of
managing the relatively disposable Pod instances.

A given Pod (as defined by a UID) is never &#34;rescheduled&#34; to a different node; instead,
that Pod can be replaced by a new, near-identical Pod, with even the same name i
desired, but with a different UID.

When something is said to have the same lifetime as a Pod, such as a
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;volume&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;,
that means that the thing exists as long as that specific Pod (with that exact UID)
exists. If that Pod is deleted for any reason, and even if an identical replacement
is created, the related thing (a volume, in this example) is also destroyed and
created anew.

&lt;figure&gt;
    &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/images/docs/pod.svg&#34; width=&#34;50%&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Pod diagram&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


*A multi-container Pod that contains a file puller and a
web server that uses a persistent volume for shared storage between the containers.*

 --&gt;
&lt;h2 id=&#34;pod-的一生&#34;&gt;Pod 的一生&lt;/h2&gt;
&lt;p&gt;与单独使用应用容器一样, Pod 可以被认为是一个相对临时(而不是长期存在)的实体. Pod 在创建时会被分配
一个唯一的 ID(&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names/#uids&#34;&gt;UID&lt;/a&gt;),
然后被到一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;node&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; 上,直到被终止(依照重启策略)或者被删除.
如果一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;node&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; 挂了, 这个节点上的 Pod 会在超时后
&lt;a href=&#34;#pod-garbage-collection&#34;&gt;因删除被调度&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Pod 并不能独自实现自愈. 如果 Pod 被调度的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;node&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; 挂了,
或者是调度操作本身失败, Pod 就被删除了, Pod 也不会在因为资源不足或 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;node&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; 节点被驱逐中幸存.
k8s 通过一个叫 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; 的更高层级的抽象,来处理这些相对来说是一次的的 Pod 实例的管理工作.&lt;/p&gt;
&lt;p&gt;某个 Pod(拥有特定 UID) 是永远不会被重新调度到另一个节点上; 而是被一个基本相同, 甚至可以名称也相同,但 UID 不同的 Pod 所取代.&lt;/p&gt;
&lt;p&gt;当某些对象(如 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;volume&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;) 被描述为与 Pod 拥有一致的生命期,
表示这些对象会与指定的 (拥有那个 UID 的) &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 同时存在,
如果那个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 因为某些原为被删除, 即便同样的代替 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 已经被创建,
相关的对象(如 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;卷(Volume)&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt; 也会被随同 Pod 一起被销毁重建).&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
    &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/docs/pod.svg&#34;
         alt=&#34;Pod diagram&#34; width=&#34;50%&#34;/&gt; 
&lt;/figure&gt;

&lt;em&gt;有一个容器作为 web 服务，为共享数据卷的文件提供访问服务，另一个独立的容器作为 边车，负责从远程的源更新这些文件&lt;/em&gt;&lt;/p&gt;
&lt;!--  
## Pod phase

A Pod&#39;s `status` field is a
[PodStatus](/k8sDocs/reference/generated/kubernetes-api/v1.19/#podstatus-v1-core)
object, which has a `phase` field.

The phase of a Pod is a simple, high-level summary of where the Pod is in its
lifecycle. The phase is not intended to be a comprehensive rollup of observations
of container or Pod state, nor is it intended to be a comprehensive state machine.

The number and meanings of Pod phase values are tightly guarded.
Other than what is documented here, nothing should be assumed about Pods that
have a given `phase` value.

Here are the possible values for `phase`:

Value | Description
:-----|:-----------
`Pending` | The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to bescheduled as well as the time spent downloading container images over the network.
`Running` | The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.
`Succeeded` | All containers in the Pod have terminated in success, and will not be restarted.
`Failed` | All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system.
`Unknown` | For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.

If a node dies or is disconnected from the rest of the cluster, Kubernetes
applies a policy for setting the `phase` of all Pods on the lost node to Failed.
--&gt;
&lt;h2 id=&#34;pod-的人生阶段&#34;&gt;Pod 的人生阶段&lt;/h2&gt;
&lt;p&gt;在 Pod 的 &lt;code&gt;status&lt;/code&gt; 字段是一个 &lt;a href=&#34;https://kubernetes.io/k8sDocs/reference/generated/kubernetes-api/v1.19/#podstatus-v1-core&#34;&gt;PodStatus&lt;/a&gt;
对象, 上面有一个 &lt;code&gt;phase&lt;/code&gt; 字段.&lt;/p&gt;
&lt;p&gt;Pod 的人生阶段是对 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 生命周期的调度总结.
Pod 的人生阶段并不是对容器或 Pod 状态的容易理解的总结, 也不是一个容易理解的状态机.&lt;/p&gt;
&lt;p&gt;Pod 的人生阶段的数量与意义与其值都是很有限的. 除了以下对各阶段的说明, Pod 不会有其它的阶段.
以下为 &lt;code&gt;phase&lt;/code&gt; 字段的可能值:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;字段值&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Pending&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 已经被集群确立， 但是其中的一个或多个容器还没有完成配置并准备就绪。 包括 Pod 等调度的时间和从网上下载容器镜像的时间&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Running&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 已经在节点上，并且其中的所有容器已经完成创建，至少有一个容器正在运行，或在启动或重启的过程中&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Succeeded&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 中所有的容器都已经成功运行完成并终止，并且不会再被重启&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Failed&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 中所有的容器被终止，且至少有一个容器是因为失败而被终止的。 容器失败的原因可能是因返回非零而退出或被系统终止&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Unknown&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;因为某些原因导至无法获取 Pod 的状态。 这个阶段一般是因为与 Pod 所在的节点无法通信。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;如果一个节点挂了或者与集群失联， k8s 会执行一个策略，让节点上所有的 Pod 的 &lt;code&gt;phase&lt;/code&gt; 字段设置为 &lt;code&gt;Failed&lt;/code&gt;。&lt;/p&gt;
&lt;!--
## Container states

As well as the [phase](#pod-phase) of the Pod overall, Kubernetes tracks the state of
each container inside a Pod. You can use
[container lifecycle hooks](/k8sDocs/docs/concepts/containers/container-lifecycle-hooks/) to
trigger events to run at certain points in a container&#39;s lifecycle.

Once the &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-scheduler/&#39; target=&#39;_blank&#39;&gt;scheduler&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.&lt;/span&gt;
&lt;/a&gt;
assigns a Pod to a Node, the kubelet starts creating containers for that Pod
using a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;container runtime&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt;.
There are three possible container states: `Waiting`, `Running`, and `Terminated`.

To the check state of a Pod&#39;s containers, you can use
`kubectl describe pod &lt;name-of-pod&gt;`. The output shows the state for each container
within that Pod.

Each state has a specific meaning:

 --&gt;
&lt;h2 id=&#34;容器的状态&#34;&gt;容器的状态&lt;/h2&gt;
&lt;p&gt;与 Pod 存在几个&lt;a href=&#34;#pod-phase&#34;&gt;阶段&lt;/a&gt;一个样，k8s 也会跟踪 Pod 内的容器的状态。 用户可以通过
&lt;a href=&#34;../../../containers/container-lifecycle-hooks/&#34;&gt;容器的生命周期钩子&lt;/a&gt;
来以容器生命周期事件来触发一些需要工作的运行&lt;/p&gt;
&lt;p&gt;当一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-scheduler/&#39; target=&#39;_blank&#39;&gt;kube-scheduler&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.&lt;/span&gt;
&lt;/a&gt;
将一个 Pod 调度到一个节点时， kubelet 就会通过 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;容器运行环境&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt;
为该 Pod 创建对应的容器。 容器可能存在三种状态 &lt;code&gt;Waiting&lt;/code&gt;, &lt;code&gt;Running&lt;/code&gt;, &lt;code&gt;Terminated&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;用户可以通过命令 &lt;code&gt;kubectl describe pod &amp;lt;name-of-pod&amp;gt;&lt;/code&gt; 查看 Pod 中容器的状态。
命令输出结果会包含其中所有容器的状态。
接下来介绍每一种状的具体含义&lt;/p&gt;
&lt;h3 id=&#34;container-state-waiting&#34;&gt;&lt;code&gt;Waiting&lt;/code&gt;&lt;/h3&gt;
&lt;!--
If a container is not in either the `Running` or `Terminated` state, it `Waiting`.
A container in the `Waiting` state is still running the operations it requires in
order to complete start up: for example, pulling the container image from a container
image registry, or applying &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;存放如密码, OAuth, ssh 密钥等敏感信息&lt;/span&gt;
&lt;/a&gt;
data.
When you use `kubectl` to query a Pod with a container that is `Waiting`, you also see
a Reason field to summarize why the container is in that state.
 --&gt;
&lt;p&gt;当一个容器的状不是 &lt;code&gt;Running&lt;/code&gt; 或 &lt;code&gt;Terminated&lt;/code&gt; 就是 &lt;code&gt;Waiting&lt;/code&gt;。当一个容器状态为 &lt;code&gt;Waiting&lt;/code&gt;
表示容器正在进行启动需要的前置操作: 如， 从镜像仓库拉取容器所需要的镜像， 或配置
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;存放如密码, OAuth, ssh 密钥等敏感信息&lt;/span&gt;
&lt;/a&gt; 数据。&lt;/p&gt;
&lt;p&gt;当 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;Kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt; 查询到Pod中的状是 &lt;code&gt;Waiting&lt;/code&gt;， 同时也会看到
一个 &lt;code&gt;Reason&lt;/code&gt; 字段，其值是对容器保持在该状态原因的总结&lt;/p&gt;
&lt;h3 id=&#34;container-state-running&#34;&gt;&lt;code&gt;Running&lt;/code&gt;&lt;/h3&gt;
&lt;!--
The `Running` status indicates that a container is executing without issues. If there
was a `postStart` hook configured, it has already executed and executed. When you use
`kubectl` to query a Pod with a container that is `Running`, you also see information
about when the container entered the `Running` state.

 --&gt;
&lt;p&gt;&lt;code&gt;Running&lt;/code&gt; 状态表示容器正在欢快地运行，没啥毛病。 如果配置了钩子 &lt;code&gt;postStart&lt;/code&gt;， 这个钩子的处理器也
已经执行而且成功完成。 当使用 &lt;code&gt;kubectl&lt;/code&gt; 查询容器为 &lt;code&gt;Running&lt;/code&gt; 的 Pod 时，同时可以看到容器进入
&lt;code&gt;Running&lt;/code&gt; 状态的时长。&lt;/p&gt;
&lt;h3 id=&#34;container-state-terminated&#34;&gt;&lt;code&gt;Terminated&lt;/code&gt;&lt;/h3&gt;
&lt;!--
A container in the `Terminated` state has begin execution and has then either run to
completion or has failed for some reason. When you use `kubectl` to query a Pod with
a container that is `Terminated`, you see a reason, and exit code, and the start and
finish time for that container&#39;s period of execution.

If a container has a `preStop` hook configured, that runs before the container enters
the `Terminated` state.
 --&gt;
&lt;p&gt;当一个容器状态为 &lt;code&gt;Terminated&lt;/code&gt; 时，表示任务已经执行了，要么执行完成，要么因为某些原因失败了。
当使用 &lt;code&gt;kubectl&lt;/code&gt; 查看容器为 &lt;code&gt;Terminated&lt;/code&gt; 状态的 Pod 时， 可以看到原因和退出码， 还有
容器内任务执行的开始和结束时间。&lt;/p&gt;
&lt;p&gt;如果一个容器配置了钩子 &lt;code&gt;preStop&lt;/code&gt;， 那么钩子对应的处理器会在容器进行&lt;code&gt;Terminated&lt;/code&gt; 状态之前执行。&lt;/p&gt;
&lt;!--
## Container restart policy {#restart-policy}

The `spec` of a Pod has a `restartPolicy` field with possible values Always, OnFailure,
and Never. The default value is Always.

The `restartPolicy` applies to all containers in the Pod. `restartPolicy` only
refers to restarts of the containers by the kubelet on the same node. After containers
in a Pod exit, the kubelet restarts them with an exponential back-off delay (10s, 20s,
40s, …), that is capped at five minutes. Once a container has executed with no problems
for 10 minutes without any problems, the kubelet resets the restart backoff timer for
that container.
 --&gt;
&lt;h2 id=&#34;restart-policy&#34;&gt;容器的重启策略&lt;/h2&gt;
&lt;p&gt;在 Pod 的 &lt;code&gt;spec&lt;/code&gt; 子对象上有一个 &lt;code&gt;restartPolicy&lt;/code&gt; 字段，可能的值有 &lt;code&gt;Always&lt;/code&gt;, &lt;code&gt;OnFailure&lt;/code&gt;, &lt;code&gt;Never&lt;/code&gt;
默认为 &lt;code&gt;Always&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;restartPolicy&lt;/code&gt; 适用于 Pod 中的所有容器。 &lt;code&gt;restartPolicy&lt;/code&gt; 只能让一个节点上的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;Kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt;
来重启其上的容器。 当 Pod 中的容器退出后， &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;Kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt;
会以指数延迟(10s, 20s, 40s, …)补偿机制来重启容器，延迟时间最长为 5 分钟。 当一个容器正常运行 10 分钟
后，  &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;Kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt; 才会重置该容器的补偿时钟。&lt;/p&gt;
&lt;!--
## Pod conditions

A Pod has a PodStatus, which has an array of
[PodConditions](https://kubernetes.io/k8sDocs/reference/generated/kubernetes-api/v1.19/#podcondition-v1-core)
through which the Pod has or has not passed:

* `PodScheduled`: the Pod has been scheduled to a node.
* `ContainersReady`: all containers in the Pod are ready.
* `Initialized`: all [init containers](/k8sDocs/docs/concepts/workloads/pods/init-containers/)
  have started successfully.
* `Ready`: the Pod is able to serve requests and should be added to the load
  balancing pools of all matching Services.

Field name           | Description
:--------------------|:-----------
`type`               | Name of this Pod condition.
`status`             | Indicates whether that condition is applicable, with possible values &#34;`True`&#34;, &#34;`False`&#34;, or &#34;`Unknown`&#34;.
`lastProbeTime`      | Timestamp of when the Pod condition was last probed.
`lastTransitionTime` | Timestamp for when the Pod last transitioned from one status to another.
`reason`             | Machine-readable, UpperCamelCase text indicating the reason for the condition&#39;s last transition.
`message`            | Human-readable message indicating details about the last status transition.
 --&gt;
&lt;h2 id=&#34;pod-的就绪条件&#34;&gt;Pod 的就绪条件&lt;/h2&gt;
&lt;p&gt;在 Pod 上面有一个 &lt;code&gt;PodStatus&lt;/code&gt; 子对象，其中包含一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/generated/kubernetes-api/v1.19/#podcondition-v1-core&#34;&gt;PodConditions&lt;/a&gt;
的数组。表示这个 Pod 有没有通过这些条件， 具体如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PodScheduled&lt;/code&gt;: Pod 已经被调度到节点上.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ContainersReady&lt;/code&gt;: Pod 中所有的容器都已经就绪.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Initialized&lt;/code&gt;: 所有 &lt;a href=&#34;../init-containers/&#34;&gt;初始化容器&lt;/a&gt;
都启动成功.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Ready&lt;/code&gt;: Pod 已经能够处理请求，应该被加入对应 Service 的负载均衡池中。&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;字段名称&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;type&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;这个 Pod 条件的名称&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;status&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;表示这个条件的达成情况， 可能的值有 &amp;ldquo;&lt;code&gt;True&lt;/code&gt;&amp;rdquo;, &amp;ldquo;&lt;code&gt;False&lt;/code&gt;&amp;rdquo;, 或 &amp;ldquo;&lt;code&gt;Unknown&lt;/code&gt;&amp;rdquo;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;lastProbeTime&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;该条件上次探测的时间戳&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;lastTransitionTime&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;该条件的值最近发生变更的时间戳&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;reason&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;机器可读, 大写驼峰的文本，说明最近一次值变化的原因&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;message&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;人类可读的消息，详细说明最近一次值变化的原因&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
### Pod readiness {#pod-readiness-gate}






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [stable]&lt;/code&gt;
&lt;/div&gt;



Your application can inject extra feedback or signals into PodStatus:
_Pod readiness_. To use this, set `readinessGates` in the Pod&#39;s `spec` to
specify a list of additional conditions that the kubelet evaluates for Pod readiness.

Readiness gates are determined by the current state of `status.condition`
fields for the Pod. If Kubernetes cannot find such a condition in the
`status.conditions` field of a Pod, the status of the condition
is defaulted to &#34;`False`&#34;.

Here is an example:

```yaml
kind: Pod
...
spec:
  readinessGates:
    - conditionType: &#34;www.example.com/feature-1&#34;
status:
  conditions:
    - type: Ready                              # a built in PodCondition
      status: &#34;False&#34;
      lastProbeTime: null
      lastTransitionTime: 2018-01-01T00:00:00Z
    - type: &#34;www.example.com/feature-1&#34;        # an extra PodCondition
      status: &#34;False&#34;
      lastProbeTime: null
      lastTransitionTime: 2018-01-01T00:00:00Z
  containerStatuses:
    - containerID: docker://abcd...
      ready: true
...
```

The Pod conditions you add must have names that meet the Kubernetes [label key format](/k8sDocs/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set).
  --&gt;
&lt;h3 id=&#34;pod-readiness-gate&#34;&gt;Pod 就绪阀&lt;/h3&gt;
&lt;p&gt;用户可以向应用中注入额外的反馈或信号到 &lt;code&gt;PodStatus&lt;/code&gt;: Pod readiness. 要使用该特性，需要在 Pod 的 &lt;code&gt;spec&lt;/code&gt; 子对象上设置 &lt;code&gt;readinessGates&lt;/code&gt;，定义追加额外的就绪条件到 k8s 检测 Pod 就绪条件列表中。&lt;/p&gt;
&lt;p&gt;就绪阀由 Pod 当前 &lt;code&gt;status.condition&lt;/code&gt; 的状态决定。 如果 k8s 不能在 Pod 的 &lt;code&gt;status.condition&lt;/code&gt; 中找到该条件， 条件的默认值为 &lt;code&gt;False&lt;/code&gt;
以下为示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
...
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;readinessGates&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;conditionType&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;www.example.com/feature-1&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;status&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;conditions&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ready                              # 内置的 PodCondition&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;status&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;False&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;lastProbeTime&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;null&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;lastTransitionTime&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;2018-01-01T00:00:00Z&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;www.example.com/feature-1&amp;#34;&lt;/span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# 外挂的 PodCondition&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;status&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;False&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;lastProbeTime&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;null&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;lastTransitionTime&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;2018-01-01T00:00:00Z&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containerStatuses&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;containerID&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;docker://abcd...&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;ready&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;用户添加的条件在命名是需要符合 k8s 的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set&#34;&gt;标签命名格式&lt;/a&gt;&lt;/p&gt;
&lt;!--
### Status for Pod readiness {#pod-readiness-status}

The `kubectl patch` command does not support patching object status.
To set these `status.conditions` for the pod, applications and
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/extend-kubernetes/operator/&#39; target=&#39;_blank&#39;&gt;operators&lt;span class=&#39;tooltip-text&#39;&gt;A specialized controller used to manage a custom resource 一个用于管理自定义资源的专用控制器&lt;/span&gt;
&lt;/a&gt; should use
the `PATCH` action.
You can use a [Kubernetes client library](/k8sDocs/reference/using-api/client-libraries/) to
write code that sets custom Pod conditions for Pod readiness.

For a Pod that uses custom conditions, that Pod is evaluated to be ready **only**
when both the following statements apply:

* All containers in the Pod are ready.
* All conditions specified in `readinessGates` are `True`.

When a Pod&#39;s containers are Ready but at least one custom condition is missing or
`False`, the kubelet sets the Pod&#39;s [condition](#pod-condition) to `ContainersReady`.
 --&gt;
&lt;h3 id=&#34;pod-readiness-status&#34;&gt;Pod 就绪条件的状态&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl patch&lt;/code&gt; 命令不支持对对象状态的修改。 想要对 Pod 的 &lt;code&gt;status.conditions&lt;/code&gt;， 应用，或其它进行 &lt;code&gt;PATCH&lt;/code&gt; 的操作
可以通过 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/using-api/client-libraries/&#34;&gt;k8s 客户端库&lt;/a&gt;
写代码的方式来自定义 Pod 就绪条件。&lt;/p&gt;
&lt;p&gt;对于使用自定义就绪条件的 Pod， 只有在达成以下条件时才能进入就绪状态:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod 中所有的容器都已经就绪&lt;/li&gt;
&lt;li&gt;所有有容器配置的 &lt;code&gt;readinessGates&lt;/code&gt; 的值都为 &lt;code&gt;True&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当一个 Pod 中所有容器已经就绪，但至少有一个自定义条件不存在或值为 &lt;code&gt;False&lt;/code&gt;，
kubelet 会将 Pod 的&lt;a href=&#34;#pod-condition&#34;&gt;就绪条件&lt;/a&gt;值设置为 &lt;code&gt;ContainersReady&lt;/code&gt;&lt;/p&gt;
&lt;!--
## Container probes

A [Probe](/k8sDocs/reference/generated/kubernetes-api/v1.19/#probe-v1-core) is a diagnostic
performed periodically by the [kubelet](/k8sDocs/admin/kubelet/)
on a Container. To perform a diagnostic,
the kubelet calls a
[Handler](/k8sDocs/reference/generated/kubernetes-api/v1.19/#handler-v1-core) implemented by
the container. There are three types of handlers:

* [ExecAction](/k8sDocs/reference/generated/kubernetes-api/v1.19/#execaction-v1-core):
  Executes a specified command inside the container. The diagnostic
  is considered successful if the command exits with a status code of 0.

* [TCPSocketAction](/k8sDocs/reference/generated/kubernetes-api/v1.19/#tcpsocketaction-v1-core):
  Performs a TCP check against the Pod&#39;s IP address on
  a specified port. The diagnostic is considered successful if the port is open.

* [HTTPGetAction](/k8sDocs/reference/generated/kubernetes-api/v1.19/#httpgetaction-v1-core):
  Performs an HTTP `GET` request against the Pod&#39;s IP
  address on a specified port and path. The diagnostic is considered successful
  if the response has a status code greater than or equal to 200 and less than 400.

Each probe has one of three results:

* `Success`: The container passed the diagnostic.
* `Failure`: The container failed the diagnostic.
* `Unknown`: The diagnostic failed, so no action should be taken.

The kubelet can optionally perform and react to three kinds of probes on running
containers:

* `livenessProbe`: Indicates whether the container is running. If
   the liveness probe fails, the kubelet kills the container, and the container
   is subjected to its [restart policy](#restart-policy). If a Container does not
   provide a liveness probe, the default state is `Success`.

* `readinessProbe`: Indicates whether the container is ready to respond to requests.
   If the readiness probe fails, the endpoints controller removes the Pod&#39;s IP
   address from the endpoints of all Services that match the Pod. The default
   state of readiness before the initial delay is `Failure`. If a Container does
   not provide a readiness probe, the default state is `Success`.

* `startupProbe`: Indicates whether the application within the container is started.
   All other probes are disabled if a startup probe is provided, until it succeeds.
   If the startup probe fails, the kubelet kills the container, and the container
   is subjected to its [restart policy](#restart-policy). If a Container does not
   provide a startup probe, the default state is `Success`.

For more information about how to set up a liveness, readiness, or startup probe,
see [Configure Liveness, Readiness and Startup Probes](/k8sDocs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).
 --&gt;
&lt;h2 id=&#34;容器探针&#34;&gt;容器探针&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#probe-v1-core&#34;&gt;探针&lt;/a&gt;
就是由 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/admin/kubelet/&#34;&gt;kubelet&lt;/a&gt; 定时对容器进行诊断操作，
诊断操作则是由 kubelet 调用一个由容器实现的
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#handler-v1-core&#34;&gt;处理器&lt;/a&gt;。
有以下三种类型处理器:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#execaction-v1-core&#34;&gt;ExecAction&lt;/a&gt;:
在容器内执行一个指定命令. 如果命令执行结束代码为 0 则表示诊断结果为成功&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#tcpsocketaction-v1-core&#34;&gt;TCPSocketAction&lt;/a&gt;:
向指定 IP 地址和端口发起 TCP 请求。 如果成功打开端口，表示诊断结果为成功&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#httpgetaction-v1-core&#34;&gt;HTTPGetAction&lt;/a&gt;:
向指定IP 地址，端口和路径发起 HTTP &lt;code&gt;GET&lt;/code&gt; 请求。如果响应码在 200 &amp;lt;= code &amp;lt; 400
表示诊断结果为成功&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上探针的结果的值可能为以下任意一个:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Success&lt;/code&gt;: 容器通过了诊断.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Failure&lt;/code&gt;: 容器没有通过诊断.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Unknown&lt;/code&gt;: 诊断过程失败，不执行任何操作.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;kubelet 可以选择是否对容器中以下探针的结果作出相应的操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;livenessProbe&lt;/code&gt;(存活探针): 指示容器是否正在运行.&lt;/p&gt;
&lt;p&gt;如果存活探针的诊断结果为未通过， 则 kubelet 会杀掉这个容器，而后容器操作则由其 &lt;a href=&#34;#restart-policy&#34;&gt;重启策略&lt;/a&gt;决定。
如果容器没有配置存活探针，则默认状态为成功。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;readinessProbe&lt;/code&gt;(就绪探针): 指示容器是否可以响应请求
如果就绪探针的诊断结果为未通过， 则 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-endpoint&#39; target=&#39;_blank&#39;&gt;Endpoints&lt;span class=&#39;tooltip-text&#39;&gt;Endpoint 跟踪匹配 Service 选择器的 Pod 的 IP 地址&lt;/span&gt;
&lt;/a&gt; 控制器
就会把该 Pod 从所有配置该 Pod 的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 的
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-endpoint&#39; target=&#39;_blank&#39;&gt;Endpoints&lt;span class=&#39;tooltip-text&#39;&gt;Endpoint 跟踪匹配 Service 选择器的 Pod 的 IP 地址&lt;/span&gt;
&lt;/a&gt; 中移出。
如果容器没有配置就绪探针，则默认状态为成功。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;startupProbe&lt;/code&gt;(启动探针): 指示容器内的应用是否启动
如果配置了启动探针，除非启动探针诊断结果为通过，否则所有其它探针都不会工作。 如果启动探针的诊断
结果为未通过，则 kubelet 会杀掉是这个容器， 而后容器操作则由其 &lt;a href=&#34;#restart-policy&#34;&gt;重启策略&lt;/a&gt;决定。
如果容器没有配置启动探针，则默认状态为成功。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;了角更多关于如何配置 存活探针，就绪探针，启动探针，见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/&#34;&gt;存活探针，就绪探针，启动探针配置&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### When should you use a liveness probe?






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.0 [stable]&lt;/code&gt;
&lt;/div&gt;



If the process in your container is able to crash on its own whenever it
encounters an issue or becomes unhealthy, you do not necessarily need a liveness
probe; the kubelet will automatically perform the correct action in accordance
with the Pod&#39;s `restartPolicy`.

If you&#39;d like your container to be killed and restarted if a probe fails, then
specify a liveness probe, and specify a `restartPolicy` of Always or OnFailure.
  --&gt;
&lt;h3 id=&#34;为啥需要用就绪readiness探针&#34;&gt;为啥需要用就绪(readiness)探针?&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.0 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;如果应用内的进程在出毛病或变得不健康是就能够自己挂掉，那么就是需要配置生存探针， kubelet 会
自动根据 Pod 的 &lt;code&gt;restartPolicy&lt;/code&gt; 正确处理这些问题。&lt;/p&gt;
&lt;p&gt;如果用户需要在探针诊断结果为未通过时杀掉容器并重启，就可以配置一个存活探针，并将 &lt;code&gt;restartPolicy&lt;/code&gt;
的值设置为 &lt;code&gt;Always&lt;/code&gt; 或 &lt;code&gt;OnFailure&lt;/code&gt;.&lt;/p&gt;
&lt;!--
### When should you use a readiness probe?






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.0 [stable]&lt;/code&gt;
&lt;/div&gt;



If you&#39;d like to start sending traffic to a Pod only when a probe succeeds,
specify a readiness probe. In this case, the readiness probe might be the same
as the liveness probe, but the existence of the readiness probe in the spec means
that the Pod will start without receiving any traffic and only start receiving
traffic after the probe starts succeeding.
If your container needs to work on loading large data, configuration files, or
migrations during startup, specify a readiness probe.

If you want your container to be able to take itself down for maintenance, you
can specify a readiness probe that checks an endpoint specific to readiness that
is different from the liveness probe.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If you just want to be able to drain requests when the Pod is deleted, you do not
necessarily need a readiness probe; on deletion, the Pod automatically puts itself
into an unready state regardless of whether the readiness probe exists.
The Pod remains in the unready state while it waits for the containers in the Pod
to stop.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;啥时候应该用就绪readiness探针&#34;&gt;啥时候应该用就绪(readiness)探针?&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.0 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;如果用户期望仅在探针诊断状态为通过时才向 Pod 调度流量，此时应该配置就绪探针。
在这种情况下，就绪探针可能与存活探针区别不大， 但就绪探针存在的意义在于 Pod 不会在就绪探针通过之前
接收到任何流量，只有在通过之后才会开始接收流量。
如果用户容器在启动时需要加载大量数据，配置文件，或迁移数据，这时就需要配置就绪探针。&lt;/p&gt;
&lt;p&gt;如果用户期望在需要维护容器可以自挂东南枝， 就可以设置一个与存活探针不同的探测接口作为就绪探针。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果用户期望仅在 Pod 被删除是不接收流量， 则不需要配置就绪探针。 在 Pod 被删除时，无论有没有就绪探针都自动将其状态
设置为未就绪状态。 Pod 在等待其中容器正常停止的过程中状态一直也都是未就绪。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### When should you use a startup probe?






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [alpha]&lt;/code&gt;
&lt;/div&gt;



Startup probes are useful for Pods that have containers that take a long time to
come into service. Rather than set a long liveness interval, you can configure
a separate configuration for probing the container as it starts up, allowing
a time longer than the liveness interval would allow.

If your container usually starts in more than
`initialDelaySeconds + failureThreshold × periodSeconds`, you should specify a
startup probe that checks the same endpoint as the liveness probe. The default for
`periodSeconds` is 30s. You should then set its `failureThreshold` high enough to
allow the container to start, without changing the default values of the liveness
probe. This helps to protect against deadlocks.
 --&gt;
&lt;h3 id=&#34;啥时候应该用启动探针&#34;&gt;啥时候应该用启动探针？&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;启动探针对于那些其中容器需要花费很长时间才能提供服务的 Pod 是相当有用的。并不需要配置一个长时间间隔的存活探针，
只需要配置一个独立的配置的探测容器的启动。而这样可以允许比存活探针时间间隔更长的时间来等待容器启动。&lt;/p&gt;
&lt;p&gt;如果用户容器通过启动时间大于 &lt;code&gt;initialDelaySeconds + failureThreshold × periodSeconds&lt;/code&gt;，
就应该配置一个与存活探针检查点相同的启动探针。 &lt;code&gt;periodSeconds&lt;/code&gt; 默认为 30秒。 所以需要设置一个
足够大的 &lt;code&gt;failureThreshold&lt;/code&gt; 值，以保证容器能够有足够的时间启动， 而不需要修改存活探针的默认配置。
这也能避免出现死锁。&lt;/p&gt;
&lt;!--
## Termination of Pods {#pod-termination}

Because Pods represent processes running on nodes in the cluster, it is important to
allow those processes to gracefully terminate when they are no longer needed (rather
than being abruptly stopped with a `KILL` signal and having no chance to clean up).

The design aim is for you to be able to request deletion and know when processes
terminate, but also be able to ensure that deletes eventually complete.
When you request deletion of a Pod, the cluster records and tracks the intended grace period
before the Pod is allowed to be forcefully killed. With that forceful shutdown tracking in
place, the &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt; attempts graceful
shutdown.

Typically, the container runtime sends a a TERM signal is sent to the main process in each
container. Once the grace period has expired, the KILL signal is sent to any remainig
processes, and the Pod is then deleted from the
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API Server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt;. If the kubelet or the
container runtime&#39;s management service is restarted while waiting for processes to terminate, the
cluster retries from the start including the full original grace period.

An example flow:

1. You use the `kubectl` tool to manually delete a specific Pod, with the default grace period
   (30 seconds).
1. The Pod in the API server is updated with the time beyond which the Pod is considered &#34;dead&#34;
   along with the grace period.  
   If you use `kubectl describe` to check on the Pod you&#39;re deleting, that Pod shows up as
   &#34;Terminating&#34;.  
   On the node where the Pod is running: as soon as the kubelet sees that a Pod has been marked
   as terminating (a graceful shutdown duration has been set), the kubelet begins the local Pod
   shutdown process.
   1. If one of the Pod&#39;s containers has defined a `preStop`
      [hook](/k8sDocs/docs/concepts/containers/container-lifecycle-hooks/#hook-details), the kubelet
      runs that hook inside of the container. If the `preStop` hook is still running after the
      grace period expires, the kubelet requests a small, one-off grace period extension of 2
      seconds.
      &lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If the &lt;code&gt;preStop&lt;/code&gt; hook needs longer to complete than the default grace period allows,
you must modify &lt;code&gt;terminationGracePeriodSeconds&lt;/code&gt; to suit this.&lt;/div&gt;
&lt;/blockquote&gt;

   1. The kubelet triggers the container runtime to send a TERM signal to process 1 inside each
      container.
      &lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The containers in the Pod receive the TERM signal at different times and in an arbitrary
order. If the order of shutdowns matters, consider using a &lt;code&gt;preStop&lt;/code&gt; hook to synchronize.&lt;/div&gt;
&lt;/blockquote&gt;

1. At the same time as the kubelet is starting graceful shutdown, the control plane removes that
   shutting-down Pod from Endpoints (and, if enabled, EndpointSlice) objects where these represent
   a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; with a configured
   &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/&#39; target=&#39;_blank&#39;&gt;selector&lt;span class=&#39;tooltip-text&#39;&gt;允许用户基于标签过滤资源列表&lt;/span&gt;
&lt;/a&gt;.
   &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#39; target=&#39;_blank&#39;&gt;ReplicaSets&lt;span class=&#39;tooltip-text&#39;&gt;ReplicaSet 保证在任意时间点上某个 Pod 有指定数量副本在运行&lt;/span&gt;
&lt;/a&gt; and other workload resources
   no longer treat the shutting-down Pod as a valid, in-service replica. Pods that shut down slowly
   cannot continue to serve traffic as load balancers (like the service proxy) remove the Pod from
   the list of endpoints as soon as the termination grace period _begins_.
1. When the grace period expires, the kubelet triggers forcible shutdown. The container runtime sends
   `SIGKILL` to any processes still running in any container in the Pod.
   The kubelet also cleans up a hidden `pause` container if that container runtime uses one.
1. The kubelet triggers forcible removal of Pod object from the API server, by setting grace period
   to 0 (immediate deletion).  
1. The API server deletes the Pod&#39;s API object, which is then no longer visible from any client.
 --&gt;
&lt;h2 id=&#34;pod-termination&#34;&gt;Pod 的终结过程&lt;/h2&gt;
&lt;p&gt;因为 Pod 代表运行在集群节点上的一系列进程， 而要让这些进程在不需要时能够死得瞑目(而不是通过 KILL 信号突然被停止，连收尾的机会都没得)。&lt;/p&gt;
&lt;p&gt;在设计上旨在用户能够在发起删除请求并能够知晓啥时候进程终止， 但最终还要保证删除操作最终需要完成。
当用户发起删除一个 Pod 的请求， 集群会在预期的时间内跟踪和记录，如果超过这个时间则会强制终止 Pod 的进程。
在强制终止之前， kubelet 都会尝试平滑关闭。&lt;/p&gt;
&lt;p&gt;通常情况下，&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;容器运行环境&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt; 会向每个容器的主进程
发送一个 &lt;code&gt;TERM&lt;/code&gt; 信号。如果超过预期时间，再和仍然存在的进程发送 &lt;code&gt;KILL&lt;/code&gt; 信号， 然后从
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 中删除该 Pod 对象。 如果在这个等待过程中
kubelet 或 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;容器运行环境&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt; 发生重启， 集群会尝试对该删除操作
重启开启计时。&lt;/p&gt;
&lt;p&gt;以下为一个示例：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用户使用 kubectl 命令手动删除一个 Pod，使用默认的预期时间(30s).&lt;/li&gt;
&lt;li&gt;从命令执行开始到预期时间内 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 中的
Pod 对象就会更新，并标记为已经挂了。 如果使用 &lt;code&gt;kubectl describe&lt;/code&gt; 查看正在删除的 Pod，
看到它的状态应该是 &lt;code&gt;Terminating&lt;/code&gt;。 在 Pod 所在的节点上： 当 kubelet 看到 Pod 被标记为终止时(添加一个平滑关闭标记)
kubelet 就开始并本地的 Pod 的进程。
&lt;ol&gt;
&lt;li&gt;如果 Pod 中有任意容器配置了&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/containers/container-lifecycle-hooks/#hook-details&#34;&gt;钩子&lt;/a&gt; &lt;code&gt;preStop&lt;/code&gt;,
kubelet 会在对应容器中执行这个钩子。 如果在预期时间到达时 &lt;code&gt;preStop&lt;/code&gt; 钩子仍在运行，则 kubelet 一次性多给 2 秒。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果 &lt;code&gt;preStop&lt;/code&gt; 需要比默认的预期时间更长的时间，则需要设置一个合适的 &lt;code&gt;terminationGracePeriodSeconds&lt;/code&gt; 值&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;kubelet 触发 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;容器运行环境&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt; 向 Pod 中的每个容器的 1 号进程
发送 TERM 信号
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Pod 中的容器可能会以不同的顺序和时间接收到 TERM 信号， 如果需要进行有序关闭，考虑使用 &lt;code&gt;preStop&lt;/code&gt; 钩子来实现同步锁&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;在 kubelet 开始平滑关闭 Pod 的进程的同时， 控制中心将正在删除的 Pod 从 对应配置选择的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;
所代表的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-endpoint&#39; target=&#39;_blank&#39;&gt;Endpoints&lt;span class=&#39;tooltip-text&#39;&gt;Endpoint 跟踪匹配 Service 选择器的 Pod 的 IP 地址&lt;/span&gt;
&lt;/a&gt; (如果开启也可能是 EndpointSlice)中移出。
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#39; target=&#39;_blank&#39;&gt;ReplicaSet&lt;span class=&#39;tooltip-text&#39;&gt;ReplicaSet 保证在任意时间点上某个 Pod 有指定数量副本在运行&lt;/span&gt;
&lt;/a&gt; 和其它的工作负载资源都会将该Pod认作是失效的，对于那些半天关不掉又不能提供服务的Pod
负载均衡(如 service proxy)会在删除预期时间开始时就从 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-endpoint&#39; target=&#39;_blank&#39;&gt;Endpoints&lt;span class=&#39;tooltip-text&#39;&gt;Endpoint 跟踪匹配 Service 选择器的 Pod 的 IP 地址&lt;/span&gt;
&lt;/a&gt; 列表中移出。&lt;/li&gt;
&lt;li&gt;当预期时间用完后，就会触发 kubelet 强制删除。 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;容器运行环境&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt;
会向所有剩余的进程发送 &lt;code&gt;SIGKILL&lt;/code&gt; 信号。 如果容器用到了隐藏的 &lt;code&gt;pause&lt;/code&gt; 容器 kubelet 也会一起清理&lt;/li&gt;
&lt;li&gt;kubectl 通过将预期时间设置为0(立马删除)触发从 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt;
中强制删除 Pod 对象。&lt;/li&gt;
&lt;li&gt;&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 会在 Pod 对象对所有客户端不可见时，将其删除&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
### Forced Pod termination {#pod-termination-forced}

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; Forced deletions can be potentially disruptiove for some workloads and their Pods.&lt;/div&gt;
&lt;/blockquote&gt;


By default, all deletes are graceful within 30 seconds. The `kubectl delete` command supports
the `--grace-period=&lt;seconds&gt;` option which allows you to override the default and specify your
own value.

Setting the grace period to `0` forcibly and immediately deletes the Pod from the API
server. If the pod was still running on a node, that forcible deletion triggers the kubelet to
begin immediate cleanup.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You must specify an additional flag &lt;code&gt;--force&lt;/code&gt; along with &lt;code&gt;--grace-period=0&lt;/code&gt; in order to perform force deletions.&lt;/div&gt;
&lt;/blockquote&gt;


When a force deletion is performed, the API server does not wait for confirmation
from the kubelet that the Pod has been terminated on the node it was running on. It
removes the Pod in the API immediately so a new Pod can be created with the same
name. On the node, Pods that are set to terminate immediately will still be given
a small grace period before being force killed.

If you need to force-delete Pods that are part of a StatefulSet, refer to the task
documentation for
[deleting Pods from a StatefulSet](/k8sDocs/tasks/run-application/force-delete-stateful-set-pod/).
 --&gt;
&lt;h3 id=&#34;pod-termination-forced&#34;&gt;Pod 的强制删除&lt;/h3&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 强制删除存在破坏一些工作负载或其 Pod的潜在风险。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;在默认情况下，所有的删除操作的预期时间都是 30 秒，&lt;code&gt;kubectl delete&lt;/code&gt; 命令支持通过
&lt;code&gt;--grace-period=&amp;lt;seconds&amp;gt;&lt;/code&gt; 选择来自定义预期时间。&lt;/p&gt;
&lt;p&gt;将预期时间设置为 0， 会强制立马从 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 中删除 Pod 对象。
如果 Pod 仍然运行在某个节点上， 这种强制删除会触发 kubelet 开始立即清理。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 需要同时使用 &lt;code&gt;--force&lt;/code&gt; 和 &lt;code&gt;--grace-period=0&lt;/code&gt; 在能实现强制删除。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;当一个强制删除被执行时， &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 不会等待
Pod 所在节点的的 kubelet 确认 Pod 已经被终止。 只是立马删除 Pod 对象，这时可以马上创建一个同名的新 Pod
而在节点上，被设置为立马终止的 Pod 在被强制杀死前也会给予一小会时间，以期可能平滑关闭。&lt;/p&gt;
&lt;p&gt;如果用户需要强制删除一个 StatefulSet 的 Pod，
请见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/force-delete-stateful-set-pod/&#34;&gt;删除一个属于StatefulSet的Pod&lt;/a&gt;.&lt;/p&gt;
&lt;!--  
### Garbage collection of failed Pods {#pod-garbage-collection}

For failed Pods, the API objects remain in the cluster&#39;s API until a human or
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;controller&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; process
explicitly removes them.

The control plane cleans up terminated Pods (with a phase of `Succeeded` or
`Failed`), when the number of Pods exceeds the configured threshold
(determined by `terminated-pod-gc-threshold` in the kube-controller-manager).
This avoids a resource leak as Pods are created and terminated over time.
--&gt;
&lt;h3 id=&#34;pod-garbage-collection&#34;&gt;对失效 Pod 的垃圾回收&lt;/h3&gt;
&lt;p&gt;对于失效的 Pod, 其对应会存在于集群 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 中，
直至人工或 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;controller&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; 明确的删除它们&lt;/p&gt;
&lt;p&gt;控制中心清理终止的Pod (阶段的 &lt;code&gt;Succeeded&lt;/code&gt; 或 &lt;code&gt;Failed&lt;/code&gt;)， 如 Pod 的数量超过配置的阈值(由 kube-controller-manager 中的&lt;code&gt;terminated-pod-gc-threshold&lt;/code&gt;配置决定)
这会在长时间 Pod 创建和终止的过程中避免出现资源泄漏。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;实践
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/attach-handler-lifecycle-event/&#34;&gt;attaching handlers to Container lifecycle events&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这溃
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/&#34;&gt;configuring Liveness, Readiness and Startup Probes&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;了解&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/containers/container-lifecycle-hooks/&#34;&gt;container lifecycle hooks&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;更多关于 Pod / Container 状态的 API, 见 &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#podstatus-v1-core&#34;&gt;PodStatus&lt;/a&gt;,
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#containerstatus-v1-core&#34;&gt;ContainerStatus&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 卷(Volume)</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/</link>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jsafrane
- saad-ali
- thockin
- msau42
title: Volumes
content_type: concept
weight: 10
--- --&gt;
&lt;!-- overview --&gt;
&lt;!--
On-disk files in a Container are ephemeral, which presents some problems for
non-trivial applications when running in Containers.  First, when a Container
crashes, kubelet will restart it, but the files will be lost - the
Container starts with a clean state.  Second, when running Containers together
in a `Pod` it is often necessary to share files between those Containers.  The
Kubernetes `Volume` abstraction solves both of these problems.

Familiarity with [Pods](/docs/concepts/workloads/pods/pod/) is suggested.
 --&gt;
&lt;p&gt;容器中写到硬盘的文件是临时的，这会导致有些需要写入硬盘文件的应用出现一些问题。第一个问题是当一个容器
崩溃后，kubelet 会将其重启，但其所写的文件会全部丢失 - 容器会以全新的状态启动。 第二个问题是
在一个 &lt;code&gt;Pod&lt;/code&gt; 中的不同容器之间需要共享文件。 k8s 的 &lt;code&gt;Volume&lt;/code&gt; 抽象概念就是解决这些问题的。&lt;/p&gt;
&lt;p&gt;建议先看看 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/pod/&#34;&gt;Pods&lt;/a&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Background

Docker also has a concept of
[volumes](https://docs.docker.com/storage/), though it is
somewhat looser and less managed.  In Docker, a volume is simply a directory on
disk or in another Container.  Lifetimes are not managed and until very
recently there were only local-disk-backed volumes.  Docker now provides volume
drivers, but the functionality is very limited for now (e.g. as of Docker 1.7
only one volume driver is allowed per Container and there is no way to pass
parameters to volumes).

A Kubernetes volume, on the other hand, has an explicit lifetime - the same as
the Pod that encloses it.  Consequently, a volume outlives any Containers that run
within the Pod, and data is preserved across Container restarts. Of course, when a
Pod ceases to exist, the volume will cease to exist, too.  Perhaps more
importantly than this, Kubernetes supports many types of volumes, and a Pod can
use any number of them simultaneously.

At its core, a volume is just a directory, possibly with some data in it, which
is accessible to the Containers in a Pod.  How that directory comes to be, the
medium that backs it, and the contents of it are determined by the particular
volume type used.

To use a volume, a Pod specifies what volumes to provide for the Pod (the
`.spec.volumes`
field) and where to mount those into Containers (the
`.spec.containers[*].volumeMounts`
field).

A process in a container sees a filesystem view composed from their Docker
image and volumes.  The [Docker
image](https://docs.docker.com/userguide/dockerimages/) is at the root of the
filesystem hierarchy, and any volumes are mounted at the specified paths within
the image.  Volumes can not mount onto other volumes or have hard links to
other volumes.  Each Container in the Pod must independently specify where to
mount each volume.
 --&gt;
&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;p&gt;Docker 同样有 &lt;a href=&#34;https://docs.docker.com/storage/&#34;&gt;volumes&lt;/a&gt; 概念，但是 Docker 的数据卷(volume)
不那么好管理。在 Docker 中，一个数据卷就是简单地在磁盘上或另一个容器中的一个目录。存在期不受管理，
并且直到最近(文档最早提交为 2018年5月6日)都只支持本地磁盘的数据卷。 现在 Docker 提供了数据卷
驱动， 但其功能也还是相当有限(例如，Docker v1.7 中每个容器只支持一个数据卷驱动，并且没办法向
数据卷传递参数)。&lt;/p&gt;
&lt;p&gt;k8s 的数据卷，相对来说就更加强大，拥有明确的生命期 - 与其绑定的 Pod 相同。 因此，一个数据卷可能
比 Pod 中所有的容器的命都长， 并且在容器重启之后数据依然存在。 当然，当 Pod 不存在进，与其绑定的
数据卷也就不存在了。 可能比这些更重要的是 k8s 支持许多类型的数据卷，并且一个 Pod 可以同时使用
任意类型任意数量的数据卷。&lt;/p&gt;
&lt;p&gt;数据卷的核心也只是一个目录，可能其中还有数据，它可以被 Pod 中的容器访问。 这个目录是怎么来的，
它所用的介质是什么，它其中的内存是什么，这些都是由数据卷所使用的类型所决定的。&lt;/p&gt;
&lt;p&gt;要使用一个数据卷，需要在 Pod 配置文件中 &lt;code&gt;.spec.volumes&lt;/code&gt; 字段上配置可用数据卷，并在容器的
&lt;code&gt;.spec.containers[*].volumeMounts&lt;/code&gt; 字段指定数据卷和在容器内的挂载点&lt;/p&gt;
&lt;p&gt;在一个容器内的进程看到的文件系统是由它们的 Docker 镜像和数据卷共同组成的。 &lt;a href=&#34;https://docs.docker.com/userguide/dockerimages/&#34;&gt;Docker
镜像&lt;/a&gt;位于文件系统层级的底层， 然后各个
数据卷挂载到镜像内相应的目录上。 数据卷不能再挂载到其它的数据卷上，也不创建硬连接到其它的数据卷。
Pod 中的每个容器都需要独立的指定每个数据卷的挂载点。&lt;/p&gt;
&lt;!--
## Types of Volumes

Kubernetes supports several types of Volumes:

   * [awsElasticBlockStore](#awselasticblockstore)
   * [azureDisk](#azuredisk)
   * [azureFile](#azurefile)
   * [cephfs](#cephfs)
   * [cinder](#cinder)
   * [configMap](#configmap)
   * [csi](#csi)
   * [downwardAPI](#downwardapi)
   * [emptyDir](#emptydir)
   * [fc (fibre channel)](#fc)
   * [flexVolume](#flexVolume)
   * [flocker](#flocker)
   * [gcePersistentDisk](#gcepersistentdisk)
   * [gitRepo (deprecated)](#gitrepo)
   * [glusterfs](#glusterfs)
   * [hostPath](#hostpath)
   * [iscsi](#iscsi)
   * [local](#local)
   * [nfs](#nfs)
   * [persistentVolumeClaim](#persistentvolumeclaim)
   * [projected](#projected)
   * [portworxVolume](#portworxvolume)
   * [quobyte](#quobyte)
   * [rbd](#rbd)
   * [scaleIO](#scaleio)
   * [secret](#secret)
   * [storageos](#storageos)
   * [vsphereVolume](#vspherevolume)

We welcome additional contributions.

 --&gt;
&lt;h2 id=&#34;types-of-volumes&#34;&gt;数据卷(Volume)的类型&lt;/h2&gt;
&lt;p&gt;以下为 k8s 支持的数据卷(Volume)类型:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#awselasticblockstore&#34;&gt;awsElasticBlockStore&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#azuredisk&#34;&gt;azureDisk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#azurefile&#34;&gt;azureFile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cephfs&#34;&gt;cephfs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cinder&#34;&gt;cinder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#configmap&#34;&gt;configMap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#csi&#34;&gt;csi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#downwardapi&#34;&gt;downwardAPI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#emptydir&#34;&gt;emptyDir&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fc&#34;&gt;fc (fibre channel)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#flexVolume&#34;&gt;flexVolume&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#flocker&#34;&gt;flocker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gcepersistentdisk&#34;&gt;gcePersistentDisk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gitrepo&#34;&gt;gitRepo (deprecated)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#glusterfs&#34;&gt;glusterfs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hostpath&#34;&gt;hostPath&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#iscsi&#34;&gt;iscsi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#local&#34;&gt;local&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nfs&#34;&gt;nfs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#persistentvolumeclaim&#34;&gt;persistentVolumeClaim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#projected&#34;&gt;projected&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#portworxvolume&#34;&gt;portworxVolume&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quobyte&#34;&gt;quobyte&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rbd&#34;&gt;rbd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scaleio&#34;&gt;scaleIO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#secret&#34;&gt;secret&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#storageos&#34;&gt;storageos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vspherevolume&#34;&gt;vsphereVolume&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;欢迎贡献更多类型.&lt;/p&gt;
&lt;!--
### awsElasticBlockStore {#awselasticblockstore}

An `awsElasticBlockStore` volume mounts an Amazon Web Services (AWS) [EBS
Volume](https://aws.amazon.com/ebs/) into your Pod.  Unlike
`emptyDir`, which is erased when a Pod is removed, the contents of an EBS
volume are preserved and the volume is merely unmounted.  This means that an
EBS volume can be pre-populated with data, and that data can be &#34;handed off&#34;
between Pods.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must create an EBS volume using &lt;code&gt;aws ec2 create-volume&lt;/code&gt; or the AWS API before you can use it.&lt;/div&gt;
&lt;/blockquote&gt;


There are some restrictions when using an `awsElasticBlockStore` volume:

* the nodes on which Pods are running must be AWS EC2 instances
* those instances need to be in the same region and availability-zone as the EBS volume
* EBS only supports a single EC2 instance mounting a volume
 --&gt;
&lt;h3 id=&#34;awselasticblockstore&#34;&gt;awsElasticBlockStore&lt;/h3&gt;
&lt;p&gt;一个 &lt;code&gt;awsElasticBlockStore&lt;/code&gt; 数据卷会挂载一个 AWS 的 &lt;a href=&#34;https://aws.amazon.com/ebs/&#34;&gt;EBS 卷&lt;/a&gt;
到 Pod 中。 与 &lt;code&gt;emptyDir&lt;/code&gt; 在 Pod 删除时清除数据不同， 在 EBS 卷中的内存会在卸载后依然会保存
其中的内容。也就是说一个 EBS 卷 可以预先存在数据，其中的数据也可以在不同的 Pod 之间传递。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 必须要先使用 &lt;code&gt;aws ec2 create-volume&lt;/code&gt; 或 AWS API 创建一个 EBS 卷 才能在 &lt;code&gt;awsElasticBlockStore&lt;/code&gt; 中使用。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;使用 &lt;code&gt;awsElasticBlockStore&lt;/code&gt; 卷有如下限制:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod 运行的节点必须是 AWS EC2 实例&lt;/li&gt;
&lt;li&gt;节点实例必须与 EBS 卷 在同一个地址或可用域&lt;/li&gt;
&lt;li&gt;EBS 只支持一个 EC2 实例挂载一个卷&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
#### Creating an EBS volume

Before you can use an EBS volume with a Pod, you need to create it.

```shell
aws ec2 create-volume --availability-zone=eu-west-1a --size=10 --volume-type=gp2
```

Make sure the zone matches the zone you brought up your cluster in.  (And also check that the size and EBS volume
type are suitable for your use!)
 --&gt;
&lt;h4 id=&#34;创建一个-ebs-卷&#34;&gt;创建一个 EBS 卷&lt;/h4&gt;
&lt;p&gt;在 Pod 中使用一个 EBS 卷 之前需要先创建。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;aws ec2 create-volume --availability-zone&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;eu-west-1a --size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt; --volume-type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;gp2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;需要确保卷所在的可用区域与集群所在的可用区域相同。(同时还要检查卷的大小和类型是合用的)&lt;/p&gt;
&lt;!--
#### AWS EBS Example configuration

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-ebs
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-ebs
      name: test-volume
  volumes:
  - name: test-volume
    # This AWS EBS volume must already exist.
    awsElasticBlockStore:
      volumeID: &lt;volume-id&gt;
      fsType: ext4
```
 --&gt;
&lt;h4 id=&#34;一个使用-aws-ebs-的配置示例&#34;&gt;一个使用 AWS EBS 的配置示例&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-ebs&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/test-ebs&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# This AWS EBS volume must already exist.&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;awsElasticBlockStore&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeID&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;&amp;lt;volume-id&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
#### CSI Migration






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;



The CSI Migration feature for awsElasticBlockStore, when enabled, shims all plugin operations
from the existing in-tree plugin to the `ebs.csi.aws.com` Container
Storage Interface (CSI) Driver. In order to use this feature, the [AWS EBS CSI
Driver](https://github.com/kubernetes-sigs/aws-ebs-csi-driver)
must be installed on the cluster and the `CSIMigration` and `CSIMigrationAWS`
Beta features must be enabled.
 --&gt;
&lt;h4 id=&#34;csi-迁移&#34;&gt;CSI 迁移&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;code&gt;awsElasticBlockStore&lt;/code&gt; 的 CSI 迁移特性，在启用时，会将来自已经存在的插件的插件操作转移到
&lt;code&gt;ebs.csi.aws.com&lt;/code&gt; 容器存储接口(CSI)驱动。 为了使用该特性， 需要在集群中先安装
&lt;a href=&#34;https://github.com/kubernetes-sigs/aws-ebs-csi-driver&#34;&gt;AWS EBS CSI 驱动&lt;/a&gt;
同时启用 &lt;code&gt;CSIMigration&lt;/code&gt; 和 &lt;code&gt;CSIMigrationAWS&lt;/code&gt; 两个 &lt;code&gt;beta&lt;/code&gt; 特性&lt;/p&gt;
&lt;!--
#### CSI Migration Complete





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [alpha]&lt;/code&gt;
&lt;/div&gt;



To turn off the awsElasticBlockStore storage plugin from being loaded by controller manager and kubelet, you need to set this feature flag to true. This requires `ebs.csi.aws.com` Container Storage Interface (CSI) driver being installed on all worker nodes.
 --&gt;
&lt;h4 id=&#34;csi-迁移完成&#34;&gt;CSI 迁移完成&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;为了关闭由控制器管理器和 kubelet 加载的 &lt;code&gt;awsElasticBlockStore&lt;/code&gt; 存储插件， 需要将这个标记设置
为 &lt;code&gt;true&lt;/code&gt;. 这需要在所有的工作节点安装 &lt;code&gt;ebs.csi.aws.com&lt;/code&gt; 容器存储接口(CSI)驱动&lt;/p&gt;
&lt;!--
### azureDisk {#azuredisk}

A `azureDisk` is used to mount a Microsoft Azure [Data Disk](https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-linux-about-disks-vhds/) into a Pod.

More details can be found [here](https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_disk/README.md).
 --&gt;
&lt;h3 id=&#34;azuredisk&#34;&gt;azureDisk&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;azureDisk&lt;/code&gt; 是用来将微软 Azure 的
&lt;a href=&#34;https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-linux-about-disks-vhds/&#34;&gt;Data Disk&lt;/a&gt;
数据盘挂载到 Pod 中的。
更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_disk/README.md&#34;&gt;这里&lt;/a&gt;.&lt;/p&gt;
&lt;!--
#### CSI Migration






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;



The CSI Migration feature for azureDisk, when enabled, shims all plugin operations
from the existing in-tree plugin to the `disk.csi.azure.com` Container
Storage Interface (CSI) Driver. In order to use this feature, the [Azure Disk CSI
Driver](https://github.com/kubernetes-sigs/azuredisk-csi-driver)
must be installed on the cluster and the `CSIMigration` and `CSIMigrationAzureDisk`
features must be enabled.
 --&gt;
&lt;h4 id=&#34;csi-迁移-1&#34;&gt;CSI 迁移&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;code&gt;azureDisk&lt;/code&gt; 的 CSI 迁移特性，在启用时，会将来自已经存在的插件的插件操作转移到
&lt;code&gt;disk.csi.azure.com&lt;/code&gt;  容器存储接口(CSI)驱动。 为了使用该特性， 需要在集群中先安装
&lt;a href=&#34;https://github.com/kubernetes-sigs/azuredisk-csi-driver&#34;&gt;Azure Disk CSI Driver&lt;/a&gt;
同时启用 &lt;code&gt;CSIMigration&lt;/code&gt; 和 &lt;code&gt;CSIMigrationAzureDisk&lt;/code&gt; 两个特性&lt;/p&gt;
&lt;!--
### azureFile {#azurefile}

A `azureFile` is used to mount a Microsoft Azure File Volume (SMB 2.1 and 3.0)
into a Pod.

More details can be found [here](https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_file/README.md).
 --&gt;
&lt;h3 id=&#34;azurefile&#34;&gt;azureFile&lt;/h3&gt;
&lt;p&gt;A &lt;code&gt;azureFile&lt;/code&gt; is used to mount a Microsoft Azure File Volume (SMB 2.1 and 3.0)
into a Pod.
&lt;code&gt;azureFile&lt;/code&gt; 用于将微软 Azure 文件卷(SMB 2.1 and 3.0) 挂载到 Pod 中。
更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_file/README.md&#34;&gt;这里&lt;/a&gt;.&lt;/p&gt;
&lt;!--
#### CSI Migration






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.15 [alpha]&lt;/code&gt;
&lt;/div&gt;



The CSI Migration feature for azureFile, when enabled, shims all plugin operations
from the existing in-tree plugin to the `file.csi.azure.com` Container
Storage Interface (CSI) Driver. In order to use this feature, the [Azure File CSI
Driver](https://github.com/kubernetes-sigs/azurefile-csi-driver)
must be installed on the cluster and the `CSIMigration` and `CSIMigrationAzureFile`
Alpha features must be enabled.
 --&gt;
&lt;h4 id=&#34;csi-迁移-2&#34;&gt;CSI 迁移&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.15 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;code&gt;azureFile&lt;/code&gt; 的 CSI 迁移特性，在启用时，会将来自已经存在的插件的插件操作转移到
&lt;code&gt;file.csi.azure.com&lt;/code&gt;  容器存储接口(CSI)驱动。 为了使用该特性， 需要在集群中先安装
&lt;a href=&#34;https://github.com/kubernetes-sigs/azurefile-csi-driver&#34;&gt;Azure File CSI Driver&lt;/a&gt;
同时启用 &lt;code&gt;CSIMigration&lt;/code&gt; 和 &lt;code&gt;CSIMigrationAzureFile&lt;/code&gt; 两个 &lt;code&gt;Alpha&lt;/code&gt; 特性&lt;/p&gt;
&lt;h3 id=&#34;cephfs&#34;&gt;cephfs&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;cephfs&lt;/code&gt; 卷可以将一个已经存在的 CephFS 卷挂载到一个 Pod 中。
与 &lt;code&gt;emptyDir&lt;/code&gt; 在 Pod 删除时清除数据不同， 在 &lt;code&gt;cephfs&lt;/code&gt; 卷中的内存会在卸载后依然会保存
其中的内容。也就是说一个 &lt;code&gt;cephfs&lt;/code&gt; 卷可以预先存在数据，其中的数据也可以在不同的 Pod 之间传递。
CephFS 还可以被同时多个挂载拥有写权限&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在使用之前需要有自己的 Ceph 服务在运行，并且相应的分离已经配置好&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/volumes/cephfs/&#34;&gt;CephFS 示例&lt;/a&gt;&lt;/p&gt;
&lt;!--
### cinder {#cinder}

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Prerequisite: Kubernetes with OpenStack Cloud Provider configured.&lt;/div&gt;
&lt;/blockquote&gt;


`cinder` is used to mount OpenStack Cinder Volume into your Pod.
 --&gt;
&lt;h3 id=&#34;cinder&#34;&gt;cinder&lt;/h3&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 前置条件: 配置好 OpenStack 云提供商的 k8s 集群。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;cinder&lt;/code&gt; 用于将 OpenStack Cinder 卷挂载到 Pod.&lt;/p&gt;
&lt;!--
#### Cinder Volume Example configuration

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-cinder
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-cinder-container
    volumeMounts:
    - mountPath: /test-cinder
      name: test-volume
  volumes:
  - name: test-volume
    # This OpenStack volume must already exist.
    cinder:
      volumeID: &lt;volume-id&gt;
      fsType: ext4
```
--&gt;
&lt;h4 id=&#34;cinder-volume-配置示例&#34;&gt;Cinder Volume 配置示例&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-cinder&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-cinder-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/test-cinder&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# 这个 OpenStack 卷必须已经存在.&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;cinder&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeID&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;&amp;lt;volume-id&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;csi-迁移-3&#34;&gt;CSI 迁移&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;code&gt;Cinder&lt;/code&gt; 的 CSI 迁移特性，在启用时，会将来自已经存在的插件的插件操作转移到 &lt;code&gt;cinder.csi.openstack.org&lt;/code&gt;  容器存储接口(CSI)驱动。 为了使用该特性， 需要在集群中先安装
&lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-cinder-csi-plugin.md&#34;&gt;Openstack Cinder CSI
Driver&lt;/a&gt;
同时启用 &lt;code&gt;CSIMigration&lt;/code&gt; 和 &lt;code&gt;CSIMigrationOpenStack&lt;/code&gt; 两个 &lt;code&gt;beta&lt;/code&gt; 特性&lt;/p&gt;
&lt;!--
### configMap {#configmap}

The [`configMap`](/docs/tasks/configure-pod-container/configure-pod-configmap/) resource
provides a way to inject configuration data into Pods.
The data stored in a `ConfigMap` object can be referenced in a volume of type
`configMap` and then consumed by containerized applications running in a Pod.

When referencing a `configMap` object, you can simply provide its name in the
volume to reference it. You can also customize the path to use for a specific
entry in the ConfigMap.
For example, to mount the `log-config` ConfigMap onto a Pod called `configmap-pod`,
you might use the YAML below:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
    - name: test
      image: busybox
      volumeMounts:
        - name: config-vol
          mountPath: /etc/config
  volumes:
    - name: config-vol
      configMap:
        name: log-config
        items:
          - key: log_level
            path: log_level
```

The `log-config` ConfigMap is mounted as a volume, and all contents stored in
its `log_level` entry are mounted into the Pod at path &#34;`/etc/config/log_level`&#34;.
Note that this path is derived from the volume&#39;s `mountPath` and the `path`
keyed with `log_level`.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must create a &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/configure-pod-container/configure-pod-configmap/&#34;&gt;ConfigMap&lt;/a&gt; before you can use it.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A Container using a ConfigMap as a &lt;a href=&#34;#using-subpath&#34;&gt;subPath&lt;/a&gt; volume mount will not
receive ConfigMap updates.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Text data is exposed as files using the UTF-8 character encoding. To use some other character encoding, use binaryData.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;configmap&#34;&gt;configMap&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/configure-pod-container/configure-pod-configmap/&#34;&gt;&lt;code&gt;configMap&lt;/code&gt;&lt;/a&gt; 资源提供了一种将配置数据注入 Pod 的方式。&lt;/p&gt;
&lt;p&gt;存在于 &lt;code&gt;ConfigMap&lt;/code&gt; 对象中的数据可以通过 &lt;code&gt;configMap&lt;/code&gt; 类型的数据卷被 Pod 引用，然后被存在于 Pod 中的容器化应用所使用。&lt;/p&gt;
&lt;p&gt;在引用 &lt;code&gt;configMap&lt;/code&gt; 对象时，只需要提供这个对象的名称在引用它的卷上面。
也可以通过自定义路径的方式使用 ConfigMap 中指定的条目。
例如， 将一个叫 &lt;code&gt;log-config&lt;/code&gt; ConfigMap 挂载到一个 叫 &lt;code&gt;configmap-pod&lt;/code&gt; 的 Pod上，配置 YAML 如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;configmap-pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;config-vol&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/etc/config&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;config-vol&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;configMap&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;log-config&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;items&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;log_level&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;log_level&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;log-config&lt;/code&gt; ConfigMap 以卷的方式挂载， 其中所有存在于条目 &lt;code&gt;log_level&lt;/code&gt; 的内容会被挂载到 Pod 的 &lt;code&gt;/etc/config/log_level&lt;/code&gt; 上
其中这个路径继承了卷上面的 &lt;code&gt;mountPath&lt;/code&gt; 和 键名为 &lt;code&gt;log_level&lt;/code&gt; 的
&lt;code&gt;path&lt;/code&gt; 的值 -  &lt;code&gt;log_level&lt;/code&gt;&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在使用之前必须要先创建对应的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-pod-configmap/&#34;&gt;ConfigMap&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 一个以 &lt;a href=&#34;#using-subpath&#34;&gt;subPath&lt;/a&gt; 卷方式使用 ConfigMap 的容器是
不能接收到 ConfigMap 的更新的&lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 文本内存数据以 UTF-8编码的文件方式提供。 要使用其它的编码方式请使用
二进制数据(&lt;code&gt;binaryData&lt;/code&gt;)&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### downwardAPI {#downwardapi}

A `downwardAPI` volume is used to make downward API data available to applications.
It mounts a directory and writes the requested data in plain text files.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A Container using Downward API as a &lt;a href=&#34;#using-subpath&#34;&gt;subPath&lt;/a&gt; volume mount will not
receive Downward API updates.&lt;/div&gt;
&lt;/blockquote&gt;


See the [`downwardAPI` volume example](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)  for more details.

 --&gt;
&lt;h3 id=&#34;downwardapi&#34;&gt;downwardAPI&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;downwardAPI&lt;/code&gt; 卷用来让 downward API 的数据在应用中可用。
它通过挂载一个目录然后将请求的数据以纯文本文件的形式写在这个目录中。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 一个以 &lt;a href=&#34;#using-subpath&#34;&gt;subPath&lt;/a&gt; 卷方式使用 Downward API 的容器是 不能接收到 Downward API 的更新的&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;更多信息见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/&#34;&gt;&lt;code&gt;downwardAPI&lt;/code&gt; 卷示例&lt;/a&gt;&lt;/p&gt;
&lt;!--

### emptyDir {#emptydir}

An `emptyDir` volume is first created when a Pod is assigned to a Node, and
exists as long as that Pod is running on that node.  As the name says, it is
initially empty.  Containers in the Pod can all read and write the same
files in the `emptyDir` volume, though that volume can be mounted at the same
or different paths in each Container.  When a Pod is removed from a node for
any reason, the data in the `emptyDir` is deleted forever.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A Container crashing does &lt;em&gt;NOT&lt;/em&gt; remove a Pod from a node, so the data in an &lt;code&gt;emptyDir&lt;/code&gt; volume is safe across Container crashes.&lt;/div&gt;
&lt;/blockquote&gt;


Some uses for an `emptyDir` are:

* scratch space, such as for a disk-based merge sort
* checkpointing a long computation for recovery from crashes
* holding files that a content-manager Container fetches while a webserver
  Container serves the data

By default, `emptyDir` volumes are stored on whatever medium is backing the
node - that might be disk or SSD or network storage, depending on your
environment.  However, you can set the `emptyDir.medium` field to `&#34;Memory&#34;`
to tell Kubernetes to mount a tmpfs (RAM-backed filesystem) for you instead.
While tmpfs is very fast, be aware that unlike disks, tmpfs is cleared on
node reboot and any files you write will count against your Container&#39;s
memory limit.
 --&gt;
&lt;h3 id=&#34;emptydir&#34;&gt;emptyDir&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;emptyDir&lt;/code&gt; 卷在 Pod 分配到节点时创建，只要这个 Pod 还在这个节点上运行，
那么这个郑就会存在。 就像名称所示，它就是个空目录，所以初始化时是空的。
Pod 中 所有的容器都可以读写这个 &lt;code&gt;emptyDir&lt;/code&gt; 卷中的文件， 这个卷可以挂载到每个
容器的同一个路径或不同的路径。当 Pod 因为任何原因从节点移除时， 存在于  &lt;code&gt;emptyDir&lt;/code&gt;
中的数据会被永久删除。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 容器崩溃并 &lt;em&gt;不会&lt;/em&gt; 导致 Pod 从节点删除，所以容器崩溃并不会导致 &lt;code&gt;emptyDir&lt;/code&gt; 中的数据丢失。&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;Some uses for an &lt;code&gt;emptyDir&lt;/code&gt; are:
一些用到 &lt;code&gt;emptyDir&lt;/code&gt; 的地方:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;临时空间，例如基于磁盘的合并排序&lt;/li&gt;
&lt;li&gt;作为一个恢复崩溃的长时计划的检查点
Container serves the data&lt;/li&gt;
&lt;li&gt;存放由 内容管理容器抓取然后用于 web 服务器使用的数据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;默认情况下， &lt;code&gt;emptyDir&lt;/code&gt; 卷使用所在节点所使用的介质存储数据，基于环境可能是机械硬盘
SSD 或 网络存储。 但是，用户可以通过设置 &lt;code&gt;emptyDir.medium&lt;/code&gt; 字段值为 &lt;code&gt;&amp;quot;Memory&amp;quot;&lt;/code&gt;
让 k8s 将其挂载为 tmpfs (基于内存的文件系统)。
因为 tmpfs 的速度很快，但要注意与磁盘不同， tmpfs 会在节点重启时被清除，
可用空间的大小受内存大小限制。&lt;/p&gt;
&lt;h4 id=&#34;示例&#34;&gt;示例&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-pd&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/cache&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cache-volume&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cache-volume&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;emptyDir&lt;/span&gt;: {}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;fc&#34;&gt;fc (fibre channel)&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;fc&lt;/code&gt; 卷可以将光纤信道(fibre channel)卷挂载到 Pod 中。 可以在卷的配置中使用
&lt;code&gt;targetWWNs&lt;/code&gt; 参数指定一个或多个目标全局名称(World Wide Names).
如果指定了多个 WWNs， targetWWNs 使用的是那些来自 multi-path 连接的  WWNs
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 必须要预先配置 FC SAN Zoning 分配并且标记这些目标 WWN 的 LUNs (卷)
这样 k8s 主机才能访问它们&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/fibre_channel&#34;&gt;FC 示例&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;flocker&#34;&gt;flocker&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ClusterHQ/flocker&#34;&gt;Flocker&lt;/a&gt; 是一个开源的集群
容器数据卷管理器。 它提供了管理和编排多种存储后端的数据卷&lt;/p&gt;
&lt;p&gt;&lt;code&gt;flocker&lt;/code&gt; 让 Flocker 数据集可以被挂载到 Pod 中。 如果数据集还没有存在于 Flocker， 必须要使用 Flocker
CLI 或 Flocker API 创建。 如果数据集已经存在于 Flocker， 当 Pod 被调度到
节点时会被重新通过 Flocker 挂载。这就是说这些数据可以在需要的 Pod 传递。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 必须要在使用之前先安装运行 Flocker&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/flocker&#34;&gt;Flocker 示例&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;gcepersistentdisk&#34;&gt;gcePersistentDisk&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;gcePersistentDisk&lt;/code&gt; 卷可以将 Google 计算引擎(GCE)
&lt;a href=&#34;https://cloud.google.com/compute/docs/disks&#34;&gt;持久化磁盘(PD)&lt;/a&gt;
挂载到 Pod 中。 与 &lt;code&gt;emptyDir&lt;/code&gt; 在 Pod 删除时同时删除不同， PD 中的内存会依旧保留 仅仅卸载卷。 这就是说 PD 可以预先添加数据，并且可以在不同 Pod 之间传递。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 必须要先使用 &lt;code&gt;gcloud&lt;/code&gt; 或 GCE API 或 UI 创建 PD 才能使用。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;使用 &lt;code&gt;gcePersistentDisk&lt;/code&gt; 有如下限制:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod 运行的节点必须是 GCE 虚拟机&lt;/li&gt;
&lt;li&gt;这些虚拟机必须与 PD 在同一个 GCE 项目和区域&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PD 有一个特性是如果要同时挂载到多个 Pod 只能以只读方式挂载。
也就是可以预先将数据存入 PD 中，然后就可以在需要的任意个多 Pod 中使用。
不幸的是只能以读写方式挂载一次，不能以写方式多次挂载
在一个用 ReplicationController 管理的 Pod 中使用 PD 时，只有在 PD 为只读
或副本数为 0 或 1 时才能成功否则就会失败。&lt;/p&gt;
&lt;!--
#### Creating a PD

Before you can use a GCE PD with a Pod, you need to create it.

```shell
gcloud compute disks create --size=500GB --zone=us-central1-a my-data-disk
```

#### Example Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    # This GCE PD must already exist.
    gcePersistentDisk:
      pdName: my-data-disk
      fsType: ext4
```
 --&gt;
&lt;h4 id=&#34;创建-pd&#34;&gt;创建 PD&lt;/h4&gt;
&lt;p&gt;在 Pod 中使用 GCE PD 前需要先创建&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;gcloud compute disks create --size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;500GB --zone&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;us-central1-a my-data-disk
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;示例-pod&#34;&gt;示例 Pod&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-pd&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/test-pd&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# 这个 GCE PD 必须要先存在&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;gcePersistentDisk&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;pdName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-data-disk&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
#### Regional Persistent Disks
The [Regional Persistent Disks](https://cloud.google.com/compute/docs/disks/#repds) feature allows the creation of Persistent Disks that are available in two zones within the same region. In order to use this feature, the volume must be provisioned as a PersistentVolume; referencing the volume directly from a pod is not supported.
 --&gt;
&lt;h4 id=&#34;地区性持久化磁盘&#34;&gt;地区性持久化磁盘&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/compute/docs/disks/#repds&#34;&gt;地区性持久化磁盘&lt;/a&gt; 特性允许在同一个地区的两个区域创建 地区性持久化磁盘。
要使用这个特性， 这个卷必须由 PersistentVolume 管理；不支持在 Pod 直接使用&lt;/p&gt;
&lt;!--
#### Manually provisioning a Regional PD PersistentVolume
Dynamic provisioning is possible using a [StorageClass for GCE PD](/docs/concepts/storage/storage-classes/#gce).
Before creating a PersistentVolume, you must create the PD:
```shell
gcloud compute disks create --size=500GB my-data-disk
    --region us-central1
    --replica-zones us-central1-a,us-central1-b
```
Example PersistentVolume spec:

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: test-volume
spec:
  capacity:
    storage: 400Gi
  accessModes:
  - ReadWriteOnce
  gcePersistentDisk:
    pdName: my-data-disk
    fsType: ext4
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: failure-domain.beta.kubernetes.io/zone
          operator: In
          values:
          - us-central1-a
          - us-central1-b
```
 --&gt;
&lt;h4 id=&#34;手动管理一个地区性-pd-persistentvolume&#34;&gt;手动管理一个地区性 PD PersistentVolume&lt;/h4&gt;
&lt;p&gt;动态管理使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-classes/#gce&#34;&gt;StorageClass for GCE PD&lt;/a&gt;.
在创建 PersistentVolume 之前，需要先创建 PD:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;gcloud compute disks create --size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;500GB my-data-disk
    --region us-central1
    --replica-zones us-central1-a,us-central1-b
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;示例 PersistentVolume spec:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;capacity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;400Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;gcePersistentDisk&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;pdName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-data-disk&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;nodeAffinity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;required&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelectorTerms&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;failure-domain.beta.kubernetes.io/zone&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
          - &lt;span style=&#34;color:#ae81ff&#34;&gt;us-central1-a&lt;/span&gt;
          - &lt;span style=&#34;color:#ae81ff&#34;&gt;us-central1-b&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
#### CSI Migration






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;



The CSI Migration feature for GCE PD, when enabled, shims all plugin operations
from the existing in-tree plugin to the `pd.csi.storage.gke.io` Container
Storage Interface (CSI) Driver. In order to use this feature, the [GCE PD CSI
Driver](https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver)
must be installed on the cluster and the `CSIMigration` and `CSIMigrationGCE`
Beta features must be enabled.
 --&gt;
&lt;h4 id=&#34;csi-迁移-4&#34;&gt;CSI 迁移&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;GCE PD 的 CSI 迁移特性，在启用时，会将来自已经存在的插件的插件操作转移到 &lt;code&gt;pd.csi.storage.gke.io&lt;/code&gt;  容器存储接口(CSI)驱动。 为了使用该特性， 需要在集群中先安装
&lt;a href=&#34;https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver&#34;&gt;GCE PD CSI
Driver&lt;/a&gt;
同时启用 &lt;code&gt;CSIMigration&lt;/code&gt; 和 &lt;code&gt;CSIMigrationGCE&lt;/code&gt; 两个 &lt;code&gt;beta&lt;/code&gt; 特性&lt;/p&gt;
&lt;!--
### gitRepo (deprecated) {#gitrepo}

&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; The gitRepo volume type is deprecated. To provision a container with a git repo, mount an &lt;a href=&#34;#emptydir&#34;&gt;EmptyDir&lt;/a&gt; into an InitContainer that clones the repo using git, then mount the &lt;a href=&#34;#emptydir&#34;&gt;EmptyDir&lt;/a&gt; into the Pod&amp;rsquo;s container.&lt;/div&gt;
&lt;/blockquote&gt;


A `gitRepo` volume is an example of what can be done as a volume plugin.  It
mounts an empty directory and clones a git repository into it for your Pod to
use.  In the future, such volumes may be moved to an even more decoupled model,
rather than extending the Kubernetes API for every such use case.

Here is an example of gitRepo volume:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: server
spec:
  containers:
  - image: nginx
    name: nginx
    volumeMounts:
    - mountPath: /mypath
      name: git-volume
  volumes:
  - name: git-volume
    gitRepo:
      repository: &#34;git@somewhere:me/my-git-repository.git&#34;
      revision: &#34;22f1d8406d464b0c0874075539c1f2e96c253775&#34;
```
 --&gt;
&lt;h3 id=&#34;gitrepo&#34;&gt;gitRepo (废弃)&lt;/h3&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; gitRepo 卷类型已经被废弃。 要管理一个包含 git 创建的容器，可以在初始化容器
(InitContainer) 中挂载一个 &lt;a href=&#34;#emptydir&#34;&gt;EmptyDir&lt;/a&gt; 使用 git 克隆这个仓库，
然后在需要使用的容器的 Pod 上挂载这个 &lt;a href=&#34;#emptydir&#34;&gt;EmptyDir&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;gitRepo&lt;/code&gt; 卷是一个能够使用卷插件的示例。 它挂载一个空目录然后在其中克隆一个
git 创建到其中，供 Pod 使用。 在未来，这些卷可能被改为更加松耦合的模式，
而不是每次在这些情况下都通过扩展 k8s  API 来实现。
以下为 gitRepo 卷示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;server&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/mypath&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;git-volume&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;git-volume&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;gitRepo&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;repository&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git@somewhere:me/my-git-repository.git&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;revision&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;22f1d8406d464b0c0874075539c1f2e96c253775&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### glusterfs {#glusterfs}

A `glusterfs` volume allows a [Glusterfs](https://www.gluster.org) (an open
source networked filesystem) volume to be mounted into your Pod.  Unlike
`emptyDir`, which is erased when a Pod is removed, the contents of a
`glusterfs` volume are preserved and the volume is merely unmounted.  This
means that a glusterfs volume can be pre-populated with data, and that data can
be &#34;handed off&#34; between Pods.  GlusterFS can be mounted by multiple writers
simultaneously.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must have your own GlusterFS installation running before you can use it.&lt;/div&gt;
&lt;/blockquote&gt;


See the [GlusterFS example](https://github.com/kubernetes/examples/tree/master/volumes/glusterfs) for more details.
 --&gt;
&lt;h3 id=&#34;glusterfs&#34;&gt;glusterfs&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;glusterfs&lt;/code&gt; 卷可以将 &lt;a href=&#34;https://www.gluster.org&#34;&gt;Glusterfs&lt;/a&gt;
(一个开源的网络文件系统)卷挂载到 Pod 中。
与 &lt;code&gt;emptyDir&lt;/code&gt; 在 Pod 删除时清除数据不同， 在 &lt;code&gt;glusterfs&lt;/code&gt; 卷中的内存会在卸载后依然会保存
其中的内容。也就是说一个 &lt;code&gt;glusterfs&lt;/code&gt; 卷可以预先存在数据，其中的数据也可以在不同的 Pod 之间传递。
GlusterFS 还可以被同时多个挂载拥有写权限
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在使用之前必须要先安装运行自己的 GlusterFS&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;更多个信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/volumes/glusterfs&#34;&gt;GlusterFS 示例&lt;/a&gt;&lt;/p&gt;
&lt;!--
### hostPath {#hostpath}

A `hostPath` volume mounts a file or directory from the host node&#39;s filesystem
into your Pod. This is not something that most Pods will need, but it offers a
powerful escape hatch for some applications.

For example, some uses for a `hostPath` are:

* running a Container that needs access to Docker internals; use a `hostPath`
  of `/var/lib/docker`
* running cAdvisor in a Container; use a `hostPath` of `/sys`
* allowing a Pod to specify whether a given `hostPath` should exist prior to the
  Pod running, whether it should be created, and what it should exist as

In addition to the required `path` property, user can optionally specify a `type` for a `hostPath` volume.

The supported values for field `type` are:


| Value | Behavior |
|:------|:---------|
| | Empty string (default) is for backward compatibility, which means that no checks will be performed before mounting the hostPath volume. |
| `DirectoryOrCreate` | If nothing exists at the given path, an empty directory will be created there as needed with permission set to 0755, having the same group and ownership with Kubelet. |
| `Directory` | A directory must exist at the given path |
| `FileOrCreate` | If nothing exists at the given path, an empty file will be created there as needed with permission set to 0644, having the same group and ownership with Kubelet. |
| `File` | A file must exist at the given path |
| `Socket` | A UNIX socket must exist at the given path |
| `CharDevice` | A character device must exist at the given path |
| `BlockDevice` | A block device must exist at the given path |

Watch out when using this type of volume, because:

* Pods with identical configuration (such as created from a podTemplate) may
  behave differently on different nodes due to different files on the nodes
* when Kubernetes adds resource-aware scheduling, as is planned, it will not be
  able to account for resources used by a `hostPath`
* the files or directories created on the underlying hosts are only writable by root. You
  either need to run your process as root in a
  [privileged Container](/docs/tasks/configure-pod-container/security-context/) or modify the file
  permissions on the host to be able to write to a `hostPath` volume
 --&gt;
&lt;h3 id=&#34;hostpath&#34;&gt;hostPath&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;hostPath&lt;/code&gt; 卷可以将节点上的一个文件或目录挂载到 Pod 中. 这种方式不是大多数
Pod 所需要要的， 但也能为有些应用提供强力支持。&lt;/p&gt;
&lt;p&gt;For example, some uses for a &lt;code&gt;hostPath&lt;/code&gt; are:
例如以下为可以用到 &lt;code&gt;hostPath&lt;/code&gt; 的地方:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;运行一个需要访问 Docker 内部的应用，使用 &lt;code&gt;hostPath&lt;/code&gt; 挂载 &lt;code&gt;/var/lib/docker&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在容器中运行 &lt;code&gt;cAdvisor&lt;/code&gt;； 使用 &lt;code&gt;hostPath&lt;/code&gt; 挂载 &lt;code&gt;/sys&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;允许 Pod 指定某个 &lt;code&gt;hostPath&lt;/code&gt; 在 Pod 运行之前需要存在，这个目录是否需要创建，以什么方式存在(在说啥？)
In addition to the required &lt;code&gt;path&lt;/code&gt; property, user can optionally specify a &lt;code&gt;type&lt;/code&gt; for a &lt;code&gt;hostPath&lt;/code&gt; volume.
在需要的 &lt;code&gt;path&lt;/code&gt; 属性外， 用户还可以为 &lt;code&gt;hostPath&lt;/code&gt; 指定一个可选的 &lt;code&gt;type&lt;/code&gt; 属性。
The supported values for field &lt;code&gt;type&lt;/code&gt; are:
&lt;code&gt;type&lt;/code&gt; 字段支持的值有:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;值&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;行为&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;空字符器(默认)为了向后兼容，含义是在执行挂载 &lt;code&gt;hostPath&lt;/code&gt; 卷之前不执行任何检查&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;DirectoryOrCreate&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;如果指定路径不存在，则创建一个空目录，权限为 0755，组和所属关系与 kubelet 相同&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Directory&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;路径必须是一个已经存在的目录&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;FileOrCreate&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;如果指定路径不存在，则创建一个空文件，权限为 0644，组和所属关系与 kubelet 相同&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;File&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;路径必须是一个已经存在的文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Socket&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;路径必须是一个已经存在的 UNIX 套接字&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;CharDevice&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;路径必须是一个已经存在的 字符设备(character device)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;BlockDevice&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;路径必须是一个已经存在的 块设备(block device)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Watch out when using this type of volume, because:
在使用带类型的卷时需要小心，因为:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拥有相同配置(如由同一个 podTemplate 创建的) Pod 可以会因为不同节点上拥有的文件不同为表现不同行为&lt;/li&gt;
&lt;li&gt;根据设计，当 k8s 添加资源感知(resource-aware) 调度时，不会计算 &lt;code&gt;hostPath&lt;/code&gt; 使用的资源。&lt;/li&gt;
&lt;li&gt;在底层主机上创建的文件或目录只有 root 拥有写权限。 所以必须要在一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/security-context/&#34;&gt;提权容器&lt;/a&gt;
以 root 权限运行进程或在主机上修改文件权限以便可以让 &lt;code&gt;hostPath&lt;/code&gt; 卷为可读&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
#### Example Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data
      # this field is optional
      type: Directory
```

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; It should be noted that the &lt;code&gt;FileOrCreate&lt;/code&gt; mode does not create the parent directory of the file. If the parent directory of the mounted file does not exist, the pod fails to start. To ensure that this mode works, you can try to mount directories and files separately, as shown below.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h4 id=&#34;示例-pod-1&#34;&gt;示例 Pod&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-pd&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/test-pd&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
      &lt;span style=&#34;color:#75715e&#34;&gt;# 主机上的目录&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/data&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# 该字段可选&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Directory&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 要注意 &lt;code&gt;FileOrCreate&lt;/code&gt; 模式时，如果文件的父目录不存在是不会自动创建的。 如果挂载文件的父目录不
存在， Pod 就会启动失败。 为了保证该模式正常工作，可以参考下面的方式，将目录和文件分开挂载。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
#### Example Pod FileOrCreate

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-webserver
spec:
  containers:
  - name: test-webserver
    image: k8s.gcr.io/test-webserver:latest
    volumeMounts:
    - mountPath: /var/local/aaa
      name: mydir
    - mountPath: /var/local/aaa/1.txt
      name: myfile
  volumes:
  - name: mydir
    hostPath:
      # Ensure the file directory is created.
      path: /var/local/aaa
      type: DirectoryOrCreate
  - name: myfile
    hostPath:
      path: /var/local/aaa/1.txt
      type: FileOrCreate
```
 --&gt;
&lt;h4 id=&#34;fileorcreate-示例-pod&#34;&gt;FileOrCreate 示例 Pod&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-webserver&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver:latest&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/local/aaa&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mydir&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/local/aaa/1.txt&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myfile&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mydir&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
      &lt;span style=&#34;color:#75715e&#34;&gt;# 确保文件所在的目录是存在的&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/local/aaa&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DirectoryOrCreate&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myfile&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/local/aaa/1.txt&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;FileOrCreate&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### iscsi {#iscsi}

An `iscsi` volume allows an existing iSCSI (SCSI over IP) volume to be mounted
into your Pod.  Unlike `emptyDir`, which is erased when a Pod is removed, the
contents of an `iscsi` volume are preserved and the volume is merely
unmounted.  This means that an iscsi volume can be pre-populated with data, and
that data can be &#34;handed off&#34; between Pods.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must have your own iSCSI server running with the volume created before you can use it.&lt;/div&gt;
&lt;/blockquote&gt;


A feature of iSCSI is that it can be mounted as read-only by multiple consumers
simultaneously.  This means that you can pre-populate a volume with your dataset
and then serve it in parallel from as many Pods as you need.  Unfortunately,
iSCSI volumes can only be mounted by a single consumer in read-write mode - no
simultaneous writers allowed.

See the [iSCSI example](https://github.com/kubernetes/examples/tree/master/volumes/iscsi) for more details.
 --&gt;
&lt;h3 id=&#34;iscsi&#34;&gt;iscsi&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;iscsi&lt;/code&gt; 卷可以将已经存在的 iSCSI (SCSI over IP) 卷挂载到 Pod 中。
与 &lt;code&gt;emptyDir&lt;/code&gt; 在 Pod 删除时清除数据不同， 在 &lt;code&gt;iscsi&lt;/code&gt; 卷中的内存会在卸载后依然会保存
其中的内容。也就是说一个 &lt;code&gt;iscsi&lt;/code&gt; 卷可以预先存在数据，其中的数据也可以在不同的 Pod 之间传递。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在创建和使用卷之前必须为先有运行的 iSCSI 服务。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;iSCSI&lt;/code&gt; 一个特性是可以以只读的方式同时挂载到多个消费者中。 这就代表着可以预先将数据集放入卷中
然后根据需要使用任意数量的 Pod 来提供访问服务。 不过 iSCSI 卷以读写方式挂载时只能有一个消费者，
不允许并行写。
更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/volumes/iscsi&#34;&gt;iSCSI 示例&lt;/a&gt;&lt;/p&gt;
&lt;!--
### local {#local}






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [stable]&lt;/code&gt;
&lt;/div&gt;



A `local` volume represents a mounted local storage device such as a disk,
partition or directory.

Local volumes can only be used as a statically created PersistentVolume. Dynamic
provisioning is not supported yet.

Compared to `hostPath` volumes, local volumes can be used in a durable and
portable manner without manually scheduling Pods to nodes, as the system is aware
of the volume&#39;s node constraints by looking at the node affinity on the PersistentVolume.

However, local volumes are still subject to the availability of the underlying
node and are not suitable for all applications. If a node becomes unhealthy,
then the local volume will also become inaccessible, and a Pod using it will not
be able to run. Applications using local volumes must be able to tolerate this
reduced availability, as well as potential data loss, depending on the
durability characteristics of the underlying disk.

The following is an example of PersistentVolume spec using a `local` volume and
`nodeAffinity`:

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 100Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /mnt/disks/ssd1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - example-node
```

PersistentVolume `nodeAffinity` is required when using local volumes. It enables
the Kubernetes scheduler to correctly schedule Pods using local volumes to the
correct node.

PersistentVolume `volumeMode` can be set to &#34;Block&#34; (instead of the default
value &#34;Filesystem&#34;) to expose the local volume as a raw block device.

When using local volumes, it is recommended to create a StorageClass with
`volumeBindingMode` set to `WaitForFirstConsumer`. See the
[example](/docs/concepts/storage/storage-classes/#local). Delaying volume binding ensures
that the PersistentVolumeClaim binding decision will also be evaluated with any
other node constraints the Pod may have, such as node resource requirements, node
selectors, Pod affinity, and Pod anti-affinity.

An external static provisioner can be run separately for improved management of
the local volume lifecycle. Note that this provisioner does not support dynamic
provisioning yet. For an example on how to run an external local provisioner,
see the [local volume provisioner user
guide](https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner).

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The local PersistentVolume requires manual cleanup and deletion by the
user if the external static provisioner is not used to manage the volume
lifecycle.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;local&#34;&gt;local&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;code&gt;local&lt;/code&gt; 卷代表一个挂载的本地存储，例如磁盘，分区或目录
&lt;code&gt;local&lt;/code&gt; 卷只能以 静态创建 &lt;code&gt;PersistentVolume&lt;/code&gt; 的方式使用。 目前还不支持动态管理&lt;/p&gt;
&lt;p&gt;Compared to &lt;code&gt;hostPath&lt;/code&gt; volumes, local volumes can be used in a durable and
portable manner without manually scheduling Pods to nodes, as the system is aware
of the volume&amp;rsquo;s node constraints by looking at the node affinity on the PersistentVolume.
与 &lt;code&gt;hostPath&lt;/code&gt; 相比， &lt;code&gt;local&lt;/code&gt; 卷可以通过手动在节点上管理实现持久的和可移植的， 然后根据
PersistentVolume 的节点亲和性来打开对应节点上的卷。&lt;/p&gt;
&lt;p&gt;但是， &lt;code&gt;local&lt;/code&gt; 卷依然受底层节点的可用性影响，不是对所有应用都适用。 如果有一个节点应得不健康，
这个节点上的 &lt;code&gt;local&lt;/code&gt; 也会变得不可用，使用这个卷的 Pod 也不能运行。 使用 &lt;code&gt;local&lt;/code&gt; 卷的这些应用必要
要能忍受这样可用性降低的情况，同时还因为底层磁盘的持久性特性而产生的潜在的数据丢失的风险&lt;/p&gt;
&lt;p&gt;以下示例中使用了 &lt;code&gt;local&lt;/code&gt;卷和 &lt;code&gt;nodeAffinity&lt;/code&gt; 的 &lt;code&gt;PersistentVolume&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;example-pv&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;capacity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;100Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Filesystem&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeReclaimPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Delete&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;local-storage&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;local&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/mnt/disks/ssd1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;nodeAffinity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;required&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelectorTerms&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/hostname&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
          - &lt;span style=&#34;color:#ae81ff&#34;&gt;example-node&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在使用 &lt;code&gt;local&lt;/code&gt; 卷时 PersistentVolume 需要使用 &lt;code&gt;nodeAffinity&lt;/code&gt;。 这让 k8s 可以正确地将
使用 &lt;code&gt;local&lt;/code&gt; 卷的 Pod 调度到正确的节点上。&lt;/p&gt;
&lt;p&gt;PersistentVolume 的 &lt;code&gt;volumeMode&lt;/code&gt; 可以被设置为 &amp;ldquo;Block&amp;rdquo; (而不是默认值 &amp;ldquo;Filesystem&amp;rdquo;)
将这个 &lt;code&gt;local&lt;/code&gt; 卷作为块设备&lt;/p&gt;
&lt;p&gt;当使用 &lt;code&gt;local&lt;/code&gt; 卷时， 推荐在创建 StorageClass 时将 &lt;code&gt;volumeBindingMode&lt;/code&gt; 设置为 &lt;code&gt;WaitForFirstConsumer&lt;/code&gt;。
可以见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes/#local&#34;&gt;示例&lt;/a&gt;.
卷延迟绑定确定 PersistentVolumeClaim 在作绑定决策时检查 Pod 可能有的其它的节点约束，如果节点
的资源需求，节点的选择器，Pod 的亲和性和反亲和性。&lt;/p&gt;
&lt;p&gt;可以在外部独立运行一个静态提供者来改善 &lt;code&gt;local&lt;/code&gt; 卷生命周期管理。 要注意这个提供者目前还不支持动态管理。
运行外部提供者的示例见
&lt;a href=&#34;https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner&#34;&gt;local 卷提供者用户指南&lt;/a&gt;.
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果外部的静态提供都没有用来管理 &lt;code&gt;local&lt;/code&gt; PersistentVolume 的生命周期， 需要手动清理和删除。&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;!--
### nfs {#nfs}

An `nfs` volume allows an existing NFS (Network File System) share to be
mounted into your Pod. Unlike `emptyDir`, which is erased when a Pod is
removed, the contents of an `nfs` volume are preserved and the volume is merely
unmounted.  This means that an NFS volume can be pre-populated with data, and
that data can be &#34;handed off&#34; between Pods.  NFS can be mounted by multiple
writers simultaneously.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must have your own NFS server running with the share exported before you can use it.&lt;/div&gt;
&lt;/blockquote&gt;


See the [NFS example](https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs) for more details.
 --&gt;
&lt;h3 id=&#34;nfs&#34;&gt;nfs&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;nfs&lt;/code&gt; 卷可以将现有的 NFS (网络文件系统)挂载到 Pod 中。 与 &lt;code&gt;emptyDir&lt;/code&gt; 在 Pod 删除时清除数据不同，
在 &lt;code&gt;iscsi&lt;/code&gt; 卷中的内存会在卸载后依然会保存 其中的内容。也就是说一个 &lt;code&gt;iscsi&lt;/code&gt; 卷可以预先存在数据，
其中的数据也可以在不同的 Pod 之间传递。 NFS 可以以读写方式同时挂载到多个 Pod 中。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在使用之前必须要先搭建并运行好 NFS 服务器&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;更多详情见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs&#34;&gt;NFS 示例&lt;/a&gt;&lt;/p&gt;
&lt;!--
### persistentVolumeClaim {#persistentvolumeclaim}

A `persistentVolumeClaim` volume is used to mount a
[PersistentVolume](/docs/concepts/storage/persistent-volumes/) into a Pod. PersistentVolumeClaims
are a way for users to &#34;claim&#34; durable storage (such as a GCE PersistentDisk or an
iSCSI volume) without knowing the details of the particular cloud environment.

See the [PersistentVolumes example](/docs/concepts/storage/persistent-volumes/) for more
details.
 --&gt;
&lt;h3 id=&#34;persistentvolumeclaim&#34;&gt;persistentVolumeClaim&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;persistentVolumeClaim&lt;/code&gt; 卷用于将
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/&#34;&gt;PersistentVolume&lt;/a&gt; 挂载到 Pod 中。
&lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 是用户&amp;quot;声明&amp;quot;持久存储(如GCE PersistentDisk 或 iSCSI 卷)时不需要
知道具体云环境细节的一种方式
更多信息见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/&#34;&gt;PersistentVolumes 示例&lt;/a&gt;&lt;/p&gt;
&lt;!--
### projected {#projected}

A `projected` volume maps several existing volume sources into the same directory.

Currently, the following types of volume sources can be projected:

- [`secret`](#secret)
- [`downwardAPI`](#downwardapi)
- [`configMap`](#configmap)
- `serviceAccountToken`

All sources are required to be in the same namespace as the Pod. For more details,
see the [all-in-one volume design document](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/all-in-one-volume.md).

The projection of service account tokens is a feature introduced in Kubernetes
1.11 and promoted to Beta in 1.12.
To enable this feature on 1.11, you need to explicitly set the `TokenRequestProjection`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) to
True.
 --&gt;
&lt;h3 id=&#34;projected&#34;&gt;projected&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;projected&lt;/code&gt; 卷可以将几种现有的不同的卷作为源映射到同一个目录中&lt;/p&gt;
&lt;p&gt;目前， 支持以下几种卷作为 &lt;code&gt;projected&lt;/code&gt; 的源&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#secret&#34;&gt;&lt;code&gt;secret&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#downwardapi&#34;&gt;&lt;code&gt;downwardAPI&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#configmap&#34;&gt;&lt;code&gt;configMap&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;serviceAccountToken&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有的源都需要要与 Pod 在同一个命令空间。 更多细节见
&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/all-in-one-volume.md&#34;&gt;all-in-one 卷设计文档&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;对服务账号凭据(Service Account Token)的投射支持是从 k8s 1.11 加入的， v1.12 提升为 beta 版本。
在 v1.11 中要启用该特性，需要显示地将
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;
&lt;code&gt;TokenRequestProjection&lt;/code&gt; 设置为 &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
&lt;!--
#### Example Pod with a secret, a downward API, and a configmap.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts:
    - name: all-in-one
      mountPath: &#34;/projected-volume&#34;
      readOnly: true
  volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: mysecret
          items:
            - key: username
              path: my-group/my-username
      - downwardAPI:
          items:
            - path: &#34;labels&#34;
              fieldRef:
                fieldPath: metadata.labels
            - path: &#34;cpu_limit&#34;
              resourceFieldRef:
                containerName: container-test
                resource: limits.cpu
      - configMap:
          name: myconfigmap
          items:
            - key: config
              path: my-group/my-config
```
 --&gt;
&lt;h4 id=&#34;示例-pod-中包含-secret-downwardapi-configmap&#34;&gt;示例： Pod 中包含 Secret, DownwardAPI, Configmap.&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;volume-test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;container-test&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;all-in-one&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/projected-volume&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;all-in-one&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;projected&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;sources&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysecret&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;items&lt;/span&gt;:
            - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;username&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-group/my-username&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;downwardAPI&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;items&lt;/span&gt;:
            - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;labels&amp;#34;&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;fieldRef&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;fieldPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;metadata.labels&lt;/span&gt;
            - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;cpu_limit&amp;#34;&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;resourceFieldRef&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;containerName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;container-test&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;resource&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;limits.cpu&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;configMap&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myconfigmap&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;items&lt;/span&gt;:
            - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;config&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-group/my-config&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
#### Example Pod with multiple secrets with a non-default permission mode set.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts:
    - name: all-in-one
      mountPath: &#34;/projected-volume&#34;
      readOnly: true
  volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: mysecret
          items:
            - key: username
              path: my-group/my-username
      - secret:
          name: mysecret2
          items:
            - key: password
              path: my-group/my-password
              mode: 511
```

Each projected volume source is listed in the spec under `sources`. The
parameters are nearly the same with two exceptions:

* For secrets, the `secretName` field has been changed to `name` to be consistent
  with ConfigMap naming.
* The `defaultMode` can only be specified at the projected level and not for each
  volume source. However, as illustrated above, you can explicitly set the `mode`
  for each individual projection.

When the `TokenRequestProjection` feature is enabled, you can inject the token
for the current [service account](/docs/reference/access-authn-authz/authentication/#service-account-tokens)
into a Pod at a specified path. Below is an example:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sa-token-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts:
    - name: token-vol
      mountPath: &#34;/service-account&#34;
      readOnly: true
  volumes:
  - name: token-vol
    projected:
      sources:
      - serviceAccountToken:
          audience: api
          expirationSeconds: 3600
          path: token
```

The example Pod has a projected volume containing the injected service account
token. This token can be used by Pod containers to access the Kubernetes API
server, for example. The `audience` field contains the intended audience of the
token. A recipient of the token must identify itself with an identifier specified
in the audience of the token, and otherwise should reject the token. This field
is optional and it defaults to the identifier of the API server.

The `expirationSeconds` is the expected duration of validity of the service account
token. It defaults to 1 hour and must be at least 10 minutes (600 seconds). An administrator
can also limit its maximum value by specifying the `--service-account-max-token-expiration`
option for the API server. The `path` field specifies a relative path to the mount point
of the projected volume.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A Container using a projected volume source as a &lt;a href=&#34;#using-subpath&#34;&gt;subPath&lt;/a&gt; volume mount will not
receive updates for those volume sources.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h4 id=&#34;示例-pod-包含多有设置非默认权限模式的-secret&#34;&gt;示例： Pod 包含多有设置非默认权限模式的 Secret&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;volume-test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;container-test&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;all-in-one&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/projected-volume&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;all-in-one&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;projected&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;sources&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysecret&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;items&lt;/span&gt;:
            - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;username&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-group/my-username&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysecret2&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;items&lt;/span&gt;:
            - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;password&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-group/my-password&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;mode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;511&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;每一个投射(&lt;code&gt;projected&lt;/code&gt;)卷的源都在配置 &lt;code&gt;sources&lt;/code&gt; 下。 除了以下两个例外，其它的参数都基本相同:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于 Secret, &lt;code&gt;secretName&lt;/code&gt; 字段变为 &lt;code&gt;name&lt;/code&gt; 以便与 ConfigMap 的命名一致&lt;/li&gt;
&lt;li&gt;&lt;code&gt;defaultMode&lt;/code&gt; 只能在 projected 级别上设置不能在每个卷源上设置。 但是，就像上面例子中所示，
可以为每一个源设置各自独立的 &lt;code&gt;mode&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当 &lt;code&gt;TokenRequestProjection&lt;/code&gt; 特性开启时，可能将当前[服务账号(ServiceAccount)]的凭据(token)
注入到 Pod 的指定路径，下面为示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;sa-token-test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;container-test&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;token-vol&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/service-account&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;token-vol&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;projected&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;sources&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;serviceAccountToken&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;audience&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;api&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;expirationSeconds&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3600&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;token&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上面的例子中， &lt;code&gt;projected&lt;/code&gt; 卷中包含了被注入的 服务账号凭据。 这个凭据可以被 Pod 的容器用来
访问 k8s API 服务. &lt;code&gt;audience&lt;/code&gt; 字段中包含的是凭据的目标用户， 凭据的的接收者必须要验证凭据中
提供的 &lt;code&gt;audience&lt;/code&gt; 与其本身的标识进行验证，否则就应该拒绝这个凭据。 这个字段为可选，默认的标识符为 API 服务&lt;/p&gt;
&lt;p&gt;&lt;code&gt;expirationSeconds&lt;/code&gt; 为服务账号凭据的有效时长。 默认为1小时，最少为 10 分钟(即 600 秒)。
管理员也可能通过API 服务的 &lt;code&gt;--service-account-max-token-expiration&lt;/code&gt; 选项限制其最大值。
&lt;code&gt;path&lt;/code&gt; 字段指定 &lt;code&gt;projected&lt;/code&gt; 卷挂载点的相对路径。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果一个容器以将一个 &lt;code&gt;projected&lt;/code&gt; 卷以 &lt;a href=&#34;#using-subpath&#34;&gt;subPath&lt;/a&gt; 方式挂载，则不会接收到
卷源的更新&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;!--
### portworxVolume {#portworxvolume}

A `portworxVolume` is an elastic block storage layer that runs hyperconverged with
Kubernetes. [Portworx](https://portworx.com/use-case/kubernetes-storage/) fingerprints storage in a server, tiers based on capabilities,
and aggregates capacity across multiple servers. Portworx runs in-guest in virtual machines or on bare metal Linux nodes.

A `portworxVolume` can be dynamically created through Kubernetes or it can also
be pre-provisioned and referenced inside a Kubernetes Pod.
Here is an example Pod referencing a pre-provisioned PortworxVolume:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-portworx-volume-pod
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /mnt
      name: pxvol
  volumes:
  - name: pxvol
    # This Portworx volume must already exist.
    portworxVolume:
      volumeID: &#34;pxvol&#34;
      fsType: &#34;&lt;fs-type&gt;&#34;
```

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; Make sure you have an existing PortworxVolume with name &lt;code&gt;pxvol&lt;/code&gt;
before using it in the Pod.&lt;/div&gt;
&lt;/blockquote&gt;


More details and examples can be found [here](https://github.com/kubernetes/examples/tree/master/staging/volumes/portworx/README.md).
 --&gt;
&lt;h3 id=&#34;portworxvolume&#34;&gt;portworxVolume&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;portworxVolume&lt;/code&gt; 是一个弹性块存储层(elastic block storage layer), 它通过 k8s 运行
&lt;code&gt;hyperconverged&lt;/code&gt;。
&lt;a href=&#34;https://portworx.com/use-case/kubernetes-storage/&#34;&gt;Portworx&lt;/a&gt; (这句有点绕)
Portworx 可以运行在虚拟机或裸金属的 Linux 节点中。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;portworxVolume&lt;/code&gt; 可以通过 k8s 动态创建或预先创建然后在一个 k8s 的 Pod 中引用。
以下为在 Pod 中使用一个预先创建好的 &lt;code&gt;PortworxVolume&lt;/code&gt; 的示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-portworx-volume-pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/mnt&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pxvol&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pxvol&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# 这个 Portworx 卷必须要已经存在.&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;portworxVolume&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeID&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pxvol&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;lt;fs-type&amp;gt;&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在 Pod 中使用之前需要已经存在一个名叫 &lt;code&gt;pxvol&lt;/code&gt; 的 &lt;code&gt;PortworxVolume&lt;/code&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;更多详情及示例见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/portworx/README.md&#34;&gt;这里&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### quobyte {#quobyte}

A `quobyte` volume allows an existing [Quobyte](https://www.quobyte.com) volume to
be mounted into your Pod.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must have your own Quobyte setup running with the volumes
created before you can use it.&lt;/div&gt;
&lt;/blockquote&gt;


Quobyte supports the &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;Container Storage Interface&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt;.
CSI is the recommended plugin to use Quobyte volumes inside Kubernetes. Quobyte&#39;s
GitHub project has [instructions](https://github.com/quobyte/quobyte-csi#quobyte-csi) for deploying Quobyte using CSI, along with examples.
 --&gt;
&lt;h3 id=&#34;quobyte&#34;&gt;quobyte&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;quobyte&lt;/code&gt; 可以将已经存在的 &lt;a href=&#34;https://www.quobyte.com&#34;&gt;Quobyte&lt;/a&gt; 卷挂载到 Pod 中。
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在使用之前必须要配置好 Quobyte 并创建使用的卷&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;Quobyte 支持 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;容器存储接口 (CSI)&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt;.
CSI 是在 k8s 中使用 Quobyte 卷的推荐插件。 Quobyte 的 GitHub 项目中有介绍使用 CSI 部署
Quobyte 的介绍&lt;a href=&#34;https://github.com/quobyte/quobyte-csi#quobyte-csi&#34;&gt;介绍&lt;/a&gt;
和示例。&lt;/p&gt;
&lt;!--
### rbd {#rbd}

An `rbd` volume allows a
[Rados Block Device](https://ceph.com/docs/master/rbd/rbd/) volume to be mounted into your
Pod.  Unlike `emptyDir`, which is erased when a Pod is removed, the contents of
a `rbd` volume are preserved and the volume is merely unmounted.  This
means that a RBD volume can be pre-populated with data, and that data can
be &#34;handed off&#34; between Pods.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must have your own Ceph installation running before you can use RBD.&lt;/div&gt;
&lt;/blockquote&gt;


A feature of RBD is that it can be mounted as read-only by multiple consumers
simultaneously.  This means that you can pre-populate a volume with your dataset
and then serve it in parallel from as many Pods as you need.  Unfortunately,
RBD volumes can only be mounted by a single consumer in read-write mode - no
simultaneous writers allowed.

See the [RBD example](https://github.com/kubernetes/examples/tree/master/volumes/rbd) for more details.
 --&gt;
&lt;h3 id=&#34;rbd&#34;&gt;rbd&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;rbd&lt;/code&gt; 卷可以让 &lt;a href=&#34;https://ceph.com/docs/master/rbd/rbd/&#34;&gt;Rados 块设备&lt;/a&gt;卷挂载到 Pod 中。
与 &lt;code&gt;emptyDir&lt;/code&gt; 在 Pod 删除时清除数据不同， 在 &lt;code&gt;rbd&lt;/code&gt; 卷中的内存会在卸载后依然会保存
其中的内容。也就是说一个 &lt;code&gt;rbd&lt;/code&gt; 卷可以预先存在数据，其中的数据也可以在不同的 Pod 之间传递。
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在使用 RBD 之前需要先配置运行 Ceph&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RBD&lt;/code&gt; 一个特性是可以以只读的方式同时挂载到多个消费者中。 这就代表着可以预先将数据集放入卷中
然后根据需要使用任意数量的 Pod 来提供访问服务。 不过 &lt;code&gt;RBD&lt;/code&gt; 卷以读写方式挂载时只能有一个消费者，
不允许并行写。
更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/volumes/rbd&#34;&gt;RBD 示例&lt;/a&gt;&lt;/p&gt;
&lt;!--
### scaleIO {#scaleio}

ScaleIO is a software-based storage platform that can use existing hardware to
create clusters of scalable shared block networked storage. The `scaleIO` volume
plugin allows deployed Pods to access existing ScaleIO
volumes (or it can dynamically provision new volumes for persistent volume claims, see
[ScaleIO Persistent Volumes](/docs/concepts/storage/persistent-volumes/#scaleio)).

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must have an existing ScaleIO cluster already setup and
running with the volumes created before you can use them.&lt;/div&gt;
&lt;/blockquote&gt;


The following is an example of Pod configuration with ScaleIO:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-0
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: pod-0
    volumeMounts:
    - mountPath: /test-pd
      name: vol-0
  volumes:
  - name: vol-0
    scaleIO:
      gateway: https://localhost:443/api
      system: scaleio
      protectionDomain: sd0
      storagePool: sp1
      volumeName: vol-0
      secretRef:
        name: sio-secret
      fsType: xfs
```

For further detail, please see the [ScaleIO examples](https://github.com/kubernetes/examples/tree/master/staging/volumes/scaleio).
 --&gt;
&lt;h3 id=&#34;scaleio&#34;&gt;scaleIO&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;ScaleIO&lt;/code&gt; 是一个基于软件的存储平台，它可以使用已经存在的硬件来创建可扩展的共享块网络存储集群。
&lt;code&gt;scaleIO&lt;/code&gt; 卷插件让 Pod 可以使用 已经存在的 ScaleIO 卷(也可以为 持久化卷声明(PersistentVolumeClaim)
动态地创建新的卷， 见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/#scaleio&#34;&gt;ScaleIO 持久化卷&lt;/a&gt;)&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在使用 &lt;code&gt;ScaleIO&lt;/code&gt; 卷之前，需要先部署配置运行 ScaleIO 集群。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;以下为一个使用 ScaleIO 的 Pod 配置示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod-0&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod-0&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/test-pd&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vol-0&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vol-0&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;scaleIO&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;gateway&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://localhost:443/api&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;system&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;scaleio&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;protectionDomain&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;sd0&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;storagePool&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;sp1&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vol-0&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;secretRef&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;sio-secret&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;xfs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/scaleio&#34;&gt;ScaleIO 示例&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### secret {#secret}

A `secret` volume is used to pass sensitive information, such as passwords, to
Pods.  You can store secrets in the Kubernetes API and mount them as files for
use by Pods without coupling to Kubernetes directly.  `secret` volumes are
backed by tmpfs (a RAM-backed filesystem) so they are never written to
non-volatile storage.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must create a secret in the Kubernetes API before you can use it.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A Container using a Secret as a &lt;a href=&#34;#using-subpath&#34;&gt;subPath&lt;/a&gt; volume mount will not
receive Secret updates.&lt;/div&gt;
&lt;/blockquote&gt;


Secrets are described in more detail [here](/docs/concepts/configuration/secret/).
 --&gt;
&lt;h3 id=&#34;secret&#34;&gt;secret&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;secret&lt;/code&gt; 卷用于将如密码这样的敏感数据传入 Pod 中。 用户可以将 Secret 存在 k8s API 中，然后
将它们以文件的方式挂载到 Pod 中而不需要直接与 k8s 耦合在一起。 &lt;code&gt;secret&lt;/code&gt; 卷基于 tmpfs(
一个基于内存的文件系统)，所以这给永远不会写入持久性存储。
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在使用之前需要先在 k8s API 中先创建相应的 Secret&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 一个以 &lt;a href=&#34;#using-subpath&#34;&gt;subPath&lt;/a&gt; 挂载 Secret 的容器不会接收到 Secret 的更新。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;更多关于 Secret 的信息见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#34;&gt;这里&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### storageOS {#storageos}

A `storageos` volume allows an existing [StorageOS](https://www.storageos.com)
volume to be mounted into your Pod.

StorageOS runs as a Container within your Kubernetes environment, making local
or attached storage accessible from any node within the Kubernetes cluster.
Data can be replicated to protect against node failure. Thin provisioning and
compression can improve utilization and reduce cost.

At its core, StorageOS provides block storage to Containers, accessible via a file system.

The StorageOS Container requires 64-bit Linux and has no additional dependencies.
A free developer license is available.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must run the StorageOS Container on each node that wants to
access StorageOS volumes or that will contribute storage capacity to the pool.
For installation instructions, consult the
&lt;a href=&#34;https://docs.storageos.com&#34;&gt;StorageOS documentation&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;


```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: redis
    role: master
  name: test-storageos-redis
spec:
  containers:
    - name: master
      image: kubernetes/redis:v1
      env:
        - name: MASTER
          value: &#34;true&#34;
      ports:
        - containerPort: 6379
      volumeMounts:
        - mountPath: /redis-master-data
          name: redis-data
  volumes:
    - name: redis-data
      storageos:
        # The `redis-vol01` volume must already exist within StorageOS in the `default` namespace.
        volumeName: redis-vol01
        fsType: ext4
```

For more information including Dynamic Provisioning and Persistent Volume Claims, please see the
[StorageOS examples](https://github.com/kubernetes/examples/blob/master/volumes/storageos).
 --&gt;
&lt;h3 id=&#34;storageos&#34;&gt;storageOS&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;storageos&lt;/code&gt; 卷可以将现有的 &lt;a href=&#34;https://www.storageos.com&#34;&gt;StorageOS&lt;/a&gt; 卷挂载到 Pod 中。&lt;/p&gt;
&lt;p&gt;StorageOS 以容器方式运行在 k8s 环境中，可以将集群中任意节点的本地存储或附加存储变为可访问。
数据可以被复制以提高可用性。瘦供给或压缩可以改善利用率并减少开销。
StorageOS 核心是向容器提供块存储，可以通过文件系统访问。
StorageOS 容器需要 64 位 Linux 外不需要其它额外的信赖。
有一个免费的开发者许可可用。
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 需要在想让 StorageOS 访问的节点上运行 StorageOS 容器这样可以将它的存储容量加到池中。
更新安装指导见 &lt;a href=&#34;https://docs.storageos.com&#34;&gt;StorageOS 文档&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;role&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;master&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-storageos-redis&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;master&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes/redis:v1&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MASTER&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;6379&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/redis-master-data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis-data&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis-data&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;storageos&lt;/span&gt;:
        &lt;span style=&#34;color:#75715e&#34;&gt;# The `redis-vol01` volume must already exist within StorageOS in the `default` namespace.&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis-vol01&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;更多关于动态管理和 PersistentVolumeClaim 的信息见
&lt;a href=&#34;https://github.com/kubernetes/examples/blob/master/volumes/storageos&#34;&gt;StorageOS 示例&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### vsphereVolume {#vspherevolume}

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Prerequisite: Kubernetes with vSphere Cloud Provider configured. For cloudprovider
configuration please refer &lt;a href=&#34;https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/&#34;&gt;vSphere getting started guide&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;


A `vsphereVolume` is used to mount a vSphere VMDK Volume into your Pod.  The contents
of a volume are preserved when it is unmounted. It supports both VMFS and VSAN datastore.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must create VMDK using one of the following methods before using with Pod.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;vspherevolume&#34;&gt;vsphereVolume&lt;/h3&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 前置条件: k8s 需要配置好 vSphere 云提供商。 cloudprovider 配置见
&lt;a href=&#34;https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/&#34;&gt;vSphere 上手指导&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;vsphereVolume&lt;/code&gt; 用于将  vSphere VMDK 卷挂载到 Pod 中。 卷中的数据在卸载后依然存在。
支持 VMFS 和 VSAN 数据存储&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 需要使用以下方式中的其中一种创建 VMDK 后才能在 Pod 中使用。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
#### Creating a VMDK volume

Choose one of the following methods to create a VMDK.

&lt;ul class=&#34;nav nav-tabs&#34; id=&#34;tabs_volumes&#34; role=&#34;tablist&#34;&gt;&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link active&#34; href=&#34;#tabs_volumes-0&#34; role=&#34;tab&#34; aria-controls=&#34;tabs_volumes-0&#34; aria-selected=&#34;true&#34;&gt;Create using vmkfstools&lt;/a&gt;&lt;/li&gt;
	  
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#tabs_volumes-1&#34; role=&#34;tab&#34; aria-controls=&#34;tabs_volumes-1&#34;&gt;Create using vmware-vdiskmanager&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;div class=&#34;tab-content&#34; id=&#34;tabs_volumes&#34;&gt;&lt;div id=&#34;tabs_volumes-0&#34; class=&#34;tab-pane show active&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;tabs_volumes-0&#34;&gt;

&lt;p&gt;&lt;p&gt;First ssh into ESX, then use the following command to create a VMDK:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;vmkfstools -c 2G /vmfs/volumes/DatastoreName/volumes/myDisk.vmdk
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;div id=&#34;tabs_volumes-1&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;tabs_volumes-1&#34;&gt;

&lt;p&gt;&lt;p&gt;Use the following command to create a VMDK:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;vmware-vdiskmanager -c -t &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; -s 40GB -a lsilogic myDisk.vmdk
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;

 --&gt;
&lt;h4 id=&#34;创建-vmdk-卷&#34;&gt;创建 VMDK 卷&lt;/h4&gt;
&lt;p&gt;选择以下方式中的一种创建 VMDK&lt;/p&gt;
&lt;ul class=&#34;nav nav-tabs&#34; id=&#34;tabs_volumes&#34; role=&#34;tablist&#34;&gt;&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link active&#34; href=&#34;#tabs_volumes-0&#34; role=&#34;tab&#34; aria-controls=&#34;tabs_volumes-0&#34; aria-selected=&#34;true&#34;&gt;Create using vmkfstools&lt;/a&gt;&lt;/li&gt;
	  
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#tabs_volumes-1&#34; role=&#34;tab&#34; aria-controls=&#34;tabs_volumes-1&#34;&gt;Create using vmware-vdiskmanager&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;div class=&#34;tab-content&#34; id=&#34;tabs_volumes&#34;&gt;&lt;div id=&#34;tabs_volumes-0&#34; class=&#34;tab-pane show active&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;tabs_volumes-0&#34;&gt;

&lt;p&gt;&lt;p&gt;先 ssh 到 ESX 中，然后使用用下面的命令创建一个 VMDK&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;vmkfstools -c 2G /vmfs/volumes/DatastoreName/volumes/myDisk.vmdk
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;div id=&#34;tabs_volumes-1&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;tabs_volumes-1&#34;&gt;

&lt;p&gt;&lt;p&gt;使用下面的命令创建一个 VMDK&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;vmware-vdiskmanager -c -t &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; -s 40GB -a lsilogic myDisk.vmdk
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!--
#### vSphere VMDK Example configuration

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-vmdk
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-vmdk
      name: test-volume
  volumes:
  - name: test-volume
    # This VMDK volume must already exist.
    vsphereVolume:
      volumePath: &#34;[DatastoreName] volumes/myDisk&#34;
      fsType: ext4
```
 --&gt;
&lt;h4 id=&#34;vsphere-vmdk-配置示例&#34;&gt;vSphere VMDK 配置示例&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-vmdk&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/test-vmdk&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# 这个 VMDK 必须要已经存在.&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;vsphereVolume&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;volumePath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;[DatastoreName] volumes/myDisk&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere&#34;&gt;这里&lt;/a&gt;.&lt;/p&gt;
&lt;!--
#### CSI migration






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;



The CSI Migration feature for vsphereVolume, when enabled, shims all plugin operations
from the existing in-tree plugin to the `csi.vsphere.vmware.com` &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;CSI&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt; driver. In order to use this feature, the [vSphere CSI
Driver](https://github.com/kubernetes-sigs/vsphere-csi-driver)
must be installed on the cluster and the `CSIMigration` and `CSIMigrationvSphere`
[feature gates](/docs/reference/command-line-tools-reference/feature-gates/) must be enabled.

This also requires minimum vSphere vCenter/ESXi Version to be 7.0u1 and minimum HW Version to be VM version 15.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;The following StorageClass parameters from the built-in vsphereVolume plugin are not supported by the vSphere CSI driver:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;diskformat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hostfailurestotolerate&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;forceprovisioning&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cachereservation&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;diskstripes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;objectspacereservation&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;iopslimit&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Existing volumes created using these parameters will be migrated to the vSphere CSI driver, but new volumes created by the vSphere CSI driver will not be honoring these parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h4 id=&#34;csi-migration-5&#34;&gt;CSI 迁移&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;的 CSI 迁移特性，在启用时，会将来自已经存在的插件的插件操作转移到
&lt;code&gt;csi.vsphere.vmware.com&lt;/code&gt; &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;CSI&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt;(CSI)驱动。
为了使用该特性， 需要在集群中先安装
&lt;a href=&#34;https://github.com/kubernetes-sigs/vsphere-csi-driver&#34;&gt;vSphere CSI Driver&lt;/a&gt;
同时在
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;
启用 &lt;code&gt;CSIMigration&lt;/code&gt; 和 &lt;code&gt;CSIMigrationAzureFile&lt;/code&gt; 两个 &lt;code&gt;Alpha&lt;/code&gt; 特性
This also requires minimum vSphere vCenter/ESXi Version to be 7.0u1 and minimum HW Version to be VM version 15.
同时还需要 vSphere vCenter/ESXi v7.0u1+ 和 VM v15+
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;以下来自 vsphereVolume 插件内置的 StorageClass 参数不受 vSphere CSI 驱动支持:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;diskformat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hostfailurestotolerate&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;forceprovisioning&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cachereservation&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;diskstripes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;objectspacereservation&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;iopslimit&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;已经存在使用这些参数创建的卷会被迁移到 vSphere CSI 驱动， 但是使用 vSphere CSI 驱动创建新
的卷时就不能再使用这些参数了&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;!--
#### CSI Migration Complete





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;



To turn off the vsphereVolume plugin from being loaded by controller manager and kubelet, you need to set this feature flag to true. This requires `csi.vsphere.vmware.com` &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;CSI&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt; driver being installed on all worker nodes.
 --&gt;
&lt;h4 id=&#34;csi-migration-complete&#34;&gt;CSI Migration Complete&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;为了关闭由控制器管理器和 kubelet 加载的 vsphereVolume 插件， 需要将这具功能特性标记为 true
同时还需要在每个节点上安装 &lt;code&gt;csi.vsphere.vmware.com&lt;/code&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;容器存储接口 (CSI)&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt;
驱动&lt;/p&gt;
&lt;!--
## Using subPath

Sometimes, it is useful to share one volume for multiple uses in a single Pod. The `volumeMounts.subPath`
property can be used to specify a sub-path inside the referenced volume instead of its root.

Here is an example of a Pod with a LAMP stack (Linux Apache Mysql PHP) using a single, shared volume.
The HTML contents are mapped to its `html` folder, and the databases will be stored in its `mysql` folder:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-lamp-site
spec:
    containers:
    - name: mysql
      image: mysql
      env:
      - name: MYSQL_ROOT_PASSWORD
        value: &#34;rootpasswd&#34;
      volumeMounts:
      - mountPath: /var/lib/mysql
        name: site-data
        subPath: mysql
    - name: php
      image: php:7.0-apache
      volumeMounts:
      - mountPath: /var/www/html
        name: site-data
        subPath: html
    volumes:
    - name: site-data
      persistentVolumeClaim:
        claimName: my-lamp-site-data
```
 --&gt;
&lt;h2 id=&#34;使用-subpath&#34;&gt;使用 subPath&lt;/h2&gt;
&lt;p&gt;有时，在同一个 Pod 中的多个用户分享同一个卷是相当有用的。 &lt;code&gt;volumeMounts.subPath&lt;/code&gt; 属性可以用来
在引用卷时不使用其根路径而是指定子路径&lt;/p&gt;
&lt;p&gt;以下示例 Pod 为一个使用一个共享卷的 LAMP (Linux Apache Mysql PHP) 栈。
HTML 内容映射到 &lt;code&gt;html&lt;/code&gt; 目录， 数据库会使用 &lt;code&gt;mysql&lt;/code&gt; 目录。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-lamp-site&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MYSQL_ROOT_PASSWORD&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rootpasswd&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/lib/mysql&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;site-data&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;subPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;php&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;php:7.0-apache&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/www/html&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;site-data&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;subPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;html&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;site-data&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaim&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;claimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-lamp-site-data&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Using subPath with expanded environment variables






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [stable]&lt;/code&gt;
&lt;/div&gt;




Use the `subPathExpr` field to construct `subPath` directory names from Downward API environment variables.
The `subPath` and `subPathExpr` properties are mutually exclusive.

In this example, a Pod uses `subPathExpr` to create a directory `pod1` within the hostPath volume `/var/log/pods`, using the pod name from the Downward API.  The host directory `/var/log/pods/pod1` is mounted at `/logs` in the container.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
  - name: container1
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    image: busybox
    command: [ &#34;sh&#34;, &#34;-c&#34;, &#34;while [ true ]; do echo &#39;Hello&#39;; sleep 10; done | tee -a /logs/hello.txt&#34; ]
    volumeMounts:
    - name: workdir1
      mountPath: /logs
      subPathExpr: $(POD_NAME)
  restartPolicy: Never
  volumes:
  - name: workdir1
    hostPath:
      path: /var/log/pods
```
 --&gt;
&lt;h3 id=&#34;在-subpath-配置上使用环境变量&#34;&gt;在 subPath 配置上使用环境变量&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;使用 &lt;code&gt;subPathExpr&lt;/code&gt; 字段来通过 Downward API 的环境变量构建 &lt;code&gt;subPath&lt;/code&gt; 的名称。
&lt;code&gt;subPath&lt;/code&gt; 和 &lt;code&gt;subPathExpr&lt;/code&gt; 相互独立。&lt;/p&gt;
&lt;p&gt;在这个例子中， 一个 Pod 使用 &lt;code&gt;subPathExpr&lt;/code&gt; 创建一个 &lt;code&gt;pod1&lt;/code&gt; 目录在 hostPath 卷 &lt;code&gt;/var/log/pods&lt;/code&gt; 中，
使用来自 Downward API 的 Pod 名称。 主机目录 &lt;code&gt;/var/log/pods/pod1&lt;/code&gt; 挂载到容器中的 &lt;code&gt;/logs&lt;/code&gt; 上。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;container1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;POD_NAME&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;fieldRef&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;fieldPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;metadata.name&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sh&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;while [ true ]; do echo &amp;#39;Hello&amp;#39;; sleep 10; done | tee -a /logs/hello.txt&amp;#34;&lt;/span&gt; ]
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;workdir1&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/logs&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;subPathExpr&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;$(POD_NAME)&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;workdir1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/log/pods&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Resources

The storage media (Disk, SSD, etc.) of an `emptyDir` volume is determined by the
medium of the filesystem holding the kubelet root dir (typically
`/var/lib/kubelet`).  There is no limit on how much space an `emptyDir` or
`hostPath` volume can consume, and no isolation between Containers or between
Pods.

In the future, we expect that `emptyDir` and `hostPath` volumes will be able to
request a certain amount of space using a [resource](/docs/concepts/configuration/manage-resources-containers/)
specification, and to select the type of media to use, for clusters that have
several media types.
 --&gt;
&lt;h2 id=&#34;资源&#34;&gt;资源&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;emptyDir&lt;/code&gt; 卷的存储介质(Disk, SSD, etc.)是由 kubelet 根目录(通常为 &lt;code&gt;/var/lib/kubelet&lt;/code&gt;)
所在的文件系统的存储介质决定的。 &lt;code&gt;emptyDir&lt;/code&gt; 和 &lt;code&gt;hostPath&lt;/code&gt; 卷没有使用容量限制，容器和 Pod
之间也没有限离。&lt;/p&gt;
&lt;p&gt;在将来，我们预计将通过
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/manage-resources-containers/&#34;&gt;资源&lt;/a&gt;
来确定 &lt;code&gt;emptyDir&lt;/code&gt; 和 &lt;code&gt;hostPath&lt;/code&gt; 请求确定数量的空间，如果集群中有多种介质也 选择要使用的介质&lt;/p&gt;
&lt;!--
## Out-of-Tree Volume Plugins

The Out-of-tree volume plugins include the Container Storage Interface (CSI)
and FlexVolume. They enable storage vendors to create custom storage plugins
without adding them to the Kubernetes repository.

Before the introduction of CSI and FlexVolume, all volume plugins (like
volume types listed above) were &#34;in-tree&#34; meaning they were built, linked,
compiled, and shipped with the core Kubernetes binaries and extend the core
Kubernetes API. This meant that adding a new storage system to Kubernetes (a
volume plugin) required checking code into the core Kubernetes code repository.

Both CSI and FlexVolume allow volume plugins to be developed independent of
the Kubernetes code base, and deployed (installed) on Kubernetes clusters as
extensions.

For storage vendors looking to create an out-of-tree volume plugin, please refer
to [this FAQ](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md).
 --&gt;
&lt;h2 id=&#34;外部卷插件&#34;&gt;外部卷插件&lt;/h2&gt;
&lt;p&gt;外部卷插件包括 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;容器存储接口 (CSI)&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt; 和 FlexVolume。它们使得存储提供商
可能创建自定义的存储插件而不需要将其添加到 k8s 代码库中。&lt;/p&gt;
&lt;p&gt;在引入 CSI 和 FlexVolume 之前， 所有的卷插件(像所有前面列举的那些)都是内置的，它们在构建，连接
编译，发布都是与 k8s 核心二进制文件在一起的并且扩展了 k8s 核心 API。 这意味着如果我们要添加
一种新的存储系统(一个卷插件)到 k8s 中时，需要将其代码检入到 k8s 核心代码库中。&lt;/p&gt;
&lt;p&gt;CSI 和 FlexVolume 都允许独立于 k8s 核心代码独立开发卷插件，并且独立以插件的方式部署(安装)
到 k8s 集群中。&lt;/p&gt;
&lt;p&gt;要看看有哪些外部卷插件，请见
&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md&#34;&gt;这个 FAQ&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### CSI

[Container Storage Interface](https://github.com/container-storage-interface/spec/blob/master/spec.md) (CSI)
defines a standard interface for container orchestration systems (like
Kubernetes) to expose arbitrary storage systems to their container workloads.

Please read the [CSI design proposal](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md) for more information.

CSI support was introduced as alpha in Kubernetes v1.9, moved to beta in
Kubernetes v1.10, and is GA in Kubernetes v1.13.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Support for CSI spec versions 0.2 and 0.3 are deprecated in Kubernetes
v1.13 and will be removed in a future release.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; CSI drivers may not be compatible across all Kubernetes releases.
Please check the specific CSI driver&amp;rsquo;s documentation for supported
deployments steps for each Kubernetes release and a compatibility matrix.&lt;/div&gt;
&lt;/blockquote&gt;


Once a CSI compatible volume driver is deployed on a Kubernetes cluster, users
may use the `csi` volume type to attach, mount, etc. the volumes exposed by the
CSI driver.

A `csi` volume can be used in a pod in three different ways:
- through a reference to a [`persistentVolumeClaim`](#persistentvolumeclaim)
- with a [generic ephemeral volume](/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volume) (alpha feature)
- with a [CSI ephemeral volume](/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volume) if the driver
  supports that (beta feature)

The following fields are available to storage administrators to configure a CSI
persistent volume:

- `driver`: A string value that specifies the name of the volume driver to use.
  This value must correspond to the value returned in the `GetPluginInfoResponse`
  by the CSI driver as defined in the [CSI spec](https://github.com/container-storage-interface/spec/blob/master/spec.md#getplugininfo).
  It is used by Kubernetes to identify which CSI driver to call out to, and by
  CSI driver components to identify which PV objects belong to the CSI driver.
- `volumeHandle`: A string value that uniquely identifies the volume. This value
  must correspond to the value returned in the `volume.id` field of the
  `CreateVolumeResponse` by the CSI driver as defined in the [CSI spec](https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume).
  The value is passed as `volume_id` on all calls to the CSI volume driver when
  referencing the volume.
- `readOnly`: An optional boolean value indicating whether the volume is to be
  &#34;ControllerPublished&#34; (attached) as read only. Default is false. This value is
  passed to the CSI driver via the `readonly` field in the
  `ControllerPublishVolumeRequest`.
- `fsType`: If the PV&#39;s `VolumeMode` is `Filesystem` then this field may be used
  to specify the filesystem that should be used to mount the volume. If the
  volume has not been formatted and formatting is supported, this value will be
  used to format the volume.
  This value is passed to the CSI driver via the `VolumeCapability` field of
  `ControllerPublishVolumeRequest`, `NodeStageVolumeRequest`, and
  `NodePublishVolumeRequest`.
- `volumeAttributes`: A map of string to string that specifies static properties
  of a volume. This map must correspond to the map returned in the
  `volume.attributes` field of the `CreateVolumeResponse` by the CSI driver as
  defined in the [CSI spec](https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume).
  The map is passed to the CSI driver via the `volume_context` field in the
  `ControllerPublishVolumeRequest`, `NodeStageVolumeRequest`, and
  `NodePublishVolumeRequest`.
- `controllerPublishSecretRef`: A reference to the secret object containing
  sensitive information to pass to the CSI driver to complete the CSI
  `ControllerPublishVolume` and `ControllerUnpublishVolume` calls. This field is
  optional, and may be empty if no secret is required. If the secret object
  contains more than one secret, all secrets are passed.
- `nodeStageSecretRef`: A reference to the secret object containing
  sensitive information to pass to the CSI driver to complete the CSI
  `NodeStageVolume` call. This field is optional, and may be empty if no secret
  is required. If the secret object contains more than one secret, all secrets
  are passed.
- `nodePublishSecretRef`: A reference to the secret object containing
  sensitive information to pass to the CSI driver to complete the CSI
  `NodePublishVolume` call. This field is optional, and may be empty if no
  secret is required. If the secret object contains more than one secret, all
  secrets are passed.
--&gt;
&lt;h3 id=&#34;csi&#34;&gt;CSI&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34;&gt;容器存储接口&lt;/a&gt; (CSI)
defines a standard interface for container orchestration systems (like
Kubernetes) to expose arbitrary storage systems to their container workloads.
定义一个标准接口用于容器编排系统(像 k8s)暴露任意存储系统到它们的容器工作负载中。
更多信息请见 &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md&#34;&gt;CSI 设计方案&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;CSI 支持是在 k8s v1.9 中引入为 alpha， 在 k8s v1.10 进变为 beta 版本， k8s v1.13 变为 GA
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 对 CSI 0.2 和 0.3 标准的支持在 k8s v1.13 中被废弃并会在将来被移除&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; CSI 驱动可能不能与所有的 k8s 发行版本兼容。请查阅对应 CSI 驱动 文档，了解其在每一个 k8s 版本
中的部署方式和对应的版本兼容表格&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;当一个 CSI 兼容的卷驱动被部署到 k8s 集群中时， 用户可以使用 &lt;code&gt;csi&lt;/code&gt; 卷类型为装载，挂载那些
通过 CSI 驱动暴露的卷。&lt;/p&gt;
&lt;p&gt;一个 &lt;code&gt;csi&lt;/code&gt; 卷可以以下三种方式在 Pod 中使用:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过一个 &lt;a href=&#34;#persistentvolumeclaim&#34;&gt;&lt;code&gt;persistentVolumeClaim&lt;/code&gt;&lt;/a&gt; 引用&lt;/li&gt;
&lt;li&gt;通过&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volume&#34;&gt;generic ephemeral volume&lt;/a&gt;(alpha 特性)&lt;/li&gt;
&lt;li&gt;如果驱动支持，通过 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volume&#34;&gt;CSI ephemeral volume&lt;/a&gt;  (beta 特性)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下字段可以用于存储管理员对 CSI 持久卷的配置:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;driver&lt;/code&gt;: 一个字符串，用于指定卷所使用的驱动的名称。 取值范围是
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#getplugininfo&#34;&gt;CSI spec&lt;/a&gt;
中定义的 CSI 驱动 &lt;code&gt;GetPluginInfoResponse&lt;/code&gt; 的返回值。 它被 k8s 用来识别，应该调用哪个 CSI 驱动
我大兵由 CSI 驱动组件来识别哪些 PV 对象是属于该 CSI 驱动的&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;volumeHandle&lt;/code&gt;: 一个字符串， 用于唯一标识该卷。 这个值必须是在
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume&#34;&gt;CSI spec&lt;/a&gt;.
定义的 CSI 驱动 &lt;code&gt;CreateVolumeResponse&lt;/code&gt; 返回值中的 &lt;code&gt;volume.id&lt;/code&gt; 的值。 在所有引用这个
卷时会该这个值 以 &lt;code&gt;volume_id&lt;/code&gt; 调用参数发给 CSI 卷驱动&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;readOnly&lt;/code&gt;: 一个可选布尔值，用于指定该卷是否以只读方式挂载(ControllerPublished)。
默认值为 false. 这个值通过 &lt;code&gt;ControllerPublishVolumeRequest&lt;/code&gt; 中 &lt;code&gt;readonly&lt;/code&gt; 传递给
CSI 驱动&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;fsType&lt;/code&gt;: 如果 PV 的 &lt;code&gt;VolumeMode&lt;/code&gt; 是 &lt;code&gt;Filesystem&lt;/code&gt;，这个字段就可以用来指定挂载该卷所使用
的文件系统。 如果这个卷还没有被格式化并支持格式化，则这个被被用来格式化这个卷。 这个值通过
&lt;code&gt;ControllerPublishVolumeRequest&lt;/code&gt;, &lt;code&gt;NodeStageVolumeRequest&lt;/code&gt;, &lt;code&gt;NodePublishVolumeRequest&lt;/code&gt;
的 &lt;code&gt;VolumeCapability&lt;/code&gt; 字段传递给 CSI 驱动。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;volumeAttributes&lt;/code&gt;: 一个键值都是字符串的字典，用于指定卷的静态属性。这个字段的键值必须是
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume&#34;&gt;CSI spec&lt;/a&gt;
中定义 CSI 驱动 &lt;code&gt;CreateVolumeResponse&lt;/code&gt; 返回值的 &lt;code&gt;volume.attributes&lt;/code&gt; 字段的键值的子集
这个字典通过 &lt;code&gt;ControllerPublishVolumeRequest&lt;/code&gt;, &lt;code&gt;NodeStageVolumeRequest&lt;/code&gt;,
&lt;code&gt;NodePublishVolumeRequest&lt;/code&gt; 的 &lt;code&gt;volume_context&lt;/code&gt; 字段传递给 CSI 驱动&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;controllerPublishSecretRef&lt;/code&gt;:  完成 &lt;code&gt;ControllerPublishVolume&lt;/code&gt; 和
&lt;code&gt;ControllerUnpublishVolume&lt;/code&gt; 调用向 CSI 驱动传递敏感信息的 Secret 对象的引用。该字段为
可选， 如果没使用 Secret 则留空。 如果 Secret 对象中有多个 Secret, 则所有的都会传递。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;nodeStageSecretRef&lt;/code&gt;: 为了完成 &lt;code&gt;NodeStageVolume&lt;/code&gt; 调用， 向 CSI 驱动传递敏感信息的
Secret 对象的引用。该字段为 可选， 如果没使用 Secret 则留空。
如果 Secret 对象中有多个 Secret, 则所有的都会传递。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;nodePublishSecretRef&lt;/code&gt;:  为了完成 &lt;code&gt;NodePublishVolume&lt;/code&gt; 调用， 向 CSI 驱动传递敏感信息的
Secret 对象的引用。该字段为 可选， 如果没使用 Secret 则留空。
如果 Secret 对象中有多个 Secret, 则所有的都会传递。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
#### CSI raw block volume support






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;



Vendors with external CSI drivers can implement raw block volumes support
in Kubernetes workloads.

You can [setup your PV/PVC with raw block volume support](/docs/concepts/storage/persistent-volumes/#raw-block-volume-support)
as usual, without any CSI specific changes.
 --&gt;
&lt;h4 id=&#34;csi-原生块卷支持&#34;&gt;CSI 原生块卷支持&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;有外部 CSI 驱动的供应商可以实现对 k8s 工作负载原生块卷的支持&lt;/p&gt;
&lt;p&gt;用户可以在不作任何 CSI 配置修改的情况下
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/#raw-block-volume-support&#34;&gt;设置支持原生块卷的 PV/PVC&lt;/a&gt;&lt;/p&gt;
&lt;!--
#### CSI ephemeral volumes






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [beta]&lt;/code&gt;
&lt;/div&gt;



You can directly configure CSI volumes within the Pod
specification. Volumes specified in this way are ephemeral and do not
persist across Pod restarts. See [Ephemeral
Volumes](/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volume)
for more information.
 --&gt;
&lt;h4 id=&#34;csi-临时卷&#34;&gt;CSI 临时卷&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;You can directly configure CSI volumes within the Pod
specification. Volumes specified in this way are ephemeral and do not
persist across Pod restarts. See &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volume&#34;&gt;Ephemeral
Volumes&lt;/a&gt;
for more information.
用户可以在 Pod 定义中直接配置 CSI 卷。 但是通过这种方式创建的卷是临时，Pod 重启后就不会存在。
更多信息请见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volume&#34;&gt;临时卷&lt;/a&gt;&lt;/p&gt;
&lt;!--
#### 相关资料

For more information on how to develop a CSI driver, refer to the [kubernetes-csi
documentation](https://kubernetes-csi.github.io/docs/)
 --&gt;
&lt;h4 id=&#34;相关资料&#34;&gt;相关资料&lt;/h4&gt;
&lt;p&gt;更多关于如何开发 CSI 驱动的信息，请见
&lt;a href=&#34;https://kubernetes-csi.github.io/docs/&#34;&gt;kubernetes-csi 文档&lt;/a&gt;&lt;/p&gt;
&lt;!--
#### Migrating to CSI drivers from in-tree plugins






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [alpha]&lt;/code&gt;
&lt;/div&gt;



The CSI Migration feature, when enabled, directs operations against existing in-tree
plugins to corresponding CSI plugins (which are expected to be installed and configured).
The feature implements the necessary translation logic and shims to re-route the
operations in a seamless fashion. As a result, operators do not have to make any
configuration changes to existing Storage Classes, PVs or PVCs (referring to
in-tree plugins) when transitioning to a CSI driver that supersedes an in-tree plugin.

In the alpha state, the operations and features that are supported include
provisioning/delete, attach/detach, mount/unmount and resizing of volumes.

In-tree plugins that support CSI Migration and have a corresponding CSI driver implemented
are listed in the &#34;Types of Volumes&#34; section above.
 --&gt;
&lt;h4 id=&#34;从内部插件迁移到-csi-驱动&#34;&gt;从内部插件迁移到 CSI 驱动&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;当启用 CSI 迁移特性后，当内部插件有对应的 CSI 插件(已经完成安装配置)时。这个特性实现了必要的翻译
逻辑将操作无缝地转移。 最终，操作者不需要对已经存在的 StorageClass， PV， PVC(使用内部插件的)
配置做任何修改而实现 从内部插件迁移到 CSI 驱动上。&lt;/p&gt;
&lt;p&gt;在 alpha 状态，支持的操作和特性包括 管理/删除 attach/detach, mount/unmount 和修改卷大小。&lt;/p&gt;
&lt;p&gt;支持 CSI 迁移并且有对应 CSI 驱动实现的内部插件见上面的 &lt;a href=&#34;#types-of-volumes&#34;&gt;卷类型&lt;/a&gt; 章节&lt;/p&gt;
&lt;h3 id=&#34;flexVolume&#34;&gt;FlexVolume&lt;/h3&gt;
&lt;p&gt;FlexVolume 是一个外部插件接口，它从 k8v v1.2 (在 CSI 之间)就存在了。 它使用一个基于 exec
的模式与驱动交互。 &lt;code&gt;FlexVolume&lt;/code&gt; 驱动的二进制必须要预先安装在节点上一个预定义插件路径中(有些情况 master 也要装)&lt;/p&gt;
&lt;p&gt;Pod 通过内部插件 &lt;code&gt;flexvolume&lt;/code&gt; 与 FlexVolume 驱动交互，更多信息请见
&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md&#34;&gt;这里&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Mount propagation

Mount propagation allows for sharing volumes mounted by a Container to
other Containers in the same Pod, or even to other Pods on the same node.

Mount propagation of a volume is controlled by `mountPropagation` field in Container.volumeMounts.
Its values are:

 * `None` - This volume mount will not receive any subsequent mounts
   that are mounted to this volume or any of its subdirectories by the host.
   In similar fashion, no mounts created by the Container will be visible on
   the host. This is the default mode.

   This mode is equal to `private` mount propagation as described in the
   [Linux kernel documentation](https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt)

 * `HostToContainer` - This volume mount will receive all subsequent mounts
   that are mounted to this volume or any of its subdirectories.

   In other words, if the host mounts anything inside the volume mount, the
   Container will see it mounted there.

   Similarly, if any Pod with `Bidirectional` mount propagation to the same
   volume mounts anything there, the Container with `HostToContainer` mount
   propagation will see it.

   This mode is equal to `rslave` mount propagation as described in the
   [Linux kernel documentation](https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt)

 * `Bidirectional` - This volume mount behaves the same the `HostToContainer` mount.
   In addition, all volume mounts created by the Container will be propagated
   back to the host and to all Containers of all Pods that use the same volume.

   A typical use case for this mode is a Pod with a FlexVolume or CSI driver or
   a Pod that needs to mount something on the host using a `hostPath` volume.

   This mode is equal to `rshared` mount propagation as described in the
   [Linux kernel documentation](https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt)

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; &lt;code&gt;Bidirectional&lt;/code&gt; mount propagation can be dangerous. It can damage
the host operating system and therefore it is allowed only in privileged
Containers. Familiarity with Linux kernel behavior is strongly recommended.
In addition, any volume mounts created by Containers in Pods must be destroyed
(unmounted) by the Containers on termination.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;挂载传播&#34;&gt;挂载传播&lt;/h2&gt;
&lt;p&gt;挂载传播可以让同一个 Pod 中的一个容器上挂载的卷可以共享给另一个容器，甚至可以共享给同一个节点上
的其它 Pod&lt;/p&gt;
&lt;p&gt;一个卷的挂载传播受容 &lt;code&gt;Container.volumeMounts&lt;/code&gt; 的 &lt;code&gt;mountPropagation&lt;/code&gt; 字段控制。可选值有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;None&lt;/code&gt; 这个卷挂载不会接收到任何后续由主机挂载到这个卷或任意其子目录的挂载。
类似地，由容器创建的挂载在主机上不可见。 这是默认模式。&lt;/p&gt;
&lt;p&gt;这个模式与
&lt;a href=&#34;https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt&#34;&gt;Linux 内核文档&lt;/a&gt;
中描述的 &lt;code&gt;private&lt;/code&gt; 挂载传播等效。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;HostToContainer&lt;/code&gt; 这个卷挂载会接收所有后继到这个卷或其子目录的挂载。&lt;/p&gt;
&lt;p&gt;换种方式来说， 如果主机挂载了任意内容到这个卷挂载，容器中就会看到挂载的内容。&lt;/p&gt;
&lt;p&gt;相似地，如果任意使用 &lt;code&gt;Bidirectional&lt;/code&gt; 挂载传播的 Pod 挂载了任意内容， 使用 &lt;code&gt;HostToContainer&lt;/code&gt;
挂载传播的容器也会看到这个挂载。&lt;/p&gt;
&lt;p&gt;这种模式与
&lt;a href=&#34;https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt&#34;&gt;Linux 内核文档&lt;/a&gt;
中描述的 &lt;code&gt;rslave&lt;/code&gt; 挂载传播等效。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Bidirectional&lt;/code&gt; 这种卷挂载的表现方式与 &lt;code&gt;HostToContainer&lt;/code&gt; 挂载相同。 同时，所以由容器创建
卷挂载也会反向传播给主机和所以使用这个卷的所有 Pod 的所有容器。&lt;/p&gt;
&lt;p&gt;使用这种模式的一个典型场景是一个使用 &lt;code&gt;FlexVolume&lt;/code&gt; 或 CSI 驱动容器 或 一个需要使用 &lt;code&gt;hostPath&lt;/code&gt;
卷挂载一些主机内容的容器。
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; &lt;code&gt;Bidirectional&lt;/code&gt; 挂载传播可能会相当危险。 它可能危害主机操作系统并且只能在提权容器上使用。
强烈建议操作使用前要相当熟悉  Linux 内核行为。 另外，由 Pod 中容器创建的卷挂载必须要在容器被终结时销毁
(卸载)&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Configuration
Before mount propagation can work properly on some deployments (CoreOS,
RedHat/Centos, Ubuntu) mount share must be configured correctly in
Docker as shown below.

Edit your Docker&#39;s `systemd` service file.  Set `MountFlags` as follows:
```shell
MountFlags=shared
```
Or, remove `MountFlags=slave` if present.  Then restart the Docker daemon:
```shell
sudo systemctl daemon-reload
sudo systemctl restart docker
```
 --&gt;
&lt;h3 id=&#34;配置&#34;&gt;配置&lt;/h3&gt;
&lt;p&gt;Before mount propagation can work properly on some deployments (CoreOS,
RedHat/Centos, Ubuntu) mount share must be configured correctly in
Docker as shown below.
想要让挂载传播在某些部署(CoreOS, RedHat/Centos, Ubuntu)上正常工作，需要在 Docker 上
进行正确的共享挂载配置，具体如下:
编辑 Docker &lt;code&gt;systemd&lt;/code&gt; 服务配置文件，将 &lt;code&gt;MountFlags&lt;/code&gt; 设置如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MountFlags=shared
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;或者，如果有配置为 &lt;code&gt;MountFlags=slave&lt;/code&gt; 则将其移除。然后重启 Docker 守护进程:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo systemctl daemon-reload
sudo systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;相关资料-1&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
* Follow an example of [deploying WordPress and MySQL with Persistent Volumes](/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/).
--&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/&#34;&gt;部署带有持久卷的 WordPress 和 MySQL&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Service</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service/</link>
      <pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- bprashanth
title: Service
feature:
  title: Service discovery and load balancing
  description: &gt;
    No need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them.

content_type: concept
weight: 10
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
&lt;p&gt;以网络服务的方式让一个由一组
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
组成的应用能够对外提供服务的一种抽象方式&lt;/p&gt;

With Kubernetes you don&#39;t need to modify your application to use an unfamiliar service discovery mechanism.
Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods,
and can load-balance across them.
 --&gt;
&lt;p&gt;以网络服务的方式让一个由一组
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
组成的应用能够对外提供服务的一种抽象方式&lt;/p&gt;
&lt;p&gt;在使用 k8s 时并不需要修改应用来使用不熟悉的服务发现机制。 k8s 为 Pod 提供了自己的 IP 地址和
也为 Pod 集合提供单个 DNS 名称，并为其提供负载均衡。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Motivation

Kubernetes &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; are mortal.
They are born and when they die, they are not resurrected.
If you use a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#39; target=&#39;_blank&#39;&gt;Deployment&lt;span class=&#39;tooltip-text&#39;&gt;管理集群中的一个多副本应用&lt;/span&gt;
&lt;/a&gt; to run your app,
it can create and destroy Pods dynamically.

Each Pod gets its own IP address, however in a Deployment, the set of Pods
running in one moment in time could be different from
the set of Pods running that application a moment later.

This leads to a problem: if some set of Pods (call them &#34;backends&#34;) provides
functionality to other Pods (call them &#34;frontends&#34;) inside your cluster,
how do the frontends find out and keep track of which IP address to connect
to, so that the frontend can use the backend part of the workload?

Enter _Services_.
 --&gt;
&lt;h2 id=&#34;动机&#34;&gt;动机&lt;/h2&gt;
&lt;p&gt;k8s 的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 是会挂掉的。它们出生然后挂掉，
它们挂了以后就不能再重生了。 如果使用
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#39; target=&#39;_blank&#39;&gt;Deployment&lt;span class=&#39;tooltip-text&#39;&gt;管理集群中的一个多副本应用&lt;/span&gt;
&lt;/a&gt;&lt;br&gt;
来运行应用，则它会动态地创建和销毁 Pod。&lt;/p&gt;
&lt;p&gt;每个 Pod 都会有一个自己的 IP 地址， 但是在 Deployment 中，它所管理的 Pod 在这一个时间点和
另一个时间点可能是不一样的。&lt;/p&gt;
&lt;p&gt;这就会导致一个问题: 如果在集群中有一组 Pod (称作 &amp;ldquo;后端&amp;rdquo;)为另一组 Pod (称作 &amp;ldquo;前端&amp;rdquo;)提供功能，
那么前端的 Pod 怎么能一直找到后端的连接 IP 地址，然后使用后端作为工作负载呢。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Services&lt;/em&gt; 就闪亮登场了.&lt;/p&gt;
&lt;!--
## Service resources {#service-resource}

In Kubernetes, a Service is an abstraction which defines a logical set of Pods
and a policy by which to access them (sometimes this pattern is called
a micro-service). The set of Pods targeted by a Service is usually determined
by a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/&#39; target=&#39;_blank&#39;&gt;selector&lt;span class=&#39;tooltip-text&#39;&gt;允许用户基于标签过滤资源列表&lt;/span&gt;
&lt;/a&gt;
(see [below](#services-without-selectors) for why you might want a Service
_without_ a selector).

For example, consider a stateless image-processing backend which is running with
3 replicas.  Those replicas are fungible&amp;mdash;frontends do not care which backend
they use.  While the actual Pods that compose the backend set may change, the
frontend clients should not need to be aware of that, nor should they need to keep
track of the set of backends themselves.

The Service abstraction enables this decoupling.

### Cloud-native service discovery

If you&#39;re able to use Kubernetes APIs for service discovery in your application,
you can query the &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt;
for Endpoints, that get updated whenever the set of Pods in a Service changes.

For non-native applications, Kubernetes offers ways to place a network port or load
balancer in between your application and the backend Pods.

 --&gt;
&lt;h2 id=&#34;service-resource&#34;&gt;Service 资源&lt;/h2&gt;
&lt;p&gt;在 k8s 中， Service 是一个抽象概念，它定义的是逻辑组上的一组 Pod 与访问它们的策略(有时候这种模式也被称为 微服务)。
Service 所指向的是哪些 Pod 通常是由&lt;br&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/&#39; target=&#39;_blank&#39;&gt;选择器&lt;span class=&#39;tooltip-text&#39;&gt;允许用户基于标签过滤资源列表&lt;/span&gt;
&lt;/a&gt;
决定的(&lt;a href=&#34;#services-without-selectors&#34;&gt;下面&lt;/a&gt;还介绍了可能 &lt;em&gt;不需要&lt;/em&gt; 选择器的 Service).&lt;/p&gt;
&lt;p&gt;例如，假如有一个无状的图片处理后端，有3个副本在运行。 这些副本是可替代的 — 前端不关心它们
用的是哪个后台。 当组成后端的 Pod 可能发生变化， 但前端的客户端应该不能感知到，它们也不需要自己
来跟踪后端的具体成员。&lt;/p&gt;
&lt;p&gt;Service 的抽象实现了这样的解耦。&lt;/p&gt;
&lt;h3 id=&#34;云原生服务发现&#34;&gt;云原生服务发现&lt;/h3&gt;
&lt;p&gt;如果能够在应用中使用 k8s API 来实现服务发现， 可以通过
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt;
查询 Endpoint, 通过这种方式可以实时更新到 Service 的 Pod 变更。&lt;/p&gt;
&lt;p&gt;对于非原生应用， k8s 为应用与后端 Pod 之间通信提供了网络端口或负载均衡等方式。&lt;/p&gt;
&lt;!--
## Defining a Service

A Service in Kubernetes is a REST object, similar to a Pod.  Like all of the
REST objects, you can `POST` a Service definition to the API server to create
a new instance.
The name of a Service object must be a valid
[DNS label name](/docs/concepts/overview/working-with-objects/names#dns-label-names).

For example, suppose you have a set of Pods that each listen on TCP port 9376
and carry a label `app=MyApp`:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
```

This specification creates a new Service object named &#34;my-service&#34;, which
targets TCP port 9376 on any Pod with the `app=MyApp` label.

Kubernetes assigns this Service an IP address (sometimes called the &#34;cluster IP&#34;),
which is used by the Service proxies
(see [Virtual IPs and service proxies](#virtual-ips-and-service-proxies) below).

The controller for the Service selector continuously scans for Pods that
match its selector, and then POSTs any updates to an Endpoint object
also named &#34;my-service&#34;.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A Service can map &lt;em&gt;any&lt;/em&gt; incoming &lt;code&gt;port&lt;/code&gt; to a &lt;code&gt;targetPort&lt;/code&gt;. By default and
for convenience, the &lt;code&gt;targetPort&lt;/code&gt; is set to the same value as the &lt;code&gt;port&lt;/code&gt;
field.&lt;/div&gt;
&lt;/blockquote&gt;


Port definitions in Pods have names, and you can reference these names in the
`targetPort` attribute of a Service. This works even if there is a mixture
of Pods in the Service using a single configured name, with the same network
protocol available via different port numbers.
This offers a lot of flexibility for deploying and evolving your Services.
For example, you can change the port numbers that Pods expose in the next
version of your backend software, without breaking clients.

The default protocol for Services is TCP; you can also use any other
[supported protocol](#protocol-support).

As many Services need to expose more than one port, Kubernetes supports multiple
port definitions on a Service object.
Each port definition can have the same `protocol`, or a different one.
 --&gt;
&lt;h2 id=&#34;service-定义&#34;&gt;Service 定义&lt;/h2&gt;
&lt;p&gt;Service 在 k8s 中是一个 &lt;code&gt;REST&lt;/code&gt; 对象， 与 Pod 类似。 与其它所有 &lt;code&gt;REST&lt;/code&gt; 对象一样，
可以通过 &lt;code&gt;POST&lt;/code&gt; 请求将 Service 定义发送到 api-server 来创建一个新的实例。
Service 的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-label-names&#34;&gt;DNS 标签名称&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;例如， 假如有一组 Pod， 每个 Pod 监听的端口都是 &lt;code&gt;9376&lt;/code&gt;， 都打着一个标签为 &lt;code&gt;app=MyApp&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上面的配置定义了一个 Service 对象，名字叫 &amp;ldquo;my-service&amp;rdquo;， 指向所有 TCP 端口为 &lt;code&gt;9376&lt;/code&gt;， 带有
&lt;code&gt;app=MyApp&lt;/code&gt; 标签的 Pod。&lt;/p&gt;
&lt;p&gt;k8s 会为 Service 分配一个 IP 地址(有时称为 &amp;ldquo;集群IP (cluster IP)&amp;quot;), 这个 IP 地址会被
Service 代理使用。
(见下面的 &lt;a href=&#34;#virtual-ips-and-service-proxies&#34;&gt;虚拟IP 和 service 代理&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Service 选择器的控制器会持续扫描匹配其选择器的 Pod，然后把这些变更以 POST 请求方式发送到一个
叫 &amp;ldquo;my-service&amp;rdquo; 的 Endpoint 对象。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Service 可以映射 &lt;em&gt;任意&lt;/em&gt; 输入 &lt;code&gt;port&lt;/code&gt; 到 &lt;code&gt;targetPort&lt;/code&gt;。 默认情况和为了方便， &lt;code&gt;targetPort&lt;/code&gt;
会设置与 &lt;code&gt;port&lt;/code&gt; 字段相同的值。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;Pod 中的 Port 定义是有名字的， 这个名字可以在 Service &lt;code&gt;targetPort&lt;/code&gt; 属性上引用。
这种方式甚至可以用在当 Service 中使用同一个配置名称的不同 Pod，使用相同的网络协议，不同的端口
这个特性为 Service 的部署和演进提供了很高的灵活性。
例如， 用户可以修改用于下一版后端软件 Pod 暴露的端口，而不影响客户端。&lt;/p&gt;
&lt;p&gt;Services 默认协议为 TCP; 也可以使用其它&lt;a href=&#34;#protocol-support&#34;&gt;支持的协议&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;当许多的服务需要显露不止一个端口， k8s 支持在一个 Service 对象上定义多个端口。 每个端口定义
可以使用同样的 协议(&lt;code&gt;protocol&lt;/code&gt;), 也可以使用不同的.&lt;/p&gt;
&lt;h3 id=&#34;无标签选择器-service&#34;&gt;无标签选择器 Service&lt;/h3&gt;
&lt;p&gt;Service 最常见的用户就是作为 k8s Pod 的入口， 但它也可以作为其它类型的后端的抽象入口。
例如:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在生产环境使用的集群外的数库，但是在测试环境用的是内部的数据。&lt;/li&gt;
&lt;li&gt;想要将 Service 集群中另一个命名空间中的 Service 或另一个集群的服务。&lt;/li&gt;
&lt;li&gt;在迁移工作负载到 k8s 时，为了评估是否可行，先使用一部分后端服务在 k8s 中。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In any of these scenarios you can define a Service &lt;em&gt;without&lt;/em&gt; a Pod selector.
For example:
在以上的任意一种场景中都需要定义 &lt;em&gt;没有&lt;/em&gt; Pod 选择器的 Service。
例如:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;因为这些 Service 没有选择器，所以对象的 Endpoint 也 &lt;em&gt;不会&lt;/em&gt; 自动创建。 可以通过手动创建 Endpoint
对象的方式将 Service 映射到实际运行的网络地址和端口。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Endpoints&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;subsets&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;addresses&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;ip&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;192.0.2.42&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Endpoint 对象的名称必以是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;Endpoint 对象所用的 IP &lt;em&gt;必须不能&lt;/em&gt; 是: 回环地址 (127.0.0.0/8 IPv4, ::1/128 IPv6)，或
链路本地(link-local) (169.254.0.0/16 和 224.0.0.0/24  IPv4, fe80::/64 IPv6).&lt;/p&gt;
&lt;p&gt;Endpoint IP 地址也不能是其它 k8s Service 的集群 IP， 因为
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/kube-proxy/&#39; target=&#39;_blank&#39;&gt;kube-proxy&lt;span class=&#39;tooltip-text&#39;&gt;kube-proxy is a network proxy that runs on each node in the cluster.&lt;/span&gt;
&lt;/a&gt;
不支持将虚拟IP作为目的地址。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;访问无选择器的 Service 与有选择器的 Service 是一样的。在上面的例子中， 流量会路由到YAML定义中
唯一的 Endpoint &lt;code&gt;192.0.2.42:9376&lt;/code&gt; (TCP)&lt;/p&gt;
&lt;p&gt;一个 ExternalName Service 是 Service 中的一种特殊情景， 它没有选择器而是 DNS 名称。
更多信息见本文下面的 &lt;a href=&#34;#externalname&#34;&gt;ExternalName&lt;/a&gt;。&lt;/p&gt;
&lt;!--
### EndpointSlices





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;



EndpointSlices are an API resource that can provide a more scalable alternative
to Endpoints. Although conceptually quite similar to Endpoints, EndpointSlices
allow for distributing network endpoints across multiple resources. By default,
an EndpointSlice is considered &#34;full&#34; once it reaches 100 endpoints, at which
point additional EndpointSlices will be created to store any additional
endpoints.

EndpointSlices provide additional attributes and functionality which is
described in detail in [EndpointSlices](/docs/concepts/services-networking/endpoint-slices/).
 --&gt;
&lt;h3 id=&#34;endpointslices&#34;&gt;EndpointSlices&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;EndpointSlice 是一种可比 Endpoint 提供更新好伸缩性替代方案的 API 资源。尽管在概念与 Endpoint
很相近， EndpointSlice 允许对铆中资源的网络末端进行分发. 默认情况下当一个 EndpointSlice
的网络末端数量达到 100 时就认为是 &amp;ldquo;满了&amp;rdquo;, 这时候就会创建新的 EndpointSlice 来存储更多的网络末端。&lt;/p&gt;
&lt;p&gt;更多 EndpointSlice 提供的属性和功能请见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/endpoint-slices/&#34;&gt;EndpointSlices&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### Application protocol






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;



The AppProtocol field provides a way to specify an application protocol to be
used for each Service port. The value of this field is mirrored by corresponding
Endpoints and EndpointSlice resources.
--&gt;
&lt;h3 id=&#34;应用协议&#34;&gt;应用协议&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;code&gt;AppProtocol&lt;/code&gt; 字段提供了指定每个 Service 端口对应的应用协议的一种方式。
这个字段是对 Endpoint 和 EndpointSlice 对应字段的镜像。&lt;/p&gt;
&lt;!--
## Virtual IPs and service proxies

Every node in a Kubernetes cluster runs a `kube-proxy`. `kube-proxy` is
responsible for implementing a form of virtual IP for `Services` of type other
than [`ExternalName`](#externalname).
--&gt;
&lt;h2 id=&#34;虚拟-ip-和-service-代理&#34;&gt;虚拟 IP 和 Service 代理&lt;/h2&gt;
&lt;p&gt;Every node in a Kubernetes cluster runs a &lt;code&gt;kube-proxy&lt;/code&gt;. &lt;code&gt;kube-proxy&lt;/code&gt; is
responsible for implementing a form of virtual IP for &lt;code&gt;Services&lt;/code&gt; of type other
than &lt;a href=&#34;#externalname&#34;&gt;&lt;code&gt;ExternalName&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;k8s 集群中的每一个节点上都运行了 &lt;code&gt;kube-proxy&lt;/code&gt;， &lt;code&gt;kube-proxy&lt;/code&gt; 负责实现除了
&lt;a href=&#34;#externalname&#34;&gt;&lt;code&gt;ExternalName&lt;/code&gt;&lt;/a&gt;
外其它类型的 &lt;code&gt;Services&lt;/code&gt; 虚拟 IP 的实现方式&lt;/p&gt;
&lt;!--
### Why not use round-robin DNS?

A question that pops up every now and then is why Kubernetes relies on
proxying to forward inbound traffic to backends. What about other
approaches? For example, would it be possible to configure DNS records that
have multiple A values (or AAAA for IPv6), and rely on round-robin name
resolution?

There are a few reasons for using proxying for Services:

 * There is a long history of DNS implementations not respecting record TTLs,
   and caching the results of name lookups after they should have expired.
 * Some apps do DNS lookups only once and cache the results indefinitely.
 * Even if apps and libraries did proper re-resolution, the low or zero TTLs
   on the DNS records could impose a high load on DNS that then becomes
   difficult to manage.
 --&gt;
&lt;h3 id=&#34;为嘛不用-round-robin-dns-&#34;&gt;为嘛不用 round-robin DNS ?&lt;/h3&gt;
&lt;p&gt;一个时不时被提起的问题就是为啥 k8s 信赖于代理来转发入站流量到后端。 为啥不用其它的方式？
例如，有没有可能通过配置包含多个 A 值的 DNS 记录(IPv6 用 AAAA)， 然后通过轮询域名解析结果。
以下为 Service 使用代理的几个原因:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DNS 实现不遵循记录的 TTL有长久的历史， 并且在结果过期后继续使用缓存查询结果&lt;/li&gt;
&lt;li&gt;有些应用一次查询 DNS 后永远使用缓存的查询结果&lt;/li&gt;
&lt;li&gt;即便每个应用规范地来查 DNS， 但是 DNS 记录上的 TTL 的值很低或为0 会导致 DNS 的负载很高，并且变得难于管理&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### User space proxy mode {#proxy-mode-userspace}

In this mode, kube-proxy watches the Kubernetes master for the addition and
removal of Service and Endpoint objects. For each Service it opens a
port (randomly chosen) on the local node.  Any connections to this &#34;proxy port&#34;
are
proxied to one of the Service&#39;s backend Pods (as reported via
Endpoints). kube-proxy takes the `SessionAffinity` setting of the Service into
account when deciding which backend Pod to use.

Lastly, the user-space proxy installs iptables rules which capture traffic to
the Service&#39;s `clusterIP` (which is virtual) and `port`. The rules
redirect that traffic to the proxy port which proxies the backend Pod.

By default, kube-proxy in userspace mode chooses a backend via a round-robin algorithm.

![Services overview diagram for userspace proxy](/images/docs/services-userspace-overview.svg)
 --&gt;
&lt;h3 id=&#34;proxy-mode-userspace&#34;&gt;user-space 代理模式&lt;/h3&gt;
&lt;p&gt;在这种模式下， kube-proxy 监听 k8s 主控节点上 Service 和 Endpoint 对象的添加和删除。
对每一个 Service 它会在本地节点打开一个端口(随机选择)。任意一个连接到该 &amp;ldquo;代理端口&amp;quot;的流量都会代理
到 Service 后端 Pod(由 Endpoint 报告) 中的一个上。 kube-proxy 使用 Service 的 &lt;code&gt;SessionAffinity&lt;/code&gt;
设置为决定使用哪个后端 Pod。&lt;/p&gt;
&lt;p&gt;最后 user-space 代理会添加相应的 iptables 规则将捕获到 Service &lt;code&gt;clusterIP&lt;/code&gt;(是一个虚拟IP) 和 &lt;code&gt;port&lt;/code&gt; 的流量
然后这些规则将这些流量重定向到刚提供的会代理到后端 Pod 的代理端口上。&lt;/p&gt;
&lt;p&gt;默认情况下， kube-proxy 在使用 user-space 代理模式是使用轮询算法选择后端的 Pod。
&lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/docs/services-userspace-overview.svg&#34; alt=&#34;Services overview diagram for userspace proxy&#34;&gt;&lt;/p&gt;
&lt;!--
### `iptables` proxy mode {#proxy-mode-iptables}

In this mode, kube-proxy watches the Kubernetes control plane for the addition and
removal of Service and Endpoint objects. For each Service, it installs
iptables rules, which capture traffic to the Service&#39;s `clusterIP` and `port`,
and redirect that traffic to one of the Service&#39;s
backend sets.  For each Endpoint object, it installs iptables rules which
select a backend Pod.

By default, kube-proxy in iptables mode chooses a backend at random.

Using iptables to handle traffic has a lower system overhead, because traffic
is handled by Linux netfilter without the need to switch between userspace and the
kernel space. This approach is also likely to be more reliable.

If kube-proxy is running in iptables mode and the first Pod that&#39;s selected
does not respond, the connection fails. This is different from userspace
mode: in that scenario, kube-proxy would detect that the connection to the first
Pod had failed and would automatically retry with a different backend Pod.

You can use Pod [readiness probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes)
to verify that backend Pods are working OK, so that kube-proxy in iptables mode
only sees backends that test out as healthy. Doing this means you avoid
having traffic sent via kube-proxy to a Pod that&#39;s known to have failed.

![Services overview diagram for iptables proxy](/k8sDocs/images/docs/services-iptables-overview.svg)
--&gt;
&lt;h3 id=&#34;proxy-mode-iptables&#34;&gt;&lt;code&gt;iptables&lt;/code&gt; 代理模式&lt;/h3&gt;
&lt;p&gt;在这种模式下， kube-proxy 监听 k8s 主控节点上 Service 和 Endpoint 对象的添加和删除。
对于每个 Service， 会添加一个 iptables 规则， 这个规则会捕获所有目标是 Service 的 &lt;code&gt;clusterIP&lt;/code&gt; 和 &lt;code&gt;port&lt;/code&gt;
的流量并将其重定向到 Service 后端 Pod 中的一个上。 对于每个 Endpoint 也会添加一个 iptables
规则，用来选择后端的 Pod。&lt;/p&gt;
&lt;p&gt;默认情况下，kube-proxy 在 &lt;code&gt;iptables&lt;/code&gt; 代理模式下，随机选择后端 Pod。&lt;/p&gt;
&lt;p&gt;使用 iptables 来处理流量系统开销更低， 因为流量由 Linux netfilter处理，而不需要在 userspace 和
kernelspace 之间来回切换。 这种方式也可能更加可靠。&lt;/p&gt;
&lt;p&gt;如果 kube-proxy 使用的 iptables 模式， 如果选择的第一个 Pod 没有响应，这个连接就失败了。
这与 userspace 模式不同： 在这种场景下， kube-proxy 会检测到第一个 Pod 的连接失败了，会自动
地尝试其它的后端 Pod。&lt;/p&gt;
&lt;p&gt;可以使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/#container-probes&#34;&gt;就绪探针&lt;/a&gt;
来验证后端的 Pod 是在正常工作的， 因此 kube-proxy 在 iptables 模式下只会看到检测结果为健康的
后端 Pod。 这么做的意义在于避免了通过 kube-proxy 将流量发送到已经知道挂了的 Pod 上。
&lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/docs/services-iptables-overview.svg&#34; alt=&#34;Services overview diagram for iptables proxy&#34;&gt;&lt;/p&gt;
&lt;!--
### IPVS proxy mode {#proxy-mode-ipvs}






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [stable]&lt;/code&gt;
&lt;/div&gt;



In `ipvs` mode, kube-proxy watches Kubernetes Services and Endpoints,
calls `netlink` interface to create IPVS rules accordingly and synchronizes
IPVS rules with Kubernetes Services and Endpoints periodically.
This control loop ensures that IPVS status matches the desired
state.
When accessing a Service, IPVS directs traffic to one of the backend Pods.

The IPVS proxy mode is based on netfilter hook function that is similar to
iptables mode, but uses a hash table as the underlying data structure and works
in the kernel space.
That means kube-proxy in IPVS mode redirects traffic with lower latency than
kube-proxy in iptables mode, with much better performance when synchronising
proxy rules. Compared to the other proxy modes, IPVS mode also supports a
higher throughput of network traffic.

IPVS provides more options for balancing traffic to backend Pods;
these are:

- `rr`: round-robin
- `lc`: least connection (smallest number of open connections)
- `dh`: destination hashing
- `sh`: source hashing
- `sed`: shortest expected delay
- `nq`: never queue

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;To run kube-proxy in IPVS mode, you must make IPVS available on
the node before starting kube-proxy.&lt;/p&gt;
&lt;p&gt;When kube-proxy starts in IPVS proxy mode, it verifies whether IPVS
kernel modules are available. If the IPVS kernel modules are not detected, then kube-proxy
falls back to running in iptables proxy mode.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;


![Services overview diagram for IPVS proxy](/k8sDocs/images/docs/services-ipvs-overview.svg)

In these proxy models, the traffic bound for the Service&#39;s IP:Port is
proxied to an appropriate backend without the clients knowing anything
about Kubernetes or Services or Pods.

If you want to make sure that connections from a particular client
are passed to the same Pod each time, you can select the session affinity based
on the client&#39;s IP addresses by setting `service.spec.sessionAffinity` to &#34;ClientIP&#34;
(the default is &#34;None&#34;).
You can also set the maximum session sticky time by setting
`service.spec.sessionAffinityConfig.clientIP.timeoutSeconds` appropriately.
(the default value is 10800, which works out to be 3 hours).
 --&gt;
&lt;h3 id=&#34;proxy-mode-ipvs&#34;&gt;IPVS 代理&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;在 &lt;code&gt;ipvs&lt;/code&gt; 模式下， kube-proxy 监听 k8s Services 和 Endpoint,
调用 &lt;code&gt;netlink&lt;/code&gt; 接口来创建 IPVS 规则， 并定时根据 k8s Services 和 Endpoint 更新 IPVS 规则。
这个控制回环确保 IPVS 的状态与期望状态一至。 当访问一个 Service 时， IPVS 重定向流量到
后端 Pod 中的一个上。&lt;/p&gt;
&lt;p&gt;IPVS 代理模式基于 netfilter 钩子功能，与 iptables 类似， 但底层使用的数据结构是一个哈希表
并且是在内核空间中工作的。
也就是 kube-proxy 在 IPVS 模式下， 重定向流量会比 iptables 模式有更低的延迟，在同步代理
规则时也会有更好的恨不能。 与其它的代理模式相比， IPVS 模式也支持更高吞吐量的网络流量。&lt;/p&gt;
&lt;p&gt;IPVS 还提供了更多到后端 Pod 的负载均衡选择；
具体如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;rr&lt;/code&gt;: 轮询&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lc&lt;/code&gt;: 最少连接 (打开连接数最小的)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dh&lt;/code&gt;: 目标哈希&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sh&lt;/code&gt;: 源哈希&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sed&lt;/code&gt;: 最短期望延迟 (shortest expected delay)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nq&lt;/code&gt;: 无须队列等待 (never queue)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;要让 kube-proxy 以 IPVS 运行，必须要在 kube-proxy 启动之前让 IPVS 在节点上是可用的。&lt;/p&gt;
&lt;p&gt;当 kube-proxy 以 IPVS 代理模式启动时， 会检测 IPVS 内核模块是否可用。 如 IPVS 内核模块没有检测到，
则 kube-proxy 会回退以 iptables 模式运行。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/docs/services-ipvs-overview.svg&#34; alt=&#34;Services overview diagram for IPVS proxy&#34;&gt;&lt;/p&gt;
&lt;p&gt;In these proxy models, the traffic bound for the Service&amp;rsquo;s IP:Port is
proxied to an appropriate backend without the clients knowing anything
about Kubernetes or Services or Pods.
在使用这种代理模式时， 访问 Service IP:Port 的流量被代理到适当的后端时，客户端不会感知到
k8s 或 Service 或 Pod 这些的存在。&lt;/p&gt;
&lt;p&gt;如果需要保证一个特定客户端的连接每次都要转发到同一个 Pod 上面， 可能设置
&lt;code&gt;service.spec.sessionAffinity&lt;/code&gt; 为 &amp;ldquo;ClientIP&amp;rdquo; (默认为 &amp;ldquo;None&amp;rdquo;) 来选择基于客户端IP的会话亲和性(session affinity).
也可以设置基于时间的会话黏性，为 &lt;code&gt;service.spec.sessionAffinityConfig.clientIP.timeoutSeconds&lt;/code&gt;
设置一个合适的值。 (默认值为 10800， 也就是 3 个小时)&lt;/p&gt;
&lt;!--
## Multi-Port Services

For some Services, you need to expose more than one port.
Kubernetes lets you configure multiple port definitions on a Service object.
When using multiple ports for a Service, you must give all of your ports names
so that these are unambiguous.
For example:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 9376
    - name: https
      protocol: TCP
      port: 443
      targetPort: 9377
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;As with Kubernetes &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/names&#39; target=&#39;_blank&#39;&gt;names&lt;span class=&#39;tooltip-text&#39;&gt;A client-provided string that refers to an object in a resource URL, such as /api/v1/pods/some-name.&lt;/span&gt;
&lt;/a&gt; in general, names for ports
must only contain lowercase alphanumeric characters and &lt;code&gt;-&lt;/code&gt;. Port names must
also start and end with an alphanumeric character.&lt;/p&gt;
&lt;p&gt;For example, the names &lt;code&gt;123-abc&lt;/code&gt; and &lt;code&gt;web&lt;/code&gt; are valid, but &lt;code&gt;123_abc&lt;/code&gt; and &lt;code&gt;-web&lt;/code&gt; are not.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;多端口的-service&#34;&gt;多端口的 Service&lt;/h2&gt;
&lt;p&gt;对于有些 Service 需要显露多于一个端口， k8s 允许用户在 Service 对象上定义多个端口。
当在 Service 上使用多个端口时，必须要给所有的端口配置名字，这样才便于区分。
例如:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;443&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9377&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;依照 k8s 通用 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/names&#39; target=&#39;_blank&#39;&gt;Name&lt;span class=&#39;tooltip-text&#39;&gt;A client-provided string that refers to an object in a resource URL, such as /api/v1/pods/some-name.&lt;/span&gt;
&lt;/a&gt;， 端口的名称只能包含小写字母，数字，和 中划线(&lt;code&gt;-&lt;/code&gt;)
且端口名只能以字母数字开始和结尾。&lt;/p&gt;
&lt;p&gt;例如， &lt;code&gt;123-abc&lt;/code&gt; 和 &lt;code&gt;web&lt;/code&gt; 是有效的，&lt;code&gt;123_abc&lt;/code&gt; 和 &lt;code&gt;-web&lt;/code&gt; 是无效的&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Choosing your own IP address

You can specify your own cluster IP address as part of a `Service` creation
request.  To do this, set the `.spec.clusterIP` field. For example, if you
already have an existing DNS entry that you wish to reuse, or legacy systems
that are configured for a specific IP address and difficult to re-configure.

The IP address that you choose must be a valid IPv4 or IPv6 address from within the
`service-cluster-ip-range` CIDR range that is configured for the API server.
If you try to create a Service with an invalid clusterIP address value, the API
server will return a 422 HTTP status code to indicate that there&#39;s a problem.

--&gt;
&lt;h2 id=&#34;选择自己的-ip-地址&#34;&gt;选择自己的 IP 地址&lt;/h2&gt;
&lt;p&gt;在创建 &lt;code&gt;Service&lt;/code&gt; 的时候可以通过设置 &lt;code&gt;.spec.clusterIP&lt;/code&gt; 字段指定自己的集群IP地址。
例如， 希望复用已经存在的 DNS 记录，或者一个已经设置了IP 然后很难重新配置的旧系统。&lt;/p&gt;
&lt;p&gt;选择设置的IP 必须要是 api-server 上配置的 &lt;code&gt;service-cluster-ip-range&lt;/code&gt; CIDR 范围内有效的
IPv4 或 IPv6 地址。 如果尝试使用一个无效的集群IP地址，api-server 会返回一个 422 的 HTTP
状态码，表示配置有问题&lt;/p&gt;
&lt;!--
## Discovering services

Kubernetes supports 2 primary modes of finding a Service - environment
variables and DNS.
 --&gt;
&lt;h2 id=&#34;service-查找&#34;&gt;Service 查找&lt;/h2&gt;
&lt;p&gt;k8s 主要支持 2 种查找一个 Service的方式: 环境变量 和 DNS&lt;/p&gt;
&lt;!--
### Environment variables

When a Pod is run on a Node, the kubelet adds a set of environment variables
for each active Service.  It supports both [Docker links
compatible](https://docs.docker.com/userguide/dockerlinks/) variables (see
[makeLinkVariables](https://releases.k8s.io/master/pkg/kubelet/envvars/envvars.go#L49))
and simpler `{SVCNAME}_SERVICE_HOST` and `{SVCNAME}_SERVICE_PORT` variables,
where the Service name is upper-cased and dashes are converted to underscores.

For example, the Service `&#34;redis-master&#34;` which exposes TCP port 6379 and has been
allocated cluster IP address 10.0.0.11, produces the following environment
variables:

```shell
REDIS_MASTER_SERVICE_HOST=10.0.0.11
REDIS_MASTER_SERVICE_PORT=6379
REDIS_MASTER_PORT=tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP_PROTO=tcp
REDIS_MASTER_PORT_6379_TCP_PORT=6379
REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;When you have a Pod that needs to access a Service, and you are using
the environment variable method to publish the port and cluster IP to the client
Pods, you must create the Service &lt;em&gt;before&lt;/em&gt; the client Pods come into existence.
Otherwise, those client Pods won&amp;rsquo;t have their environment variables populated.&lt;/p&gt;
&lt;p&gt;If you only use DNS to discover the cluster IP for a Service, you don&amp;rsquo;t need to
worry about this ordering issue.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

--&gt;
&lt;h3 id=&#34;环境变量&#34;&gt;环境变量&lt;/h3&gt;
&lt;p&gt;当一个 Pod 运行到一个节点时， kubelet 会把每个活跃的 Service  作为环境变量添加到 Pod 中。
它支持
&lt;a href=&#34;https://docs.docker.com/userguide/dockerlinks/&#34;&gt;Docker 连接兼容&lt;/a&gt;
的变量
(见 &lt;a href=&#34;https://releases.k8s.io/master/pkg/kubelet/envvars/envvars.go#L49&#34;&gt;makeLinkVariables&lt;/a&gt;)
和简单些的 &lt;code&gt;{SVCNAME}_SERVICE_HOST&lt;/code&gt; 和 &lt;code&gt;{SVCNAME}_SERVICE_PORT&lt;/code&gt; 变量，其中 Service
的名称为大写，中划线会被转化为下划线。&lt;/p&gt;
&lt;p&gt;例如， 一个叫 &lt;code&gt;&amp;quot;redis-master&amp;quot;&lt;/code&gt; 的 Service， 暴露的端口是 TCP &lt;code&gt;6379&lt;/code&gt;， 分配的集群IP地址为
&lt;code&gt;10.0.0.11&lt;/code&gt;， 就会产生如下环境变量:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;REDIS_MASTER_SERVICE_HOST&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;10.0.0.11
REDIS_MASTER_SERVICE_PORT&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6379&lt;/span&gt;
REDIS_MASTER_PORT&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP_PROTO&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tcp
REDIS_MASTER_PORT_6379_TCP_PORT&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6379&lt;/span&gt;
REDIS_MASTER_PORT_6379_TCP_ADDR&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;10.0.0.11
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;当有一个 Pod 需要要访问一个 Service， 并且是使用环境变量的方式将端口和集群IP传递给客户端 Pod 的，
那么 Service 必须要在客户端 Pod 创建 &lt;em&gt;之前&lt;/em&gt; 就要存在。 否则客户端 Pod 中就不会加入它对应的环境变量。&lt;/p&gt;
&lt;p&gt;如果只使用 DNS 为查找 Service 的集群IP，则不需要担心这个顺序问题&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### DNS

You can (and almost always should) set up a DNS service for your Kubernetes
cluster using an [add-on](/docs/concepts/cluster-administration/addons/).

A cluster-aware DNS server, such as CoreDNS, watches the Kubernetes API for new
Services and creates a set of DNS records for each one.  If DNS has been enabled
throughout your cluster then all Pods should automatically be able to resolve
Services by their DNS name.

For example, if you have a Service called `&#34;my-service&#34;` in a Kubernetes
Namespace `&#34;my-ns&#34;`, the control plane and the DNS Service acting together
create a DNS record for `&#34;my-service.my-ns&#34;`. Pods in the `&#34;my-ns&#34;` Namespace
should be able to find it by simply doing a name lookup for `my-service`
(`&#34;my-service.my-ns&#34;` would also work).

Pods in other Namespaces must qualify the name as `my-service.my-ns`. These names
will resolve to the cluster IP assigned for the Service.

Kubernetes also supports DNS SRV (Service) records for named ports.  If the
`&#34;my-service.my-ns&#34;` Service has a port named `&#34;http&#34;` with the protocol set to
`TCP`, you can do a DNS SRV query for `_http._tcp.my-service.my-ns` to discover
the port number for `&#34;http&#34;`, as well as the IP address.

The Kubernetes DNS server is the only way to access `ExternalName` Services.
You can find more information about `ExternalName` resolution in
[DNS Pods and Services](/docs/concepts/services-networking/dns-pod-service/).
--&gt;
&lt;h3 id=&#34;dns&#34;&gt;DNS&lt;/h3&gt;
&lt;p&gt;用户可以(并且几乎绝大多时候应该)通过使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/cluster-administration/addons/&#34;&gt;插件&lt;/a&gt;.
为你的集群设置 DNS 服务。&lt;/p&gt;
&lt;p&gt;一个可感知集群的 DNS 服务， 例如 CoreDNS， 会监听 k8s API 创建的 Service 并创建对应的 DNS 记录。
如果集群启用的 DNS 服务，则所以的 Pod 都应该会自动地通过 DNS 名称解析 Service。&lt;/p&gt;
&lt;p&gt;例如，如果有一个名叫 &lt;code&gt;&amp;quot;my-service&amp;quot;&lt;/code&gt; Service 于 &lt;code&gt;&amp;quot;my-ns&amp;quot;&lt;/code&gt; 命名空间，控制中心和 DNS 服务会
协作创建一条 DNS 记录为 &lt;code&gt;&amp;quot;my-service.my-ns&amp;quot;&lt;/code&gt;。 在 &lt;code&gt;&amp;quot;my-ns&amp;quot;&lt;/code&gt; 命名空间的 Pod 可以只需要简单地
使用 &lt;code&gt;my-service&lt;/code&gt; 就能查到(&lt;code&gt;&amp;quot;my-service.my-ns&amp;quot;&lt;/code&gt; 也是可以的)&lt;/p&gt;
&lt;p&gt;在其它命名空间的 Pod 必须使用 &lt;code&gt;my-service.my-ns&lt;/code&gt; 这样的限定名。 这些名称会解析为 Service
分配的集群IP。&lt;/p&gt;
&lt;p&gt;k8s 还支持命名端口的 DNS SRV (Service) 记录。 如果叫 &lt;code&gt;&amp;quot;my-service.my-ns&amp;quot;&lt;/code&gt; 的 Service
有一个叫 &lt;code&gt;&amp;quot;http&amp;quot;&lt;/code&gt; 的 &lt;code&gt;TCP&lt;/code&gt; 端口， 就可以使用 &lt;code&gt;DNS SRV&lt;/code&gt; 查询 &lt;code&gt;_http._tcp.my-service.my-ns&lt;/code&gt;
得到 &lt;code&gt;&amp;quot;http&amp;quot;&lt;/code&gt; 对应的端口号和 IP 地址。&lt;/p&gt;
&lt;p&gt;k8s DNS 服务是访问 &lt;code&gt;ExternalName&lt;/code&gt; Service 的唯一方式。更多关于 &lt;code&gt;ExternalName&lt;/code&gt; 的信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/dns-pod-service/&#34;&gt;DNS Pod 和 Service&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Headless Services

Sometimes you don&#39;t need load-balancing and a single Service IP.  In
this case, you can create what are termed &#34;headless&#34; Services, by explicitly
specifying `&#34;None&#34;` for the cluster IP (`.spec.clusterIP`).

You can use a headless Service to interface with other service discovery mechanisms,
without being tied to Kubernetes&#39; implementation.

For headless `Services`, a cluster IP is not allocated, kube-proxy does not handle
these Services, and there is no load balancing or proxying done by the platform
for them. How DNS is automatically configured depends on whether the Service has
selectors defined:
 --&gt;
&lt;h2 id=&#34;headless-services&#34;&gt;Headless Services&lt;/h2&gt;
&lt;p&gt;有时候并不需要负载均衡和一个 Service 的 IP。 在这种情况下就可以创建一个被称为 &amp;ldquo;无头&amp;rdquo; 的 Service，
更确切的说就是将集群 IP (&lt;code&gt;.spec.clusterIP&lt;/code&gt;) 设置为 &lt;code&gt;&amp;quot;None&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;用户可以使用 无头 Service 作为其它服务发现机制的接口，而不需要与 k8s 实现耦合在一起。&lt;/p&gt;
&lt;p&gt;对于 无头的 &lt;code&gt;Services&lt;/code&gt; 是不会分配集群IP的， &lt;code&gt;kube-proxy&lt;/code&gt; 不会处理这些 Service，
平台也不会对它们提供负载均衡或代理。 DNS 是如何自动配置的基于 Service 是否定义了选择器:&lt;/p&gt;
&lt;!--
### With selectors

For headless Services that define selectors, the endpoints controller creates
`Endpoints` records in the API, and modifies the DNS configuration to return
records (addresses) that point directly to the `Pods` backing the `Service`.

### Without selectors

For headless Services that do not define selectors, the endpoints controller does
not create `Endpoints` records. However, the DNS system looks for and configures
either:

  * CNAME records for [`ExternalName`](#externalname)-type Services.
  * A records for any `Endpoints` that share a name with the Service, for all
    other types.
    --&gt;
&lt;h3 id=&#34;有选择器的&#34;&gt;有选择器的&lt;/h3&gt;
&lt;p&gt;对于有选择器的无头 Service， Endpoint 选择器会创建 &lt;code&gt;Endpoints&lt;/code&gt; 记录， 并修改 DNS 配置
直接返回记录为 &lt;code&gt;Service&lt;/code&gt; 后端 &lt;code&gt;Pod&lt;/code&gt; 的地址。&lt;/p&gt;
&lt;h3 id=&#34;没有选择器的&#34;&gt;没有选择器的&lt;/h3&gt;
&lt;p&gt;For headless Services that do not define selectors, the endpoints controller does
not create &lt;code&gt;Endpoints&lt;/code&gt; records. However, the DNS system looks for and configures
either:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CNAME records for &lt;a href=&#34;#externalname&#34;&gt;&lt;code&gt;ExternalName&lt;/code&gt;&lt;/a&gt;-type Services.&lt;/li&gt;
&lt;li&gt;A records for any &lt;code&gt;Endpoints&lt;/code&gt; that share a name with the Service, for all
other types.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于没有选择器的无头 Service， Endpoint 选择器不会创建 &lt;code&gt;Endpoints&lt;/code&gt; 记录，但是 DNS 系统会根据以
下情况来配置:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#externalname&#34;&gt;&lt;code&gt;ExternalName&lt;/code&gt;&lt;/a&gt; 类型的 Service 创建 CNAME&lt;/li&gt;
&lt;li&gt;所有其它类型，为其它任意与该 Service 同名的 &lt;code&gt;Endpoints&lt;/code&gt; 创建 A 记录&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Publishing Services (ServiceTypes) {#publishing-services-service-types}

For some parts of your application (for example, frontends) you may want to expose a
Service onto an external IP address, that&#39;s outside of your cluster.

Kubernetes `ServiceTypes` allow you to specify what kind of Service you want.
The default is `ClusterIP`.

`Type` values and their behaviors are:

   * `ClusterIP`: Exposes the Service on a cluster-internal IP. Choosing this value
     makes the Service only reachable from within the cluster. This is the
     default `ServiceType`.
   * [`NodePort`](#nodeport): Exposes the Service on each Node&#39;s IP at a static port
     (the `NodePort`). A `ClusterIP` Service, to which the `NodePort` Service
     routes, is automatically created.  You&#39;ll be able to contact the `NodePort` Service,
     from outside the cluster,
     by requesting `&lt;NodeIP&gt;:&lt;NodePort&gt;`.
   * [`LoadBalancer`](#loadbalancer): Exposes the Service externally using a cloud
     provider&#39;s load balancer. `NodePort` and `ClusterIP` Services, to which the external
     load balancer routes, are automatically created.
   * [`ExternalName`](#externalname): Maps the Service to the contents of the
     `externalName` field (e.g. `foo.bar.example.com`), by returning a `CNAME` record

     with its value. No proxying of any kind is set up.
     &lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You need either kube-dns version 1.7 or CoreDNS version 0.0.8 or higher to use the &lt;code&gt;ExternalName&lt;/code&gt; type.&lt;/div&gt;
&lt;/blockquote&gt;


You can also use [Ingress](/docs/concepts/services-networking/ingress/) to expose your Service. Ingress is not a Service type, but it acts as the entry point for your cluster. It lets you consolidate your routing rules into a single resource as it can expose multiple services under the same IP address.
--&gt;
&lt;h2 id=&#34;publishing-services-service-types&#34;&gt;发布 Service (ServiceTypes)&lt;/h2&gt;
&lt;p&gt;对于应用的一些部分(例如，前端)，需要将 Service 暴露到集群外部 IP 地址。&lt;/p&gt;
&lt;p&gt;k8s 可以通过 &lt;code&gt;ServiceTypes&lt;/code&gt; 来指定想要创建的 Service 类型， 默认为 &lt;code&gt;ClusterIP&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Type&lt;/code&gt; 的值各行为如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;ClusterIP&lt;/code&gt;: 以集群内部 IP 的形式暴露 Service， 使用这种方式 Service 只能在集群内部访问。
这是默认的 &lt;code&gt;ServiceType&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;#nodeport&#34;&gt;&lt;code&gt;NodePort&lt;/code&gt;&lt;/a&gt;: 将 Service 暴露到每个节点 IP和一个静态端口上(可以通过 &lt;code&gt;NodePort&lt;/code&gt;指定)
(&lt;code&gt;ClusterIP&lt;/code&gt; Service 到 &lt;code&gt;NodePort&lt;/code&gt; Service 的路由会自动创建)。
用户可以通过在集群外请求 &lt;code&gt;&amp;lt;NodeIP&amp;gt;:&amp;lt;NodePort&amp;gt;&lt;/code&gt; 方式访问 &lt;code&gt;NodePort&lt;/code&gt; Service。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;#loadbalancer&#34;&gt;&lt;code&gt;LoadBalancer&lt;/code&gt;&lt;/a&gt;: 使用云提供商的负载均衡器对外暴露 Service。
由 &lt;code&gt;NodePort&lt;/code&gt; 和 &lt;code&gt;ClusterIP&lt;/code&gt; Service 到外部负载均衡的路由会自动创建.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;#externalname&#34;&gt;&lt;code&gt;ExternalName&lt;/code&gt;&lt;/a&gt;:
通过返回 &lt;code&gt;CNAME&lt;/code&gt; 的方式 将 Service 映射到 &lt;code&gt;externalName&lt;/code&gt; 字段 (e.g. &lt;code&gt;foo.bar.example.com&lt;/code&gt;)的服务
没有设置任何类型的代理
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果使用 &lt;code&gt;ExternalName&lt;/code&gt; 类型，需要 kube-dns &lt;code&gt;v1.7+&lt;/code&gt; 或 CoreDNS &lt;code&gt;v0.0.8+&lt;/code&gt;&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用户也可以使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/ingress/&#34;&gt;Ingress&lt;/a&gt; 来暴露 Service。
Ingress 不是 Service 的一个类型， 但它扮演的是集群切入点的角色。 它让路由规则可以统一为一个资源。
并可以在同一个IP地址上暴露多个 Service&lt;/p&gt;
&lt;!--
### Type NodePort {#nodeport}

If you set the `type` field to `NodePort`, the Kubernetes control plane
allocates a port from a range specified by `--service-node-port-range` flag (default: 30000-32767).
Each node proxies that port (the same port number on every Node) into your Service.
Your Service reports the allocated port in its `.spec.ports[*].nodePort` field.


If you want to specify particular IP(s) to proxy the port, you can set the `--nodeport-addresses` flag in kube-proxy to particular IP block(s); this is supported since Kubernetes v1.10.
This flag takes a comma-delimited list of IP blocks (e.g. 10.0.0.0/8, 192.0.2.0/25) to specify IP address ranges that kube-proxy should consider as local to this node.

For example, if you start kube-proxy with the `--nodeport-addresses=127.0.0.0/8` flag, kube-proxy only selects the loopback interface for NodePort Services. The default for `--nodeport-addresses` is an empty list. This means that kube-proxy should consider all available network interfaces for NodePort. (That&#39;s also compatible with earlier Kubernetes releases).

If you want a specific port number, you can specify a value in the `nodePort`
field. The control plane will either allocate you that port or report that
the API transaction failed.
This means that you need to take care of possible port collisions yourself.
You also have to use a valid port number, one that&#39;s inside the range configured
for NodePort use.

Using a NodePort gives you the freedom to set up your own load balancing solution,
to configure environments that are not fully supported by Kubernetes, or even
to just expose one or more nodes&#39; IPs directly.

Note that this Service is visible as `&lt;NodeIP&gt;:spec.ports[*].nodePort`
and `.spec.clusterIP:spec.ports[*].port`. (If the `--nodeport-addresses` flag in kube-proxy is set, &lt;NodeIP&gt; would be filtered NodeIP(s).)

For example:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: NodePort
  selector:
    app: MyApp
  ports:
      # By default and for convenience, the `targetPort` is set to the same value as the `port` field.
    - port: 80
      targetPort: 80
      # Optional field
      # By default and for convenience, the Kubernetes control plane will allocate a port from a range (default: 30000-32767)
      nodePort: 30007
```
 --&gt;
&lt;h3 id=&#34;nodeport&#34;&gt;NodePort&lt;/h3&gt;
&lt;p&gt;如果将 &lt;code&gt;type&lt;/code&gt; 字段设置为 &lt;code&gt;NodePort&lt;/code&gt;， k8s 控制中心会从
&lt;code&gt;--service-node-port-range&lt;/code&gt; 选择配置的范围(默认 30000-32767)中分配一个端口。
集群中的每个节点都会将那个端口(每个节点使用相同的端口)代理到 Service 上。
Service 会将分配的的端口存放在它的 &lt;code&gt;.spec.ports[*].nodePort&lt;/code&gt; 字段。&lt;/p&gt;
&lt;p&gt;如果想要指定某些IP来代理这个端口，可以通过 kube-proxy 中的 &lt;code&gt;--nodeport-addresses&lt;/code&gt; 选择来
指定 IP 或 IP 段；这个特性自 k8s &lt;code&gt;v1.10&lt;/code&gt; 开始支持。
这个选择支持逗号分隔的 IP 段列表(e.g. 10.0.0.0/8, 192.0.2.0/25) 来指定 kube-proxy 是不是应该认为是应该代理的 IP 地址范围。&lt;/p&gt;
&lt;p&gt;例如，如果将 kube-proxy 设置 &lt;code&gt;--nodeport-addresses=127.0.0.0/8&lt;/code&gt;， 则 kube-proxy 只会
选择本地回环接口用作 Service 的 NodePort。 默认的  &lt;code&gt;--nodeport-addresses&lt;/code&gt; 是一个空列表。
其含义是 kube-proxy 会将所以可用的网络接口应用到 NodePort (这也与早期的 k8s 版本兼容)&lt;/p&gt;
&lt;p&gt;如果用户想要指定端口，可以通过设置 &lt;code&gt;nodePort&lt;/code&gt; 的值实现。 控制中心会分配那个端口或报告业务失败。
也就是说在设置该字段时需要用户自己解决端口冲突的问题。
并且首先设置的端口是一个有效的端口，其次端口是在之前提到所配置的 NodePort 的可用范围内。&lt;/p&gt;
&lt;p&gt;使用 NodePort 时就将负载均衡的解决方案选择交到用户手上, 可以用于配置那些 k8s 不完全支持的环境，
甚至直接暴露一个或多个节点的IP&lt;/p&gt;
&lt;p&gt;要注意 Service 可以通过 &lt;code&gt;&amp;lt;NodeIP&amp;gt;:spec.ports[*].nodePort&lt;/code&gt;:&lt;code&gt;.spec.clusterIP:spec.ports[*].port&lt;/code&gt; 访问。
(如果 kube-proxy 设置了 &lt;code&gt;--nodeport-addresses&lt;/code&gt;， 则 &lt;NodeIP&gt; 只能是配置的范围内的IP)&lt;/p&gt;
&lt;p&gt;例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NodePort&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
      &lt;span style=&#34;color:#75715e&#34;&gt;# 默认情况下和为了方便， `targetPort` 会与 `port` 字段使用相同的值&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# 可选字段&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# 默认情况下和为了方便，k8s 控制中心会在配置的端口范围(默认30000-32767) 内分配一个端口&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;nodePort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;30007&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Type LoadBalancer {#loadbalancer}

On cloud providers which support external load balancers, setting the `type`
field to `LoadBalancer` provisions a load balancer for your Service.
The actual creation of the load balancer happens asynchronously, and
information about the provisioned balancer is published in the Service&#39;s
`.status.loadBalancer` field.
For example:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  clusterIP: 10.0.171.239
  type: LoadBalancer
status:
  loadBalancer:
    ingress:
    - ip: 192.0.2.127
```

Traffic from the external load balancer is directed at the backend Pods. The cloud provider decides how it is load balanced.

For LoadBalancer type of Services, when there is more than one port defined, all
ports must have the same protocol and the protocol must be one of `TCP`, `UDP`,
and `SCTP`.

Some cloud providers allow you to specify the `loadBalancerIP`. In those cases, the load-balancer is created
with the user-specified `loadBalancerIP`. If the `loadBalancerIP` field is not specified,
the loadBalancer is set up with an ephemeral IP address. If you specify a `loadBalancerIP`
but your cloud provider does not support the feature, the `loadbalancerIP` field that you
set is ignored.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If you&amp;rsquo;re using SCTP, see the &lt;a href=&#34;#caveat-sctp-loadbalancer-service-type&#34;&gt;caveat&lt;/a&gt; below about the
&lt;code&gt;LoadBalancer&lt;/code&gt; Service type.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;On &lt;strong&gt;Azure&lt;/strong&gt;, if you want to use a user-specified public type &lt;code&gt;loadBalancerIP&lt;/code&gt;, you first need
to create a static type public IP address resource. This public IP address resource should
be in the same resource group of the other automatically created resources of the cluster.
For example, &lt;code&gt;MC_myResourceGroup_myAKSCluster_eastus&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Specify the assigned IP address as loadBalancerIP. Ensure that you have updated the securityGroupName in the cloud provider configuration file. For information about troubleshooting &lt;code&gt;CreatingLoadBalancerFailed&lt;/code&gt; permission issues see, &lt;a href=&#34;https://docs.microsoft.com/en-us/azure/aks/static-ip&#34;&gt;Use a static IP address with the Azure Kubernetes Service (AKS) load balancer&lt;/a&gt; or &lt;a href=&#34;https://github.com/Azure/AKS/issues/357&#34;&gt;CreatingLoadBalancerFailed on AKS cluster with advanced networking&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;loadbalancer&#34;&gt;LoadBalancer&lt;/h3&gt;
&lt;p&gt;云提供商还支持外部的负载均衡器， 通过设置 &lt;code&gt;type&lt;/code&gt; 的值为 &lt;code&gt;LoadBalancer&lt;/code&gt; 为 Service 提供一个负载均衡器
实际上对负载均衡器的创建是异步的，关于添加的负载均衡器的信息会添加到 Service 的 &lt;code&gt;.status.loadBalancer&lt;/code&gt; 字段。&lt;/p&gt;
&lt;p&gt;例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;clusterIP&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10.0.171.239&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;LoadBalancer&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;status&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;loadBalancer&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;ingress&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;ip&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;192.0.2.127&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从外部负载均衡器进来的流量会直接转发到后端的 Pod上。 云提供商会决定负载均衡方式。&lt;/p&gt;
&lt;p&gt;对于 LoadBalancer 类型的 Service， 当一个 Service 上有不止一个端口时， 所以的端口必须要使用
相同的协议，并且协议只能是 &lt;code&gt;TCP&lt;/code&gt;, &lt;code&gt;UDP&lt;/code&gt;, &lt;code&gt;SCTP&lt;/code&gt; 中的一种.&lt;/p&gt;
&lt;p&gt;有些云提供商支持指定 &lt;code&gt;loadBalancerIP&lt;/code&gt;， 在这个情况下， 负载均衡是使用指定的 &lt;code&gt;loadBalancerIP&lt;/code&gt; 创建。
如果 &lt;code&gt;loadBalancerIP&lt;/code&gt; 没有指定则会使用一个临时 IP 地址。 如果指定了 &lt;code&gt;loadBalancerIP&lt;/code&gt;
但云提供商不支持该特性，则指定的 &lt;code&gt;loadbalancerIP&lt;/code&gt; 字段会被忽略。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果使用的是 SCTP 协议，见下面的 &lt;a href=&#34;#caveat-sctp-loadbalancer-service-type&#34;&gt;caveat&lt;/a&gt; 关于 &lt;code&gt;LoadBalancer&lt;/code&gt;
Service 类型的相关信息&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;在 &lt;strong&gt;Azure&lt;/strong&gt; 上， 如果想要使用用户指定的 &lt;code&gt;loadBalancerIP&lt;/code&gt;， 首先需要创建一个静态类型的
公网 IP 地址资源。 这个公网 IP 地址资源应该与集群其它自动创建的资源在同一个资源组。
例如，&lt;code&gt;MC_myResourceGroup_myAKSCluster_eastus&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;通过 loadBalancerIP 指定分配 IP， 需要确保已经更新云提供商配置文件中的 securityGroupName。
更多 &lt;code&gt;CreatingLoadBalancerFailed&lt;/code&gt; 权限问题的调度信息见
&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/aks/static-ip&#34;&gt;Use a static IP address with the Azure Kubernetes Service (AKS) load balancer&lt;/a&gt;
或
&lt;a href=&#34;https://github.com/Azure/AKS/issues/357&#34;&gt;CreatingLoadBalancerFailed on AKS cluster with advanced networking&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
#### Internal load balancer
In a mixed environment it is sometimes necessary to route traffic from Services inside the same
(virtual) network address block.

In a split-horizon DNS environment you would need two Services to be able to route both external and internal traffic to your endpoints.

You can achieve this by adding one the following annotations to a Service.
The annotation to add depends on the cloud Service provider you&#39;re using.

&lt;ul class=&#34;nav nav-tabs&#34; id=&#34;service_tabs&#34; role=&#34;tablist&#34;&gt;&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link active&#34; href=&#34;#service_tabs-0&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-0&#34; aria-selected=&#34;true&#34;&gt;Default&lt;/a&gt;&lt;/li&gt;
	  
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#service_tabs-1&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-1&#34;&gt;GCP&lt;/a&gt;&lt;/li&gt;
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#service_tabs-2&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-2&#34;&gt;AWS&lt;/a&gt;&lt;/li&gt;
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#service_tabs-3&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-3&#34;&gt;Azure&lt;/a&gt;&lt;/li&gt;
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#service_tabs-4&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-4&#34;&gt;IBM Cloud&lt;/a&gt;&lt;/li&gt;
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#service_tabs-5&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-5&#34;&gt;OpenStack&lt;/a&gt;&lt;/li&gt;
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#service_tabs-6&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-6&#34;&gt;Baidu Cloud&lt;/a&gt;&lt;/li&gt;
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#service_tabs-7&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-7&#34;&gt;Tencent Cloud&lt;/a&gt;&lt;/li&gt;
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#service_tabs-8&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-8&#34;&gt;Alibaba Cloud&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;div class=&#34;tab-content&#34; id=&#34;service_tabs&#34;&gt;&lt;div id=&#34;service_tabs-0&#34; class=&#34;tab-pane show active&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-0&#34;&gt;

&lt;p&gt;&lt;p&gt;Select one of the tabs.&lt;/p&gt;
&lt;/div&gt;
  &lt;div id=&#34;service_tabs-1&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-1&#34;&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;cloud.google.com/load-balancer-type&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Internal&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;div id=&#34;service_tabs-2&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-2&#34;&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-internal&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;div id=&#34;service_tabs-3&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-3&#34;&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/azure-load-balancer-internal&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;div id=&#34;service_tabs-4&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-4&#34;&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;private&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;div id=&#34;service_tabs-5&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-5&#34;&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/openstack-internal-load-balancer&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;div id=&#34;service_tabs-6&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-6&#34;&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/cce-load-balancer-internal-vpc&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;div id=&#34;service_tabs-7&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-7&#34;&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/qcloud-loadbalancer-internal-subnetid&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;subnet-xxxxx&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;div id=&#34;service_tabs-8&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-8&#34;&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;intranet&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;

 --&gt;
&lt;h4 id=&#34;内部负载均衡器&#34;&gt;内部负载均衡器&lt;/h4&gt;
&lt;p&gt;在一个混搭的环境中，有时候需要在同一个(虚拟)网络地址段从 Service 间路由流量。&lt;/p&gt;
&lt;p&gt;在一个水平分割的 DNS 环境，需要有两个 Service 才能够分别路由外部和内部的流量到 Endpoint.&lt;/p&gt;
&lt;p&gt;可以通过在 Service 上添加以下注解中的一个来达成这个目的。 添加哪个注解基于你用的云提供商。&lt;/p&gt;
&lt;ul class=&#34;nav nav-tabs&#34; id=&#34;service_tabs&#34; role=&#34;tablist&#34;&gt;&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link active&#34; href=&#34;#service_tabs-0&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-0&#34; aria-selected=&#34;true&#34;&gt;Default&lt;/a&gt;&lt;/li&gt;
	  
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#service_tabs-1&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-1&#34;&gt;GCP&lt;/a&gt;&lt;/li&gt;
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#service_tabs-2&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-2&#34;&gt;AWS&lt;/a&gt;&lt;/li&gt;
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#service_tabs-3&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-3&#34;&gt;Azure&lt;/a&gt;&lt;/li&gt;
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#service_tabs-4&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-4&#34;&gt;IBM Cloud&lt;/a&gt;&lt;/li&gt;
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#service_tabs-5&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-5&#34;&gt;OpenStack&lt;/a&gt;&lt;/li&gt;
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#service_tabs-6&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-6&#34;&gt;Baidu Cloud&lt;/a&gt;&lt;/li&gt;
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#service_tabs-7&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-7&#34;&gt;Tencent Cloud&lt;/a&gt;&lt;/li&gt;
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#service_tabs-8&#34; role=&#34;tab&#34; aria-controls=&#34;service_tabs-8&#34;&gt;Alibaba Cloud&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;div class=&#34;tab-content&#34; id=&#34;service_tabs&#34;&gt;&lt;div id=&#34;service_tabs-0&#34; class=&#34;tab-pane show active&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-0&#34;&gt;

&lt;p&gt;&lt;p&gt;选择其中一个标签&lt;/p&gt;
&lt;/div&gt;
  &lt;div id=&#34;service_tabs-1&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-1&#34;&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;cloud.google.com/load-balancer-type&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Internal&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;div id=&#34;service_tabs-2&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-2&#34;&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-internal&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;div id=&#34;service_tabs-3&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-3&#34;&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/azure-load-balancer-internal&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;div id=&#34;service_tabs-4&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-4&#34;&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;private&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;div id=&#34;service_tabs-5&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-5&#34;&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/openstack-internal-load-balancer&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;div id=&#34;service_tabs-6&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-6&#34;&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/cce-load-balancer-internal-vpc&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;div id=&#34;service_tabs-7&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-7&#34;&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/qcloud-loadbalancer-internal-subnetid&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;subnet-xxxxx&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;div id=&#34;service_tabs-8&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;service_tabs-8&#34;&gt;

&lt;p&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;intranet&amp;#34;&lt;/span&gt;
[&lt;span style=&#34;color:#ae81ff&#34;&gt;...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!--
#### TLS support on AWS {#ssl-support-on-aws}

For partial TLS / SSL support on clusters running on AWS, you can add three
annotations to a `LoadBalancer` service:

```yaml
metadata:
  name: my-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012
```

The first specifies the ARN of the certificate to use. It can be either a
certificate from a third party issuer that was uploaded to IAM or one created
within AWS Certificate Manager.

```yaml
metadata:
  name: my-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: (https|http|ssl|tcp)
```

The second annotation specifies which protocol a Pod speaks. For HTTPS and
SSL, the ELB expects the Pod to authenticate itself over the encrypted
connection, using a certificate.

HTTP and HTTPS selects layer 7 proxying: the ELB terminates
the connection with the user, parses headers, and injects the `X-Forwarded-For`
header with the user&#39;s IP address (Pods only see the IP address of the
ELB at the other end of its connection) when forwarding requests.

TCP and SSL selects layer 4 proxying: the ELB forwards traffic without
modifying the headers.

In a mixed-use environment where some ports are secured and others are left unencrypted,
you can use the following annotations:

```yaml
    metadata:
      name: my-service
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
        service.beta.kubernetes.io/aws-load-balancer-ssl-ports: &#34;443,8443&#34;
```

In the above example, if the Service contained three ports, `80`, `443`, and
`8443`, then `443` and `8443` would use the SSL certificate, but `80` would just
be proxied HTTP.

From Kubernetes v1.9 onwards you can use [predefined AWS SSL policies](https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-policy-table.html) with HTTPS or SSL listeners for your Services.
To see which policies are available for use, you can use the `aws` command line tool:

```bash
aws elb describe-load-balancer-policies --query &#39;PolicyDescriptions[].PolicyName&#39;
```

You can then specify any one of those policies using the
&#34;`service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy`&#34;
annotation; for example:

```yaml
    metadata:
      name: my-service
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: &#34;ELBSecurityPolicy-TLS-1-2-2017-01&#34;
```
 --&gt;
&lt;h4 id=&#34;ssl-support-on-aws&#34;&gt;AWS 对 TLS 的支持&lt;/h4&gt;
&lt;p&gt;运行在 AWS 上的集群部分支持 TLS / SSL，可以添加以下三个注解到一个 &lt;code&gt;LoadBalancer&lt;/code&gt; 的 Service:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-ssl-cert&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;第一个指定 证书使用的 ARN。 可以是上传到  IAM 的第三方发行者的证书 或者是在 AWS 证书管理器创建的证书。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-backend-protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;(https|http|ssl|tcp)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;connection, using a certificate.
第二个注解指定 Pod 使用的是哪个协议。 对于 HTTPS 和 SSL，负载均衡期望的是 Pod 使用证书的安全连接来认证它自己。&lt;/p&gt;
&lt;p&gt;HTTP 和 HTTPS 使用第 7 层代理: 转发请求是由负载均衡器来终止用户的连接，解析头，插入包含用户 IP 地址
的 &lt;code&gt;X-Forwarded-For&lt;/code&gt; 头(Pod 只能看到连接另一头的负载均衡的IP地址)&lt;/p&gt;
&lt;p&gt;TCP 和 SSL 使用 4 层代理: 负载均衡器转发流量是不会修改头信息&lt;/p&gt;
&lt;p&gt;在混搭环境中，有些端口是安全的有是又是未加密的，可以使用如下注解:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-backend-protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-ssl-ports&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;443,8443&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在上面的例子中， 如果 Service 包含三个端口， &lt;code&gt;80&lt;/code&gt;, &lt;code&gt;443&lt;/code&gt;, &lt;code&gt;8443&lt;/code&gt;, 其中 &lt;code&gt;443&lt;/code&gt; 和 &lt;code&gt;8443&lt;/code&gt;
是使用 SSL 证书的，但 &lt;code&gt;80&lt;/code&gt; 只通过 HTTP 代理&lt;/p&gt;
&lt;p&gt;从 k8s v1.9 开始，可以使用
&lt;a href=&#34;https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-policy-table.html&#34;&gt;predefined AWS SSL policies&lt;/a&gt;
来配置 Service 的 HTTPS 或 SSL 监控器。
可以通过以下 aws 命令行工具来查看哪些可用的策略:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;aws elb describe-load-balancer-policies --query &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;PolicyDescriptions[].PolicyName&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;然后可以使
&amp;ldquo;&lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy&lt;/code&gt;&amp;rdquo;
注解来使用其中的某个策略，例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ELBSecurityPolicy-TLS-1-2-2017-01&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
#### PROXY protocol support on AWS

To enable [PROXY protocol](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt)
support for clusters running on AWS, you can use the following service
annotation:

```yaml
    metadata:
      name: my-service
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: &#34;*&#34;
```

Since version 1.3.0, the use of this annotation applies to all ports proxied by the ELB
and cannot be configured otherwise. --&gt;
&lt;h4 id=&#34;aws-支持的-proxy-协议&#34;&gt;AWS 支持的 PROXY 协议&lt;/h4&gt;
&lt;p&gt;要在 AWS 运行的集群中启用 &lt;a href=&#34;https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt&#34;&gt;PROXY protocol&lt;/a&gt;
可以在 Service 上添加以下注解:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-proxy-protocol&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从 v1.3.0 开始，就只能通过这个注解来让所以的端口通过 ELB 代理，不能通过其它方式配置了。&lt;/p&gt;
&lt;!--
#### ELB Access Logs on AWS

There are several annotations to manage access logs for ELB Services on AWS.

The annotation `service.beta.kubernetes.io/aws-load-balancer-access-log-enabled`
controls whether access logs are enabled.

The annotation `service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval`
controls the interval in minutes for publishing the access logs. You can specify
an interval of either 5 or 60 minutes.

The annotation `service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name`
controls the name of the Amazon S3 bucket where load balancer access logs are
stored.

The annotation `service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix`
specifies the logical hierarchy you created for your Amazon S3 bucket.

```yaml
    metadata:
      name: my-service
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-access-log-enabled: &#34;true&#34;
        # Specifies whether access logs are enabled for the load balancer
        service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval: &#34;60&#34;
        # The interval for publishing the access logs. You can specify an interval of either 5 or 60 (minutes).
        service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name: &#34;my-bucket&#34;
        # The name of the Amazon S3 bucket where the access logs are stored
        service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix: &#34;my-bucket-prefix/prod&#34;
        # The logical hierarchy you created for your Amazon S3 bucket, for example `my-bucket-prefix/prod`
```
 --&gt;
&lt;h4 id=&#34;aws-上-elb-的访问日志&#34;&gt;AWS 上 ELB 的访问日志&lt;/h4&gt;
&lt;p&gt;AWS 上 ELB 的访问日志 可以通过几个注解来管理。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-access-log-enabled&lt;/code&gt; 注解控制是否开启访问日志&lt;/p&gt;
&lt;p&gt;&lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval&lt;/code&gt; 注解控制发布
日志的时间间隔(单位为分钟)。 时间间隔可以是 5 分钟 或 60 分钟&lt;/p&gt;
&lt;p&gt;&lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name&lt;/code&gt; 注解控制访问
日志存储的 Amazon S3 bucket 名称
&lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix&lt;/code&gt; 注解
它指定创建的 Amazon S3 bucket 的逻辑层次&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-access-log-enabled&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 否开启访问日志&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;60&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 发布 日志的时间间隔(单位为分钟)。 时间间隔可以是 5 分钟 或 60 分钟&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;my-bucket&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 日志存储的 Amazon S3 bucket 名称&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;my-bucket-prefix/prod&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 创建的 Amazon S3 bucket 的逻辑层次, 例如 `my-bucket-prefix/prod`&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
#### Connection Draining on AWS

Connection draining for Classic ELBs can be managed with the annotation
`service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled` set
to the value of `&#34;true&#34;`. The annotation
`service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout` can
also be used to set maximum time, in seconds, to keep the existing connections open before deregistering the instances.


```yaml
    metadata:
      name: my-service
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled: &#34;true&#34;
        service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout: &#34;60&#34;
``` --&gt;
&lt;h4 id=&#34;aws-连接控制&#34;&gt;AWS 连接控制&lt;/h4&gt;
&lt;p&gt;经典ELB 的连接使用可以通过将注解
&lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled&lt;/code&gt;
的值设置为 &lt;code&gt;&amp;quot;true&amp;quot;&lt;/code&gt; 来管理。
还可以通过注解
&lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout&lt;/code&gt;
也可以用来设置保证已有连接的最大时间(单位秒)，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;60&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
#### Other ELB annotations

There are other annotations to manage Classic Elastic Load Balancers that are described below.

```yaml
    metadata:
      name: my-service
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: &#34;60&#34;
        # The time, in seconds, that the connection is allowed to be idle (no data has been sent over the connection) before it is closed by the load balancer

        service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: &#34;true&#34;
        # Specifies whether cross-zone load balancing is enabled for the load balancer

        service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: &#34;environment=prod,owner=devops&#34;
        # A comma-separated list of key-value pairs which will be recorded as
        # additional tags in the ELB.

        service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: &#34;&#34;
        # The number of successive successful health checks required for a backend to
        # be considered healthy for traffic. Defaults to 2, must be between 2 and 10

        service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: &#34;3&#34;
        # The number of unsuccessful health checks required for a backend to be
        # considered unhealthy for traffic. Defaults to 6, must be between 2 and 10

        service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: &#34;20&#34;
        # The approximate interval, in seconds, between health checks of an
        # individual instance. Defaults to 10, must be between 5 and 300

        service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: &#34;5&#34;
        # The amount of time, in seconds, during which no response means a failed
        # health check. This value must be less than the service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval
        # value. Defaults to 5, must be between 2 and 60

        service.beta.kubernetes.io/aws-load-balancer-extra-security-groups: &#34;sg-53fae93f,sg-42efd82e&#34;
        # A list of additional security groups to be added to the ELB

        service.beta.kubernetes.io/aws-load-balancer-target-node-labels: &#34;ingress-gw,gw-name=public-api&#34;
        # A comma separated list of key-value pairs which are used
        # to select the target nodes for the load balancer
```
 --&gt;
&lt;h4 id=&#34;elb-其它注解&#34;&gt;ELB 其它注解&lt;/h4&gt;
&lt;p&gt;以下介绍管理经典弹性负载均衡器的其它注解.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;60&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 连接在被负载均衡关闭前允许空闲(没有从连接发送数据)的时间，单位秒&lt;/span&gt;

        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 指定负载均衡器是否开启跨区负载均衡&lt;/span&gt;

        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;environment=prod,owner=devops&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 一个以逗号分隔的键值对列表，用于记在 ELB 中的额外标签&lt;/span&gt;

        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 一个后端服务实例被认为是健康可以处理流量所需要的成功健康检查次数，默认为 2， 只能在 2 到 10 之间&lt;/span&gt;

        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 一个后端服务实例被认为是不健康不能处理流量需要的失败的健康检查次数， 默认为 6， 必须在 2 到 10 之间&lt;/span&gt;

        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;20&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 针对每个独立实例进行健康检查的时间间隔，单位秒，默认为 10， 必须在 5 到 300 之间&lt;/span&gt;

        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;5&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 如果在这个时间(单位秒)内健康检查没有响应，则认为检测结果为失败。这个值必须小于&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval 的值&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 默认为 5， 必须在 2 到 10 之间。&lt;/span&gt;

        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-extra-security-groups&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sg-53fae93f,sg-42efd82e&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 被添加到 ELB 的额外的安全组列表&lt;/span&gt;

        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-target-node-labels&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ingress-gw,gw-name=public-api&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 一个以逗号分隔的键值对列表，用于选择负载均衡的节点&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
#### Network Load Balancer support on AWS {#aws-nlb-support}






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.15 [beta]&lt;/code&gt;
&lt;/div&gt;



To use a Network Load Balancer on AWS, use the annotation `service.beta.kubernetes.io/aws-load-balancer-type` with the value set to `nlb`.

```yaml
    metadata:
      name: my-service
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-type: &#34;nlb&#34;
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; NLB only works with certain instance classes; see the &lt;a href=&#34;https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-register-targets.html#register-deregister-targets&#34;&gt;AWS documentation&lt;/a&gt;
on Elastic Load Balancing for a list of supported instance types.&lt;/div&gt;
&lt;/blockquote&gt;


Unlike Classic Elastic Load Balancers, Network Load Balancers (NLBs) forward the
client&#39;s IP address through to the node. If a Service&#39;s `.spec.externalTrafficPolicy`
is set to `Cluster`, the client&#39;s IP address is not propagated to the end
Pods.

By setting `.spec.externalTrafficPolicy` to `Local`, the client IP addresses is
propagated to the end Pods, but this could result in uneven distribution of
traffic. Nodes without any Pods for a particular LoadBalancer Service will fail
the NLB Target Group&#39;s health check on the auto-assigned
`.spec.healthCheckNodePort` and not receive any traffic.

In order to achieve even traffic, either use a DaemonSet or specify a
[pod anti-affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)
to not locate on the same node.

You can also use NLB Services with the [internal load balancer](/docs/concepts/services-networking/service/#internal-load-balancer)
annotation.

In order for client traffic to reach instances behind an NLB, the Node security
groups are modified with the following IP rules:

| Rule | Protocol | Port(s) | IpRange(s) | IpRange Description |
|------|----------|---------|------------|---------------------|
| Health Check | TCP | NodePort(s) (`.spec.healthCheckNodePort` for `.spec.externalTrafficPolicy = Local`) | VPC CIDR | kubernetes.io/rule/nlb/health=\&lt;loadBalancerName\&gt; |
| Client Traffic | TCP | NodePort(s) | `.spec.loadBalancerSourceRanges` (defaults to `0.0.0.0/0`) | kubernetes.io/rule/nlb/client=\&lt;loadBalancerName\&gt; |
| MTU Discovery | ICMP | 3,4 | `.spec.loadBalancerSourceRanges` (defaults to `0.0.0.0/0`) | kubernetes.io/rule/nlb/mtu=\&lt;loadBalancerName\&gt; |

In order to limit which client IP&#39;s can access the Network Load Balancer,
specify `loadBalancerSourceRanges`.

```yaml
spec:
  loadBalancerSourceRanges:
    - &#34;143.231.0.0/16&#34;
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If &lt;code&gt;.spec.loadBalancerSourceRanges&lt;/code&gt; is not set, Kubernetes
allows traffic from &lt;code&gt;0.0.0.0/0&lt;/code&gt; to the Node Security Group(s). If nodes have
public IP addresses, be aware that non-NLB traffic can also reach all instances
in those modified security groups.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h4 id=&#34;aws-nlb-support&#34;&gt;AWS 上支持的网络负载均衡(NLB)&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.15 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;要使用 AWS 上的网络负载均衡器，使用注解 &lt;code&gt;service.beta.kubernetes.io/aws-load-balancer-type&lt;/code&gt;
设置值为 &lt;code&gt;nlb&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;service.beta.kubernetes.io/aws-load-balancer-type&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nlb&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; NLB 只适用的特定类型的实例，ELB 支持的实例类型见
&lt;a href=&#34;https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-register-targets.html#register-deregister-targets&#34;&gt;AWS 文档&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;与经典 ELB 不同， NLB 转发的客户端 IP 地址穿透节点。 如果 Service 的
&lt;code&gt;.spec.externalTrafficPolicy&lt;/code&gt; 设置为 &lt;code&gt;Cluster&lt;/code&gt;， 客户端 IP 地址不会传递到最终的 Pod。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;.spec.healthCheckNodePort&lt;/code&gt; and not receive any traffic.
通过设置 &lt;code&gt;.spec.externalTrafficPolicy&lt;/code&gt; 为 &lt;code&gt;Local&lt;/code&gt;， 客户端 IP 地址会传递到最终的 Pod。
但这会导向流量分发不均衡。 没有包含该负载均衡器对应 Service 的 Pod 的节点，会在 NLB 目标组
健康检查时分配的 &lt;code&gt;.spec.healthCheckNodePort&lt;/code&gt; 检查失败。 并不会收到任何流量&lt;/p&gt;
&lt;p&gt;为了达成平衡的负载，要么使用 DaemonSet，要么设置
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity&#34;&gt;pod anti-affinity&lt;/a&gt;
让 Pod 不要调度到同一个节点上&lt;/p&gt;
&lt;p&gt;也可以在使用 NLB Service 时在其中包含
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service/#internal-load-balancer&#34;&gt;内部负载均衡器&lt;/a&gt;
注解&lt;/p&gt;
&lt;p&gt;为了能让客户端流量到达 NLB 后面的实例。 节点安全组需要以以下 IP 规则进行修改:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Rule&lt;/th&gt;
&lt;th&gt;Protocol&lt;/th&gt;
&lt;th&gt;Port(s)&lt;/th&gt;
&lt;th&gt;IpRange(s)&lt;/th&gt;
&lt;th&gt;IpRange Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Health Check&lt;/td&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;NodePort(s) (&lt;code&gt;.spec.healthCheckNodePort&lt;/code&gt; for &lt;code&gt;.spec.externalTrafficPolicy = Local&lt;/code&gt;)&lt;/td&gt;
&lt;td&gt;VPC CIDR&lt;/td&gt;
&lt;td&gt;kubernetes.io/rule/nlb/health=&amp;lt;loadBalancerName&amp;gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Client Traffic&lt;/td&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;NodePort(s)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;.spec.loadBalancerSourceRanges&lt;/code&gt; (defaults to &lt;code&gt;0.0.0.0/0&lt;/code&gt;)&lt;/td&gt;
&lt;td&gt;kubernetes.io/rule/nlb/client=&amp;lt;loadBalancerName&amp;gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MTU Discovery&lt;/td&gt;
&lt;td&gt;ICMP&lt;/td&gt;
&lt;td&gt;3,4&lt;/td&gt;
&lt;td&gt;&lt;code&gt;.spec.loadBalancerSourceRanges&lt;/code&gt; (defaults to &lt;code&gt;0.0.0.0/0&lt;/code&gt;)&lt;/td&gt;
&lt;td&gt;kubernetes.io/rule/nlb/mtu=&amp;lt;loadBalancerName&amp;gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;为限所哪个客户端 IP 可以访问 NLB， 指定 &lt;code&gt;loadBalancerSourceRanges&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;loadBalancerSourceRanges&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;143.231.0.0/16&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果 &lt;code&gt;.spec.loadBalancerSourceRanges&lt;/code&gt; 没有设置，k8s 允许来自 &lt;code&gt;0.0.0.0/0&lt;/code&gt; 流量到节点安全组。
如果节点有公网 IP 地址，要注意那些非 NLB 流量也会到达所有这些修改过的安全组。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;对 AWS 使用不熟悉，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
#### Other CLB annotations on Tencent Kubernetes Engine (TKE)

There are other annotations for managing Cloud Load Balancers on TKE as shown below.

```yaml
    metadata:
      name: my-service
      annotations:
        # Bind Loadbalancers with specified nodes
        service.kubernetes.io/qcloud-loadbalancer-backends-label: key in (value1, value2)

        # ID of an existing load balancer
        service.kubernetes.io/tke-existed-lbid：lb-6swtxxxx

        # Custom parameters for the load balancer (LB), does not support modification of LB type yet
        service.kubernetes.io/service.extensiveParameters: &#34;&#34;

        # Custom parameters for the LB listener
        service.kubernetes.io/service.listenerParameters: &#34;&#34;

        # Specifies the type of Load balancer;
        # valid values: classic (Classic Cloud Load Balancer) or application (Application Cloud Load Balancer)
        service.kubernetes.io/loadbalance-type: xxxxx

        # Specifies the public network bandwidth billing method;
        # valid values: TRAFFIC_POSTPAID_BY_HOUR(bill-by-traffic) and BANDWIDTH_POSTPAID_BY_HOUR (bill-by-bandwidth).
        service.kubernetes.io/qcloud-loadbalancer-internet-charge-type: xxxxxx

        # Specifies the bandwidth value (value range: [1,2000] Mbps).
        service.kubernetes.io/qcloud-loadbalancer-internet-max-bandwidth-out: &#34;10&#34;

        # When this annotation is set，the loadbalancers will only register nodes
        # with pod running on it, otherwise all nodes will be registered.
        service.kubernetes.io/local-svc-only-bind-node-with-pod: true
```
 --&gt;
&lt;h4 id=&#34;tencent-kubernetes-engine-tke-上其它-clb-注解&#34;&gt;Tencent Kubernetes Engine (TKE) 上其它 CLB 注解&lt;/h4&gt;
&lt;p&gt;以下为管理 TKE 上 CLB 的其它注解说明。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
        &lt;span style=&#34;color:#75715e&#34;&gt;# 将 负载均衡器与指定节点绑定&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/qcloud-loadbalancer-backends-label&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;key in (value1, value2)&lt;/span&gt;

        &lt;span style=&#34;color:#75715e&#34;&gt;# 已经存在的负载均衡器的 ID&lt;/span&gt;
        &lt;span style=&#34;color:#ae81ff&#34;&gt;service.kubernetes.io/tke-existed-lbid：lb-6swtxxxx&lt;/span&gt;

        &lt;span style=&#34;color:#75715e&#34;&gt;# Custom parameters for the load balancer (LB), does not support modification of LB type yet&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 负载均衡器的自定义参数， 还不支持对负载均衡类型的修改&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/service.extensiveParameters&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;

        &lt;span style=&#34;color:#75715e&#34;&gt;# Custom parameters for the LB listener&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 负载均衡监听器的自定义参数&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/service.listenerParameters&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;

        &lt;span style=&#34;color:#75715e&#34;&gt;# 设置负载均衡器的类型&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 有效值为: classic (Classic Cloud Load Balancer) 或  application (Application Cloud Load Balancer)&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/loadbalance-type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;xxxxx&lt;/span&gt;

        &lt;span style=&#34;color:#75715e&#34;&gt;# 设置公网带宽收费方式&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# 有效值为: TRAFFIC_POSTPAID_BY_HOUR(bill-by-traffic) 或 BANDWIDTH_POSTPAID_BY_HOUR (bill-by-bandwidth).&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/qcloud-loadbalancer-internet-charge-type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;xxxxxx&lt;/span&gt;

        &lt;span style=&#34;color:#75715e&#34;&gt;# 设置带宽的值(范围: [1,2000] Mbps)&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/qcloud-loadbalancer-internet-max-bandwidth-out&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10&amp;#34;&lt;/span&gt;

        &lt;span style=&#34;color:#75715e&#34;&gt;# 当这个注解被设置，负载均衡器只会注册有 Pod 在上面运行的节点，否则所有节点都会注册&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;service.kubernetes.io/local-svc-only-bind-node-with-pod&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Type ExternalName {#externalname}

Services of type ExternalName map a Service to a DNS name, not to a typical selector such as
`my-service` or `cassandra`. You specify these Services with the `spec.externalName` parameter.

This Service definition, for example, maps
the `my-service` Service in the `prod` namespace to `my.database.example.com`:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.database.example.com
```
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; ExternalName accepts an IPv4 address string, but as a DNS names comprised of digits, not as an IP address. ExternalNames that resemble IPv4 addresses are not resolved by CoreDNS or ingress-nginx because ExternalName
is intended to specify a canonical DNS name. To hardcode an IP address, consider using
&lt;a href=&#34;#headless-services&#34;&gt;headless Services&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;


When looking up the host `my-service.prod.svc.cluster.local`, the cluster DNS Service
returns a `CNAME` record with the value `my.database.example.com`. Accessing
`my-service` works in the same way as other Services but with the crucial
difference that redirection happens at the DNS level rather than via proxying or
forwarding. Should you later decide to move your database into your cluster, you
can start its Pods, add appropriate selectors or endpoints, and change the
Service&#39;s `type`.

&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; &lt;p&gt;You may have trouble using ExternalName for some common protocols, including HTTP and HTTPS. If you use ExternalName then the hostname used by clients inside your cluster is different from the name that the ExternalName references.&lt;/p&gt;
&lt;p&gt;For protocols that use hostnames this difference may lead to errors or unexpected responses. HTTP requests will have a &lt;code&gt;Host:&lt;/code&gt; header that the origin server does not recognize; TLS servers will not be able to provide a certificate matching the hostname that the client connected to.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; This section is indebted to the &lt;a href=&#34;https://akomljen.com/kubernetes-tips-part-1/&#34;&gt;Kubernetes Tips - Part
1&lt;/a&gt; blog post from &lt;a href=&#34;https://akomljen.com/&#34;&gt;Alen Komljen&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;externalname&#34;&gt;ExternalName&lt;/h3&gt;
&lt;p&gt;ExternalName 类型的 Service 将 Service 映射到一个 DNS 名称， 而不是一个常见的选择器，如
&lt;code&gt;my-service&lt;/code&gt; 或 &lt;code&gt;cassandra&lt;/code&gt;. 通过 Service &lt;code&gt;spec.externalName&lt;/code&gt; 字段设置 DNS 名称。&lt;/p&gt;
&lt;p&gt;以面示例配置中将在 &lt;code&gt;prod&lt;/code&gt; 命名空间中一个名叫 &lt;code&gt;my-service&lt;/code&gt; 的 Service 映射到 &lt;code&gt;my.database.example.com&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ExternalName&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;externalName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my.database.example.com&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; ExternalName 可以使用一个 IPv4 地址的字符串， 但是会被当作一个由数字组成的 DNS 名称，而不是
一个 IP 地址。 ExternalName 值与 IPv4 地址相似的不会被 CoreDNS 或 ingress-nginx 解析，
因为 ExternalName 在设计上就是用作一个标准的 DNS 名称的。 如果要硬编码一个 IP 地址，
考虑使用 &lt;a href=&#34;#headless-services&#34;&gt;无头 Services&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;当查询主机名 &lt;code&gt;my-service.prod.svc.cluster.local&lt;/code&gt; 时， 集群 DNS 服务会返回一个 &lt;code&gt;CNAME&lt;/code&gt;
记录，值为 &lt;code&gt;my.database.example.com&lt;/code&gt;. 访问 &lt;code&gt;my-service&lt;/code&gt; 的效果与访问其它的 Service 一样，
关键不同点在于重定义发生在 DNS 层， 而不是通过代理或转发。 如果用户决定以后会将数据库移到集群内，
就可以先启动 Pod， 再添加恰当 的 选择器 或 Endpoint, 再修改 Service 的类型就完成迁移了，而客户端不会有感知。&lt;/p&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; 那么集群内客户端使用的主机名与 ExternalName 所引用的名称必然是不一样的。
对于使用主机名的协议，这个不同可能会导致错误或意外的响应。 HTTP 请求会有一个 &lt;code&gt;Host:&lt;/code&gt; 头，原始
的服务会认不得。 TLS 服务也不能提供客户端连接主机名匹配的证书。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 这节感谢
&lt;a href=&#34;https://akomljen.com/&#34;&gt;Alen Komljen&lt;/a&gt;
的博客
&lt;a href=&#34;https://akomljen.com/kubernetes-tips-part-1/&#34;&gt;Kubernetes Tips - Part 1&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### External IPs

If there are external IPs that route to one or more cluster nodes, Kubernetes Services can be exposed on those
`externalIPs`. Traffic that ingresses into the cluster with the external IP (as destination IP), on the Service port,
will be routed to one of the Service endpoints. `externalIPs` are not managed by Kubernetes and are the responsibility
of the cluster administrator.

In the Service spec, `externalIPs` can be specified along with any of the `ServiceTypes`.
In the example below, &#34;`my-service`&#34; can be accessed by clients on &#34;`80.11.12.10:80`&#34; (`externalIP:port`)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 9376
  externalIPs:
    - 80.11.12.10
```
 --&gt;
&lt;h3 id=&#34;外部-ip&#34;&gt;外部 IP&lt;/h3&gt;
&lt;p&gt;如果有外部IP 能够路由到集群的一个或多个节点， k8s Service 可以通过这些 &lt;code&gt;externalIPs&lt;/code&gt; 来对外暴露。
通过外部IP(作为目标IP)的流量进入集群，到 Service 的端口，会被路由到一个 Service 的 Endpoint
上。 &lt;code&gt;externalIPs&lt;/code&gt; 不是由 k8s 管理的，这是集群管理员负责的。&lt;/p&gt;
&lt;p&gt;在 Service 中，&lt;code&gt;externalIPs&lt;/code&gt; 可以用在任意  &lt;code&gt;ServiceTypes&lt;/code&gt; 的 Service 上。 在下面的示例中
客户端可以通过 &lt;code&gt;80.11.12.10:80&lt;/code&gt; (&lt;code&gt;externalIP:port&lt;/code&gt;) 访问 &lt;code&gt;my-service&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;externalIPs&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;80.11.12.10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Shortcomings

Using the userspace proxy for VIPs, work at small to medium scale, but will
not scale to very large clusters with thousands of Services.  The
[original design proposal for portals](https://github.com/kubernetes/kubernetes/issues/1107)
has more details on this.

Using the userspace proxy obscures the source IP address of a packet accessing
a Service.
This makes some kinds of network filtering (firewalling) impossible.  The iptables
proxy mode does not
obscure in-cluster source IPs, but it does still impact clients coming through
a load balancer or node-port.

The `Type` field is designed as nested functionality - each level adds to the
previous.  This is not strictly required on all cloud providers (e.g. Google Compute Engine does
not need to allocate a `NodePort` to make `LoadBalancer` work, but AWS does)
but the current API requires it.
 --&gt;
&lt;h2 id=&#34;缺陷&#34;&gt;缺陷&lt;/h2&gt;
&lt;p&gt;使用 userspace 代理的 VIP，只适用于中小规模，但不适用于有上千 Service 的大规模集群。
更多细节见
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/1107&#34;&gt;original design proposal for portals&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使用 userspace 代理会吃掉进入 Service 的包的源 IP 地址。 这会使得某些类型的网络过虑(防火墙)
变得不可能， iptables 代理模式不会吃掉进入集群的源 IP 地址，但依然与来自负载均衡或 NodePort
的客户端有冲突。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Type&lt;/code&gt; 在设计上是可以嵌套的，每一级添加到前一级上。 但这不是所有的云提供商都严格必须要的(例如
GCE 就不会分配一个 &lt;code&gt;NodePort&lt;/code&gt; 来让 &lt;code&gt;LoadBalancer&lt;/code&gt; 工作，但 AWS 又是需要的)但目前的
API请求必须得有它&lt;/p&gt;
&lt;!--
## Virtual IP implementation {#the-gory-details-of-virtual-ips}

The previous information should be sufficient for many people who just want to
use Services.  However, there is a lot going on behind the scenes that may be
worth understanding.
 --&gt;
&lt;h2 id=&#34;the-gory-details-of-virtual-ips&#34;&gt;虚拟 IP 的实现&lt;/h2&gt;
&lt;p&gt;The previous information should be sufficient for many people who just want to
use Services.  However, there is a lot going on behind the scenes that may be
worth understanding.
之间的信息对于许多只想用 Service 的用户来说已经足够了。但是这些信息之下还有许多值得理解的东西。&lt;/p&gt;
&lt;!--
### Avoiding collisions

One of the primary philosophies of Kubernetes is that you should not be
exposed to situations that could cause your actions to fail through no fault
of your own. For the design of the Service resource, this means not making
you choose your own port number if that choice might collide with
someone else&#39;s choice.  That is an isolation failure.

In order to allow you to choose a port number for your Services, we must
ensure that no two Services can collide. Kubernetes does that by allocating each
Service its own IP address.

To ensure each Service receives a unique IP, an internal allocator atomically
updates a global allocation map in &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/administer-cluster/configure-upgrade-etcd/&#39; target=&#39;_blank&#39;&gt;etcd&lt;span class=&#39;tooltip-text&#39;&gt;用来存储 k8s 所有集群数据的一致性和高可用键值存储&lt;/span&gt;
&lt;/a&gt;
prior to creating each Service. The map object must exist in the registry for
Services to get IP address assignments, otherwise creations will
fail with a message indicating an IP address could not be allocated.

In the control plane, a background controller is responsible for creating that
map (needed to support migrating from older versions of Kubernetes that used
in-memory locking). Kubernetes also uses controllers to check for invalid
assignments (eg due to administrator intervention) and for cleaning up allocated
IP addresses that are no longer used by any Services.
 --&gt;
&lt;h3 id=&#34;避免冲突&#34;&gt;避免冲突&lt;/h3&gt;
&lt;p&gt;k8s 一个主要的宗旨就是让用户不是因为自己的错误而引起出错， 就拿 Service 资源的设计来说，就是如果
你选择的端口可能与别人选择的端口可能冲突，则不你来选择这个端口。 这是一种故障隔离。&lt;/p&gt;
&lt;p&gt;为了允许用户为 Service  选择一个端口，我们必须要确保不能有两个 Service 端口冲突。k8s 通过为每
个 Service 分配 IP 地址来避免这个问题。&lt;/p&gt;
&lt;p&gt;为了保证每个 Service 接收的 IP 地址都是唯一的，在创建每个 Service 之间一个内部分配器原子地
在 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/administer-cluster/configure-upgrade-etcd/&#39; target=&#39;_blank&#39;&gt;etcd&lt;span class=&#39;tooltip-text&#39;&gt;用来存储 k8s 所有集群数据的一致性和高可用键值存储&lt;/span&gt;
&lt;/a&gt; 中更新一个全局的分配字典。 必须要能在这个映射中
为 Service 分配指定 IP， 否则就会失败，错误信息为不能分配该 IP 地址。&lt;/p&gt;
&lt;p&gt;在控制中心中，一个后台控制器负载创建这个映射(需要支持使用内存锁的老版本k8s迁移)。k8s 还使用
控制器检查每个分配(例如，因为管理员介入)并清除那些不被任何 Service 使用的 IP 地址。&lt;/p&gt;
&lt;!--
### Service IP addresses {#ips-and-vips}

Unlike Pod IP addresses, which actually route to a fixed destination,
Service IPs are not actually answered by a single host.  Instead, kube-proxy
uses iptables (packet processing logic in Linux) to define _virtual_ IP addresses
which are transparently redirected as needed.  When clients connect to the
VIP, their traffic is automatically transported to an appropriate endpoint.
The environment variables and DNS for Services are actually populated in
terms of the Service&#39;s virtual IP address (and port).

kube-proxy supports three proxy modes&amp;mdash;userspace, iptables and IPVS&amp;mdash;which
each operate slightly differently.
 --&gt;
&lt;h3 id=&#34;ips-and-vips&#34;&gt;Service IP 地址&lt;/h3&gt;
&lt;p&gt;与 Pod 的 IP 地址不同， 当 Pod 实际是路由到一个固定的地址， Service  IP 通常不是由单个主机响应的。
而是 kube-proxy 使用 iptables (Linux 中的包处理逻辑) 来定义一个 &lt;em&gt;虚拟的&lt;/em&gt; IP 地址，根据需要透明地
转发流量。 当客户端连接到 VIP 时， 它们的流量自动传输到恰当和端点上。 Pod 中被注入的环境变量
和 Service DNS 记录指向的也是 Service 的虚拟 IP 地址(和端口)。&lt;/p&gt;
&lt;p&gt;kube-proxy 支持三种代理模式 —userspace, iptables 和 IPVS， 它们之间的运转方式各有不同。&lt;/p&gt;
&lt;!--
#### Userspace

As an example, consider the image processing application described above.
When the backend Service is created, the Kubernetes master assigns a virtual
IP address, for example 10.0.0.1.  Assuming the Service port is 1234, the
Service is observed by all of the kube-proxy instances in the cluster.
When a proxy sees a new Service, it opens a new random port, establishes an
iptables redirect from the virtual IP address to this new port, and starts accepting
connections on it.

When a client connects to the Service&#39;s virtual IP address, the iptables
rule kicks in, and redirects the packets to the proxy&#39;s own port.
The &#34;Service proxy&#34; chooses a backend, and starts proxying traffic from the client to the backend.

This means that Service owners can choose any port they want without risk of
collision.  Clients can simply connect to an IP and port, without being aware
of which Pods they are actually accessing.
 --&gt;
&lt;h4 id=&#34;userspace&#34;&gt;userspace&lt;/h4&gt;
&lt;p&gt;例如， 上面提到过那个处理图片的应用。 当后端的 Service 创建时， k8s 控制中心为它分配一个虚拟
的 IP 地址，假如是 &lt;code&gt;10.0.0.1&lt;/code&gt;。 假定 Service 的端口为 &lt;code&gt;1234&lt;/code&gt;， 该 Service 会被集群中所有
的 kube-proxy 实例监控。 当 一个代理发现一个新的 Service， 它会打开一个随机端口， 创建一个
iptables 规则将虚拟 IP 地址重定向到这个新创建的端口，并开始接收连接。&lt;/p&gt;
&lt;p&gt;当有一个客户端连接到这个 Service 的虚拟 IP 时， iptables 规则工作将包转发到代理自己的端口。
然后 Service 的代理选择后端，然后开始将代理从客户端到后端的流量。&lt;/p&gt;
&lt;p&gt;这么做的意义在于 Service 拥有者可以选择任意端口这就避免的端口冲突的风险。 客户端只是简单地连接
到一个 IP 地址和端口， 并不会感知到它实际上是连接到后端的哪个 Pod。&lt;/p&gt;
&lt;!--
#### iptables

Again, consider the image processing application described above.
When the backend Service is created, the Kubernetes control plane assigns a virtual
IP address, for example 10.0.0.1.  Assuming the Service port is 1234, the
Service is observed by all of the kube-proxy instances in the cluster.
When a proxy sees a new Service, it installs a series of iptables rules which
redirect from the virtual IP address  to per-Service rules.  The per-Service
rules link to per-Endpoint rules which redirect traffic (using destination NAT)
to the backends.

When a client connects to the Service&#39;s virtual IP address the iptables rule kicks in.
A backend is chosen (either based on session affinity or randomly) and packets are
redirected to the backend.  Unlike the userspace proxy, packets are never
copied to userspace, the kube-proxy does not have to be running for the virtual
IP address to work, and Nodes see traffic arriving from the unaltered client IP
address.

This same basic flow executes when traffic comes in through a node-port or
through a load-balancer, though in those cases the client IP does get altered.
 --&gt;
&lt;h4 id=&#34;iptables&#34;&gt;iptables&lt;/h4&gt;
&lt;p&gt;再来，还是上面那个处理图片的应用。 当后端的 Service 创建后， k8s 控制中心分配了一个虚拟IP地址
，假如是 &lt;code&gt;10.0.0.1&lt;/code&gt;.假定 Service 的端口为 &lt;code&gt;1234&lt;/code&gt;， 该 Service 会被集群中所有 的 kube-proxy 实例监控。
当代理发现一个新的 Service, 它会插入一系统 iptables 规则， 这些规则将重定向到虚拟IP地址的流量
到每个 Service 的规则。 每个 Service 的规则又与每个 Endpoint 的规则相连，将流量重定向(通过目标 NAT)
到后端。&lt;/p&gt;
&lt;p&gt;当一个客户端连接到 Service 的虚拟 IP 地址时， iptables 开始插一脚。 选择一个后端(要么基于会话粘性，要么随机)
并将数据包转发到该后端。 与 userspace 代理模式不同， 网络包不会拷贝到用户空间，kube-proxy 也不
需要为为虚拟 IP 运行工作任务， 节点可以看到接收到流量中未修改的客户端 IP 地址&lt;/p&gt;
&lt;p&gt;来自 NodePort 或 负载均衡器的流量也是使用这样的基本执行流程， 在这种情况下客户端 IP 不会被修改。&lt;/p&gt;
&lt;!--
#### IPVS

iptables operations slow down dramatically in large scale cluster e.g 10,000 Services.
IPVS is designed for load balancing and based on in-kernel hash tables. So you can achieve performance consistency in large number of Services from IPVS-based kube-proxy. Meanwhile, IPVS-based kube-proxy has more sophisticated load balancing algorithms (least conns, locality, weighted, persistence).
 --&gt;
&lt;h4 id=&#34;ipvs&#34;&gt;IPVS&lt;/h4&gt;
&lt;p&gt;iptables 在大规模集群中(如，上万个 Service)是急剧下降。IPVS 设计上是基于内核内部的哈希表来
实现负载均衡的。 所以基于 IPVS 的 kube-proxy 可以实现在大量 Service 的情况下性能稳定。
同时基于 IPVS kube-proxy 也包含更丰富的负载均衡算法(最少连接，位置，权重，维持)&lt;/p&gt;
&lt;!--
## API Object

Service is a top-level resource in the Kubernetes REST API. You can find more details
about the API object at: [Service API object](/docs/reference/generated/kubernetes-api/v1.19/#service-v1-core).
 --&gt;
&lt;h2 id=&#34;api-对象&#34;&gt;API 对象&lt;/h2&gt;
&lt;p&gt;Service is a top-level resource in the Kubernetes REST API. You can find more details
about the API object at: &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubernetes-api/v1.19/#service-v1-core&#34;&gt;Service API object&lt;/a&gt;.
Service 是 k8s REST API 中的顶级资源， 更多相关信息见
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#service-v1-core&#34;&gt;Service API 对象&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Supported protocols {#protocol-support}

### TCP

You can use TCP for any kind of Service, and it&#39;s the default network protocol.

### UDP

You can use UDP for most Services. For type=LoadBalancer Services, UDP support
depends on the cloud provider offering this facility.

### HTTP

If your cloud provider supports it, you can use a Service in LoadBalancer mode
to set up external HTTP / HTTPS reverse proxying, forwarded to the Endpoints
of the Service.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You can also use &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/ingress/&#39; target=&#39;_blank&#39;&gt;Ingress&lt;span class=&#39;tooltip-text&#39;&gt;一个用于管理外部访问集群内 Service 的 API 对象，通常是 HTTP。&lt;/span&gt;
&lt;/a&gt; in place of Service
to expose HTTP / HTTPS Services.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;protocol-support&#34;&gt;支持的协议&lt;/h2&gt;
&lt;h3 id=&#34;tcp&#34;&gt;TCP&lt;/h3&gt;
&lt;p&gt;TCP 可用于任意类型的 Service， 也是 Service  的默认网络协议&lt;/p&gt;
&lt;h3 id=&#34;udp&#34;&gt;UDP&lt;/h3&gt;
&lt;p&gt;UDP 可用于大多数 Service, 对于 type=LoadBalancer 的 Service, 是否支持 UDP 基于云提供商
是否提供该功能。&lt;/p&gt;
&lt;h3 id=&#34;http&#34;&gt;HTTP&lt;/h3&gt;
&lt;p&gt;如果云提供商支持，则可以使用 type=LoadBalancer 的 Service 通过外部的 HTTP / HTTPS 反向代理
转发到 Service 的 Endpoint&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 在 Service 暴露的是 HTTP / HTTPS 时也可以使用 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/ingress/&#39; target=&#39;_blank&#39;&gt;Ingress&lt;span class=&#39;tooltip-text&#39;&gt;一个用于管理外部访问集群内 Service 的 API 对象，通常是 HTTP。&lt;/span&gt;
&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### PROXY protocol

If your cloud provider supports it (eg, [AWS](/docs/concepts/cluster-administration/cloud-providers/#aws)),
you can use a Service in LoadBalancer mode to configure a load balancer outside
of Kubernetes itself, that will forward connections prefixed with
[PROXY protocol](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt).

The load balancer will send an initial series of octets describing the
incoming connection, similar to this example

```
PROXY TCP4 192.0.2.202 10.0.42.7 12345 7\r\n
```
followed by the data from the client.
 --&gt;
&lt;h3 id=&#34;proxy-协议&#34;&gt;PROXY 协议&lt;/h3&gt;
&lt;p&gt;如果云提供商支持， (比如 , &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/cluster-administration/cloud-providers/#aws&#34;&gt;AWS&lt;/a&gt;)，
可以在使用 LoadBalancer 类型的 Service 时在 k8s 外部配置负载均衡器， 它会转发带前缀
&lt;a href=&#34;https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt&#34;&gt;PROXY 协议&lt;/a&gt;.
的连接。&lt;/p&gt;
&lt;p&gt;负载均衡器会发送一个初始化八进制序列描述进入的连接，类似如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;PROXY TCP4 192.0.2.202 10.0.42.7 12345 7\r\n
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;紧哪关就是客户端发送的数据。&lt;/p&gt;
&lt;!--
### SCTP






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;



Kubernetes supports SCTP as a `protocol` value in Service, Endpoints, EndpointSlice, NetworkPolicy and Pod definitions. As a beta feature, this is enabled by default. To disable SCTP at a cluster level, you (or your cluster administrator) will need to disable the `SCTPSupport` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) for the API server with `--feature-gates=SCTPSupport=false,…`.

When the feature gate is enabled, you can set the `protocol` field of a Service, Endpoints, EndpointSlice, NetworkPolicy or Pod to `SCTP`. Kubernetes sets up the network accordingly for the SCTP associations, just like it does for TCP connections.
 --&gt;
&lt;h3 id=&#34;sctp&#34;&gt;SCTP&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;k8s 支持在 Service, Endpoints, EndpointSlice, NetworkPolicy 和 Pod 定义中 &lt;code&gt;protocol&lt;/code&gt;
的值为 &lt;code&gt;SCTP&lt;/code&gt;。 因为这是一个 beta 版本的特性，所以默认是开启的。要在集群级别禁用 &lt;code&gt;SCTP&lt;/code&gt;，
需要在 api-server 上通过设置 &lt;code&gt;--feature-gates=SCTPSupport=false,…&lt;/code&gt; 禁用 &lt;code&gt;SCTPSupport&lt;/code&gt;
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;当该特性被启用时，可以设置 Service, Endpoints, EndpointSlice, NetworkPolicy, Pod 的
&lt;code&gt;protocol&lt;/code&gt; 为 &lt;code&gt;SCTP&lt;/code&gt;。 k8s 会根据 SCTP 设置网络，就像设置 TCP 连接一样。&lt;/p&gt;
&lt;!--
#### Warnings {#caveat-sctp-overview}

##### Support for multihomed SCTP associations {#caveat-sctp-multihomed}

&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; &lt;p&gt;The support of multihomed SCTP associations requires that the CNI plugin can support the assignment of multiple interfaces and IP addresses to a Pod.&lt;/p&gt;
&lt;p&gt;NAT for multihomed SCTP associations requires special logic in the corresponding kernel modules.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;


##### Service with type=LoadBalancer {#caveat-sctp-loadbalancer-service-type}

&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; You can only create a Service with &lt;code&gt;type&lt;/code&gt; LoadBalancer plus &lt;code&gt;protocol&lt;/code&gt; SCTP if the cloud provider&amp;rsquo;s load balancer implementation supports SCTP as a protocol. Otherwise, the Service creation request is rejected. The current set of cloud load balancer providers (Azure, AWS, CloudStack, GCE, OpenStack) all lack support for SCTP.&lt;/div&gt;
&lt;/blockquote&gt;


##### Windows {#caveat-sctp-windows-os}

&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; SCTP is not supported on Windows based nodes.&lt;/div&gt;
&lt;/blockquote&gt;


##### Userspace kube-proxy {#caveat-sctp-kube-proxy-userspace}

&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; The kube-proxy does not support the management of SCTP associations when it is in userspace mode.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h4 id=&#34;caveat-sctp-overview&#34;&gt;警告&lt;/h4&gt;
&lt;h5 id=&#34;caveat-sctp-multihomed&#34;&gt;SCTP 多重连接支持&lt;/h5&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; SCTP 多重连接支持的前提是 CNI 插件支持为一个 Pod 分配多个网上和IP地址
SCTP 多重连接的 NAT 需要在对应的逻辑模块中有特殊逻辑&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h5 id=&#34;caveat-sctp-loadbalancer-service-type&#34;&gt;type=LoadBalancer 的 Service&lt;/h5&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; 如有在云提供商的负载均衡器实现了对 &lt;code&gt;SCTP&lt;/code&gt; 协议支持时才能够创建一个类型为 LoadBalancer 并且
&lt;code&gt;protocol&lt;/code&gt; 为 SCTP 的 Service. 否则 Service 的创建请求会被拒绝。 目前的云负载均衡提供者
(Azure, AWS, CloudStack, GCE, OpenStack) 都缺乏对 SCTP 的支持。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h5 id=&#34;caveat-sctp-windows-os&#34;&gt;Windows&lt;/h5&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; 基于 Windows 的节点不支持 SCTP&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h5 id=&#34;caveat-sctp-kube-proxy-userspace&#34;&gt;userspace kube-proxy&lt;/h5&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; 使用 userspace 模式的 kube-proxy 不支持对 SCTP 连接的管理&lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;对SCTP不了解，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/connect-applications-service/&#34;&gt;通过 Service 连接应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/ingress/&#34;&gt;Ingress&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/endpoint-slices/&#34;&gt;EndpointSlices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 云原生安全概述</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/security/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/security/overview/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- zparnold
title: Overview of Cloud Native Security
content_type: concept
weight: 10
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This overview defines a model for thinking about Kubernetes security in the context of Cloud Native security.
 --&gt;
&lt;p&gt;本文定义了一个模式用于思考在云原生安全语境下的 k8s 安全&lt;/p&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; &lt;!--
This container security model provides suggestions, not proven information security policies.
 --&gt;
&lt;p&gt;这个容器安全模型只提供建议，并不提供安全策略信息。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!-- body --&gt;
&lt;!--
## The 4C&#39;s of Cloud Native security

You can think about security in layers. The 4C&#39;s of Cloud Native security are Cloud,
Clusters, Containers, and Code.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; This layered approach augments the &lt;a href=&#34;https://en.wikipedia.org/wiki/Defense_in_depth_(computing)&#34;&gt;defense in depth&lt;/a&gt;
computing approach to security, which is widely regarded as a best practice for securing
software systems.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;figure&gt;
    &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/images/docs/4c.png&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;The 4C&amp;#39;s of Cloud Native Security&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


Each layer of the Cloud Native security model builds upon the next outermost layer.
The Code layer benefits from strong base (Cloud, Cluster, Container) security layers.
You cannot safeguard against poor security standards in the base layers by addressing
security at the Code level.
 --&gt;
&lt;h2 id=&#34;the-4cs-of-cloud-native-security&#34;&gt;云原生安全的 4C 概念&lt;/h2&gt;
&lt;p&gt;可以将安全进行分层。 云原生安全的 4 个 C 就是云(Cloud), 集群(Clusters), 容器(Containers),
和 代码(Code).&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 这种计算机安全通过分层的方式实现在
&lt;a href=&#34;https://en.wikipedia.org/wiki/Defense_in_depth_(computing)&#34;&gt;纵深防御&lt;/a&gt;
中有讨论，也是广泛认可作为软件系统安全的最佳实践。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;figure&gt;
    &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/docs/4c.png&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;云原生安全的 4C 概念&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;云原生安全模型的每一层都是构建于其外层的。 代码(Code)层受益于 (Cloud, Cluster, Container)
层的安全基础。 不能希望在基础层安全薄弱的情况下只通过代码层的安全策略就能提供良好的安全保障。&lt;/p&gt;
&lt;!--
## Cloud

In many ways, the Cloud (or co-located servers, or the corporate datacenter) is the
[trusted computing base](https://en.wikipedia.org/wiki/Trusted_computing_base)
of a Kubernetes cluster. If the Cloud layer is vulnerable (or
configured in a vulnerable way) then there is no guarantee that the components built
on top of this base are secure. Each cloud provider makes security recommendations
for running workloads securely in their environment.
 --&gt;
&lt;h2 id=&#34;cloud&#34;&gt;云环境&lt;/h2&gt;
&lt;p&gt;在大多数情况下， 云环境(或同一个地方的服务器，或公司的数据中心) 在 k8s 集群中是
&lt;a href=&#34;https://en.wikipedia.org/wiki/Trusted_computing_base&#34;&gt;受信任的计算基础&lt;/a&gt;。 如果在云
环境层是不安全的(或者以一个不安全的方式配置的)，那么基于这个基础构建的组建也是没有安全保证的。
每个云提供商都对在其环境上安全运行工作负载的安全建议&lt;/p&gt;
&lt;!--
### Cloud provider security

If you are running a Kubernetes cluster on your own hardware or a different cloud provider,
consult your documentation for security best practices.
Here are links to some of the popular cloud providers&#39; security documentation:






&lt;table&gt;&lt;caption style=&#34;display: none;&#34;&gt;Cloud provider security&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;IaaS Provider&lt;/th&gt;
&lt;th&gt;Link&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Alibaba Cloud&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.alibabacloud.com/trust-center&#34;&gt;https://www.alibabacloud.com/trust-center&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Amazon Web Services&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://aws.amazon.com/security/&#34;&gt;https://aws.amazon.com/security/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Google Cloud Platform&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://cloud.google.com/security/&#34;&gt;https://cloud.google.com/security/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IBM Cloud&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.ibm.com/cloud/security&#34;&gt;https://www.ibm.com/cloud/security&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Microsoft Azure&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/security/azure-security&#34;&gt;https://docs.microsoft.com/en-us/azure/security/azure-security&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VMWare VSphere&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.vmware.com/security/hardening-guides.html&#34;&gt;https://www.vmware.com/security/hardening-guides.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

 --&gt;
&lt;h3 id=&#34;Cloud-provider-security&#34;&gt;云提供商安全&lt;/h3&gt;
&lt;p&gt;如果 k8s 集群运行在自己的硬件上或不同的云提供商，请查询相应文档获取最佳安全实践。
下面是一些主流云提供商的安全文档的链接地址.&lt;/p&gt;





&lt;table&gt;&lt;caption style=&#34;display: none;&#34;&gt;云提供商安全&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;IaaS 提供商&lt;/th&gt;
&lt;th&gt;链接&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Alibaba Cloud&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.alibabacloud.com/trust-center&#34;&gt;https://www.alibabacloud.com/trust-center&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Amazon Web Services&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://aws.amazon.com/security/&#34;&gt;https://aws.amazon.com/security/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Google Cloud Platform&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://cloud.google.com/security/&#34;&gt;https://cloud.google.com/security/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IBM Cloud&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.ibm.com/cloud/security&#34;&gt;https://www.ibm.com/cloud/security&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Microsoft Azure&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/security/azure-security&#34;&gt;https://docs.microsoft.com/en-us/azure/security/azure-security&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VMWare VSphere&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://www.vmware.com/security/hardening-guides.html&#34;&gt;https://www.vmware.com/security/hardening-guides.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;!--
### Infrastructure security {#infrastructure-security}

Suggestions for securing your infrastructure in a Kubernetes cluster:






&lt;table&gt;&lt;caption style=&#34;display: none;&#34;&gt;Infrastructure security&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Area of Concern for Kubernetes Infrastructure&lt;/th&gt;
&lt;th&gt;Recommendation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Network access to API Server (Control plane)&lt;/td&gt;
&lt;td&gt;All access to the Kubernetes control plane is not allowed publicly on the internet and is controlled by network access control lists restricted to the set of IP addresses needed to administer the cluster.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Network access to Nodes (nodes)&lt;/td&gt;
&lt;td&gt;Nodes should be configured to &lt;em&gt;only&lt;/em&gt; accept connections (via network access control lists)from the control plane on the specified ports, and accept connections for services in Kubernetes of type NodePort and LoadBalancer. If possible, these nodes should not be exposed on the public internet entirely.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kubernetes access to Cloud Provider API&lt;/td&gt;
&lt;td&gt;Each cloud provider needs to grant a different set of permissions to the Kubernetes control plane and nodes. It is best to provide the cluster with cloud provider access that follows the &lt;a href=&#34;https://en.wikipedia.org/wiki/Principle_of_least_privilege&#34;&gt;principle of least privilege&lt;/a&gt; for the resources it needs to administer. The &lt;a href=&#34;https://github.com/kubernetes/kops/blob/master/docs/iam_roles.md#iam-roles&#34;&gt;Kops documentation&lt;/a&gt; provides information about IAM policies and roles.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Access to etcd&lt;/td&gt;
&lt;td&gt;Access to etcd (the datastore of Kubernetes) should be limited to the control plane only. Depending on your configuration, you should attempt to use etcd over TLS. More information can be found in the &lt;a href=&#34;https://github.com/etcd-io/etcd/tree/master/Documentation&#34;&gt;etcd documentation&lt;/a&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;etcd Encryption&lt;/td&gt;
&lt;td&gt;Wherever possible it&amp;rsquo;s a good practice to encrypt all drives at rest, but since etcd holds the state of the entire cluster (including Secrets) its disk should especially be encrypted at rest.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

 --&gt;
&lt;!--
### Infrastructure security {#infrastructure-security}

Suggestions for securing your infrastructure in a Kubernetes cluster:






&lt;table&gt;&lt;caption style=&#34;display: none;&#34;&gt;Infrastructure security&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Area of Concern for Kubernetes Infrastructure&lt;/th&gt;
&lt;th&gt;Recommendation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Network access to API Server (Control plane)&lt;/td&gt;
&lt;td&gt;All access to the Kubernetes control plane is not allowed publicly on the internet and is controlled by network access control lists restricted to the set of IP addresses needed to administer the cluster.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Network access to Nodes (nodes)&lt;/td&gt;
&lt;td&gt;Nodes should be configured to &lt;em&gt;only&lt;/em&gt; accept connections (via network access control lists)from the control plane on the specified ports, and accept connections for services in Kubernetes of type NodePort and LoadBalancer. If possible, these nodes should not be exposed on the public internet entirely.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kubernetes access to Cloud Provider API&lt;/td&gt;
&lt;td&gt;Each cloud provider needs to grant a different set of permissions to the Kubernetes control plane and nodes. It is best to provide the cluster with cloud provider access that follows the &lt;a href=&#34;https://en.wikipedia.org/wiki/Principle_of_least_privilege&#34;&gt;principle of least privilege&lt;/a&gt; for the resources it needs to administer. The &lt;a href=&#34;https://github.com/kubernetes/kops/blob/master/docs/iam_roles.md#iam-roles&#34;&gt;Kops documentation&lt;/a&gt; provides information about IAM policies and roles.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Access to etcd&lt;/td&gt;
&lt;td&gt;Access to etcd (the datastore of Kubernetes) should be limited to the control plane only. Depending on your configuration, you should attempt to use etcd over TLS. More information can be found in the &lt;a href=&#34;https://github.com/etcd-io/etcd/tree/master/Documentation&#34;&gt;etcd documentation&lt;/a&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;etcd Encryption&lt;/td&gt;
&lt;td&gt;Wherever possible it&amp;rsquo;s a good practice to encrypt all drives at rest, but since etcd holds the state of the entire cluster (including Secrets) its disk should especially be encrypted at rest.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

 --&gt;
&lt;h3 id=&#34;infrastructure-security&#34;&gt;基础设施安全&lt;/h3&gt;
&lt;p&gt;k8s 集群基础设计安全加固建议:&lt;/p&gt;





&lt;table&gt;&lt;caption style=&#34;display: none;&#34;&gt;基础设施安全&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;k8s 基础设计关注领域&lt;/th&gt;
&lt;th&gt;推荐&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;访问 API 服务 (控制中心) 的网络&lt;/td&gt;
&lt;td&gt;k8s 控制中心是不允许任意来自公共互联网的访问，并且对控制中心的访问应该限制在那些需要管理集群的 IP 集合&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;访问节点 (nodes) 的网络&lt;/td&gt;
&lt;td&gt;节点应该配置成 &lt;em&gt;只&lt;/em&gt; 允许来自控制中心访问指定端口的连接(通过网络访问控制列表)，和接收来自 k8s 中 NodePort 和 LoadBalancer 类型 Service 的连接。 如果可能，避免让所有的节点都暴露到互联网。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;k8s 对云提供商 API 的访问&lt;/td&gt;
&lt;td&gt;每个云提供商需要给 k8s 控制中心和节点授予不同个权限集。 在授予集群云提供商访问权限时，依照&lt;a href=&#34;https://en.wikipedia.org/wiki/Principle_of_least_privilege&#34;&gt;最小权限原则&lt;/a&gt;，只给予其所需要管理的资源。 &lt;a href=&#34;https://github.com/kubernetes/kops/blob/master/docs/iam_roles.md#iam-roles&#34;&gt;Kops documentation&lt;/a&gt; 文档提供了关于 IAM 和角色的信息。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;访问 etcd&lt;/td&gt;
&lt;td&gt;对 etcd (k8s 的数据源)应该限制到只给控制中心。 基于配置，应该使用配置有 TLS 的 etcd。 更多信息可以在 &lt;a href=&#34;https://github.com/etcd-io/etcd/tree/master/Documentation&#34;&gt;etcd 文档&lt;/a&gt;中找到.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;etcd 加密&lt;/td&gt;
&lt;td&gt;如果可能，对所有驱动器在落盘时加密是一种好的实践，但因为 etcd 中存放了整个集群的状态(包括 Secret)它的数据最应该在落盘时加密。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Cluster

There are two areas of concern for securing Kubernetes:

* Securing the cluster components that are configurable
* Securing the applications which run in the cluster
 --&gt;
&lt;h2 id=&#34;cluster&#34;&gt;集群&lt;/h2&gt;
&lt;p&gt;在对 k8s 安全加固时有两个值得关注的领域：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;安全加固那些可配置的集群组件&lt;/li&gt;
&lt;li&gt;安全加固那些运行在集群中的应用&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Components of the Cluster {#cluster-components}

If you want to protect your cluster from accidental or malicious access and adopt
good information practices, read and follow the advice about
[securing your cluster](/docs/tasks/administer-cluster/securing-a-cluster/).
 --&gt;
&lt;h3 id=&#34;cluster-components&#34;&gt;集群的组件&lt;/h3&gt;
&lt;p&gt;如果希望保护集群免于偶然或恶意的访问和彩良好的信息实践，请阅读下面的建议
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/administer-cluster/securing-a-cluster/&#34;&gt;集群安全加固&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### Components in the cluster (your application) {#cluster-applications}

Depending on the attack surface of your application, you may want to focus on specific
aspects of security. For example: If you are running a service (Service A) that is critical
in a chain of other resources and a separate workload (Service B) which is
vulnerable to a resource exhaustion attack then the risk of compromising Service A
is high if you do not limit the resources of Service B. The following table lists
areas of security concerns and recommendations for securing workloads running in Kubernetes:

Area of Concern for Workload Security | Recommendation |
------------------------------ | --------------------- |
RBAC Authorization (Access to the Kubernetes API) | https://kubernetes.io/docs/reference/access-authn-authz/rbac/
Authentication | https://kubernetes.io/docs/concepts/security/controlling-access/
Application secrets management (and encrypting them in etcd at rest) | https://kubernetes.io/docs/concepts/configuration/secret/ &lt;br&gt; https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/
Pod Security Policies | https://kubernetes.io/docs/concepts/policy/pod-security-policy/
Quality of Service (and Cluster resource management) | https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/
Network Policies | https://kubernetes.io/docs/concepts/services-networking/network-policies/
TLS For Kubernetes Ingress | https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
 --&gt;
&lt;h3 id=&#34;cluster-applications&#34;&gt;集群中的组件 (你的应用)&lt;/h3&gt;
&lt;p&gt;基于应用的攻击面，可能希望专注于某个方面的安全。 例如: 如果运行了一个服务(Service A)它是与其它资源
组成的链中的关键，另一个工作负载 (Service B), 这应用面对资源枯竭攻击是很脆弱的，如果不对 Service B
进行资源限制则会让 Service A 被攻陷的风险变高。下面表格里面列举安全关注领域和加固集群中运行的工作
负载安全的建议:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;工作负载安全关注领域&lt;/th&gt;
&lt;th&gt;建议&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;RBAC 授权 (对 k8s API 的访问)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/rbac/&#34;&gt;https://kubernetes.io/docs/reference/access-authn-authz/rbac/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;认证方式&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/security/controlling-access/&#34;&gt;https://kubernetes.io/docs/concepts/security/controlling-access/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;应用 Secret 管理 (和其在 etcd 落盘时加密)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/secret/&#34;&gt;https://kubernetes.io/docs/concepts/configuration/secret/&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/&#34;&gt;https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pod 安全策略&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/policy/pod-security-policy/&#34;&gt;https://kubernetes.io/docs/concepts/policy/pod-security-policy/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;服务资源 (和集群资源管理)&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/&#34;&gt;https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;网络策略&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/network-policies/&#34;&gt;https://kubernetes.io/docs/concepts/services-networking/network-policies/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;为 k8s Ingress 添加 TLS&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/#tls&#34;&gt;https://kubernetes.io/docs/concepts/services-networking/ingress/#tls&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
## Container

Container security is outside the scope of this guide. Here are general recommendations and
links to explore this topic:

Area of Concern for Containers | Recommendation |
------------------------------ | -------------- |
Container Vulnerability Scanning and OS Dependency Security | As part of an image build step, you should scan your containers for known vulnerabilities.
Image Signing and Enforcement | Sign container images to maintain a system of trust for the content of your containers.
Disallow privileged users | When constructing containers, consult your documentation for how to create users inside of the containers that have the least level of operating system privilege necessary in order to carry out the goal of the container.
 --&gt;
&lt;h2 id=&#34;container&#34;&gt;容器&lt;/h2&gt;
&lt;p&gt;容器安全不在本文的讨论范围。 下面是一些通用的建议和关于本话题的延伸阅读链接:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;容器安全关注领域&lt;/th&gt;
&lt;th&gt;建议&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;容器漏洞扫描 和 操作系统信赖安全&lt;/td&gt;
&lt;td&gt;作为一个镜像构建的步骤，应该对容器进行已知漏洞扫描。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;镜像签名和验签&lt;/td&gt;
&lt;td&gt;容器镜像的签名可以维护系统对容器内存的信任&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;禁用特权用户&lt;/td&gt;
&lt;td&gt;在构建容器时，查询文档关于怎么创建可以实现容器目标的最小操作系统权限的用户&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
## Code

Application code is one of the primary attack surfaces over which you have the most control.
While securing application code is outside of the Kubernetes security topic, here
are recommendations to protect application code:
 --&gt;
&lt;h2 id=&#34;code&#34;&gt;代码&lt;/h2&gt;
&lt;p&gt;应用代码是一个最受开发者控制的主要攻击面之一。 因为对应用代码的安全加固不在 k8s 安全话题之内，下面
是一些保护应用代码的建议:&lt;/p&gt;
&lt;!--
### Code security






&lt;table&gt;&lt;caption style=&#34;display: none;&#34;&gt;Code security&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Area of Concern for Code&lt;/th&gt;
&lt;th&gt;Recommendation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Access over TLS only&lt;/td&gt;
&lt;td&gt;If your code needs to communicate by TCP, perform a TLS handshake with the client ahead of time. With the exception of a few cases, encrypt everything in transit. Going one step further, it&amp;rsquo;s a good idea to encrypt network traffic between services. This can be done through a process known as mutual or &lt;a href=&#34;https://en.wikipedia.org/wiki/Mutual_authentication&#34;&gt;mTLS&lt;/a&gt; which performs a two sided verification of communication between two certificate holding services.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Limiting port ranges of communication&lt;/td&gt;
&lt;td&gt;This recommendation may be a bit self-explanatory, but wherever possible you should only expose the ports on your service that are absolutely essential for communication or metric gathering.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3rd Party Dependency Security&lt;/td&gt;
&lt;td&gt;It is a good practice to regularly scan your application&amp;rsquo;s third party libraries for known security vulnerabilities. Each programming language has a tool for performing this check automatically.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Static Code Analysis&lt;/td&gt;
&lt;td&gt;Most languages provide a way for a snippet of code to be analyzed for any potentially unsafe coding practices. Whenever possible you should perform checks using automated tooling that can scan codebases for common security errors. Some of the tools can be found at: &lt;a href=&#34;https://owasp.org/www-community/Source_Code_Analysis_Tools&#34;&gt;https://owasp.org/www-community/Source_Code_Analysis_Tools&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dynamic probing attacks&lt;/td&gt;
&lt;td&gt;There are a few automated tools that you can run against your service to try some of the well known service attacks. These include SQL injection, CSRF, and XSS. One of the most popular dynamic analysis tools is the &lt;a href=&#34;https://owasp.org/www-project-zap/&#34;&gt;OWASP Zed Attack proxy&lt;/a&gt; tool.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

 --&gt;
&lt;h3 id=&#34;code-security&#34;&gt;代码安全&lt;/h3&gt;





&lt;table&gt;&lt;caption style=&#34;display: none;&#34;&gt;Code security&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;代码安全关注领域&lt;/th&gt;
&lt;th&gt;建议&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;只允许通过 TLS 访问&lt;/td&gt;
&lt;td&gt;如果你的代码需要通过 TCP 通信，提前与客户端执行 TLS 握手。除了少数情况下，对所有传输内容加密。更进一步，对服务之间的网络流量加密也是一个好主意。这可以通过相互(mutual)过程或 &lt;a href=&#34;https://en.wikipedia.org/wiki/Mutual_authentication&#34;&gt;mTLS&lt;/a&gt; 也就是两个服务都包含证书，对通信进行双向加密和验证。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;限制通信端口范围&lt;/td&gt;
&lt;td&gt;这个建议应该是不言自明的， 但无论啥时候都应该只暴露服务必不可少的通信或度量采集端口&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;第三方信赖安全&lt;/td&gt;
&lt;td&gt;定期扫描应用的第三方信赖库是否有已知的安全漏洞是一个好的实践。 每种编辑语言都有自动执行这种检查的工具&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;静态代码分析&lt;/td&gt;
&lt;td&gt;大多数语言都提供了对代码片断分析检查是否有潜在的不安全代码实践。 无论啥时候都应该使用自动化工作对代码库执行这种检查，以提早发现常见的安全错误。 一些工具可以在这里找: &lt;a href=&#34;https://owasp.org/www-community/Source_Code_Analysis_Tools&#34;&gt;https://owasp.org/www-community/Source_Code_Analysis_Tools&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;动态探测攻击&lt;/td&gt;
&lt;td&gt;有几种自动化工具可以用来对着服务运行，尝试一些流行的服务攻击。 包含 SQL 注入， CSRF, 和 XSS. 最流行的动态分析工具之一就是 &lt;a href=&#34;https://owasp.org/www-project-zap/&#34;&gt;OWASP Zed Attack proxy&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
Learn about related Kubernetes security topics:

* [Pod security standards](/docs/concepts/security/pod-security-standards/)
* [Network policies for Pods](/docs/concepts/services-networking/network-policies/)
* [Controlling Access to the Kubernetes API](/docs/concepts/security/controlling-access)
* [Securing your cluster](/docs/tasks/administer-cluster/securing-a-cluster/)
* [Data encryption in transit](/docs/tasks/tls/managing-tls-in-a-cluster/) for the control plane
* [Data encryption at rest](/docs/tasks/administer-cluster/encrypt-data/)
* [Secrets in Kubernetes](/docs/concepts/configuration/secret/)
 --&gt;
&lt;p&gt;更多关于 k8s 安全的主题:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/security/pod-security-standards/&#34;&gt;Pod 安全标准&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/network-policies/&#34;&gt;Pod 网络策略&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/security/controlling-access&#34;&gt;对 k8s API 的访问控制&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/administer-cluster/securing-a-cluster/&#34;&gt;集群网络加固&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;对控制中心&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/tls/managing-tls-in-a-cluster/&#34;&gt;传输中的数据加密&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/administer-cluster/encrypt-data/&#34;&gt;数据落盘加密&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#34;&gt;k8s 中的 Secret&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 配置最佳实践</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/overview/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- mikedanese
title: Configuration Best Practices
content_type: concept
weight: 10
---
 --&gt;
&lt;!-- overview --&gt;
&lt;p&gt;本文主要加强和巩固从用户指引，入门文档，和示例中介绍过的配置的最佳实践。&lt;/p&gt;
&lt;p&gt;这是一个活动文档，如果用户觉得有些内容文档上没有但可以对其他人有帮助，不要犹豫，赶紧提交问题单或
提供 PR&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## General Configuration Tips

- When defining configurations, specify the latest stable API version.

- Configuration files should be stored in version control before being pushed to the cluster. This allows you to quickly roll back a configuration change if necessary. It also aids cluster re-creation and restoration.

- Write your configuration files using YAML rather than JSON. Though these formats can be used interchangeably in almost all scenarios, YAML tends to be more user-friendly.

- Group related objects into a single file whenever it makes sense. One file is often easier to manage than several. See the [guestbook-all-in-one.yaml](https://github.com/kubernetes/examples/tree/master/guestbook/all-in-one/guestbook-all-in-one.yaml) file as an example of this syntax.

- Note also that many `kubectl` commands can be called on a directory. For example, you can call `kubectl apply` on a directory of config files.

- Don&#39;t specify default values unnecessarily: simple, minimal configuration will make errors less likely.

- Put object descriptions in annotations, to allow better introspection.
 --&gt;
&lt;h2 id=&#34;通用配置提示&#34;&gt;通用配置提示&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在定义配置时，指定最新的稳定 API 版本。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置文件在推送到集群之前先保住到版本控制系统。 这样如果需要可以快速回滚配置变更。 也可以帮助集群重建或恢复。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用 YAML 格式编写配置文件而不是 JSON。 虽然这两个格式在几乎所有场景下都可以互换，但 YAML 对用户更友好。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;当几个对象有关系时将其放在一个文件中。 一个文件会比多个文件更容易管理。相关语法见示例
&lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/guestbook/all-in-one/guestbook-all-in-one.yaml&#34;&gt;guestbook-all-in-one.yaml&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;要知道 &lt;code&gt;kubectl&lt;/code&gt; 命令可以向一个目录调用，例如，可以对一个目录执行 &lt;code&gt;kubectl apply&lt;/code&gt;，表示
执行其中的所有配置文件&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;请不要指定不必要的默认值: 简单，最小化配置，通常可以减少错误的产生。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将对象描述加到注解中，让其可以对用户或自动化工具更友好。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## &#34;Naked&#34; Pods versus ReplicaSets, Deployments, and Jobs {#naked-pods-vs-replicasets-deployments-and-jobs}

- Don&#39;t use naked Pods (that is, Pods not bound to a [ReplicaSet](/docs/concepts/workloads/controllers/replicaset/) or [Deployment](/docs/concepts/workloads/controllers/deployment/)) if you can avoid it. Naked Pods will not be rescheduled in the event of a node failure.

  A Deployment, which both creates a ReplicaSet to ensure that the desired number of Pods is always available, and specifies a strategy to replace Pods (such as [RollingUpdate](/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment)), is almost always preferable to creating Pods directly, except for some explicit [`restartPolicy: Never`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) scenarios. A [Job](/docs/concepts/workloads/controllers/job/) may also be appropriate.
--&gt;
&lt;h2 id=&#34;naked-pods-vs-replicasets-deployments-and-jobs&#34;&gt;&amp;ldquo;裸奔&amp;quot;的 Pod VS ReplicaSet, Deployment, Job&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;如果能避免，尽量不要让 Pod 裸奔(也就是没有与
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/replicaset/&#34;&gt;ReplicaSet&lt;/a&gt;
或
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/deployment/&#34;&gt;Deployment&lt;/a&gt;
绑定的 Pod
)。
裸奔的 Pod 在节点挂掉之后不会重新调度。&lt;/p&gt;
&lt;p&gt;面 Deployment， 在创建一个 ReplicaSet 来保证期望个数的 Pod 始终可用外，还可以指定替换策略
(如
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment&#34;&gt;RollingUpdate&lt;/a&gt;
)，
除了一个具体场景
(&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy&#34;&gt;&lt;code&gt;restartPolicy: Never&lt;/code&gt;&lt;/a&gt;)
都推荐使用 Deployment 而不是直接创建 Pod。
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/job/&#34;&gt;Job&lt;/a&gt; 也是另一种选择。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Services

- Create a [Service](/docs/concepts/services-networking/service/) before its corresponding backend workloads (Deployments or ReplicaSets), and before any workloads that need to access it. When Kubernetes starts a container, it provides environment variables pointing to all the Services which were running when the container was started. For example, if a Service named `foo` exists, all containers will get the following variables in their initial environment:

  ```shell
  FOO_SERVICE_HOST=&lt;the host the Service is running on&gt;
  FOO_SERVICE_PORT=&lt;the port the Service is running on&gt;
  ```

  *This does imply an ordering requirement* - any `Service` that a `Pod` wants to access must be created before the `Pod` itself, or else the environment variables will not be populated.  DNS does not have this restriction.

- An optional (though strongly recommended) [cluster add-on](/docs/concepts/cluster-administration/addons/) is a DNS server.  The
DNS server watches the Kubernetes API for new `Services` and creates a set of DNS records for each.  If DNS has been enabled throughout the cluster then all `Pods` should be able to do name resolution of `Services` automatically.

- Don&#39;t specify a `hostPort` for a Pod unless it is absolutely necessary. When you bind a Pod to a `hostPort`, it limits the number of places the Pod can be scheduled, because each &lt;`hostIP`, `hostPort`, `protocol`&gt; combination must be unique. If you don&#39;t specify the `hostIP` and `protocol` explicitly, Kubernetes will use `0.0.0.0` as the default `hostIP` and `TCP` as the default `protocol`.

  If you only need access to the port for debugging purposes, you can use the [apiserver proxy](/docs/tasks/access-application-cluster/access-cluster/#manually-constructing-apiserver-proxy-urls) or [`kubectl port-forward`](/docs/tasks/access-application-cluster/port-forward-access-application-cluster/).

  If you explicitly need to expose a Pod&#39;s port on the node, consider using a [NodePort](/docs/concepts/services-networking/service/#nodeport) Service before resorting to `hostPort`.

- Avoid using `hostNetwork`, for the same reasons as `hostPort`.

- Use [headless Services](/docs/concepts/services-networking/service/#headless-services) (which have a `ClusterIP` of `None`) for easy service discovery when you don&#39;t need `kube-proxy` load balancing.
 --&gt;
&lt;h2 id=&#34;service&#34;&gt;Service&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在对应的后端工作负载(Deployments 或 ReplicaSets) 和任意访问它的工作负载之前创建一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service/&#34;&gt;Service&lt;/a&gt;。
当 k8s 启动一个容器时， 它会在容器启动时生成指向所有正在运行的 Service 的环境变量。
例如， 如果存在一个叫 &lt;code&gt;foo&lt;/code&gt; 的 Service，所有都会有如下初始化环境变量:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;FOO_SERVICE_HOST&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&amp;lt;the host the Service is running on&amp;gt;
FOO_SERVICE_PORT&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&amp;lt;the port the Service is running on&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;em&gt;这就存在一个隐式的启动顺序要求&lt;/em&gt; - 一个 Pod 想要访问的任意 Service 都需要在这个 Pod 创建
之前就要存在，否则对应的环境变量就不会生成。 DNS 就没有这个限制。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一个可选(但强烈推荐)的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/cluster-administration/addons/&#34;&gt;集群插件&lt;/a&gt;
就是 DNS 服务。 DNS 服务会监听 k8s API 中新增的 &lt;code&gt;Services&lt;/code&gt; 并对应创建一系列 DNS 记录。
如果集群中启动了 DNS 则 所有的 Pod 都可以自动通过 DNS 解析访问 &lt;code&gt;Services&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果不必要，请不要为一个 Pod 指定 &lt;code&gt;hostPort&lt;/code&gt;。 当将一个 Pod 绑定到一个 &lt;code&gt;hostPort&lt;/code&gt; 时，
就会限制这个 Pod 能调度的范围， 因为每个 &amp;lt;&lt;code&gt;hostIP&lt;/code&gt;, &lt;code&gt;hostPort&lt;/code&gt;, &lt;code&gt;protocol&lt;/code&gt;&amp;gt; 的组合在
全集群内是唯一的。 如果没有显示地指定 &lt;code&gt;hostIP&lt;/code&gt; 和 &lt;code&gt;protocol&lt;/code&gt;，k8s 全使用 &lt;code&gt;0.0.0.0&lt;/code&gt;
作为默认的 &lt;code&gt;hostIP&lt;/code&gt;， &lt;code&gt;TCP&lt;/code&gt; 作为默认的 &lt;code&gt;protocol&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;如果只为调试目的需要访问端口，可以使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/access-application-cluster/access-cluster/#manually-constructing-apiserver-proxy-urls&#34;&gt;apiserver proxy&lt;/a&gt;
或
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/access-application-cluster/port-forward-access-application-cluster/&#34;&gt;&lt;code&gt;kubectl port-forward&lt;/code&gt;&lt;/a&gt;.
如果确实需要将 Pod 端口在节点上显露，在使用 &lt;code&gt;hostPort&lt;/code&gt; 之前考虑使用 &lt;code&gt;hostPort&lt;/code&gt;的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/service/#nodeport&#34;&gt;NodePort&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;与 &lt;code&gt;hostPort&lt;/code&gt; 的理由一样，避免使用 &lt;code&gt;hostNetwork&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在不需要 &lt;code&gt;kube-proxy&lt;/code&gt; 负载均衡的情况下，使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/service/#headless-services&#34;&gt;headless Services&lt;/a&gt;
(就是将 &lt;code&gt;ClusterIP&lt;/code&gt; 设置为 &lt;code&gt;None&lt;/code&gt;)
来实现简单的服务发现&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Using Labels

- Define and use [labels](/docs/concepts/overview/working-with-objects/labels/) that identify __semantic attributes__ of your application or Deployment, such as `{ app: myapp, tier: frontend, phase: test, deployment: v3 }`. You can use these labels to select the appropriate Pods for other resources; for example, a Service that selects all `tier: frontend` Pods, or all `phase: test` components of `app: myapp`. See the [guestbook](https://github.com/kubernetes/examples/tree/master/guestbook/) app for examples of this approach.

A Service can be made to span multiple Deployments by omitting release-specific labels from its selector. [Deployments](/docs/concepts/workloads/controllers/deployment/) make it easy to update a running service without downtime.

A desired state of an object is described by a Deployment, and if changes to that spec are _applied_, the deployment controller changes the actual state to the desired state at a controlled rate.

- You can manipulate labels for debugging. Because Kubernetes controllers (such as ReplicaSet) and Services match to Pods using selector labels, removing the relevant labels from a Pod will stop it from being considered by a controller or from being served traffic by a Service. If you remove the labels of an existing Pod, its controller will create a new Pod to take its place. This is a useful way to debug a previously &#34;live&#34; Pod in a &#34;quarantine&#34; environment. To interactively remove or add labels, use [`kubectl label`](/docs/reference/generated/kubectl/kubectl-commands#label).
 --&gt;
&lt;h2 id=&#34;使用标签&#34;&gt;使用标签&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;为应用或 Deployment 定义和使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/labels/&#34;&gt;标签&lt;/a&gt;
来设置 &lt;strong&gt;有语义的属性&lt;/strong&gt;, 如
&lt;code&gt;{ app: myapp, tier: frontend, phase: test, deployment: v3 }&lt;/code&gt;，
这样其它的资源就可以通过这些标签来选择合适的 Pod; 例如， 一个 Service 可以选择所有
&lt;code&gt;tier: frontend&lt;/code&gt; 的 Pod，或者 &lt;code&gt;app: myapp&lt;/code&gt; 的所有 &lt;code&gt;phase: test&lt;/code&gt; 组件。
更多使用方式见示例
&lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/guestbook/&#34;&gt;guestbook&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一个 Service 可以在选择器中省略与发布版本相关的标签，这样就可以横跨多个版本。这样
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/deployment/&#34;&gt;Deployments&lt;/a&gt;
就可以很容易地在不停止提供服务的情况下实现服务的更新&lt;/p&gt;
&lt;p&gt;一个对象的期望状态是以一个 Deployment 来描述的， 如果它的配置的变更被_执行_，则 Deployment
控制器会以一个控制频率将实际状态变更到与期望状态相同。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用户可以可能操作标签来方便调试。 因为 k8s 控制器(如 ReplicaSet) 和 Service 使用选择器标签
来匹配 Pod， 移除相应的标签就可以认为是将其从一个控制器中移出或从一个 Service 服务后端移出。
当移除一个现有 Pod 的标签后，它的控制器会创建一个新的 Pod 来代替它的位置。 这是一种在隔离环境中调试之前
存活 Pod 的有效方式。 而要移除或添加标签，使用
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#label&#34;&gt;&lt;code&gt;kubectl label&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Container Images

The [imagePullPolicy](/docs/concepts/containers/images/#updating-images) and the tag of the image affect when the [kubelet](/docs/reference/command-line-tools-reference/kubelet/) attempts to pull the specified image.

- `imagePullPolicy: IfNotPresent`: the image is pulled only if it is not already present locally.

- `imagePullPolicy: Always`: every time the kubelet launches a container, the kubelet queries the container image registry to resolve the name to an image digest. If the kubelet has a container image with that exact digest cached locally, the kubelet uses its cached image; otherwise, the kubelet downloads (pulls) the image with the resolved digest, and uses that image to launch the container.

- `imagePullPolicy` is omitted and either the image tag is `:latest` or it is omitted: `Always` is applied.

- `imagePullPolicy` is omitted and the image tag is present but not `:latest`: `IfNotPresent` is applied.

- `imagePullPolicy: Never`: the image is assumed to exist locally. No attempt is made to pull the image.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; To make sure the container always uses the same version of the image, you can specify its &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier&#34;&gt;digest&lt;/a&gt;; replace &lt;code&gt;&amp;lt;image-name&amp;gt;:&amp;lt;tag&amp;gt;&lt;/code&gt; with &lt;code&gt;&amp;lt;image-name&amp;gt;@&amp;lt;digest&amp;gt;&lt;/code&gt; (for example, &lt;code&gt;image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2&lt;/code&gt;). The digest uniquely identifies a specific version of the image, so it is never updated by Kubernetes unless you change the digest value.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You should avoid using the &lt;code&gt;:latest&lt;/code&gt; tag when deploying containers in production as it is harder to track which version of the image is running and more difficult to roll back properly.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The caching semantics of the underlying image provider make even &lt;code&gt;imagePullPolicy: Always&lt;/code&gt; efficient. With Docker, for example, if the image already exists, the pull attempt is fast because all image layers are cached and no image download is needed.&lt;/div&gt;
&lt;/blockquote&gt;

--&gt;
&lt;h2 id=&#34;容器镜像&#34;&gt;容器镜像&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/containers/images/#updating-images&#34;&gt;imagePullPolicy&lt;/a&gt; 和镜像和标签会影响
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/&#34;&gt;kubelet&lt;/a&gt;
拉取指定的镜像。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;imagePullPolicy: IfNotPresent&lt;/code&gt;: 镜像只在本地不存在时才会拉取。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;imagePullPolicy: Always&lt;/code&gt;: 每次 kubelet 启动一个容器时， kubelet 查询容器镜像仓库将名称解析为镜像摘要。
如果 kubelet 发现本地缓存中有摘要完全相同的容器镜像时， kubelet 会使用缓存的镜像。否则，
kubelet 使用解析的摘要下载(拉取)镜像，并使用这个镜像来启动容器。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;imagePullPolicy&lt;/code&gt; 如果该字段未设置而 镜像标签为 &lt;code&gt;:latest&lt;/code&gt;，或没有标签 则会被忽略或者如果未设置值，则默认执行 &lt;code&gt;Always&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;imagePullPolicy&lt;/code&gt; 如果该字段未设置而镜像设置了标签并且不是 &lt;code&gt;:latest&lt;/code&gt;，则该字段会应用为 &lt;code&gt;IfNotPresent&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;imagePullPolicy: Never&lt;/code&gt;: 假定镜像是本地存在。 不会尝试拉取镜像&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 要保证容器即便使用同一个版本的镜像，可以使用
&lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier&#34;&gt;摘要&lt;/a&gt;;
用 &lt;code&gt;&amp;lt;image-name&amp;gt;@&amp;lt;digest&amp;gt;&lt;/code&gt;(例如，&lt;code&gt;image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2&lt;/code&gt;)
代替 &lt;code&gt;&amp;lt;image-name&amp;gt;:&amp;lt;tag&amp;gt;&lt;/code&gt;。 摘要可以唯一标识指定版本的镜像， 所以在不修改摘要的情况，k8s
不会改掉的这个镜像。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 在生产环境中应当避免使用 &lt;code&gt;:latest&lt;/code&gt; 标签，因为它会使得很难追踪正在运行的是哪个镜像，更难以实现
回滚。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 底层镜像供应者的缓存主义使得即便 &lt;code&gt;imagePullPolicy: Always&lt;/code&gt; 也是高效的。 例如，Docker
如果镜像已经存在， 摘取尝试会很快，因为镜像的所有层都已经缓存并不需要下载。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Using kubectl

- Use `kubectl apply -f &lt;directory&gt;`. This looks for Kubernetes configuration in all `.yaml`, `.yml`, and `.json` files in `&lt;directory&gt;` and passes it to `apply`.

- Use label selectors for `get` and `delete` operations instead of specific object names. See the sections on [label selectors](/docs/concepts/overview/working-with-objects/labels/#label-selectors) and [using labels effectively](/docs/concepts/cluster-administration/manage-deployment/#using-labels-effectively).

- Use `kubectl create deployment` and `kubectl expose` to quickly create single-container Deployments and Services. See [Use a Service to Access an Application in a Cluster](/docs/tasks/access-application-cluster/service-access-application-cluster/) for an example.
 --&gt;
&lt;h2 id=&#34;使用-kubectl&#34;&gt;使用 kubectl&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;使用&lt;code&gt;kubectl apply -f &amp;lt;directory&amp;gt;&lt;/code&gt;. 这会查找 k8s 配置，包含 &lt;code&gt;&amp;lt;directory&amp;gt;&lt;/code&gt; 中的
&lt;code&gt;.yaml&lt;/code&gt;, &lt;code&gt;.yml&lt;/code&gt;, &lt;code&gt;.json&lt;/code&gt; 文件，并作为 &lt;code&gt;apply&lt;/code&gt; 的参数&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用标签选择器作为 &lt;code&gt;get&lt;/code&gt; 和 &lt;code&gt;delete&lt;/code&gt; 操作的选择条件而不指定对象名称。 见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/labels/#label-selectors&#34;&gt;标签选择器&lt;/a&gt;
和
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/cluster-administration/manage-deployment/#using-labels-effectively&#34;&gt;高效使用标签&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用 &lt;code&gt;kubectl create deployment&lt;/code&gt; 和 &lt;code&gt;kubectl expose&lt;/code&gt; 快速创建单容器的 Deployment 和 Service
示例见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/access-application-cluster/service-access-application-cluster/&#34;&gt;使用 Service 访问集群中的应用&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Service Topology</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service-topology/</link>
      <pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service-topology/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- johnbelamaric
- imroc
title: Service Topology
feature:
  title: Service Topology
  description: &gt;
    Routing of service traffic based upon cluster topology.

content_type: concept
weight: 10
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [alpha]&lt;/code&gt;
&lt;/div&gt;



_Service Topology_ enables a service to route traffic based upon the Node
topology of the cluster. For example, a service can specify that traffic be
preferentially routed to endpoints that are on the same Node as the client, or
in the same availability zone.
 --&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;em&gt;Service Topology&lt;/em&gt; 让 Service 可以根据集群中节点的拓扑结构来路由流量。 例如， 一个 Service
可以配置为 流量优先路由到与客户端相同的节点或在同一个可用区的 Endpoint.&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Introduction

By default, traffic sent to a `ClusterIP` or `NodePort` Service may be routed to
any backend address for the Service. Since Kubernetes 1.7 it has been possible
to route &#34;external&#34; traffic to the Pods running on the Node that received the
traffic, but this is not supported for `ClusterIP` Services, and more complex
topologies &amp;mdash; such as routing zonally &amp;mdash; have not been possible. The
_Service Topology_ feature resolves this by allowing the Service creator to
define a policy for routing traffic based upon the Node labels for the
originating and destination Nodes.

By using Node label matching between the source and destination, the operator
may designate groups of Nodes that are &#34;closer&#34; and &#34;farther&#34; from one another,
using whatever metric makes sense for that operator&#39;s requirements. For many
operators in public clouds, for example, there is a preference to keep service
traffic within the same zone, because interzonal traffic has a cost associated
with it, while intrazonal traffic does not. Other common needs include being able
to route traffic to a local Pod managed by a DaemonSet, or keeping traffic to
Nodes connected to the same top-of-rack switch for the lowest latency.
 --&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;默认情况下， 发送到 Service  &lt;code&gt;ClusterIP&lt;/code&gt; 或 &lt;code&gt;NodePort&lt;/code&gt; 的流量会路由到 Service 的任意一个
后端实例地址。 从 k8s 1.7 开始让外部流量路由到接收到流量的节点上运行的 Pod 上成功可能。
但是这不支持 &lt;code&gt;ClusterIP&lt;/code&gt; 类型的 Service。 更复杂的拓扑结构  — 例如根据分区路由 —
也不可能。 &lt;em&gt;Service Topology&lt;/em&gt;  提供了让 Service 创建者基于流量源和目标节点标签定义一个流量路由策略来解决定个问题&lt;/p&gt;
&lt;p&gt;通过比对源和目标节点的标签， 使用一些必要的度量，就可以推断出这一组节点相对于另一组节点是更近还是更远，
许多在公有云的操作器，例如, 优先将 Service 的流量保持在同一个区里面， 因为一般在公有云跨区流量是
收费的，但区内流量是免费的。其它常见的包括能够路由流量到一个由 DaemonSet 管理的本地 Pod， 或
保持流量在同一个架顶式交换机以达到较低的延迟。&lt;/p&gt;
&lt;!--
## Using Service Topology

If your cluster has Service Topology enabled, you can control Service traffic
routing by specifying the `topologyKeys` field on the Service spec. This field
is a preference-order list of Node labels which will be used to sort endpoints
when accessing this Service. Traffic will be directed to a Node whose value for
the first label matches the originating Node&#39;s value for that label. If there is
no backend for the Service on a matching Node, then the second label will be
considered, and so forth, until no labels remain.

If no match is found, the traffic will be rejected, just as if there were no
backends for the Service at all. That is, endpoints are chosen based on the first
topology key with available backends. If this field is specified and all entries
have no backends that match the topology of the client, the service has no
backends for that client and connections should fail. The special value `&#34;*&#34;` may
be used to mean &#34;any topology&#34;. This catch-all value, if used, only makes sense
as the last value in the list.

If `topologyKeys` is not specified or empty, no topology constraints will be applied.

Consider a cluster with Nodes that are labeled with their hostname, zone name,
and region name. Then you can set the `topologyKeys` values of a service to direct
traffic as follows.

* Only to endpoints on the same node, failing if no endpoint exists on the node:
  `[&#34;kubernetes.io/hostname&#34;]`.
* Preferentially to endpoints on the same node, falling back to endpoints in the
  same zone, followed by the same region, and failing otherwise: `[&#34;kubernetes.io/hostname&#34;,
  &#34;topology.kubernetes.io/zone&#34;, &#34;topology.kubernetes.io/region&#34;]`.
  This may be useful, for example, in cases where data locality is critical.
* Preferentially to the same zone, but fallback on any available endpoint if
  none are available within this zone:
  `[&#34;topology.kubernetes.io/zone&#34;, &#34;*&#34;]`.

 --&gt;
&lt;h2 id=&#34;使用-service-topology&#34;&gt;使用 Service Topology&lt;/h2&gt;
&lt;p&gt;如果集群启用了 Service Topology， 可以通过 Service 配置的 &lt;code&gt;topologyKeys&lt;/code&gt; 字段来控制
Service 流量路由方式。 这个字段是一个用于在访问该 Service 时对 Endpoint 排序的一个节点标签的
优先顺序列表。 流量会优先转发到第一个标签与源节点同一个标签有相同值的节点。 如果 Service 后端
的所有节点都没有匹配到，就会尝试使用下一个，直至所有标签都试完。&lt;/p&gt;
&lt;p&gt;如果最终一个匹配都没有找到，则这个流量就会被拒绝，就像 Service 根本就没有后端一样。 也就是说，
Endpoint 是基于第一个有可用后端的拓扑键来选择的。 如果设置了这个字段，但所以有条目都没有
与客户端的条目相匹配的，则 Service 便就有针对该客户端的后端，连接就会失败。 有一个特殊的值可以
用来表示 &amp;ldquo;任意拓扑&amp;rdquo;。 这是一个匹配所有的值，如果要使用，只有作为列表的最后一个值才有意义。&lt;/p&gt;
&lt;p&gt;如果 &lt;code&gt;topologyKeys&lt;/code&gt; 没有设置或值为空，则不会应用任何拓扑约束。&lt;/p&gt;
&lt;p&gt;假定有一个集群中的节点都打上它们的主机名，分区名，地区名。则可以设置 &lt;code&gt;topologyKeys&lt;/code&gt; 如果值来转发流量&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[&amp;quot;kubernetes.io/hostname&amp;quot;]&lt;/code&gt;: 只匹配同一个节点上的 Endpoint, 如果节点上没有对应的 Endpoint 则失败&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;优先使用同节点上匹配的 Endpoint, 如果没则使用同分区匹配的 Endpoint, 要是还没有则使用同地区
匹配的 Endpoint, 最后都没匹配则失败。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;[&amp;quot;topology.kubernetes.io/zone&amp;quot;, &amp;quot;*&amp;quot;]&lt;/code&gt;: 优先使用同分区匹配的 Endpoint，
如果没则任意该 Service 的可用 Endpoint 都可以。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Constraints

* Service topology is not compatible with `externalTrafficPolicy=Local`, and
  therefore a Service cannot use both of these features. It is possible to use
  both features in the same cluster on different Services, just not on the same
  Service.

* Valid topology keys are currently limited to `kubernetes.io/hostname`,
  `topology.kubernetes.io/zone`, and `topology.kubernetes.io/region`, but will
  be generalized to other node labels in the future.

* Topology keys must be valid label keys and at most 16 keys may be specified.

* The catch-all value, `&#34;*&#34;`, must be the last value in the topology keys, if
  it is used.
 --&gt;
&lt;h2 id=&#34;限制&#34;&gt;限制&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Service Topology 与 &lt;code&gt;externalTrafficPolicy=Local&lt;/code&gt; 不兼容，因此同一个 Serice
不能同时使用这两个特性。 但可以在同一个集群中的不同的 Service 中分别使用一个特性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;目前有效的键只有 &lt;code&gt;kubernetes.io/hostname&lt;/code&gt;, &lt;code&gt;topology.kubernetes.io/zone&lt;/code&gt;,
&lt;code&gt;topology.kubernetes.io/region&lt;/code&gt;，未来可能包含其它的节点标签&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Topology 必须是有效的标签键，最多只能有 16 个键&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;匹配所有的 &lt;code&gt;&amp;quot;*&amp;quot;&lt;/code&gt;，如果要用，只能用作最后一个键&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Examples

The following are common examples of using the Service Topology feature.

### Only Node Local Endpoints

A Service that only routes to node local endpoints. If no endpoints exist on the node, traffic is dropped:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  topologyKeys:
    - &#34;kubernetes.io/hostname&#34;
```
 --&gt;
&lt;h2 id=&#34;示例&#34;&gt;示例&lt;/h2&gt;
&lt;p&gt;The following are common examples of using the Service Topology feature.
以下为 Service Topology 特性常见用法的示例&lt;/p&gt;
&lt;h3 id=&#34;仅限当前节点的-endpoint&#34;&gt;仅限当前节点的 Endpoint&lt;/h3&gt;
&lt;p&gt;这个 Service 只路由到本节点的 Endpoint， 如果本节点没有匹配的 Endpoint，流量被丢弃:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-app&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;topologyKeys&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/hostname&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Prefer Node Local Endpoints

A Service that prefers node local Endpoints but falls back to cluster wide endpoints if node local endpoints do not exist:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  topologyKeys:
    - &#34;kubernetes.io/hostname&#34;
    - &#34;*&#34;
```
 --&gt;
&lt;h3 id=&#34;优先使用本节点的-endpoint&#34;&gt;优先使用本节点的 Endpoint&lt;/h3&gt;
&lt;p&gt;这个 Service 优先使用本节点匹配的 Endpoint，如果没则使用集群中可用的 Endpoint:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-app&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;topologyKeys&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/hostname&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Only Zonal or Regional Endpoints

A Service that prefers zonal then regional endpoints. If no endpoints exist in either, traffic is dropped.


```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  topologyKeys:
    - &#34;topology.kubernetes.io/zone&#34;
    - &#34;topology.kubernetes.io/region&#34;
```
--&gt;
&lt;h3 id=&#34;仅限本分区或本地区的-endpoint&#34;&gt;仅限本分区或本地区的 Endpoint&lt;/h3&gt;
&lt;p&gt;这个 Service 优先使用本分区匹配的 Endpoint, 要是没有则使用同地区匹配的 Endpoint, 要是还没有就丢弃流量。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-app&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;topologyKeys&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;topology.kubernetes.io/zone&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;topology.kubernetes.io/region&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Prefer Node Local, Zonal, then Regional Endpoints

A Service that prefers node local, zonal, then regional endpoints but falls back to cluster wide endpoints.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  topologyKeys:
    - &#34;kubernetes.io/hostname&#34;
    - &#34;topology.kubernetes.io/zone&#34;
    - &#34;topology.kubernetes.io/region&#34;
    - &#34;*&#34;
```
 --&gt;
&lt;h3 id=&#34;本节点本分区本地区依次优先&#34;&gt;本节点，本分区，本地区依次优先&lt;/h3&gt;
&lt;p&gt;这个 Service 按照 本节点，本分区，本地区 的顺序依次匹配，如果三个都没有匹配到，则匹配全集群的 Endpoint&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-app&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;topologyKeys&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/hostname&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;topology.kubernetes.io/zone&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;topology.kubernetes.io/region&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/administer-cluster/enabling-service-topology&#34;&gt;启用 Service Topology&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/connect-applications-service/&#34;&gt;通过 Service 连接应用&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: ConfigMap</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/configmap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/configmap/</guid>
      <description>
        
        
        &lt;!--
---
title: ConfigMaps
content_type: concept
weight: 20
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--

&lt;p&gt;&lt;p&gt;A ConfigMap is 一个以键值对形式存储非敏感数据的 API 对象。
Pod 可以在环境变量，命令行参数或
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;volume&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;
中的配置文件中使用它。&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;ConfigMap 允许用户将环境相关的配置从
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-image&#39; target=&#39;_blank&#39;&gt;镜像(Image)&lt;span class=&#39;tooltip-text&#39;&gt;一个容器的存储实例，其中包含一系列运行应用所需要的软件。&lt;/span&gt;
&lt;/a&gt;
中解耦出来，这样可以很容易地实现应用的移植。&lt;/p&gt;

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; ConfigMap does not provide secrecy or encryption.
If the data you want to store are confidential, use a
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;存放如密码, OAuth, ssh 密钥等敏感信息&lt;/span&gt;
&lt;/a&gt; rather than a ConfigMap,
or use additional (third party) tools to keep your data private.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;p&gt;&lt;p&gt;ConfigMap 就是 一个以键值对形式存储非敏感数据的 API 对象。
Pod 可以在环境变量，命令行参数或
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;volume&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;
中的配置文件中使用它。&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;ConfigMap 允许用户将环境相关的配置从
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-image&#39; target=&#39;_blank&#39;&gt;镜像(Image)&lt;span class=&#39;tooltip-text&#39;&gt;一个容器的存储实例，其中包含一系列运行应用所需要的软件。&lt;/span&gt;
&lt;/a&gt;
中解耦出来，这样可以很容易地实现应用的移植。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; ConfigMap 不提供保密性或对其加密。 如果想要存放的保留信息，请使用
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;存放如密码, OAuth, ssh 密钥等敏感信息&lt;/span&gt;
&lt;/a&gt;
而不是 ConfigMap， 或使用外部(第三方)工具来保证数据的保密性。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!-- body --&gt;
&lt;!--
## Motivation

Use a ConfigMap for setting configuration data separately from application code.

For example, imagine that you are developing an application that you can run on your
own computer (for development) and in the cloud (to handle real traffic).
You write the code to look in an environment variable named `DATABASE_HOST`.
Locally, you set that variable to `localhost`. In the cloud, you set it to
refer to a Kubernetes &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;
that exposes the database component to your cluster.
This lets you fetch a container image running in the cloud and
debug the exact same code locally if needed.

A ConfigMap is not designed to hold large chunks of data. The data stored in a
ConfigMap cannot exceed 1 MiB. If you need to store settings that are
larger than this limit, you may want to consider mounting a volume or use a
separate database or file service.
 --&gt;
&lt;h2 id=&#34;动机&#34;&gt;动机&lt;/h2&gt;
&lt;p&gt;使用 ConfigMap 将配置数据与应用代码分离。&lt;/p&gt;
&lt;p&gt;例如，假设开发了一个应用，可以在本地机器上(开发)运行和云环境(处理生产流量)。 可以在代码中读取
一个叫 &lt;code&gt;DATABASE_HOST&lt;/code&gt; 环境变量。 在本地，可以将这个变量设置为 &lt;code&gt;localhost&lt;/code&gt;。 在云环境中
可以将它指定 k8s 集群中用于暴露数据库组件的
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;
名称。 如果需要，这会使得用户可以在云环境中拉取容器镜像并运行的与本地调试使用的是同一套代码。&lt;/p&gt;
&lt;p&gt;ConfigMap 并不是设计来存放大块数据。 存放于 ConfigMap 的数据不能超过 1 MiB。 如果需要存储
超过这个限制的配置数据， 可以考虑挂载一个卷，或使用独立的数据库或文件服务。&lt;/p&gt;
&lt;!--
## ConfigMap object

A ConfigMap is an API [object](/docs/concepts/overview/working-with-objects/kubernetes-objects/)
that lets you store configuration for other objects to use. Unlike most
Kubernetes objects that have a `spec`, a ConfigMap has `data` and `binaryData`
fields. These fields accepts key-value pairs as their values.  Both the `data`
field and the `binaryData` are optional. The `data` field is designed to
contain UTF-8 byte sequences while the `binaryData` field is designed to
contain binary data.

The name of a ConfigMap must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

Each key under the `data` or the `binaryData` field must consist of
alphanumeric characters, `-`, `_` or `.`. The keys stored in `data` must not
overlap with the keys in the `binaryData` field.

Starting from v1.19, you can add an `immutable` field to a ConfigMap
definition to create an [immutable ConfigMap](#configmap-immutable).
 --&gt;
&lt;h2 id=&#34;configmap-对象&#34;&gt;ConfigMap 对象&lt;/h2&gt;
&lt;p&gt;ConfigMap 是一个 API
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/kubernetes-objects/&#34;&gt;对象&lt;/a&gt;
, 它可以让用户存放供其它对象使用的配置信息。 与其它大多数据 k8s 对象有 &lt;code&gt;spec&lt;/code&gt; 不同， ConfigMap
包含的字段有 &lt;code&gt;data&lt;/code&gt; 和 &lt;code&gt;binaryData&lt;/code&gt;。 这些字段接受键值对作为他们的值。 &lt;code&gt;data&lt;/code&gt; 字段和
&lt;code&gt;binaryData&lt;/code&gt; 字段都是可选的。 &lt;code&gt;data&lt;/code&gt; 字段设计上是用来存储 UTF-8 字节顺序的而 &lt;code&gt;binaryData&lt;/code&gt;
字段设计上是用来存储二进制数据的。&lt;/p&gt;
&lt;p&gt;ConfigMap 的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;data&lt;/code&gt; 或 &lt;code&gt;binaryData&lt;/code&gt; 字段下的键必须由字母数字，&lt;code&gt;-&lt;/code&gt;, &lt;code&gt;_&lt;/code&gt; 或 &lt;code&gt;.&lt;/code&gt; 组成。 &lt;code&gt;data&lt;/code&gt; 存放的
键与 &lt;code&gt;binaryData&lt;/code&gt; 字段下面的键不能重复。&lt;/p&gt;
&lt;p&gt;从 k8s  v1.19 开始，可以在 ConfigMap 中添加 &lt;code&gt;immutable&lt;/code&gt; 字段，定义一个
&lt;a href=&#34;#configmap-immutable&#34;&gt;不可变的 ConfigMap&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## ConfigMaps and Pods

You can write a Pod `spec` that refers to a ConfigMap and configures the container(s)
in that Pod based on the data in the ConfigMap. The Pod and the ConfigMap must be in
the same &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/namespaces&#39; target=&#39;_blank&#39;&gt;namespace&lt;span class=&#39;tooltip-text&#39;&gt;一个用于在同一个物理集群中支持多个虚拟集群的抽象概念&lt;/span&gt;
&lt;/a&gt;.

Here&#39;s an example ConfigMap that has some keys with single values,
and other keys where the value looks like a fragment of a configuration
format.

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: game-demo
data:
  # property-like keys; each key maps to a simple value
  player_initial_lives: &#34;3&#34;
  ui_properties_file_name: &#34;user-interface.properties&#34;

  # file-like keys
  game.properties: |
    enemy.types=aliens,monsters
    player.maximum-lives=5
  user-interface.properties: |
    color.good=purple
    color.bad=yellow
    allow.textmode=true
```

There are four different ways that you can use a ConfigMap to configure
a container inside a Pod:

1. Inside a container command and args
1. Environment variables for a container
1. Add a file in read-only volume, for the application to read
1. Write code to run inside the Pod that uses the Kubernetes API to read a ConfigMap

These different methods lend themselves to different ways of modeling
the data being consumed.
For the first three methods, the
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt; uses the data from
the ConfigMap when it launches container(s) for a Pod.

The fourth method means you have to write code to read the ConfigMap and its data.
However, because you&#39;re using the Kubernetes API directly, your application can
subscribe to get updates whenever the ConfigMap changes, and react
when that happens. By accessing the Kubernetes API directly, this
technique also lets you access a ConfigMap in a different namespace.

Here&#39;s an example Pod that uses values from `game-demo` to configure a Pod:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: configmap-demo-pod
spec:
  containers:
    - name: demo
      image: alpine
      command: [&#34;sleep&#34;, &#34;3600&#34;]
      env:
        # Define the environment variable
        - name: PLAYER_INITIAL_LIVES # Notice that the case is different here
                                     # from the key name in the ConfigMap.
          valueFrom:
            configMapKeyRef:
              name: game-demo           # The ConfigMap this value comes from.
              key: player_initial_lives # The key to fetch.
        - name: UI_PROPERTIES_FILE_NAME
          valueFrom:
            configMapKeyRef:
              name: game-demo
              key: ui_properties_file_name
      volumeMounts:
      - name: config
        mountPath: &#34;/config&#34;
        readOnly: true
  volumes:
    # You set volumes at the Pod level, then mount them into containers inside that Pod
    - name: config
      configMap:
        # Provide the name of the ConfigMap you want to mount.
        name: game-demo
        # An array of keys from the ConfigMap to create as files
        items:
        - key: &#34;game.properties&#34;
          path: &#34;game.properties&#34;
        - key: &#34;user-interface.properties&#34;
          path: &#34;user-interface.properties&#34;
```

A ConfigMap doesn&#39;t differentiate between single line property values and
multi-line file-like values.
What matters is how Pods and other objects consume those values.

For this example, defining a volume and mounting it inside the `demo`
container as `/config` creates two files,
`/config/game.properties` and `/config/user-interface.properties`,
even though there are four keys in the ConfigMap. This is because the Pod
definition specifies an `items` array in the `volumes` section.
If you omit the `items` array entirely, every key  in the ConfigMap becomes
a file with the same name as the key, and you get 4 files.
--&gt;
&lt;h2 id=&#34;configmap-和-pod&#34;&gt;ConfigMap 和 Pod&lt;/h2&gt;
&lt;p&gt;用户可以在编写一个 Pod &lt;code&gt;spec&lt;/code&gt; 中引用一个 ConfigMap 并基于这个 ConfigMap 中的数据来配置这个
Pod 中的容器。 Pod 和 ConfigMap 必须在同一个
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/namespaces&#39; target=&#39;_blank&#39;&gt;命名空间(namespace)&lt;span class=&#39;tooltip-text&#39;&gt;一个用于在同一个物理集群中支持多个虚拟集群的抽象概念&lt;/span&gt;
&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;以下为一个 ConfigMap 的示例，其中包含几个有一个值的键和其它值看起来像是配置格式的片断的键&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ConfigMap&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;game-demo&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#75715e&#34;&gt;# 属性格式的键; 每个键与一个值相映射&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;player_initial_lives&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ui_properties_file_name&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;user-interface.properties&amp;#34;&lt;/span&gt;

  &lt;span style=&#34;color:#75715e&#34;&gt;# 文件格式的键&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;game.properties&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;|
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    enemy.types=aliens,monsters
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    player.maximum-lives=5
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  user-interface.properties: |
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    color.good=purple
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    color.bad=yellow
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    allow.textmode=true
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;有以下四种方式可以使用 ConfigMap 配置 Pod 中的容器:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在一个容器中的命令和参数&lt;/li&gt;
&lt;li&gt;容器中的环境变量&lt;/li&gt;
&lt;li&gt;添加一个文件到一个只读卷，用于应用读取&lt;/li&gt;
&lt;li&gt;在 Pod 中写代码使用 k8s API 来读取 ConfigMap&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些不同的方式使得它们可以不用模式供给数据。
对于前三种方式
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt; 在为 Pod 启动容器时
使用 ConfigMap 中的数据。&lt;/p&gt;
&lt;p&gt;第四种方式意味着需要用户写代理来读取 ConfigMap 和其中的数据。 但因为直接使用的是 k8s API,
应用可以通过订阅来获得 ConfigMap 变更时的更新， 并在其发生时做出反映。 通过直接访问 k8s API,
这种方式也让用户能够访问其它命名空间的 ConfigMap。&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an example Pod that uses values from &lt;code&gt;game-demo&lt;/code&gt; to configure a Pod:
以下示例中的 Pod 使用 &lt;code&gt;game-demo&lt;/code&gt;  中的值来配置一个这个 Pod&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;configmap-demo-pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;demo&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;alpine&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sleep&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3600&amp;#34;&lt;/span&gt;]
      &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
        &lt;span style=&#34;color:#75715e&#34;&gt;# 定义环境变量&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PLAYER_INITIAL_LIVES # 注意这个键与 ConfigMap 键名称是不一样的&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;configMapKeyRef&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;game-demo           # 引用 ConfigMap 的名称&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;player_initial_lives # 使用其中的键&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;UI_PROPERTIES_FILE_NAME&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;configMapKeyRef&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;game-demo&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ui_properties_file_name&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;config&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/config&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
    &lt;span style=&#34;color:#75715e&#34;&gt;# 可以在 Pod 级别设置卷， 然后将它们挂载到容器&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;config&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;configMap&lt;/span&gt;:
        &lt;span style=&#34;color:#75715e&#34;&gt;# 想要挂载的 ConfigMap 的名称&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;game-demo&lt;/span&gt;
        &lt;span style=&#34;color:#75715e&#34;&gt;# ConfigMap 中用来创建文件的键的列表&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;items&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;game.properties&amp;#34;&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;game.properties&amp;#34;&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;user-interface.properties&amp;#34;&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;user-interface.properties&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;ConfigMap 是的单行属性值和多行的类似文件的值是没有不同的。重要的是 Pod 和其它对象是怎么使用这些值的。&lt;/p&gt;
&lt;p&gt;对于这个示例， 定义一个卷并将其挂载到 &lt;code&gt;demo&lt;/code&gt; 容器中的 &lt;code&gt;/config&lt;/code&gt; 目录，并创建两个文件，
&lt;code&gt;/config/game.properties&lt;/code&gt; 和 &lt;code&gt;/config/user-interface.properties&lt;/code&gt;, 即便 ConfigMap
中包含了四个键。 这是因为 Pod 中的 &lt;code&gt;volumes&lt;/code&gt; 区域定义一个 &lt;code&gt;items&lt;/code&gt; 数组。 如果省略了
&lt;code&gt;items&lt;/code&gt; 数组实体， 每个 ConfigMap 中的键就会变成以这个键名称一样的文件，这样就会有四个文件。&lt;/p&gt;
&lt;!--
## Using ConfigMaps

ConfigMaps can be mounted as data volumes. ConfigMaps can also be used by other
parts of the system, without being directly exposed to the Pod. For example,
ConfigMaps can hold data that other parts of the system should use for configuration.

The most common way to use ConfigMaps is to configure settings for
containers running in a Pod in the same namespace. You can also use a
ConfigMap separately.

For example, you
might encounter &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/cluster-administration/addons/&#39; target=&#39;_blank&#39;&gt;addons&lt;span class=&#39;tooltip-text&#39;&gt;扩展 k8s 功能的资源&lt;/span&gt;
&lt;/a&gt;
or &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/extend-kubernetes/operator/&#39; target=&#39;_blank&#39;&gt;operators&lt;span class=&#39;tooltip-text&#39;&gt;A specialized controller used to manage a custom resource 一个用于管理自定义资源的专用控制器&lt;/span&gt;
&lt;/a&gt; that
adjust their behavior based on a ConfigMap.
 --&gt;
&lt;h2 id=&#34;使用-configmap&#34;&gt;使用 ConfigMap&lt;/h2&gt;
&lt;p&gt;ConfigMap 可以挂载为数据卷。 ConfigMap 也可以在不直接暴露给 Pod 的情况下被系统的其它部分使用。
例如， ConfigMap 可以包含系统其它部分用于檲的数据。&lt;/p&gt;
&lt;p&gt;ConfigMap 最常用的一种方式就为在同一个命名空间中的 Pod 中运行的容器提供配置。 也可以单独使用
ConfigMap&lt;/p&gt;
&lt;p&gt;例如，也可能遇到
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/cluster-administration/addons/&#39; target=&#39;_blank&#39;&gt;addons&lt;span class=&#39;tooltip-text&#39;&gt;扩展 k8s 功能的资源&lt;/span&gt;
&lt;/a&gt;
和
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/extend-kubernetes/operator/&#39; target=&#39;_blank&#39;&gt;operators&lt;span class=&#39;tooltip-text&#39;&gt;A specialized controller used to manage a custom resource 一个用于管理自定义资源的专用控制器&lt;/span&gt;
&lt;/a&gt;
基于 ConfigMap 来调整他们的行为。&lt;/p&gt;
&lt;!--
### Using ConfigMaps as files from a Pod

To consume a ConfigMap in a volume in a Pod:

1. Create a ConfigMap or use an existing one. Multiple Pods can reference the
   same ConfigMap.
1. Modify your Pod definition to add a volume under `.spec.volumes[]`. Name
   the volume anything, and have a `.spec.volumes[].configMap.name` field set
   to reference your ConfigMap object.
1. Add a `.spec.containers[].volumeMounts[]` to each container that needs the
   ConfigMap. Specify `.spec.containers[].volumeMounts[].readOnly = true` and
   `.spec.containers[].volumeMounts[].mountPath` to an unused directory name
   where you would like the ConfigMap to appear.
1. Modify your image or command line so that the program looks for files in
   that directory. Each key in the ConfigMap `data` map becomes the filename
   under `mountPath`.

This is an example of a Pod that mounts a ConfigMap in a volume:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: &#34;/etc/foo&#34;
      readOnly: true
  volumes:
  - name: foo
    configMap:
      name: myconfigmap
```

Each ConfigMap you want to use needs to be referred to in `.spec.volumes`.

If there are multiple containers in the Pod, then each container needs its
own `volumeMounts` block, but only one `.spec.volumes` is needed per ConfigMap.
--&gt;
&lt;h3 id=&#34;将-configmap-以文件的方式用到-pod-中&#34;&gt;将 ConfigMap 以文件的方式用到 Pod 中&lt;/h3&gt;
&lt;p&gt;在 Pod 中以卷的方式使用一个 ConfigMap:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;创建一个新的 ConfigMap 或使用一个现有的。 多个 Pod 可以引用同一个 ConfigMap&lt;/li&gt;
&lt;li&gt;修改 Pod 定义，在 &lt;code&gt;.spec.volumes[]&lt;/code&gt; 下面添加一个卷。这卷的名称随便起， 但其中
&lt;code&gt;.spec.volumes[].configMap.name&lt;/code&gt; 字段需要设置引用上一步提到的 ConfigMap 对象&lt;/li&gt;
&lt;li&gt;在每一个需要访问这个 ConfigMap 的容器中添加 &lt;code&gt;.spec.containers[].volumeMounts[]&lt;/code&gt;。
设置 &lt;code&gt;.spec.containers[].volumeMounts[].readOnly = true&lt;/code&gt; 和
将 &lt;code&gt;.spec.containers[].volumeMounts[].mountPath&lt;/code&gt; 指向一个期望的未使用的目录名&lt;/li&gt;
&lt;li&gt;修改镜像或命令让容器中的程序查看目录中的文件。 ConfigMap 中 &lt;code&gt;data&lt;/code&gt; 字典下的每一个键就会
对应 &lt;code&gt;mountPath&lt;/code&gt; 目录中的一个文件名&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下面这个示例中就一个将一个 ConfigMap 挂载为卷的 Pod:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/etc/foo&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;configMap&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myconfigmap&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;每个想要使用的 ConfigMap 都需要在 &lt;code&gt;.spec.volumes&lt;/code&gt; 中被引用。&lt;/p&gt;
&lt;p&gt;如果 Pod 中有多个容器， 每个容器都需要有自己的 &lt;code&gt;volumeMounts&lt;/code&gt; 块， 但每个 ConfigMap 只需要
一个 &lt;code&gt;.spec.volumes&lt;/code&gt;&lt;/p&gt;
&lt;!--
#### Mounted ConfigMaps are updated automatically

When a ConfigMap currently consumed in a volume is updated, projected keys are eventually updated as well.
The kubelet checks whether the mounted ConfigMap is fresh on every periodic sync.
However, the kubelet uses its local cache for getting the current value of the ConfigMap.
The type of the cache is configurable using the `ConfigMapAndSecretChangeDetectionStrategy` field in
the [KubeletConfiguration struct](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go).
A ConfigMap can be either propagated by watch (default), ttl-based, or simply redirecting
all requests directly to the API server.
As a result, the total delay from the moment when the ConfigMap is updated to the moment
when new keys are projected to the Pod can be as long as the kubelet sync period + cache
propagation delay, where the cache propagation delay depends on the chosen cache type
(it equals to watch propagation delay, ttl of cache, or zero correspondingly).

ConfigMaps consumed as environment variables are not updated automatically and require a pod restart.
 --&gt;
&lt;h4 id=&#34;让挂载的-configmap-自动更新&#34;&gt;让挂载的 ConfigMap 自动更新&lt;/h4&gt;
&lt;p&gt;当一个正在被以卷方式使用的 ConfigMap 更新时，与其相映射的键最终也会更新。 kubelet 会在每个
同步周期检查挂载的 ConfigMap 是否更新。 但是， kubelet 会使用本地缓存来获取 ConfigMap 的
当前值。 缓存的类型可以通过
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go&#34;&gt;KubeletConfiguration struct&lt;/a&gt;.
中的 &lt;code&gt;ConfigMapAndSecretChangeDetectionStrategy&lt;/code&gt; 字段来配置。 ConfigMap 的传播方式有
监视(默认)，基于 ttl, 或简单地将所有请求直接重定向给 API server. 最终， 从 ConfigMap 更新
到新的键被投射到 Pod 中的总延时就是 kubelet 同时间隔时长 + 缓存传播延时， 而其中缓存传播延时
又基于缓存的类型(相应地它可能等于 监视传播延时，缓存的 TTL, 或零)。&lt;/p&gt;
&lt;p&gt;通过环境变量引用的 ConfigMap 是不能自动更新的，需要重启 Pod 才行。&lt;/p&gt;
&lt;!--
## Immutable ConfigMaps {#configmap-immutable}






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;



The Kubernetes beta feature _Immutable Secrets and ConfigMaps_ provides an option to set
individual Secrets and ConfigMaps as immutable. For clusters that extensively use ConfigMaps
(at least tens of thousands of unique ConfigMap to Pod mounts), preventing changes to their
data has the following advantages:

- protects you from accidental (or unwanted) updates that could cause applications outages
- improves performance of your cluster by significantly reducing load on kube-apiserver, by
  closing watches for ConfigMaps marked as immutable.

This feature is controlled by the `ImmutableEphemeralVolumes`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/).
You can create an immutable ConfigMap by setting the `immutable` field to `true`.
For example:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  ...
data:
  ...
immutable: true
```

Once a ConfigMap is marked as immutable, it is _not_ possible to revert this change
nor to mutate the contents of the `data` or the `binaryData` field. You can
only delete and recreate the ConfigMap. Because existing Pods maintain a mount point
to the deleted ConfigMap, it is recommended to recreate these pods.
 --&gt;
&lt;h2 id=&#34;configmap-immutable&#34;&gt;不可变 ConfigMap&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;这个 k8s 的 bata 特性 &lt;em&gt;不可变 Secret 和 ConfigMap&lt;/em&gt; 提供了一个可选项，可以让一个 Secret
和 ConfigMap 变为不可变。 对于那些广泛使用 ConfigMap (一个 ConfigMap 至少被 10k Pod 挂载)，
防止修改它们中的数据有以下好处:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;防止误操作(或不想要)的更新可能引发的应用事故&lt;/li&gt;
&lt;li&gt;当 ConfigMap 标记为不可变时会关闭监视，这样能极大地减少 kube-apiserver 的负载，从而改善
集群性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个特性通过设置 &lt;code&gt;ImmutableEphemeralVolumes&lt;/code&gt;
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;
控制。 用户可以在 ConfigMap 中通过设置 &lt;code&gt;immutable&lt;/code&gt; 字段为 &lt;code&gt;true&lt;/code&gt; 让其成功不可变 ConfigMap&lt;/p&gt;
&lt;p&gt;示例：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ConfigMap&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;immutable&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;当一个 ConfigMap 被标记为不可变后，它就 &lt;em&gt;不&lt;/em&gt; 可能再被变会普通(可修改)的了，也不可能再修改其
中 &lt;code&gt;data&lt;/code&gt; 或 &lt;code&gt;binaryData&lt;/code&gt; 字段的值。只能删除或重建 ConfigMap。 因为现有的 Pod 会维持
对已经删除的 ConfigMap 挂载指向， 推荐对这些 Pod 也进行重建。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#34;&gt;Secrets&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/configure-pod-container/configure-pod-configmap/&#34;&gt;配置一个 Pod 使用 ConfigMap&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;阅读 &lt;a href=&#34;https://12factor.net/&#34;&gt;The Twelve-Factor App&lt;/a&gt; 以便理解分离代码和配置的动机&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Pod 安全标准</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/security/pod-security-standards/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/security/pod-security-standards/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- tallclair
title: Pod Security Standards
content_type: concept
weight: 10
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
Security settings for Pods are typically applied by using [security
contexts](/docs/tasks/configure-pod-container/security-context/). Security Contexts allow for the
definition of privilege and access controls on a per-Pod basis.

The enforcement and policy-based definition of cluster requirements of security contexts has
previously been achieved using [Pod Security Policy](/docs/concepts/policy/pod-security-policy/). A
_Pod Security Policy_ is a cluster-level resource that controls security sensitive aspects of the
Pod specification.

However, numerous means of policy enforcement have arisen that augment or replace the use of
PodSecurityPolicy. The intent of this page is to detail recommended Pod security profiles, decoupled
from any specific instantiation.
 --&gt;
&lt;p&gt;对于 Pod 的安全设置通常是使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/configure-pod-container/security-context/&#34;&gt;安全性上下文&lt;/a&gt;.
安全性上下文允许对 Pod 级的权限定义和访问控制。&lt;/p&gt;
&lt;p&gt;实施和基于策略定义的集群安全性上下文要求是通过使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/policy/pod-security-policy/&#34;&gt;Pod 安全性策略&lt;/a&gt;
实现的。 &lt;em&gt;Pod 安全性策略&lt;/em&gt; 是集群级别的资源，用来控制安全性敏感方面的 Pod 定义。&lt;/p&gt;
&lt;p&gt;但是， 多种方式的执行策略会引发争论或替换 PodSecurityPolicy 的使用。 本文的目的是详细推荐
安全策略方案， 与任意指定实例解耦。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Policy Types

There is an immediate need for base policy definitions to broadly cover the security spectrum. These
should range from highly restricted to highly flexible:

- **_Privileged_** - Unrestricted policy, providing the widest possible level of permissions. This
  policy allows for known privilege escalations.
- **_Baseline/Default_** - Minimally restrictive policy while preventing known privilege
  escalations. Allows the default (minimally specified) Pod configuration.
- **_Restricted_** - Heavily restricted policy, following current Pod hardening best practices.
 --&gt;
&lt;h2 id=&#34;policy-types&#34;&gt;策略类别&lt;/h2&gt;
&lt;p&gt;安全策略定义来大体上能覆盖安全相关问题。 下面这些从高限制到高灵活性的范围:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;Privileged&lt;/em&gt;&lt;/strong&gt; - 不受限策略， 提供最大可能级别的权限。 这个策略允许所有已知权限升级。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;Baseline/Default&lt;/em&gt;&lt;/strong&gt; - 在防止已知权限升级的情况下相对最小限制策略。 允许默认(最少配置)的 Pod 配置&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;Restricted&lt;/em&gt;&lt;/strong&gt; - 严重限制策略， 依照目前 Pod 加固最佳实践&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Policies

### Privileged

The Privileged policy is purposely-open, and entirely unrestricted. This type of policy is typically
aimed at system- and infrastructure-level workloads managed by privileged, trusted users.

The privileged policy is defined by an absence of restrictions. For allow-by-default enforcement
mechanisms (such as gatekeeper), the privileged profile may be an absence of applied constraints
rather than an instantiated policy. In contrast, for a deny-by-default mechanism (such as Pod
Security Policy) the privileged policy should enable all controls (disable all restrictions).
 --&gt;
&lt;h2 id=&#34;policies&#34;&gt;策略&lt;/h2&gt;
&lt;h3 id=&#34;privileged&#34;&gt;特权&lt;/h3&gt;
&lt;p&gt;特权策略是以开放为目的的，并且完全没有限制。这个策略类别通常是用于系统级和基础设施级别的工作负载，
它们由特权，受信的用户管理。&lt;/p&gt;
&lt;p&gt;特权策略的定义方式就是没有限制。 对于 默认允许执行机制(如守门人(&lt;code&gt;gatekeeper&lt;/code&gt;)), 特权方案是没有
执行的约束条件而不是一个实际的策略。相反的，在一个 默认拒绝的机制中(如 Pod 安全策略)，特权策略
就应该启用所有控制(关闭所有约束)。&lt;/p&gt;
&lt;!--
### Baseline/Default

The Baseline/Default policy is aimed at ease of adoption for common containerized workloads while
preventing known privilege escalations. This policy is targeted at application operators and
developers of non-critical applications. The following listed controls should be
enforced/disallowed:

&lt;table&gt;
	&lt;caption style=&#34;display:none&#34;&gt;Baseline policy specification&lt;/caption&gt;
	&lt;tbody&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;strong&gt;Control&lt;/strong&gt;&lt;/td&gt;
			&lt;td&gt;&lt;strong&gt;Policy&lt;/strong&gt;&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;Host Namespaces&lt;/td&gt;
			&lt;td&gt;
				Sharing the host namespaces must be disallowed.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Restricted Fields:&lt;/b&gt;&lt;br&gt;
				spec.hostNetwork&lt;br&gt;
				spec.hostPID&lt;br&gt;
				spec.hostIPC&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Allowed Values:&lt;/b&gt; false&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;Privileged Containers&lt;/td&gt;
			&lt;td&gt;
				Privileged Pods disable most security mechanisms and must be disallowed.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Restricted Fields:&lt;/b&gt;&lt;br&gt;
				spec.containers[*].securityContext.privileged&lt;br&gt;
				spec.initContainers[*].securityContext.privileged&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Allowed Values:&lt;/b&gt; false, undefined/nil&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;Capabilities&lt;/td&gt;
			&lt;td&gt;
				Adding additional capabilities beyond the &lt;a href=&#34;https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities&#34;&gt;default set&lt;/a&gt; must be disallowed.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Restricted Fields:&lt;/b&gt;&lt;br&gt;
				spec.containers[*].securityContext.capabilities.add&lt;br&gt;
				spec.initContainers[*].securityContext.capabilities.add&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Allowed Values:&lt;/b&gt; empty (or restricted to a known list)&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;HostPath Volumes&lt;/td&gt;
			&lt;td&gt;
				HostPath volumes must be forbidden.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Restricted Fields:&lt;/b&gt;&lt;br&gt;
				spec.volumes[*].hostPath&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Allowed Values:&lt;/b&gt; undefined/nil&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;Host Ports&lt;/td&gt;
			&lt;td&gt;
				HostPorts should be disallowed, or at minimum restricted to a known list.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Restricted Fields:&lt;/b&gt;&lt;br&gt;
				spec.containers[*].ports[*].hostPort&lt;br&gt;
				spec.initContainers[*].ports[*].hostPort&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Allowed Values:&lt;/b&gt; 0, undefined (or restricted to a known list)&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;AppArmor &lt;em&gt;(optional)&lt;/em&gt;&lt;/td&gt;
			&lt;td&gt;
				On supported hosts, the &#39;runtime/default&#39; AppArmor profile is applied by default. The default policy should prevent overriding or disabling the policy, or restrict overrides to an allowed set of profiles.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Restricted Fields:&lt;/b&gt;&lt;br&gt;
				metadata.annotations[&#39;container.apparmor.security.beta.kubernetes.io/*&#39;]&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Allowed Values:&lt;/b&gt; &#39;runtime/default&#39;, undefined&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;SELinux &lt;em&gt;(optional)&lt;/em&gt;&lt;/td&gt;
			&lt;td&gt;
				Setting custom SELinux options should be disallowed.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Restricted Fields:&lt;/b&gt;&lt;br&gt;
				spec.securityContext.seLinuxOptions&lt;br&gt;
				spec.containers[*].securityContext.seLinuxOptions&lt;br&gt;
				spec.initContainers[*].securityContext.seLinuxOptions&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Allowed Values:&lt;/b&gt; undefined/nil&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;/proc Mount Type&lt;/td&gt;
			&lt;td&gt;
				The default /proc masks are set up to reduce attack surface, and should be required.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Restricted Fields:&lt;/b&gt;&lt;br&gt;
				spec.containers[*].securityContext.procMount&lt;br&gt;
				spec.initContainers[*].securityContext.procMount&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Allowed Values:&lt;/b&gt; undefined/nil, &#39;Default&#39;&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;Sysctls&lt;/td&gt;
			&lt;td&gt;
				Sysctls can disable security mechanisms or affect all containers on a host, and should be disallowed except for an allowed &#34;safe&#34; subset.
				A sysctl is considered safe if it is namespaced in the container or the Pod, and it is isolated from other Pods or processes on the same Node.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Restricted Fields:&lt;/b&gt;&lt;br&gt;
				spec.securityContext.sysctls&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Allowed Values:&lt;/b&gt;&lt;br&gt;
				kernel.shm_rmid_forced&lt;br&gt;
				net.ipv4.ip_local_port_range&lt;br&gt;
				net.ipv4.tcp_syncookies&lt;br&gt;
				net.ipv4.ping_group_range&lt;br&gt;
				undefined/empty&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
	&lt;/tbody&gt;
&lt;/table&gt;
 --&gt;
&lt;h3 id=&#34;baseline&#34;&gt;基线/默认&lt;/h3&gt;
&lt;p&gt;基线/默认策略的目的是在防止已知的特权升级情况下，让得普通的容器化工作负载来使用。 这个策略的目标
用户是应用运维人员和非关键应用的开发者。 下面列举这这些控制应该受限/不被允许:&lt;/p&gt;
&lt;table&gt;
	&lt;caption style=&#34;display:none&#34;&gt;基线策略规范&lt;/caption&gt;
	&lt;tbody&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;strong&gt;控制&lt;/strong&gt;&lt;/td&gt;
			&lt;td&gt;&lt;strong&gt;策略&lt;/strong&gt;&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;宿主机命令空间&lt;/td&gt;
			&lt;td&gt;
				Sharing the host namespaces must be disallowed.&lt;br&gt;
        主机命名空间分离必须禁止。
				&lt;br&gt;&lt;b&gt;受限字段:&lt;/b&gt;&lt;br&gt;
				spec.hostNetwork&lt;br&gt;
				spec.hostPID&lt;br&gt;
				spec.hostIPC&lt;br&gt;
				&lt;br&gt;&lt;b&gt;可用值:&lt;/b&gt; false&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;特权容器&lt;/td&gt;
			&lt;td&gt;
        特权 Pod 禁用了多数安全机制，必须禁止。
				&lt;br&gt;&lt;b&gt;受限字段:&lt;/b&gt;&lt;br&gt;
				spec.containers[*].securityContext.privileged&lt;br&gt;
				spec.initContainers[*].securityContext.privileged&lt;br&gt;
				&lt;br&gt;&lt;b&gt;可用值:&lt;/b&gt; false, undefined/nil&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;Capabilities&lt;/td&gt;
			&lt;td&gt;
        [默认集](https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities)
        外的额外能力是不允许的
				&lt;br&gt;&lt;b&gt;受限字段:&lt;/b&gt;&lt;br&gt;
				spec.containers[*].securityContext.capabilities.add&lt;br&gt;
				spec.initContainers[*].securityContext.capabilities.add&lt;br&gt;
				&lt;br&gt;&lt;b&gt;可用值:&lt;/b&gt; empty (or restricted to a known list)&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;HostPath 卷&lt;/td&gt;
			&lt;td&gt;
				HostPath 卷必须禁止&lt;br&gt;
				&lt;br&gt;&lt;b&gt;受限字段:&lt;/b&gt;&lt;br&gt;
				spec.volumes[*].hostPath&lt;br&gt;
				&lt;br&gt;&lt;b&gt;可用值:&lt;/b&gt; undefined/nil&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;宿主机端口&lt;/td&gt;
			&lt;td&gt;
				宿主机端口应该禁用, 或最少限制在一个已知列表.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;受限字段:&lt;/b&gt;&lt;br&gt;
				spec.containers[*].ports[*].hostPort&lt;br&gt;
				spec.initContainers[*].ports[*].hostPort&lt;br&gt;
				&lt;br&gt;&lt;b&gt;可用值:&lt;/b&gt; 0, undefined (或限制在一个已知列表)&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;AppArmor &lt;em&gt;(可选)&lt;/em&gt;&lt;/td&gt;
			&lt;td&gt;
        在受支持的主机上，默认执行 &#39;runtime/default&#39; AppArmor 方案。 默认策略应该防止被覆盖或
        禁用策略，或限制对允许方案集的覆盖.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;受限字段:&lt;/b&gt;&lt;br&gt;
				metadata.annotations[&#39;container.apparmor.security.beta.kubernetes.io/*&#39;]&lt;br&gt;
				&lt;br&gt;&lt;b&gt;可用值:&lt;/b&gt; &#39;runtime/default&#39;, undefined&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;SELinux &lt;em&gt;(可选)&lt;/em&gt;&lt;/td&gt;
			&lt;td&gt;
        不允许设置自定义SELinux选项.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;受限字段:&lt;/b&gt;&lt;br&gt;
				spec.securityContext.seLinuxOptions&lt;br&gt;
				spec.containers[*].securityContext.seLinuxOptions&lt;br&gt;
				spec.initContainers[*].securityContext.seLinuxOptions&lt;br&gt;
				&lt;br&gt;&lt;b&gt;可用值:&lt;/b&gt; undefined/nil&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;/proc 挂载类别&lt;/td&gt;
			&lt;td&gt;
        默认的 /proc masks 被设置来减少攻击面， 是必要的.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;受限字段:&lt;/b&gt;&lt;br&gt;
				spec.containers[*].securityContext.procMount&lt;br&gt;
				spec.initContainers[*].securityContext.procMount&lt;br&gt;
				&lt;br&gt;&lt;b&gt;可用值:&lt;/b&gt; undefined/nil, &#39;Default&#39;&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;Sysctls&lt;/td&gt;
			&lt;td&gt;
        Sysctls 可以禁用安全机制或影响主机上的所有容器，在允许的安全子集外都应该被禁止。
        如果 sysctl 命名空间为容器内或 Pod 内被认为是安全的， 它会被与同一个节点上其它 Pod 或进程隔离.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;受限字段:&lt;/b&gt;&lt;br&gt;
				spec.securityContext.sysctls&lt;br&gt;
				&lt;br&gt;&lt;b&gt;可用值:&lt;/b&gt;&lt;br&gt;
				kernel.shm_rmid_forced&lt;br&gt;
				net.ipv4.ip_local_port_range&lt;br&gt;
				net.ipv4.tcp_syncookies&lt;br&gt;
				net.ipv4.ping_group_range&lt;br&gt;
				undefined/empty&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
	&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
### Restricted

The Restricted policy is aimed at enforcing current Pod hardening best practices, at the expense of
some compatibility. It is targeted at operators and developers of security-critical applications, as
well as lower-trust users.The following listed controls should be enforced/disallowed:


&lt;table&gt;
	&lt;caption style=&#34;display:none&#34;&gt;Restricted policy specification&lt;/caption&gt;
	&lt;tbody&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;strong&gt;Control&lt;/strong&gt;&lt;/td&gt;
			&lt;td&gt;&lt;strong&gt;Policy&lt;/strong&gt;&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td colspan=&#34;2&#34;&gt;&lt;em&gt;Everything from the default profile.&lt;/em&gt;&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;Volume Types&lt;/td&gt;
			&lt;td&gt;
				In addition to restricting HostPath volumes, the restricted profile limits usage of non-core volume types to those defined through PersistentVolumes.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Restricted Fields:&lt;/b&gt;&lt;br&gt;
				spec.volumes[*].hostPath&lt;br&gt;
				spec.volumes[*].gcePersistentDisk&lt;br&gt;
				spec.volumes[*].awsElasticBlockStore&lt;br&gt;
				spec.volumes[*].gitRepo&lt;br&gt;
				spec.volumes[*].nfs&lt;br&gt;
				spec.volumes[*].iscsi&lt;br&gt;
				spec.volumes[*].glusterfs&lt;br&gt;
				spec.volumes[*].rbd&lt;br&gt;
				spec.volumes[*].flexVolume&lt;br&gt;
				spec.volumes[*].cinder&lt;br&gt;
				spec.volumes[*].cephFS&lt;br&gt;
				spec.volumes[*].flocker&lt;br&gt;
				spec.volumes[*].fc&lt;br&gt;
				spec.volumes[*].azureFile&lt;br&gt;
				spec.volumes[*].vsphereVolume&lt;br&gt;
				spec.volumes[*].quobyte&lt;br&gt;
				spec.volumes[*].azureDisk&lt;br&gt;
				spec.volumes[*].portworxVolume&lt;br&gt;
				spec.volumes[*].scaleIO&lt;br&gt;
				spec.volumes[*].storageos&lt;br&gt;
				spec.volumes[*].csi&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Allowed Values:&lt;/b&gt; undefined/nil&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;Privilege Escalation&lt;/td&gt;
			&lt;td&gt;
				Privilege escalation (such as via set-user-ID or set-group-ID file mode) should not be allowed.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Restricted Fields:&lt;/b&gt;&lt;br&gt;
				spec.containers[*].securityContext.allowPrivilegeEscalation&lt;br&gt;
				spec.initContainers[*].securityContext.allowPrivilegeEscalation&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Allowed Values:&lt;/b&gt; false&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;Running as Non-root&lt;/td&gt;
			&lt;td&gt;
				Containers must be required to run as non-root users.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Restricted Fields:&lt;/b&gt;&lt;br&gt;
				spec.securityContext.runAsNonRoot&lt;br&gt;
				spec.containers[*].securityContext.runAsNonRoot&lt;br&gt;
				spec.initContainers[*].securityContext.runAsNonRoot&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Allowed Values:&lt;/b&gt; true&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;Non-root groups &lt;em&gt;(optional)&lt;/em&gt;&lt;/td&gt;
			&lt;td&gt;
				Containers should be forbidden from running with a root primary or supplementary GID.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Restricted Fields:&lt;/b&gt;&lt;br&gt;
				spec.securityContext.runAsGroup&lt;br&gt;
				spec.securityContext.supplementalGroups[*]&lt;br&gt;
				spec.securityContext.fsGroup&lt;br&gt;
				spec.containers[*].securityContext.runAsGroup&lt;br&gt;
				spec.initContainers[*].securityContext.runAsGroup&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Allowed Values:&lt;/b&gt;&lt;br&gt;
				non-zero&lt;br&gt;
				undefined / nil (except for `*.runAsGroup`)&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;Seccomp&lt;/td&gt;
			&lt;td&gt;
				The RuntimeDefault seccomp profile must be required, or allow specific additional profiles.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Restricted Fields:&lt;/b&gt;&lt;br&gt;
				spec.securityContext.seccompProfile.type&lt;br&gt;
				spec.containers[*].securityContext.seccompProfile&lt;br&gt;
				spec.initContainers[*].securityContext.seccompProfile&lt;br&gt;
				&lt;br&gt;&lt;b&gt;Allowed Values:&lt;/b&gt;&lt;br&gt;
				&#39;runtime/default&#39;&lt;br&gt;
				undefined / nil&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
	&lt;/tbody&gt;
&lt;/table&gt;
 --&gt;
&lt;h3 id=&#34;restricted&#34;&gt;受限的&lt;/h3&gt;
&lt;p&gt;受限策略旨在实施目前的 Pod 加固最佳实践，代价就是牺牲一些兼容性。目标用户为高安全性应用的运维和
开发人员，也可以是低信任度的用户。 下面列举的控制项应该受限/不被允许:&lt;/p&gt;
&lt;table&gt;
	&lt;caption style=&#34;display:none&#34;&gt;受限策略规范&lt;/caption&gt;
	&lt;tbody&gt;
		&lt;tr&gt;
			&lt;td&gt;&lt;strong&gt;Control&lt;/strong&gt;&lt;/td&gt;
			&lt;td&gt;&lt;strong&gt;Policy&lt;/strong&gt;&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td colspan=&#34;2&#34;&gt;&lt;em&gt;所有都来自默认方案&lt;/em&gt;&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;卷类型&lt;/td&gt;
			&lt;td&gt;
        为了进一步限制 HostPath 卷， 受限方案在定义 PV 时限制了这些非核心卷类型的使用.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;受限字段:&lt;/b&gt;&lt;br&gt;
				spec.volumes[*].hostPath&lt;br&gt;
				spec.volumes[*].gcePersistentDisk&lt;br&gt;
				spec.volumes[*].awsElasticBlockStore&lt;br&gt;
				spec.volumes[*].gitRepo&lt;br&gt;
				spec.volumes[*].nfs&lt;br&gt;
				spec.volumes[*].iscsi&lt;br&gt;
				spec.volumes[*].glusterfs&lt;br&gt;
				spec.volumes[*].rbd&lt;br&gt;
				spec.volumes[*].flexVolume&lt;br&gt;
				spec.volumes[*].cinder&lt;br&gt;
				spec.volumes[*].cephFS&lt;br&gt;
				spec.volumes[*].flocker&lt;br&gt;
				spec.volumes[*].fc&lt;br&gt;
				spec.volumes[*].azureFile&lt;br&gt;
				spec.volumes[*].vsphereVolume&lt;br&gt;
				spec.volumes[*].quobyte&lt;br&gt;
				spec.volumes[*].azureDisk&lt;br&gt;
				spec.volumes[*].portworxVolume&lt;br&gt;
				spec.volumes[*].scaleIO&lt;br&gt;
				spec.volumes[*].storageos&lt;br&gt;
				spec.volumes[*].csi&lt;br&gt;
				&lt;br&gt;&lt;b&gt;可用值:&lt;/b&gt; undefined/nil&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;权限提升&lt;/td&gt;
			&lt;td&gt;
				权限提升 (如通过设置 set-user-ID 或 set-group-ID 文件模式) 应该被禁止.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;受限字段:&lt;/b&gt;&lt;br&gt;
				spec.containers[*].securityContext.allowPrivilegeEscalation&lt;br&gt;
				spec.initContainers[*].securityContext.allowPrivilegeEscalation&lt;br&gt;
				&lt;br&gt;&lt;b&gt;可用值:&lt;/b&gt; false&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;以 非root用户运行&lt;/td&gt;
			&lt;td&gt;
				要求容器必须以非 root 用户运行 .&lt;br&gt;
				&lt;br&gt;&lt;b&gt;受限字段:&lt;/b&gt;&lt;br&gt;
				spec.securityContext.runAsNonRoot&lt;br&gt;
				spec.containers[*].securityContext.runAsNonRoot&lt;br&gt;
				spec.initContainers[*].securityContext.runAsNonRoot&lt;br&gt;
				&lt;br&gt;&lt;b&gt;可用值:&lt;/b&gt; true&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;非root 组 &lt;em&gt;(可选)&lt;/em&gt;&lt;/td&gt;
			&lt;td&gt;
				容器应该避免以 root 主要或辅助的 GID 来运行 .&lt;br&gt;
				&lt;br&gt;&lt;b&gt;受限字段:&lt;/b&gt;&lt;br&gt;
				spec.securityContext.runAsGroup&lt;br&gt;
				spec.securityContext.supplementalGroups[*]&lt;br&gt;
				spec.securityContext.fsGroup&lt;br&gt;
				spec.containers[*].securityContext.runAsGroup&lt;br&gt;
				spec.initContainers[*].securityContext.runAsGroup&lt;br&gt;
				&lt;br&gt;&lt;b&gt;可用值:&lt;/b&gt;&lt;br&gt;
				non-zero&lt;br&gt;
				undefined / nil (`*.runAsGroup` 除外)&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td&gt;Seccomp&lt;/td&gt;
			&lt;td&gt;
				必须要使用 RuntimeDefault 安全计算模式(seccomp) 方案，或允许指定额外方案.&lt;br&gt;
				&lt;br&gt;&lt;b&gt;受限字段:&lt;/b&gt;&lt;br&gt;
				spec.securityContext.seccompProfile.type&lt;br&gt;
				spec.containers[*].securityContext.seccompProfile&lt;br&gt;
				spec.initContainers[*].securityContext.seccompProfile&lt;br&gt;
				&lt;br&gt;&lt;b&gt;可用值:&lt;/b&gt;&lt;br&gt;
				&#39;runtime/default&#39;&lt;br&gt;
				undefined / nil&lt;br&gt;
			&lt;/td&gt;
		&lt;/tr&gt;
	&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
## Policy Instantiation

Decoupling policy definition from policy instantiation allows for a common understanding and
consistent language of policies across clusters, independent of the underlying enforcement
mechanism.

As mechanisms mature, they will be defined below on a per-policy basis. The methods of enforcement
of individual policies are not defined here.

[**PodSecurityPolicy**](/docs/concepts/policy/pod-security-policy/)

- [Privileged](https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/policy/privileged-psp.yaml)
- [Baseline](https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/policy/baseline-psp.yaml)
- [Restricted](https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/policy/restricted-psp.yaml)
 --&gt;
&lt;h2 id=&#34;policy-instantiation&#34;&gt;策略安装&lt;/h2&gt;
&lt;p&gt;将策略定义从策略安装中解耦出来可以使用其成功一个共识和跨集群策略的统一语言，独立于底层的执行机制。&lt;/p&gt;
&lt;p&gt;作为成熟机制， 它们会定义在单个策略基础之下。 每个策略的执行方法不是在这里定义的。
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/policy/pod-security-policy/&#34;&gt;&lt;strong&gt;PodSecurityPolicy&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/policy/privileged-psp.yaml&#34;&gt;Privileged&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/policy/baseline-psp.yaml&#34;&gt;Baseline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/policy/restricted-psp.yaml&#34;&gt;Restricted&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## FAQ

### Why isn&#39;t there a profile between privileged and default?

The three profiles defined here have a clear linear progression from most secure (restricted) to least
secure (privileged), and cover a broad set of workloads. Privileges required above the baseline
policy are typically very application specific, so we do not offer a standard profile in this
niche. This is not to say that the privileged profile should always be used in this case, but that
policies in this space need to be defined on a case-by-case basis.

SIG Auth may reconsider this position in the future, should a clear need for other profiles arise.
 --&gt;
&lt;h2 id=&#34;faq&#34;&gt;FAQ&lt;/h2&gt;
&lt;h3 id=&#34;why-isnt-there-a-profile-between-privileged-and-default&#34;&gt;为啥在 特权(privileged)和基线(default) 之间没有一个中间方案 &lt;/h3&gt;
&lt;p&gt;这里定义的三个方案有一个明显的线性变化， 从最安全(受限)到最不安全(特权)，覆盖了广泛的工作负载集。
在基线策略之上的权限需求通常是特别具体到应用的， 所以在这个层面我会不需要提供一个标准的方案。
这也不是说特权方案就会始终用于这个应用场景，但是在这个地方用到的策略就需要单个场景级别地来定义。&lt;/p&gt;
&lt;p&gt;如果有明确的其它方案的需求提出来，未来 SIG Auth 可以重新考虑加到这里。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;whats-the-difference-between-a-security-policy-and-a-security-context&#34;&gt;安全策略与安全上下文之间的区别&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/configure-pod-container/security-context/&#34;&gt;安全上下文&lt;/a&gt; 是在
Pod 和 容器的运行时上配置。 安全上下文是在Pod 配置中作为 Pod 和容器定义的一部分存在的，代表
传递给容器运行时的参数。&lt;/p&gt;
&lt;p&gt;安全策略是在安全上下文中执行指定设置的控制面板机制，连同安全上下文之外的其它参数。 从 2020 年 2 月
形如，当前原生执行这些安全策略的方案就是
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/policy/pod-security-policy/&#34;&gt;Pod 安全策略&lt;/a&gt; - 一个在集群中
以中心化方式在 Pod 上执行安全策略的机制。 其它执行安全策略的方式也已经在 k8s 生态中开发了，就
如
&lt;a href=&#34;https://github.com/open-policy-agent/gatekeeper&#34;&gt;OPA Gatekeeper&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;what-profiles-should-i-apply-to-my-windows-pods&#34;&gt;Windows Pod 应该执行哪个方案&lt;/h3&gt;
&lt;p&gt;windows 在 k8s 中是有些限制且与基于 Linux 的标准工作负载有明显差异的。 特别是  Pod 的 SecurityContext
字段在
&lt;a href=&#34;https://kubernetes.io/docs/setup/production-environment/windows/intro-windows-in-kubernetes/#v1-podsecuritycontext&#34;&gt;Windows Pod 上是没有效果的&lt;/a&gt;. 因此，目前是没有标准的 Pod 安全方案存在的。&lt;/p&gt;
&lt;h3 id=&#34;what-about-sandboxed-pods&#34;&gt;沙盒 Pod 是个啥情况&lt;/h3&gt;
&lt;p&gt;目前没有一个标准的 API 可以控制一个 Pod 是否被认为是沙盒。沙盒 Pod 可能可以通过使用一个沙盒
运行时(如 gVisor 或 Kata容器)来识别， 但没有对沙盒运行时有一个标准的定义。&lt;/p&gt;
&lt;p&gt;对沙盒工作负载的保护需求可能与其它的不同。 例如，在工作负载从底层内核隔离后对特殊权限的限制需求就减少了。
这使得工作负载在隔离的情况下提升指定的权限。&lt;/p&gt;
&lt;p&gt;另外，对沙盒工作负载的保护调度信赖沙盒的实现方式。 例如，没有一个推荐策略可以推荐给所有的沙盒工作负载。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 持久化卷(PV)</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/persistent-volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/persistent-volumes/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jsafrane
- saad-ali
- thockin
- msau42
- xing-yang
title: Persistent Volumes
feature:
  title: Storage orchestration
  description: &gt;
    Automatically mount the storage system of your choice, whether from local storage, a public cloud provider such as &lt;a href=&#34;https://cloud.google.com/storage/&#34;&gt;GCP&lt;/a&gt; or &lt;a href=&#34;https://aws.amazon.com/products/storage/&#34;&gt;AWS&lt;/a&gt;, or a network storage system such as NFS, iSCSI, Gluster, Ceph, Cinder, or Flocker.

content_type: concept
weight: 20
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This document describes the current state of _persistent volumes_ in Kubernetes. Familiarity with [volumes](/docs/concepts/storage/volumes/) is suggested.
 --&gt;
&lt;p&gt;本文主要介绍当前 k8s 中 &lt;em&gt;持久化卷(persistent volume)&lt;/em&gt; 的状态。
建议先熟悉 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#34;&gt;volumes&lt;/a&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Introduction

Managing storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed. To do this, we introduce two new API resources:  PersistentVolume and PersistentVolumeClaim.

A _PersistentVolume_ (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using [Storage Classes](/docs/concepts/storage/storage-classes/). It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.

A _PersistentVolumeClaim_ (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory).  Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see [AccessModes](#access-modes)).

While PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than just size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the _StorageClass_ resource.

See the [detailed walkthrough with working examples](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/).
 --&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;管理存储和管理实例是两个不同的问题。 PersistentVolume 子系统为用户和管理员提供了一套 API 将
存储是怎么提供的细节从存储使用中抽象出来。 而为了做到这一个我们要介绍两个新的 API 资源:
PersistentVolume 和 PersistentVolumeClaim.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PersistentVolume&lt;/em&gt; (PV) 是集群中的一块存储，它可能由管理员管理或使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-classes/&#34;&gt;StorageClass&lt;/a&gt;
动态管理。它是集群中的一个资源，就像节点是集群中的一个资源一样。 PV 是与卷(Volume)类似的卷插件，
但它的生命周期与使用它的 Pod 的生命周期是相互独立的。这个 API 对象中包含了存储的实现细节，包含
NFS, iSCSI, 云服务提供的存储系统。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PersistentVolumeClaim&lt;/em&gt; (PVC) 是一个用户对存储的请求。 它就像是一个 Pod。Pod 使用的是节点上的资源
而 PVC 使用的是 PV 资源。 Pod 可以申请指定级别的资源(CPU 和 Memory)。 PVC 可以申请指定容量的存储
和访问模式 (如，它们可以以 &lt;code&gt;ReadWriteOnce&lt;/code&gt;, &lt;code&gt;ReadOnlyMany&lt;/code&gt; 或 &lt;code&gt;ReadWriteMany&lt;/code&gt; 方式挂载，
见 &lt;a href=&#34;#access-modes&#34;&gt;AccessMode&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;因为 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 允许用户使用抽象的存储资源，通常用户都需要 PersistentVolume
中包含许多属性，如性能，来应对不同的问题。 集群管理员需要能够提供多样的 PersistentVolume，而不
仅仅是容量和访问模式，还需要向用户提供这些卷的实现细节。 为了满足这些需求，我们就有了 &lt;em&gt;StorageClass&lt;/em&gt; 资源。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-persistent-volume-storage/&#34;&gt;亲自上手试试&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Lifecycle of a volume and claim

PVs are resources in the cluster. PVCs are requests for those resources and also act as claim checks to the resource. The interaction between PVs and PVCs follows this lifecycle:
 --&gt;
&lt;h2 id=&#34;pv-和-pvc-的生命周期&#34;&gt;PV 和 PVC 的生命周期&lt;/h2&gt;
&lt;p&gt;PV 是集群中的资源。 PVC 是对这些资源的申请同时也表现为对这些资源的检查声明。PV 和 PVC 之间的
相互影响遵守以下生命周期:&lt;/p&gt;
&lt;!--
### Provisioning

There are two ways PVs may be provisioned: statically or dynamically.
 --&gt;
&lt;h3 id=&#34;provisioning&#34;&gt;供给&lt;/h3&gt;
&lt;p&gt;PV 可以被两个方式提供: 静态供给 或 动态供给&lt;/p&gt;
&lt;!--
#### Static

A cluster administrator creates a number of PVs. They carry the details of the real storage, which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.
 --&gt;
&lt;h4 id=&#34;静态供给&#34;&gt;静态供给&lt;/h4&gt;
&lt;p&gt;集群管理创建一系列 PV。 它们包含真实存储的细节，可以被集群用户使用。
它们存在于 k8s API 中，可以被取用。&lt;/p&gt;
&lt;!--
#### Dynamic

When none of the static PVs the administrator created match a user&#39;s PersistentVolumeClaim,
the cluster may try to dynamically provision a volume specially for the PVC.
This provisioning is based on StorageClasses: the PVC must request a
[storage class](/docs/concepts/storage/storage-classes/) and
the administrator must have created and configured that class for dynamic
provisioning to occur. Claims that request the class `&#34;&#34;` effectively disable
dynamic provisioning for themselves.

To enable dynamic storage provisioning based on storage class, the cluster administrator
needs to enable the `DefaultStorageClass` [admission controller](/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)
on the API server. This can be done, for example, by ensuring that `DefaultStorageClass` is
among the comma-delimited, ordered list of values for the `--enable-admission-plugins` flag of
the API server component. For more information on API server command-line flags,
check [kube-apiserver](/docs/admin/kube-apiserver/) documentation.
 --&gt;
&lt;h4 id=&#34;动态供给&#34;&gt;动态供给&lt;/h4&gt;
&lt;p&gt;当管理创建的 PV 不能满足用户的 PersistentVolumeClaim 时，集群可能就会尝试为这个 PVC 动态
提供一个卷。这种供给基于 &lt;code&gt;StorageClass&lt;/code&gt;: PVC 必须要申请一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes/&#34;&gt;StorageClass&lt;/a&gt;
并且在动态供给发生之前管理员必须要完成该 &lt;code&gt;StorageClass&lt;/code&gt; 的创建和配置。
如果 PVC 的 &lt;code&gt;StorageClass&lt;/code&gt; 被设置为 &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt; 实际上表示关闭自身的动态供给。&lt;/p&gt;
&lt;p&gt;要启用基于 &lt;code&gt;StorageClass&lt;/code&gt; 的动态存储供给需要集群管理在 api-server 中的
&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass&#34;&gt;准入控制&lt;/a&gt;
中启用 &lt;code&gt;DefaultStorageClass&lt;/code&gt;。 具体的配置方法就是确认 api-server 组件的 &lt;code&gt;--enable-admission-plugins&lt;/code&gt; 选项
的值中有没有 &lt;code&gt;DefaultStorageClass&lt;/code&gt;。 更多关于 api-server 命令行参数见
&lt;a href=&#34;https://kubernetes.io/docs/admin/kube-apiserver/&#34;&gt;kube-apiserver&lt;/a&gt; 文档&lt;/p&gt;
&lt;!--
### Binding

A user creates, or in the case of dynamic provisioning, has already created, a PersistentVolumeClaim with a specific amount of storage requested and with certain access modes. A control loop in the master watches for new PVCs, finds a matching PV (if possible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim.

Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster.
 --&gt;
&lt;h3 id=&#34;binding&#34;&gt;绑定&lt;/h3&gt;
&lt;p&gt;当一个指定容量和访问模式的 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 被创建后。 主控中心中的一个控制回环会监控新创建的 PVC，
如果有匹配的 PV 存在，则将它们绑定在一起。 如果这个 PV 是动态供给给这个新建的 PVC 的， 那么控制
回环将会始终将这个 PV 绑定到这个 PVC。 否则用户会得到至少满足其请求的卷，但这个卷容量可能实际是超出请求的。
当绑定完成，就会确定绑定关系，不管它们是怎么绑定的。 PVC 到 PV 的绑定是一对一关系，PV 使用的 &lt;code&gt;spec.claimRef&lt;/code&gt;
实现 &lt;code&gt;PersistentVolume&lt;/code&gt; 与 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 的双向绑定。&lt;/p&gt;
&lt;p&gt;如果没有匹配的 PV 存在，PVC 会永远保持在未绑定状态。 当有可用的匹配 PV 出现时 PVC 就会绑定。
例如， 集群中提供许多 50Gi PV 是不会被一个申请 100Gi 的 PVC 匹配到的。当集群中添加了一个
100Gi PV 时，这个 PVC 就可以绑定了。&lt;/p&gt;
&lt;!--
### Using

Pods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a Pod. For volumes that support multiple access modes, the user specifies which mode is desired when using their claim as a volume in a Pod.

Once a user has a claim and that claim is bound, the bound PV belongs to the user for as long as they need it. Users schedule Pods and access their claimed PVs by including a `persistentVolumeClaim` section in a Pod&#39;s `volumes` block. See [Claims As Volumes](#claims-as-volumes) for more details on this.
 --&gt;
&lt;h3 id=&#34;使用&#34;&gt;使用&lt;/h3&gt;
&lt;p&gt;Pod 会将 PVC 当作卷来使用。 集群会检视这个 PVC 找到它绑定的 PV 然后将这个 PV 挂载到 Pod 中。
对于支持多次访问模式的卷，用户在 Pod 当作卷的 PVC 中指定需要的访问模式。&lt;/p&gt;
&lt;p&gt;当一个用户拥有一个 PVC 并且这个 PVC 完成绑定，则这个被绑定的 PV 在用户需要时始终属于该用户。
用户通过 Pod 中的 &lt;code&gt;volumes&lt;/code&gt; 配置区中添加 &lt;code&gt;persistentVolumeClaim&lt;/code&gt; 配置区来访问这些由
PVC 管理的 PV。
更多信息见 &lt;a href=&#34;#claims-as-volumes&#34;&gt;将 PVC 当作卷(PV)&lt;/a&gt;&lt;/p&gt;
&lt;!--
### Storage Object in Use Protection
The purpose of the Storage Object in Use Protection feature is to ensure that PersistentVolumeClaims (PVCs) in active use by a Pod and PersistentVolume (PVs) that are bound to PVCs are not removed from the system, as this may result in data loss.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; PVC is in active use by a Pod when a Pod object exists that is using the PVC.&lt;/div&gt;
&lt;/blockquote&gt;


If a user deletes a PVC in active use by a Pod, the PVC is not removed immediately. PVC removal is postponed until the PVC is no longer actively used by any Pods. Also, if an admin deletes a PV that is bound to a PVC, the PV is not removed immediately. PV removal is postponed until the PV is no longer bound to a PVC.

You can see that a PVC is protected when the PVC&#39;s status is `Terminating` and the `Finalizers` list includes `kubernetes.io/pvc-protection`:

```shell
kubectl describe pvc hostpath
Name:          hostpath
Namespace:     default
StorageClass:  example-hostpath
Status:        Terminating
Volume:
Labels:        &lt;none&gt;
Annotations:   volume.beta.kubernetes.io/storage-class=example-hostpath
               volume.beta.kubernetes.io/storage-provisioner=example.com/hostpath
Finalizers:    [kubernetes.io/pvc-protection]
...
```

You can see that a PV is protected when the PV&#39;s status is `Terminating` and the `Finalizers` list includes `kubernetes.io/pv-protection` too:

```shell
kubectl describe pv task-pv-volume
Name:            task-pv-volume
Labels:          type=local
Annotations:     &lt;none&gt;
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    standard
Status:          Terminating
Claim:
Reclaim Policy:  Delete
Access Modes:    RWO
Capacity:        1Gi
Message:
Source:
    Type:          HostPath (bare host directory volume)
    Path:          /tmp/data
    HostPathType:
Events:            &lt;none&gt;
```
 --&gt;
&lt;h3 id=&#34;使用保护模式的存储对象&#34;&gt;使用保护模式的存储对象&lt;/h3&gt;
&lt;p&gt;保护模式的存储对象特性的目的是保证被 Pod 使用的有效的 PVC 和与 PVC 绑定的 PV 能会被系统删除，
从而可能导致数据丢失。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 当一个使用 PVC 的 Pod 对象存在时就表示 PVC 被 Pod 有效使用。&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;如果用户删除了一个被 Pod 有效使用的 PVC， 这个 PVC 不会立马被删除。 PVC 会延迟到没有被任何
Pod 有效使用后才会删除。 同样，如果管理员删除了一个与 PVC 绑定的 PV， 这个 PV 也不会被立马删除，
PV 的删除行为会被延迟到不再与 PVC 绑定时。&lt;/p&gt;
&lt;p&gt;当 PVC 的状态是 &lt;code&gt;Terminating&lt;/code&gt; 并且 &lt;code&gt;Finalizers&lt;/code&gt; 列表中包含 &lt;code&gt;kubernetes.io/pvc-protection&lt;/code&gt;
就表示这个 PVC 是被保护的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe pvc hostpath
Name:          hostpath
Namespace:     default
StorageClass:  example-hostpath
Status:        Terminating
Volume:
Labels:        &amp;lt;none&amp;gt;
Annotations:   volume.beta.kubernetes.io/storage-class&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;example-hostpath
               volume.beta.kubernetes.io/storage-provisioner&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;example.com/hostpath
Finalizers:    &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;kubernetes.io/pvc-protection&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;同样，当 PV 的状态是 &lt;code&gt;Terminating&lt;/code&gt; 并且 &lt;code&gt;Finalizers&lt;/code&gt; 列表中包含 &lt;code&gt;kubernetes.io/pvc-protection&lt;/code&gt;
就表示这个 PV 是被保护的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe pv task-pv-volume
Name:            task-pv-volume
Labels:          type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;local
Annotations:     &amp;lt;none&amp;gt;
Finalizers:      &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;kubernetes.io/pv-protection&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
StorageClass:    standard
Status:          Terminating
Claim:
Reclaim Policy:  Delete
Access Modes:    RWO
Capacity:        1Gi
Message:
Source:
    Type:          HostPath &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;bare host directory volume&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
    Path:          /tmp/data
    HostPathType:
Events:            &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Reclaiming

When a user is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained, Recycled, or Deleted.
 --&gt;
&lt;h3 id=&#34;回收&#34;&gt;回收&lt;/h3&gt;
&lt;p&gt;当用于不再需要一个卷时，可以通过 API 删除这个 PVC 对象，这样就允许对对应资源的回收。 PV 的回收
策略让集群可以在 PV 释放以后使用对应的方式回收。 目前，卷的回收策略有 保留(&lt;code&gt;Retain&lt;/code&gt;),
循环使用(&lt;code&gt;Recycle&lt;/code&gt;), 删除 (&lt;code&gt;Delete&lt;/code&gt;).&lt;/p&gt;
&lt;!--
#### Retain

The `Retain` reclaim policy allows for manual reclamation of the resource. When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered &#34;released&#34;. But it is not yet available for another claim because the previous claimant&#39;s data remains on the volume. An administrator can manually reclaim the volume with the following steps.

1. Delete the PersistentVolume. The associated storage asset in external infrastructure (such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume) still exists after the PV is deleted.
1. Manually clean up the data on the associated storage asset accordingly.
1. Manually delete the associated storage asset, or if you want to reuse the same storage asset, create a new PersistentVolume with the storage asset definition.
 --&gt;
&lt;h4 id=&#34;保留retained&#34;&gt;保留(&lt;code&gt;Retained&lt;/code&gt;)&lt;/h4&gt;
&lt;p&gt;保留(&lt;code&gt;Retained&lt;/code&gt;) 回收策略允许对资源手动回收。 当 PVC 被删除后，其之前绑定的 PV 依然存在并被
认为是已经释放。 但是它还不被另一个 PVC 使用，因为其中还有上一个 PVC 时的数据。 管理员可以
通过以下步骤手动回收这个卷:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;删除 PV 对象。 它所关联的外部存储设施(如 AWS EBS, GCE PD, Azure Disk, Cinder 卷)依然存在。&lt;/li&gt;
&lt;li&gt;根据需要手动清理对应存储资源上的数据&lt;/li&gt;
&lt;li&gt;手动删除对应的存储资源，如果想要重新使用该存储资源，可以再创建一个新的 PV 对象与之关联。&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
#### Delete

For volume plugins that support the `Delete` reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume. Volumes that were dynamically provisioned inherit the [reclaim policy of their StorageClass](#reclaim-policy), which defaults to `Delete`. The administrator should configure the StorageClass according to users&#39; expectations; otherwise, the PV must be edited or patched after it is created. See [Change the Reclaim Policy of a PersistentVolume](/docs/tasks/administer-cluster/change-pv-reclaim-policy/).
 --&gt;
&lt;h4 id=&#34;删除-delete&#34;&gt;删除 (&lt;code&gt;Delete&lt;/code&gt;)&lt;/h4&gt;
&lt;p&gt;对于支持 删除 (&lt;code&gt;Delete&lt;/code&gt;) 回收策略的卷插件，在删除 PVC 对象之后与其郑的 PV 对象也会被删除，
而且对应的外部存储资源，如 AWS EBS, GCE PD, Azure Disk, Cinder 卷也会一同被删除。
动态提供的卷的回收策略继承自 &lt;a href=&#34;#reclaim-policy&#34;&gt;它们的 StorageClass 的回收策略&lt;/a&gt;，默认为 删除 (&lt;code&gt;Delete&lt;/code&gt;)
管理应该根据用户期望设置 StorageClass 的回收策略，否则 PV 在创建后再需要再次修改才行。
见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/change-pv-reclaim-policy/&#34;&gt;修改 PersistentVolume 的回收策略&lt;/a&gt;.&lt;/p&gt;
&lt;!--
#### Recycle

&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; The &lt;code&gt;Recycle&lt;/code&gt; reclaim policy is deprecated. Instead, the recommended approach is to use dynamic provisioning.&lt;/div&gt;
&lt;/blockquote&gt;


If supported by the underlying volume plugin, the `Recycle` reclaim policy performs a basic scrub (`rm -rf /thevolume/*`) on the volume and makes it available again for a new claim.

However, an administrator can configure a custom recycler Pod template using
the Kubernetes controller manager command line arguments as described in the
[reference](/docs/reference/command-line-tools-reference/kube-controller-manager/).
The custom recycler Pod template must contain a `volumes` specification, as
shown in the example below:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pv-recycler
  namespace: default
spec:
  restartPolicy: Never
  volumes:
  - name: vol
    hostPath:
      path: /any/path/it/will/be/replaced
  containers:
  - name: pv-recycler
    image: &#34;k8s.gcr.io/busybox&#34;
    command: [&#34;/bin/sh&#34;, &#34;-c&#34;, &#34;test -e /scrub &amp;&amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  &amp;&amp; test -z \&#34;$(ls -A /scrub)\&#34; || exit 1&#34;]
    volumeMounts:
    - name: vol
      mountPath: /scrub
```

However, the particular path specified in the custom recycler Pod template in the `volumes` part is replaced with the particular path of the volume that is being recycled.
--&gt;
&lt;h4 id=&#34;循环使用recycle&#34;&gt;循环使用(&lt;code&gt;Recycle&lt;/code&gt;)&lt;/h4&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; 循环使用(&lt;code&gt;Recycle&lt;/code&gt;) 回收策略已经废弃。 推荐使用动态供给方式。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果底层卷插件支持， 循环使用(&lt;code&gt;Recycle&lt;/code&gt;) 回收策略在卷上执行基础的清理操作 (&lt;code&gt;rm -rf /thevolume/*&lt;/code&gt;)
然后它就可以再次被新的 PVC 使用了。&lt;/p&gt;
&lt;p&gt;但是，管理也可以使用 k8s 控制管理器的命令行参数配置一个自定义的回收器 Pod 模板。具体见
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/
&#34;&gt;这里&lt;/a&gt;.
这个自定义的回收器 Pod 模板必须要包含 &lt;code&gt;volumes&lt;/code&gt; 定义，下面就是一个示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pv-recycler&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vol&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/any/path/it/will/be/replaced&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pv-recycler&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;k8s.gcr.io/busybox&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/bin/sh&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;test -e /scrub &amp;amp;&amp;amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  &amp;amp;&amp;amp; test -z \&amp;#34;$(ls -A /scrub)\&amp;#34; || exit 1&amp;#34;&lt;/span&gt;]
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vol&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/scrub&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;回收器 Pod 模板中的 &lt;code&gt;volumes&lt;/code&gt; 部分指定的路径，就是要被回收的卷&lt;/p&gt;
&lt;!--
### Reserving a PersistentVolume

The control plane can [bind PersistentVolumeClaims to matching PersistentVolumes](#binding) in the
cluster. However, if you want a PVC to bind to a specific PV, you need to pre-bind them.

By specifying a PersistentVolume in a PersistentVolumeClaim, you declare a binding between that specific PV and PVC.
If the PersistentVolume exists and has not reserved PersistentVolumeClaims through its `claimRef` field, then the PersistentVolume and PersistentVolumeClaim will be bound.

The binding happens regardless of some volume matching criteria, including node affinity.
The control plane still checks that [storage class](/docs/concepts/storage/storage-classes/), access modes, and requested storage size are valid.

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: foo-pvc
  namespace: foo
spec:
  storageClassName: &#34;&#34; # Empty string must be explicitly set otherwise default StorageClass will be set
  volumeName: foo-pv
  ...
```

This method does not guarantee any binding privileges to the PersistentVolume. If other PersistentVolumeClaims could use the PV that you specify, you first need to reserve that storage volume. Specify the relevant PersistentVolumeClaim in the `claimRef` field of the PV so that other PVCs can not bind to it.

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: foo-pv
spec:
  storageClassName: &#34;&#34;
  claimRef:
    name: foo-pvc
    namespace: foo
  ...
```

This is useful if you want to consume PersistentVolumes that have their `claimPolicy` set
to `Retain`, including cases where you are reusing an existing PV.
 --&gt;
&lt;h3 id=&#34;保留-persistentvolume&#34;&gt;保留 &lt;code&gt;PersistentVolume&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;控制中心可以在集群中将&lt;a href=&#34;#binding&#34;&gt; &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 与匹配的 &lt;code&gt;PersistentVolume&lt;/code&gt; 绑定&lt;/a&gt;。
但是，如果想要将 PVC 与指定 PV 绑定， 则需要预先绑定(而不是自动绑定)。&lt;/p&gt;
&lt;p&gt;通过在 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 中指定一个 &lt;code&gt;PersistentVolume&lt;/code&gt;，就可以将 PV 绑定到这个 PVC 上。
如果 PV 已经存在， 可能通过设置它的 &lt;code&gt;claimRef&lt;/code&gt; 字段指定一个 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;，
这样 PV 和 PVC 就会绑定。&lt;/p&gt;
&lt;p&gt;这种绑定会忽略一些卷匹配条件，包括节点亲和性。 控制中心还是会检测，
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes/&#34;&gt;StorageClass&lt;/a&gt;, 访问模式，申请容量是否是有效的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo-pvc&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# 这里必须要显示地设置，否则就会使用默认的 StorageClass&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo-pv&lt;/span&gt;
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这种方式不对绑定 PV 的优先级做任何保证。 如果其它的 PVC 能够使用过这个 PV，必须要先保留存储卷。
在需要绑定的 PV &lt;code&gt;claimRef&lt;/code&gt; 的 PVC 这样其它的 PVC 才不能绑定这个 PV。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo-pv&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;claimRef&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo-pvc&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo&lt;/span&gt;
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果想要使用 &lt;code&gt;claimPolicy&lt;/code&gt; 是 &lt;code&gt;Retain&lt;/code&gt; 的 PV 这一招很有用， 包括重复使用那些已经存在的 PV。&lt;/p&gt;
&lt;!--
### Expanding Persistent Volumes Claims






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [beta]&lt;/code&gt;
&lt;/div&gt;



Support for expanding PersistentVolumeClaims (PVCs) is now enabled by default. You can expand
the following types of volumes:

* gcePersistentDisk
* awsElasticBlockStore
* Cinder
* glusterfs
* rbd
* Azure File
* Azure Disk
* Portworx
* FlexVolumes
* CSI

You can only expand a PVC if its storage class&#39;s `allowVolumeExpansion` field is set to true.

``` yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gluster-vol-default
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: &#34;http://192.168.10.100:8080&#34;
  restuser: &#34;&#34;
  secretNamespace: &#34;&#34;
  secretName: &#34;&#34;
allowVolumeExpansion: true
```

To request a larger volume for a PVC, edit the PVC object and specify a larger
size. This triggers expansion of the volume that backs the underlying PersistentVolume. A
new PersistentVolume is never created to satisfy the claim. Instead, an existing volume is resized.
 --&gt;
&lt;h3 id=&#34;扩展-pvc&#34;&gt;扩展 PVC&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;对 PVC 的扩展默认是启用的。 可以对以下类型的卷进行扩展:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;gcePersistentDisk&lt;/li&gt;
&lt;li&gt;awsElasticBlockStore&lt;/li&gt;
&lt;li&gt;Cinder&lt;/li&gt;
&lt;li&gt;glusterfs&lt;/li&gt;
&lt;li&gt;rbd&lt;/li&gt;
&lt;li&gt;Azure File&lt;/li&gt;
&lt;li&gt;Azure Disk&lt;/li&gt;
&lt;li&gt;Portworx&lt;/li&gt;
&lt;li&gt;FlexVolumes&lt;/li&gt;
&lt;li&gt;CSI&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;只能对 StorageClass 的 &lt;code&gt;allowVolumeExpansion&lt;/code&gt; 字段为 &lt;code&gt;true&lt;/code&gt; PVC 进行扩展&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gluster-vol-default&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/glusterfs&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;resturl&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://192.168.10.100:8080&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;restuser&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;secretNamespace&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;allowVolumeExpansion&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;想要为 PVC 申请一个更大的卷，只需要修改 PVC 对象，设置一个更大的容量。 这会触发卷底层的 PV 的扩充。
这样做不会创建一个新的 PV 来满足这个 PVC， 而是通过修改原来的容量来实现。&lt;/p&gt;
&lt;!--
#### CSI Volume expansion






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [beta]&lt;/code&gt;
&lt;/div&gt;



Support for expanding CSI volumes is enabled by default but it also requires a specific CSI driver to support volume expansion. Refer to documentation of the specific CSI driver for more information.
 --&gt;
&lt;h4 id=&#34;csi-卷扩容&#34;&gt;CSI 卷扩容&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;对 CSI 卷扩展 扩容默认是开启的，但也是需要相应的 CSI 驱动支持卷扩容。 具体信息请参阅相应
CSI 驱动的文档。&lt;/p&gt;
&lt;!--
#### Resizing a volume containing a file system

You can only resize volumes containing a file system if the file system is XFS, Ext3, or Ext4.

When a volume contains a file system, the file system is only resized when a new Pod is using
the PersistentVolumeClaim in `ReadWrite` mode. File system expansion is either done when a Pod is starting up
or when a Pod is running and the underlying file system supports online expansion.

FlexVolumes allow resize if the driver is set with the `RequiresFSResize` capability to `true`.
The FlexVolume can be resized on Pod restart.
 --&gt;
&lt;h4 id=&#34;修改包含文件系统的卷的容量&#34;&gt;修改包含文件系统的卷的容量&lt;/h4&gt;
&lt;p&gt;如果郑文件系统是  XFS, Ext3, Ext4 之一，则可以修改其容量。&lt;/p&gt;
&lt;p&gt;当卷中包含文件文件系统时，只有在
新的 Pod 使用 &lt;code&gt;ReadWrite&lt;/code&gt; 模式的 PVC 时才会变更容量。文件系统的扩容只有在 Pod 启动或
正在运行的 Pod 底层的文件系统支持在线扩容时才能生效。&lt;/p&gt;
&lt;p&gt;FlexVolume 允许在 驱动的 &lt;code&gt;RequiresFSResize&lt;/code&gt; 设置为 &lt;code&gt;true&lt;/code&gt; 时变更容量。
FlexVolume 的容量变更只能在 Pod 重启时生效。&lt;/p&gt;
&lt;!--
#### Resizing an in-use PersistentVolumeClaim






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.15 [beta]&lt;/code&gt;
&lt;/div&gt;



&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Expanding in-use PVCs is available as beta since Kubernetes 1.15, and as alpha since 1.11. The &lt;code&gt;ExpandInUsePersistentVolumes&lt;/code&gt; feature must be enabled, which is the case automatically for many clusters for beta features. Refer to the &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;feature gate&lt;/a&gt; documentation for more information.&lt;/div&gt;
&lt;/blockquote&gt;


In this case, you don&#39;t need to delete and recreate a Pod or deployment that is using an existing PVC.
Any in-use PVC automatically becomes available to its Pod as soon as its file system has been expanded.
This feature has no effect on PVCs that are not in use by a Pod or deployment. You must create a Pod that
uses the PVC before the expansion can complete.


Similar to other volume types - FlexVolume volumes can also be expanded when in-use by a Pod.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; FlexVolume resize is possible only when the underlying driver supports resize.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Expanding EBS volumes is a time-consuming operation. Also, there is a per-volume quota of one modification every 6 hours.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h4 id=&#34;对使用中的-persistentvolumeclaim-变更容量&#34;&gt;对使用中的 PersistentVolumeClaim 变更容量&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.15 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 对使用中的 PersistentVolumeClaim 变更容量 在 k8s &lt;code&gt;v1.15&lt;/code&gt; 是 &lt;code&gt;beta&lt;/code&gt; 状态，
&lt;code&gt;v1.11&lt;/code&gt; 是 &lt;code&gt;alpha&lt;/code&gt; 状态, &lt;code&gt;ExpandInUsePersistentVolumes&lt;/code&gt; 在 &lt;code&gt;beta&lt;/code&gt; 时是默认打开的。
更多信息请见
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;在这种情况下，不需要删除或重新创建使用现有 PVC 的 Pod 或 Deployment. 任意使用中的 PVC 在
文件系统扩容后在 Pod 中自动变得可用。 这个特性对那些没有被 Pod 或 Deployment 的 PVC 无效。
要想扩容生效必须要有一个使用 PVC 的 Pod。&lt;/p&gt;
&lt;p&gt;与其它的卷类型类似， FlexVolume 也可以在被 Pod 使用时扩容。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; FlexVolume 变量容量只能在底层驱动支持的情况下才行。&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 对 EBS 卷扩容是一个耗时的操作。并且还有一个每个卷每 6 个小时只能扩容的限制&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
#### Recovering from Failure when Expanding Volumes

If expanding underlying storage fails, the cluster administrator can manually recover the Persistent Volume Claim (PVC) state and cancel the resize requests. Otherwise, the resize requests are continuously retried by the controller without administrator intervention.

1. Mark the PersistentVolume(PV) that is bound to the PersistentVolumeClaim(PVC) with `Retain` reclaim policy.
2. Delete the PVC. Since PV has `Retain` reclaim policy - we will not lose any data when we recreate the PVC.
3. Delete the `claimRef` entry from PV specs, so as new PVC can bind to it. This should make the PV `Available`.
4. Re-create the PVC with smaller size than PV and set `volumeName` field of the PVC to the name of the PV. This should bind new PVC to existing PV.
5. Don&#39;t forget to restore the reclaim policy of the PV.
 --&gt;
&lt;h4 id=&#34;从卷扩展失败中恢复&#34;&gt;从卷扩展失败中恢复&lt;/h4&gt;
&lt;p&gt;如果底层存储扩容的失败，集群管理可以通过手动恢复 PVC 的状态，取消扩容请求。否则，在没管理员中止
的情况下控制器会不断地重试容量变更请求。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将与 PersistentVolumeClaim(PVC) 绑定的 PersistentVolume(PV) 回收策略改为 &lt;code&gt;Retain&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;删除 PVC。 因为 PV 的回收策略是 &lt;code&gt;Retain&lt;/code&gt;，所以在重新创建 PVC 之前数据不会丢失。&lt;/li&gt;
&lt;li&gt;删除 PV 对象中的 &lt;code&gt;claimRef&lt;/code&gt; 实体， 这样新的 PVC 才可以与它绑定， 可以可以将 PV 变为 &lt;code&gt;Available&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;以小于 PV 的容量重新创建一个 PVC，这个 PVC 的 &lt;code&gt;volumeName&lt;/code&gt; 设置 PV 的名称。这样就可以将
原来的 PV 绑定到新的 PVC 上&lt;/li&gt;
&lt;li&gt;不要忘记将 PV 的回收策略也改加原来的值&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
## Types of Persistent Volumes

PersistentVolume types are implemented as plugins.  Kubernetes currently supports the following plugins:

* GCEPersistentDisk
* AWSElasticBlockStore
* AzureFile
* AzureDisk
* CSI
* FC (Fibre Channel)
* FlexVolume
* Flocker
* NFS
* iSCSI
* RBD (Ceph Block Device)
* CephFS
* Cinder (OpenStack block storage)
* Glusterfs
* VsphereVolume
* Quobyte Volumes
* HostPath (Single node testing only -- local storage is not supported in any way and WILL NOT WORK in a multi-node cluster)
* Portworx Volumes
* ScaleIO Volumes
* StorageOS
 --&gt;
&lt;h2 id=&#34;持久化卷的类型&#34;&gt;持久化卷的类型&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;PersistentVolume&lt;/code&gt; 的类型随同实现插件。 目前 k8s 支持以下插件:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GCEPersistentDisk&lt;/li&gt;
&lt;li&gt;AWSElasticBlockStore&lt;/li&gt;
&lt;li&gt;AzureFile&lt;/li&gt;
&lt;li&gt;AzureDisk&lt;/li&gt;
&lt;li&gt;CSI&lt;/li&gt;
&lt;li&gt;FC (Fibre Channel)&lt;/li&gt;
&lt;li&gt;FlexVolume&lt;/li&gt;
&lt;li&gt;Flocker&lt;/li&gt;
&lt;li&gt;NFS&lt;/li&gt;
&lt;li&gt;iSCSI&lt;/li&gt;
&lt;li&gt;RBD (Ceph Block Device)&lt;/li&gt;
&lt;li&gt;CephFS&lt;/li&gt;
&lt;li&gt;Cinder (OpenStack block storage)&lt;/li&gt;
&lt;li&gt;Glusterfs&lt;/li&gt;
&lt;li&gt;VsphereVolume&lt;/li&gt;
&lt;li&gt;Quobyte Volumes&lt;/li&gt;
&lt;li&gt;HostPath (只在单节点上测试过 &amp;ndash; 本地存储现在不支持多节点集群，将来也永远不会支持)&lt;/li&gt;
&lt;li&gt;Portworx Volumes&lt;/li&gt;
&lt;li&gt;ScaleIO Volumes&lt;/li&gt;
&lt;li&gt;StorageOS&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Persistent Volumes

Each PV contains a spec and status, which is the specification and status of the volume.
The name of a PersistentVolume object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /tmp
    server: 172.17.0.2
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Helper programs relating to the volume type may be required for consumption of a PersistentVolume within a cluster.  In this example, the PersistentVolume is of type NFS and the helper program /sbin/mount.nfs is required to support the mounting of NFS filesystems.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;persistentvolume&#34;&gt;PersistentVolume&lt;/h2&gt;
&lt;p&gt;每个 PV 包含一个 &lt;code&gt;spec&lt;/code&gt; 和 &lt;code&gt;status&lt;/code&gt;, 分别是对这个卷的配置定义和状态。 &lt;code&gt;PersistentVolume&lt;/code&gt;
对象的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pv0003&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;capacity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Filesystem&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeReclaimPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Recycle&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;mountOptions&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;hard&lt;/span&gt;
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;nfsvers=4.1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;nfs&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/tmp&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;server&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;172.17.0.2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Helper programs relating to the volume type may be required for consumption of a PersistentVolume within a cluster.  In this example, the PersistentVolume is of type NFS and the helper program /sbin/mount.nfs is required to support the mounting of NFS filesystems.
集群中辅助程序应该能够正确处理相关类型的卷。 在上面的例子中，PersistentVolume 的类型是 NFS
辅助程序 &lt;code&gt;/sbin/mount.nfs&lt;/code&gt; 就需要支持挂载 NFS 文件系统&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Capacity

Generally, a PV will have a specific storage capacity.  This is set using the PV&#39;s `capacity` attribute.  See the Kubernetes [Resource Model](https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md) to understand the units expected by `capacity`.

Currently, storage size is the only resource that can be set or requested.  Future attributes may include IOPS, throughput, etc.
 --&gt;
&lt;h3 id=&#34;容量capacity&#34;&gt;容量(Capacity)&lt;/h3&gt;
&lt;p&gt;通常，一个 PV 是有指定存储容量的。 这是通过 PV 的 &lt;code&gt;capacity&lt;/code&gt; 属性设置的。 理解受 &lt;code&gt;capacity&lt;/code&gt;
见 &lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md&#34;&gt;Resource Model&lt;/a&gt;
Currently, storage size is the only resource that can be set or requested.  Future attributes may include IOPS, throughput, etc.&lt;/p&gt;
&lt;!--
### Volume Mode






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;



Kubernetes supports two `volumeModes` of PersistentVolumes: `Filesystem` and `Block`.

`volumeMode` is an optional API parameter.
`Filesystem` is the default mode used when `volumeMode` parameter is omitted.

A volume with `volumeMode: Filesystem` is *mounted* into Pods into a directory. If the volume
is backed by a block device and the device is empty, Kuberneretes creates a filesystem
on the device before mounting it for the first time.

You can set the value of `volumeMode` to `Block` to use a volume as a raw block device.
Such volume is presented into a Pod as a block device, without any filesystem on it.
This mode is useful to provide a Pod the fastest possible way to access a volume, without
any filesystem layer between the Pod and the volume. On the other hand, the application
running in the Pod must know how to handle a raw block device.
See [Raw Block Volume Support](#raw-block-volume-support)
for an example on how to use a volume with `volumeMode: Block` in a Pod.
 --&gt;
&lt;h3 id=&#34;卷模式&#34;&gt;卷模式&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;k8s 支持两种 PersistentVolume 卷模式(&lt;code&gt;volumeModes&lt;/code&gt;): 文件系统(&lt;code&gt;Filesystem&lt;/code&gt;) 和 块设备(&lt;code&gt;Block&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;volumeMode&lt;/code&gt; 是一个可选 API 参数
如果没有设置 &lt;code&gt;volumeMode&lt;/code&gt;， 则默认使用 &lt;code&gt;Filesystem&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;一个卷模式为 &lt;code&gt;volumeMode: Filesystem&lt;/code&gt; 的卷 &lt;em&gt;挂载&lt;/em&gt; 到 Pod 中的一个目录。 如果卷的后端是
一个块设备并且这个设备是空的， k8s 会在第一次挂载之前在块设备上创建一个文件系统。&lt;/p&gt;
&lt;p&gt;也可以将 &lt;code&gt;volumeMode&lt;/code&gt; 的值设置为 &lt;code&gt;Block&lt;/code&gt; 以块设备的方式来使用这个卷。 这个卷就会以块设备的
形式存在于 Pod 中， 其中不会有任何文件系统。 这种方式在为 Pod 提供该卷可能最佳的访问速度，
在 Pod 和 卷之间没有任何文件系统层。 另一方面， Pod 中运行的应用必须要能正确使用块设备。&lt;/p&gt;
&lt;p&gt;可以在
&lt;a href=&#34;#raw-block-volume-support&#34;&gt;块设备卷支持&lt;/a&gt;
查看使用 &lt;code&gt;volumeMode: Block&lt;/code&gt; 卷模式卷的 Pod 。&lt;/p&gt;
&lt;!--
### Access Modes

A PersistentVolume can be mounted on a host in any way supported by the resource provider. As shown in the table below, providers will have different capabilities and each PV&#39;s access modes are set to the specific modes supported by that particular volume.  For example, NFS can support multiple read/write clients, but a specific NFS PV might be exported on the server as read-only. Each PV gets its own set of access modes describing that specific PV&#39;s capabilities.

The access modes are:

* ReadWriteOnce -- the volume can be mounted as read-write by a single node
* ReadOnlyMany -- the volume can be mounted read-only by many nodes
* ReadWriteMany -- the volume can be mounted as read-write by many nodes

In the CLI, the access modes are abbreviated to:

* RWO - ReadWriteOnce
* ROX - ReadOnlyMany
* RWX - ReadWriteMany

&gt; __Important!__ A volume can only be mounted using one access mode at a time, even if it supports many.  For example, a GCEPersistentDisk can be mounted as ReadWriteOnce by a single node or ReadOnlyMany by many nodes, but not at the same time.


| Volume Plugin        | ReadWriteOnce          | ReadOnlyMany          | ReadWriteMany|
| :---                 | :---:                  | :---:                 | :---:        |
| AWSElasticBlockStore | &amp;#x2713;               | -                     | -            |
| AzureFile            | &amp;#x2713;               | &amp;#x2713;              | &amp;#x2713;     |
| AzureDisk            | &amp;#x2713;               | -                     | -            |
| CephFS               | &amp;#x2713;               | &amp;#x2713;              | &amp;#x2713;     |
| Cinder               | &amp;#x2713;               | -                     | -            |
| CSI                  | depends on the driver  | depends on the driver | depends on the driver |
| FC                   | &amp;#x2713;               | &amp;#x2713;              | -            |
| FlexVolume           | &amp;#x2713;               | &amp;#x2713;              | depends on the driver |
| Flocker              | &amp;#x2713;               | -                     | -            |
| GCEPersistentDisk    | &amp;#x2713;               | &amp;#x2713;              | -            |
| Glusterfs            | &amp;#x2713;               | &amp;#x2713;              | &amp;#x2713;     |
| HostPath             | &amp;#x2713;               | -                     | -            |
| iSCSI                | &amp;#x2713;               | &amp;#x2713;              | -            |
| Quobyte              | &amp;#x2713;               | &amp;#x2713;              | &amp;#x2713;     |
| NFS                  | &amp;#x2713;               | &amp;#x2713;              | &amp;#x2713;     |
| RBD                  | &amp;#x2713;               | &amp;#x2713;              | -            |
| VsphereVolume        | &amp;#x2713;               | -                     | - (works when Pods are collocated)  |
| PortworxVolume       | &amp;#x2713;               | -                     | &amp;#x2713;     |
| ScaleIO              | &amp;#x2713;               | &amp;#x2713;              | -            |
| StorageOS            | &amp;#x2713;               | -                     | -            |
 --&gt;
&lt;h3 id=&#34;访问模式&#34;&gt;访问模式&lt;/h3&gt;
&lt;p&gt;A PersistentVolume can be mounted on a host in any way supported by the resource provider. As shown in the table below, providers will have different capabilities and each PV&amp;rsquo;s access modes are set to the specific modes supported by that particular volume.  For example, NFS can support multiple read/write clients, but a specific NFS PV might be exported on the server as read-only. Each PV gets its own set of access modes describing that specific PV&amp;rsquo;s capabilities.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PersistentVolume&lt;/code&gt; 可以以任意资源提供者支持的方式挂载到主机中。 如下表所示， 不同的提供者
拥有不同能力，每个 PV 的模式可以在对应卷上设置支持的模式。 例如， NFS 支持多客户端读写,但是可以
将一个 NFS PV 以只读方式提供。 每个 PV 都可以独立设置各自的方式模式。&lt;/p&gt;
&lt;p&gt;访问模式有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ReadWriteOnce &amp;ndash; 卷只能被一个消费者以只读方式挂载&lt;/li&gt;
&lt;li&gt;ReadOnlyMany &amp;ndash; 卷能被多个消费者以只读方式挂载&lt;/li&gt;
&lt;li&gt;ReadWriteMany &amp;ndash; 卷能被多个消费者以读写方式挂载&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在命令，这些模式的简写如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RWO - ReadWriteOnce&lt;/li&gt;
&lt;li&gt;ROX - ReadOnlyMany&lt;/li&gt;
&lt;li&gt;RWX - ReadWriteMany&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;重要!&lt;/strong&gt; 一个卷一次只能以一种访问模式挂载，即便它是支持多种的。 例如， 一个 GCEPersistentDisk 可以
在一个消费者挂载为 ReadWriteOnce 或可以在多个消费者挂载为 ReadOnlyMany， 但不能同时挂载
ReadWriteOnce 和 ReadOnlyMany。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Volume Plugin&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteOnce&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadOnlyMany&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteMany&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AWSElasticBlockStore&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AzureFile&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AzureDisk&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CephFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Cinder&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CSI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;基于驱动&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;基于驱动&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;基于驱动&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FC&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FlexVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;基于驱动&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Flocker&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;GCEPersistentDisk&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Glusterfs&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HostPath&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;iSCSI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Quobyte&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;RBD&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;VsphereVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;- (works when Pods are collocated)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PortworxVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ScaleIO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;StorageOS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
### Class

A PV can have a class, which is specified by setting the
`storageClassName` attribute to the name of a
[StorageClass](/docs/concepts/storage/storage-classes/).
A PV of a particular class can only be bound to PVCs requesting
that class. A PV with no `storageClassName` has no class and can only be bound
to PVCs that request no particular class.

In the past, the annotation `volume.beta.kubernetes.io/storage-class` was used instead
of the `storageClassName` attribute. This annotation is still working; however,
it will become fully deprecated in a future Kubernetes release.
 --&gt;
&lt;h3 id=&#34;class&#34;&gt;类别&lt;/h3&gt;
&lt;p&gt;一个 PV 可以有一个类别， 可以通过 &lt;code&gt;storageClassName&lt;/code&gt; 属性来设置一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes/&#34;&gt;StorageClass&lt;/a&gt;.
一个指定类别的 PV 只能与请求对应类的 PVC 相绑定。 没有设置 &lt;code&gt;storageClassName&lt;/code&gt; 的 PV 是没有类别的
并且只能与没有请求类别的 PVC 绑定。&lt;/p&gt;
&lt;p&gt;在过去是通过 &lt;code&gt;volume.beta.kubernetes.io/storage-class&lt;/code&gt; 注解设置类别而不是 &lt;code&gt;storageClassName&lt;/code&gt;。
这个注解目前还能用；但在未来的版本中会完全废弃。&lt;/p&gt;
&lt;!--
### Reclaim Policy

Current reclaim policies are:

* Retain -- manual reclamation
* Recycle -- basic scrub (`rm -rf /thevolume/*`)
* Delete -- associated storage asset such as AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder volume is deleted

Currently, only NFS and HostPath support recycling. AWS EBS, GCE PD, Azure Disk, and Cinder volumes support deletion.
 --&gt;
&lt;h3 id=&#34;reclain-policy&#34;&gt;回收策略&lt;/h3&gt;
&lt;p&gt;目前支持的回收策略:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Retain &amp;ndash; 手动回收&lt;/li&gt;
&lt;li&gt;Recycle &amp;ndash; 基础清理 (&lt;code&gt;rm -rf /thevolume/*&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Delete &amp;ndash; 关联存储资源如 AWS EBS, GCE PD, Azure Disk, OpenStack Cinder 卷也会被删除。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前只有 NFS 和 HostPath 支持 &lt;code&gt;Recycle&lt;/code&gt;
AWS EBS, GCE PD, Azure Disk, and Cinder 卷支持 &lt;code&gt;Delete&lt;/code&gt;&lt;/p&gt;
&lt;!--
### Mount Options

A Kubernetes administrator can specify additional mount options for when a Persistent Volume is mounted on a node.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Not all Persistent Volume types support mount options.&lt;/div&gt;
&lt;/blockquote&gt;


The following volume types support mount options:

* AWSElasticBlockStore
* AzureDisk
* AzureFile
* CephFS
* Cinder (OpenStack block storage)
* GCEPersistentDisk
* Glusterfs
* NFS
* Quobyte Volumes
* RBD (Ceph Block Device)
* StorageOS
* VsphereVolume
* iSCSI

Mount options are not validated, so mount will simply fail if one is invalid.

In the past, the annotation `volume.beta.kubernetes.io/mount-options` was used instead
of the `mountOptions` attribute. This annotation is still working; however,
it will become fully deprecated in a future Kubernetes release.
 --&gt;
&lt;h3 id=&#34;挂载选项&#34;&gt;挂载选项&lt;/h3&gt;
&lt;p&gt;k8s 管理员可以在 PV 挂载时指定额外的挂载选项。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 不是所有的 PV 类型都支持挂载选项&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;以下卷类型支持挂载选项:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AWSElasticBlockStore&lt;/li&gt;
&lt;li&gt;AzureDisk&lt;/li&gt;
&lt;li&gt;AzureFile&lt;/li&gt;
&lt;li&gt;CephFS&lt;/li&gt;
&lt;li&gt;Cinder (OpenStack block storage)&lt;/li&gt;
&lt;li&gt;GCEPersistentDisk&lt;/li&gt;
&lt;li&gt;Glusterfs&lt;/li&gt;
&lt;li&gt;NFS&lt;/li&gt;
&lt;li&gt;Quobyte Volumes&lt;/li&gt;
&lt;li&gt;RBD (Ceph Block Device)&lt;/li&gt;
&lt;li&gt;StorageOS&lt;/li&gt;
&lt;li&gt;VsphereVolume&lt;/li&gt;
&lt;li&gt;iSCSI&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;挂载选项不会验证，如果其中有无效选项就会挂载失败。&lt;/p&gt;
&lt;p&gt;在过去 使用的是 &lt;code&gt;volume.beta.kubernetes.io/mount-options&lt;/code&gt; 注解设置挂载选项，而不是 &lt;code&gt;mountOptions&lt;/code&gt;
这个注解目前还可以用，但在未来的版本中会完全废弃。&lt;/p&gt;
&lt;!--
### Node Affinity

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; For most volume types, you do not need to set this field. It is automatically populated for &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/#awselasticblockstore&#34;&gt;AWS EBS&lt;/a&gt;, &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/#gcepersistentdisk&#34;&gt;GCE PD&lt;/a&gt; and &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/#azuredisk&#34;&gt;Azure Disk&lt;/a&gt; volume block types. You need to explicitly set this for &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/#local&#34;&gt;local&lt;/a&gt; volumes.&lt;/div&gt;
&lt;/blockquote&gt;


A PV can specify [node affinity](/docs/reference/generated/kubernetes-api/v1.19/#volumenodeaffinity-v1-core) to define constraints that limit what nodes this volume can be accessed from. Pods that use a PV will only be scheduled to nodes that are selected by the node affinity.
 --&gt;
&lt;h3 id=&#34;节点亲和性&#34;&gt;节点亲和性&lt;/h3&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 对于大多数卷类型， 是不需要设置这个字段的。
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#awselasticblockstore&#34;&gt;AWS EBS&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#gcepersistentdisk&#34;&gt;GCE PD&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#azuredisk&#34;&gt;Azure Disk&lt;/a&gt;
卷块设备会自动添加。
但需要为
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#local&#34;&gt;local&lt;/a&gt;
卷需要显示设置该字段&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;PV 可以通过设置
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#volumenodeaffinity-v1-core&#34;&gt;节点亲和性&lt;/a&gt;
来定义限制哪些节点能够访问这个卷。
Pod 只会被调度到节点亲和性选择的节点上。&lt;/p&gt;
&lt;!--
### Phase

A volume will be in one of the following phases:

* Available -- a free resource that is not yet bound to a claim
* Bound -- the volume is bound to a claim
* Released -- the claim has been deleted, but the resource is not yet reclaimed by the cluster
* Failed -- the volume has failed its automatic reclamation

The CLI will show the name of the PVC bound to the PV.
 --&gt;
&lt;h3 id=&#34;阶段&#34;&gt;阶段&lt;/h3&gt;
&lt;p&gt;一个卷会处于以下阶段中的一个:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Available &amp;ndash; 一个空闲资源，还没有被绑定到 PVC&lt;/li&gt;
&lt;li&gt;Bound &amp;ndash; 这个卷被绑定了一个 PVC&lt;/li&gt;
&lt;li&gt;Released &amp;ndash; 绑定的 PVC 已经被删除，但资源还没有被集群回收&lt;/li&gt;
&lt;li&gt;Failed &amp;ndash; 这个卷自动回收失败&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;命令行可以显示与 PV 绑定的 PVC 的名称&lt;/p&gt;
&lt;!--
## PersistentVolumeClaims

Each PVC contains a spec and status, which is the specification and status of the claim.
The name of a PersistentVolumeClaim object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi
  storageClassName: slow
  selector:
    matchLabels:
      release: &#34;stable&#34;
    matchExpressions:
      - {key: environment, operator: In, values: [dev]}
```
 --&gt;
&lt;h2 id=&#34;persistentvolumeclaims&#34;&gt;PersistentVolumeClaim&lt;/h2&gt;
&lt;p&gt;每个 PVC 包含 &lt;code&gt;spec&lt;/code&gt; 和 &lt;code&gt;status&lt;/code&gt;, 其中包含 PVC 的配置定义和状态。
&lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 对象的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myclaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Filesystem&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;8Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;release&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stable&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
      - {&lt;span style=&#34;color:#f92672&#34;&gt;key: environment, operator: In, values&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;dev]}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Access Modes

Claims use the same conventions as volumes when requesting storage with specific access modes.
 --&gt;
&lt;h3 id=&#34;访问模式-1&#34;&gt;访问模式&lt;/h3&gt;
&lt;p&gt;Claims use the same conventions as volumes when requesting storage with specific access modes.
PVC 的访问模式与卷在请求存储时指定的访问模式一至&lt;/p&gt;
&lt;!--
### Volume Modes

Claims use the same convention as volumes to indicate the consumption of the volume as either a filesystem or block device.
 --&gt;
&lt;h3 id=&#34;卷模式-1&#34;&gt;卷模式&lt;/h3&gt;
&lt;p&gt;PVC 的卷模式与卷指定的卷模式一至，可以是 文件系统 或 块设备&lt;/p&gt;
&lt;!--
### Resources

Claims, like Pods, can request specific quantities of a resource. In this case, the request is for storage. The same [resource model](https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md) applies to both volumes and claims.
 --&gt;
&lt;h3 id=&#34;资源&#34;&gt;资源&lt;/h3&gt;
&lt;p&gt;PVC 与 Pod 类似， 可以请求指定数量的资源。 在这种情况下， 请求是提存储。
&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md&#34;&gt;资源模式&lt;/a&gt;
可应用到卷和 PVC&lt;/p&gt;
&lt;!--
### Selector

Claims can specify a [label selector](/docs/concepts/overview/working-with-objects/labels/#label-selectors) to further filter the set of volumes. Only the volumes whose labels match the selector can be bound to the claim. The selector can consist of two fields:

* `matchLabels` - the volume must have a label with this value
* `matchExpressions` - a list of requirements made by specifying key, list of values, and operator that relates the key and values. Valid operators include In, NotIn, Exists, and DoesNotExist.

All of the requirements, from both `matchLabels` and `matchExpressions`, are ANDed together – they must all be satisfied in order to match.
 --&gt;
&lt;h3 id=&#34;选择器&#34;&gt;选择器&lt;/h3&gt;
&lt;p&gt;PVC 可以指定一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/labels/#label-selectors&#34;&gt;标签选择器&lt;/a&gt;
来选择卷集合。 只有与选择器匹配的卷可以与 PVC 绑定。 选择器可以包含以下两个字段&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;matchLabels&lt;/code&gt; - 卷必须要拥有与其对应的标签&lt;/li&gt;
&lt;li&gt;&lt;code&gt;matchExpressions&lt;/code&gt; - 一个由指定键和值列表，以及键和值相应的操作符组成的条件列表，
可用的操作符包含 &lt;code&gt;In&lt;/code&gt;, &lt;code&gt;NotIn&lt;/code&gt;, &lt;code&gt;Exists&lt;/code&gt;, &lt;code&gt;DoesNotExist&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;来自 &lt;code&gt;matchLabels&lt;/code&gt; 和 &lt;code&gt;matchExpressions&lt;/code&gt; 的所有条件以逻辑与方式组合。
必须要满足所有条件才能匹配到。&lt;/p&gt;
&lt;!--
### Class

A claim can request a particular class by specifying the name of a
[StorageClass](/docs/concepts/storage/storage-classes/)
using the attribute `storageClassName`.
Only PVs of the requested class, ones with the same `storageClassName` as the PVC, can
be bound to the PVC.

PVCs don&#39;t necessarily have to request a class. A PVC with its `storageClassName` set
equal to `&#34;&#34;` is always interpreted to be requesting a PV with no class, so it
can only be bound to PVs with no class (no annotation or one set equal to
`&#34;&#34;`). A PVC with no `storageClassName` is not quite the same and is treated differently
by the cluster, depending on whether the
[`DefaultStorageClass` admission plugin](/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)
is turned on.

* If the admission plugin is turned on, the administrator may specify a
  default StorageClass. All PVCs that have no `storageClassName` can be bound only to
  PVs of that default. Specifying a default StorageClass is done by setting the
  annotation `storageclass.kubernetes.io/is-default-class` equal to `true` in
  a StorageClass object. If the administrator does not specify a default, the
  cluster responds to PVC creation as if the admission plugin were turned off. If
  more than one default is specified, the admission plugin forbids the creation of
  all PVCs.
* If the admission plugin is turned off, there is no notion of a default
  StorageClass. All PVCs that have no `storageClassName` can be bound only to PVs that
  have no class. In this case, the PVCs that have no `storageClassName` are treated the
  same way as PVCs that have their `storageClassName` set to `&#34;&#34;`.

Depending on installation method, a default StorageClass may be deployed
to a Kubernetes cluster by addon manager during installation.

When a PVC specifies a `selector` in addition to requesting a StorageClass,
the requirements are ANDed together: only a PV of the requested class and with
the requested labels may be bound to the PVC.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Currently, a PVC with a non-empty &lt;code&gt;selector&lt;/code&gt; can&amp;rsquo;t have a PV dynamically provisioned for it.&lt;/div&gt;
&lt;/blockquote&gt;


In the past, the annotation `volume.beta.kubernetes.io/storage-class` was used instead
of `storageClassName` attribute. This annotation is still working; however,
it won&#39;t be supported in a future Kubernetes release.
 --&gt;
&lt;h3 id=&#34;类别&#34;&gt;类别&lt;/h3&gt;
&lt;p&gt;PVC 可以通过设置 &lt;code&gt;storageClassName&lt;/code&gt; 的值为一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-classes/&#34;&gt;StorageClass&lt;/a&gt;
的名称来指定其使用该类别。
PV 与 PVC 只有在拥有相同 &lt;code&gt;storageClassName&lt;/code&gt; 的情况下才能绑定。&lt;/p&gt;
&lt;p&gt;PVC 并不是必须要设置一个类别。 PVC 可以将 &lt;code&gt;storageClassName&lt;/code&gt; 设置为  &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;， 表示请求一个
没有类别的 PV，因此它也只能与没有类别的 PV 绑定(没有类别注解或类别注解值为 &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;)。
如果 PVC 没有设置 &lt;code&gt;storageClassName&lt;/code&gt;，则会基于是否开启
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass&#34;&gt;&lt;code&gt;DefaultStorageClass&lt;/code&gt; admission plugin&lt;/a&gt;
以下情况而有不同的表现行为:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果准入插件是开启的， 则管理员可以设置一个默认的 &lt;code&gt;StorageClass&lt;/code&gt;。 所有没有设置 &lt;code&gt;storageClassName&lt;/code&gt;
的 PVC 就只能与这个默认类别的 PV 绑定。 通过将 &lt;code&gt;StorageClass&lt;/code&gt; 对象的
&lt;code&gt;storageclass.kubernetes.io/is-default-class&lt;/code&gt; 注解值设置为 &lt;code&gt;true&lt;/code&gt; 可以将其设置默认。
如果管理不有设置默认类别， 集群应答 PVC 创建操作与准入插件关闭相同。 如果设置了不只一个默认
类别，则准入插件会阻止所有 PVC 的创建&lt;/li&gt;
&lt;li&gt;如果准入插件没有打开， 那就没有默认 &lt;code&gt;StorageClass&lt;/code&gt; 这回事。 所有没有设置 &lt;code&gt;storageClassName&lt;/code&gt;
的 PVC 就只能与没有设置类型的 PV 绑定。 在这种情况下， 没有设置 &lt;code&gt;storageClassName&lt;/code&gt; 与
将 &lt;code&gt;storageClassName&lt;/code&gt; 设置为 &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt; 的处理方式是一样的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;基于安装方式， 默认 &lt;code&gt;StorageClass&lt;/code&gt; 可以被插件管理器在安装时加入集群。&lt;/p&gt;
&lt;p&gt;当 PVC 设置 &lt;code&gt;selector&lt;/code&gt; 来请求 StorageClass， 所有条件是逻辑与关系: 只有包含所有选择器需要
的标签的类型才能被绑定的 PVC。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 目前，如果 PVC 的 &lt;code&gt;selector&lt;/code&gt; 是空，则不能实现动态管理 PV&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;在过去， 用的是 &lt;code&gt;volume.beta.kubernetes.io/storage-class&lt;/code&gt; 注解而不是 &lt;code&gt;storageClassName&lt;/code&gt; 属性。
这个注解目前还有用，但在未来版本中会完全废弃。&lt;/p&gt;
&lt;!--
## Claims As Volumes

Pods access storage by using the claim as a volume. Claims must exist in the same namespace as the Pod using the claim. The cluster finds the claim in the Pod&#39;s namespace and uses it to get the PersistentVolume backing the claim. The volume is then mounted to the host and into the Pod.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: &#34;/var/www/html&#34;
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim
```
 --&gt;
&lt;h2 id=&#34;claims-as-volumes&#34;&gt;将 PVC 当作卷(PV)&lt;/h2&gt;
&lt;p&gt;Pod 可以将 PVC 当作卷(PV) 来用作存储。 被引用 PVC 必须要要与 Pod 在同一个命名空间。
集群会在 Pod 所在的命名空间中寻找 PVC 然后使用与其绑定的 PV。 最后将 PV 挂载到主机，最终挂载到
Pod 中。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myfrontend&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/var/www/html&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypd&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypd&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaim&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;claimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myclaim&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### A Note on Namespaces

PersistentVolumes binds are exclusive, and since PersistentVolumeClaims are namespaced objects, mounting claims with &#34;Many&#34; modes (`ROX`, `RWX`) is only possible within one namespace.
 --&gt;
&lt;h3 id=&#34;一个需要注意命名空间的问题&#34;&gt;一个需要注意命名空间的问题&lt;/h3&gt;
&lt;p&gt;PersistentVolume 的绑定是独占的，而又因为 PVC 是命名空间级别的对象，因此在对 PVC 进行多节点
模式(&lt;code&gt;ROX&lt;/code&gt;, &lt;code&gt;RWX&lt;/code&gt;)挂载时只能针对同一个命名空间的节点。&lt;/p&gt;
&lt;!--
## Raw Block Volume Support






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;



The following volume plugins support raw block volumes, including dynamic provisioning where
applicable:

* AWSElasticBlockStore
* AzureDisk
* CSI
* FC (Fibre Channel)
* GCEPersistentDisk
* iSCSI
* Local volume
* OpenStack Cinder
* RBD (Ceph Block Device)
* VsphereVolume
 --&gt;
&lt;h2 id=&#34;raw-block-volume-support&#34;&gt;块设备卷支持&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;以下卷插件支持块设备卷，包含适配的动态管理:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AWSElasticBlockStore&lt;/li&gt;
&lt;li&gt;AzureDisk&lt;/li&gt;
&lt;li&gt;CSI&lt;/li&gt;
&lt;li&gt;FC (Fibre Channel)&lt;/li&gt;
&lt;li&gt;GCEPersistentDisk&lt;/li&gt;
&lt;li&gt;iSCSI&lt;/li&gt;
&lt;li&gt;Local volume&lt;/li&gt;
&lt;li&gt;OpenStack Cinder&lt;/li&gt;
&lt;li&gt;RBD (Ceph Block Device)&lt;/li&gt;
&lt;li&gt;VsphereVolume&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### PersistentVolume using a Raw Block Volume {#persistent-volume-using-a-raw-block-volume}

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: block-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  persistentVolumeReclaimPolicy: Retain
  fc:
    targetWWNs: [&#34;50060e801049cfd1&#34;]
    lun: 0
    readOnly: false
```
 --&gt;
&lt;h3 id=&#34;persistent-volume-using-a-raw-block-volume&#34;&gt;使用块设备卷的 PersistentVolume&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;block-pv&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;capacity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Block&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeReclaimPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Retain&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;fc&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;targetWWNs&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;50060e801049cfd1&amp;#34;&lt;/span&gt;]
    &lt;span style=&#34;color:#f92672&#34;&gt;lun&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### PersistentVolumeClaim requesting a Raw Block Volume {#persistent-volume-claim-requesting-a-raw-block-volume}

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 10Gi
```
 --&gt;
&lt;h3 id=&#34;persistent-volume-claim-requesting-a-raw-block-volume&#34;&gt;申请块设备卷的 PersistentVolumeClaim&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;block-pvc&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Block&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10Gi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Pod specification adding Raw Block Device path in container

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: [&#34;/bin/sh&#34;, &#34;-c&#34;]
      args: [ &#34;tail -f /dev/null&#34; ]
      volumeDevices:
        - name: data
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: block-pvc
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; When adding a raw block device for a Pod, you specify the device path in the container instead of a mount path.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;在-pod-定义中添加容器中的块设备路径&#34;&gt;在 Pod 定义中添加容器中的块设备路径&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod-with-block-volume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fc-container&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fedora:26&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/bin/sh&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;]
      &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tail -f /dev/null&amp;#34;&lt;/span&gt; ]
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeDevices&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;devicePath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/dev/xvda&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;data&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaim&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;claimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;block-pvc&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 在为一个 Pod 添加块设备时，在容器中指定的是设备路径，而不是挂载路径&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Binding Block Volumes

If a user requests a raw block volume by indicating this using the `volumeMode` field in the PersistentVolumeClaim spec, the binding rules differ slightly from previous releases that didn&#39;t consider this mode as part of the spec.
Listed is a table of possible combinations the user and admin might specify for requesting a raw block device. The table indicates if the volume will be bound or not given the combinations:
Volume binding matrix for statically provisioned volumes:

| PV volumeMode | PVC volumeMode  | Result           |
| --------------|:---------------:| ----------------:|
|   unspecified | unspecified     | BIND             |
|   unspecified | Block           | NO BIND          |
|   unspecified | Filesystem      | BIND             |
|   Block       | unspecified     | NO BIND          |
|   Block       | Block           | BIND             |
|   Block       | Filesystem      | NO BIND          |
|   Filesystem  | Filesystem      | BIND             |
|   Filesystem  | Block           | NO BIND          |
|   Filesystem  | unspecified     | BIND             |

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Only statically provisioned volumes are supported for alpha release. Administrators should take care to consider these values when working with raw block devices.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;绑定块设备&#34;&gt;绑定块设备&lt;/h3&gt;
&lt;p&gt;如果用户在 PVC 配置中使用 &lt;code&gt;volumeMode&lt;/code&gt; 字段指定申请一个块设备卷，则绑定规则与之前版本中
没有在配置中指定 &lt;code&gt;volumeMode&lt;/code&gt; 是有点不一样的。&lt;/p&gt;
&lt;p&gt;以下为用户/管理员在申请块设备时可能的组合列表。 这个表展示了这个组合卷是否能绑定
静态管理卷的绑定矩阵:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;PV volumeMode&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;PVC volumeMode&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Result&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NO BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NO BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NO BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NO BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 在 alpha 特性中只有静态管理的卷被支持。 管理员在操作块设备需要仔细考虑这些值。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Volume Snapshot and Restore Volume from Snapshot Support






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;



Volume snapshot feature was added to support CSI Volume Plugins only. For details, see [volume snapshots](/docs/concepts/storage/volume-snapshots/).

To enable support for restoring a volume from a volume snapshot data source, enable the
`VolumeSnapshotDataSource` feature gate on the apiserver and controller-manager.
 --&gt;
&lt;h2 id=&#34;volume-snapshot-and-restore-volume-from-snapshot-support&#34;&gt;卷快照和快照恢复支持&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;卷快照特性只对 CSI 卷插件支持。详细信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volume-snapshots/&#34;&gt;卷快照&lt;/a&gt;.
要启用对从卷快照恢复的支持，需要在 &lt;code&gt;apiserver&lt;/code&gt; &lt;code&gt;controller-manager&lt;/code&gt; 打开
&lt;code&gt;VolumeSnapshotDataSource&lt;/code&gt; 功能阀&lt;/p&gt;
&lt;!--
### Create a PersistentVolumeClaim from a Volume Snapshot {#create-persistent-volume-claim-from-volume-snapshot}

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: restore-pvc
spec:
  storageClassName: csi-hostpath-sc
  dataSource:
    name: new-snapshot-test
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```
 --&gt;
&lt;h3 id=&#34;create-persistent-volume-claim-from-volume-snapshot&#34;&gt;基于卷快照创建 PVC&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;restore-pvc&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;csi-hostpath-sc&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dataSource&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;new-snapshot-test&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshot&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;apiGroup&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10Gi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Volume Cloning

[Volume Cloning](/docs/concepts/storage/volume-pvc-datasource/) only available for CSI volume plugins.
 --&gt;
&lt;h2 id=&#34;卷克隆&#34;&gt;卷克隆&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volume-pvc-datasource/&#34;&gt;卷克隆&lt;/a&gt; 只存在于 CSI 卷插件&lt;/p&gt;
&lt;!--
### Create PersistentVolumeClaim from an existing PVC {#create-persistent-volume-claim-from-an-existing-pvc}

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cloned-pvc
spec:
  storageClassName: my-csi-plugin
  dataSource:
    name: existing-src-pvc-name
    kind: PersistentVolumeClaim
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```
 --&gt;
&lt;h3 id=&#34;create-persistent-volume-claim-from-an-existing-pvc&#34;&gt;基于存在的 PVC 创建新的 PVC&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cloned-pvc&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-csi-plugin&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dataSource&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;existing-src-pvc-name&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10Gi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Writing Portable Configuration

If you&#39;re writing configuration templates or examples that run on a wide range of clusters
and need persistent storage, it is recommended that you use the following pattern:

- Include PersistentVolumeClaim objects in your bundle of config (alongside
  Deployments, ConfigMaps, etc).
- Do not include PersistentVolume objects in the config, since the user instantiating
  the config may not have permission to create PersistentVolumes.
- Give the user the option of providing a storage class name when instantiating
  the template.
  - If the user provides a storage class name, put that value into the
    `persistentVolumeClaim.storageClassName` field.
    This will cause the PVC to match the right storage
    class if the cluster has StorageClasses enabled by the admin.
  - If the user does not provide a storage class name, leave the
    `persistentVolumeClaim.storageClassName` field as nil. This will cause a
    PV to be automatically provisioned for the user with the default StorageClass
    in the cluster. Many cluster environments have a default StorageClass installed,
    or administrators can create their own default StorageClass.
- In your tooling, watch for PVCs that are not getting bound after some time
  and surface this to the user, as this may indicate that the cluster has no
  dynamic storage support (in which case the user should create a matching PV)
  or the cluster has no storage system (in which case the user cannot deploy
  config requiring PVCs).

  ## 相关资料


* Learn more about [Creating a PersistentVolume](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume).
* Learn more about [Creating a PersistentVolumeClaim](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim).
* Read the [Persistent Storage design document](https://git.k8s.io/community/contributors/design-proposals/storage/persistent-storage.md).
 --&gt;
&lt;h2 id=&#34;编写可移植的配置&#34;&gt;编写可移植的配置&lt;/h2&gt;
&lt;p&gt;如果要编写运行在大范围集群中并且需要使用到持久化存储的模板或示例配置，建议依照以下模式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在配置 (随同 Deployments, ConfigMaps, 等)时在同一个配置文件中包含其使用的 PVC 对象。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果使用配置的用户可能没有创建 PV 的权限，则不要在配置中包含 PV 对象。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在用户使用模板时，提供设置 StorageClass 名称的选项&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果用户通过 &lt;code&gt;persistentVolumeClaim.storageClassName&lt;/code&gt;  字段设置了 StorageClass 名称。
当集群管理员启用了该 StorageClasses 时， PVC 就能正确使用存储类别。&lt;/li&gt;
&lt;li&gt;如果用户没提供 &lt;code&gt;StorageClass&lt;/code&gt; 名称。 这会导致 &lt;code&gt;persistentVolumeClaim.storageClassName&lt;/code&gt;
值为空。 这样集群中 PV 就会使用默认 &lt;code&gt;StorageClass&lt;/code&gt; 自动管理。 许多集群环境中都都有安装一个
默认的 &lt;code&gt;StorageClass&lt;/code&gt;或管理可能创建自己的默认 &lt;code&gt;StorageClass&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在工具中，在一个时间后观测未绑定的 PVC 并将其传递给用户。 这可能是集群不支持动态存储(这样用户
就要自己创建相应的 PV) 或者集群中没有存储系统(这种情况用户就不能部署包含 PVC 的配置)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
* Learn more about [Creating a PersistentVolume](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume).
* Learn more about [Creating a PersistentVolumeClaim](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim).
* Read the [Persistent Storage design document](https://git.k8s.io/community/contributors/design-proposals/storage/persistent-storage.md).
 --&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume&#34;&gt;创建 PersistentVolume&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim&#34;&gt;创建 PersistentVolumeClaim&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;参阅 &lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/storage/persistent-storage.md&#34;&gt;持久化存储设计文稿&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#persistentvolume-v1-core&#34;&gt;PersistentVolume&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#persistentvolumespec-v1-core&#34;&gt;PersistentVolumeSpec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#persistentvolumeclaim-v1-core&#34;&gt;PersistentVolumeClaim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#persistentvolumeclaimspec-v1-core&#34;&gt;PersistentVolumeClaimSpec&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Service 和 Pod 的 DNS</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/dns-pod-service/</link>
      <pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/dns-pod-service/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- davidopp
- thockin
title: DNS for Services and Pods
content_type: concept
weight: 20
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This page provides an overview of DNS support by Kubernetes.
 --&gt;
&lt;p&gt;本文简述 k8s 对 DNS 的支持&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Introduction

Kubernetes DNS schedules a DNS Pod and Service on the cluster, and configures
the kubelets to tell individual containers to use the DNS Service&#39;s IP to
resolve DNS names
 --&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;k8s DNS 先在集群中部署了一个 DNS 的 Pod 和 Service， 并配置 kubelet 让每一个容器都使用
DNS Service 的 IP 来解析 DNS 名称。&lt;/p&gt;
&lt;!--
### What things get DNS names?

Every Service defined in the cluster (including the DNS server itself) is
assigned a DNS name.  By default, a client Pod&#39;s DNS search list will
include the Pod&#39;s own namespace and the cluster&#39;s default domain.  This is best
illustrated by example:

Assume a Service named `foo` in the Kubernetes namespace `bar`.  A Pod running
in namespace `bar` can look up this service by simply doing a DNS query for
`foo`.  A Pod running in namespace `quux` can look up this service by doing a
DNS query for `foo.bar`.

The following sections detail the supported record types and layout that is
supported.  Any other layout or names or queries that happen to work are
considered implementation details and are subject to change without warning.
For more up-to-date specification, see
[Kubernetes DNS-Based Service Discovery](https://github.com/kubernetes/dns/blob/master/docs/specification.md).
 --&gt;
&lt;h3 id=&#34;啥东西会有-dns-名称&#34;&gt;啥东西会有 DNS 名称?&lt;/h3&gt;
&lt;p&gt;集群中定义的每一个 Service (包括 DNS 服务本身) 都会分配一个 DNS 名称。 默认情况下，一个客户端
Pod 的 DNS 检索列表会包含 Pod 自己所在的命名空间和集群的默认域。 一例胜千言:&lt;/p&gt;
&lt;p&gt;假设在 k8s 的 &lt;code&gt;bar&lt;/code&gt; 命名空间有一个叫 &lt;code&gt;foo&lt;/code&gt; 的 Service. 一个运行在 &lt;code&gt;bar&lt;/code&gt; 命名空间的 Pod
只需要简单地使用 &lt;code&gt;foo&lt;/code&gt; 作为 DNS 查询条目就可以找到这个 Service。 另一个运行在 &lt;code&gt;quux&lt;/code&gt; 命名空间的 Pod
同要为查询这个 Service。 DNS 的查询条目就需要是 &lt;code&gt;foo.bar&lt;/code&gt; (带上命名空间的名称)&lt;/p&gt;
&lt;p&gt;接下来的章节会详细介绍支持的 DNS 记录类型的规划。 (这一句没懂)
最新的规格说明书见
&lt;a href=&#34;https://github.com/kubernetes/dns/blob/master/docs/specification.md&#34;&gt;Kubernetes DNS-Based Service Discovery&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Services

### A/AAAA records

&#34;Normal&#34; (not headless) Services are assigned a DNS A or AAAA record,
depending on the IP family of the service, for a name of the form
`my-svc.my-namespace.svc.cluster-domain.example`.  This resolves to the cluster IP
of the Service.

&#34;Headless&#34; (without a cluster IP) Services are also assigned a DNS A or AAAA record,
depending on the IP family of the service, for a name of the form
`my-svc.my-namespace.svc.cluster-domain.example`.  Unlike normal
Services, this resolves to the set of IPs of the pods selected by the Service.
Clients are expected to consume the set or else use standard round-robin
selection from the set.

### SRV records

SRV Records are created for named ports that are part of normal or [Headless
Services](/docs/concepts/services-networking/service/#headless-services).
For each named port, the SRV record would have the form
`_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster-domain.example`.
For a regular service, this resolves to the port number and the domain name:
`my-svc.my-namespace.svc.cluster-domain.example`.
For a headless service, this resolves to multiple answers, one for each pod
that is backing the service, and contains the port number and the domain name of the pod
of the form `auto-generated-name.my-svc.my-namespace.svc.cluster-domain.example`.
 --&gt;
&lt;h2 id=&#34;service&#34;&gt;Service&lt;/h2&gt;
&lt;h3 id=&#34;aaaaa-记录&#34;&gt;A/AAAA 记录&lt;/h3&gt;
&lt;p&gt;&amp;ldquo;普通的&amp;rdquo;(不是无头(headless)的) Service 会基于使用的是 IPv4 还是 IPv6 分配一个 DNS A 或 AAAA 记录，
记录名为 &lt;code&gt;my-svc.my-namespace.svc.cluster-domain.example&lt;/code&gt;。 记录的值是 Service 的
集群 IP (cluster IP)&lt;/p&gt;
&lt;p&gt;无头(headless)(没有设置 &lt;code&gt;clusterIP&lt;/code&gt;) Service 也会基于使用的是 IPv4 还是 IPv6
分配一个 DNS A 或 AAAA 记录，记录名为 &lt;code&gt;my-svc.my-namespace.svc.cluster-domain.example&lt;/code&gt;，
与普通 Service 不同的是记录值是由 Service 选择的 Pod 的IP 的集合。 客户端在设计要需要能能够
接受 IP 集合或使用标准轮询 IP 集合。&lt;/p&gt;
&lt;h3 id=&#34;srv-记录&#34;&gt;SRV 记录&lt;/h3&gt;
&lt;p&gt;SRV 记录是为命名端口创建的，是普通或
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service/#headless-services&#34;&gt;Headless Services&lt;/a&gt;
的一部分。 对于每个命名端口的 SRV 记录格式如下:
&lt;code&gt;_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster-domain.example&lt;/code&gt;.
对于普通 Service , 解析的结果为 端口号和 域名:
&lt;code&gt;my-svc.my-namespace.svc.cluster-domain.example&lt;/code&gt;.
对于 无头(headless) 的 Service, 解析结果有多个应答， 一个是 Service 后端的每个 Pod。
另一个则包含端口号和类似这种格式
&lt;code&gt;auto-generated-name.my-svc.my-namespace.svc.cluster-domain.example&lt;/code&gt;
的 Pod 的域名&lt;/p&gt;
&lt;!--
## Pods

### A/AAAA records

In general a pod has the following DNS resolution:

`pod-ip-address.my-namespace.pod.cluster-domain.example`.

For example, if a pod in the `default` namespace has the IP address 172.17.0.3,
and the domain name for your cluster is `cluster.local`, then the Pod has a DNS name:

`172-17-0-3.default.pod.cluster.local`.

Any pods created by a Deployment or DaemonSet exposed by a Service have the
following DNS resolution available:

`pod-ip-address.deployment-name.my-namespace.svc.cluster-domain.example`.
 --&gt;
&lt;h2 id=&#34;pod&#34;&gt;Pod&lt;/h2&gt;
&lt;h3 id=&#34;aaaaa-记录-1&#34;&gt;A/AAAA 记录&lt;/h3&gt;
&lt;p&gt;通常 Pod 的 DNS 记录名为如下格式:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pod-ip-address.my-namespace.pod.cluster-domain.example&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;例如， 如果有一个在 &lt;code&gt;default&lt;/code&gt; 命名空间的 Pod， 它的 IP 地址为 &lt;code&gt;172.17.0.3&lt;/code&gt;， 集群配置的
域名叫 &lt;code&gt;cluster.local&lt;/code&gt;， 这样这个 Pod 的 DNS 名称就是:
&lt;code&gt;172-17-0-3.default.pod.cluster.local&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;任意由 Deployment 或 DaemonSet 创建，由 Service 暴露的 Pod 会拥有一个如下的 DNS 名称:
&lt;code&gt;pod-ip-address.deployment-name.my-namespace.svc.cluster-domain.example&lt;/code&gt;.&lt;/p&gt;
&lt;!--
### Pod&#39;s hostname and subdomain fields

Currently when a pod is created, its hostname is the Pod&#39;s `metadata.name` value.

The Pod spec has an optional `hostname` field, which can be used to specify the
Pod&#39;s hostname. When specified, it takes precedence over the Pod&#39;s name to be
the hostname of the pod. For example, given a Pod with `hostname` set to
&#34;`my-host`&#34;, the Pod will have its hostname set to &#34;`my-host`&#34;.

The Pod spec also has an optional `subdomain` field which can be used to specify
its subdomain. For example, a Pod with `hostname` set to &#34;`foo`&#34;, and `subdomain`
set to &#34;`bar`&#34;, in namespace &#34;`my-namespace`&#34;, will have the fully qualified
domain name (FQDN) &#34;`foo.bar.my-namespace.svc.cluster-domain.example`&#34;.

Example:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: default-subdomain
spec:
  selector:
    name: busybox
  clusterIP: None
  ports:
  - name: foo # Actually, no port is needed.
    port: 1234
    targetPort: 1234
---
apiVersion: v1
kind: Pod
metadata:
  name: busybox1
  labels:
    name: busybox
spec:
  hostname: busybox-1
  subdomain: default-subdomain
  containers:
  - image: busybox:1.28
    command:
      - sleep
      - &#34;3600&#34;
    name: busybox
---
apiVersion: v1
kind: Pod
metadata:
  name: busybox2
  labels:
    name: busybox
spec:
  hostname: busybox-2
  subdomain: default-subdomain
  containers:
  - image: busybox:1.28
    command:
      - sleep
      - &#34;3600&#34;
    name: busybox
```

If there exists a headless service in the same namespace as the pod and with
the same name as the subdomain, the cluster&#39;s DNS Server also returns an A or AAAA
record for the Pod&#39;s fully qualified hostname.
For example, given a Pod with the hostname set to &#34;`busybox-1`&#34; and the subdomain set to
&#34;`default-subdomain`&#34;, and a headless Service named &#34;`default-subdomain`&#34; in
the same namespace, the pod will see its own FQDN as
&#34;`busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example`&#34;. DNS serves an
A or AAAA record at that name, pointing to the Pod&#39;s IP. Both pods &#34;`busybox1`&#34; and
&#34;`busybox2`&#34; can have their distinct A or AAAA records.

The Endpoints object can specify the `hostname` for any endpoint addresses,
along with its IP.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Because A or AAAA records are not created for Pod names, &lt;code&gt;hostname&lt;/code&gt; is required for the Pod&amp;rsquo;s A or AAAA
record to be created. A Pod with no &lt;code&gt;hostname&lt;/code&gt; but with &lt;code&gt;subdomain&lt;/code&gt; will only create the
A or AAAA record for the headless service (&lt;code&gt;default-subdomain.my-namespace.svc.cluster-domain.example&lt;/code&gt;),
pointing to the Pod&amp;rsquo;s IP address. Also, Pod needs to become ready in order to have a
record unless &lt;code&gt;publishNotReadyAddresses=True&lt;/code&gt; is set on the Service.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;pod-的-hostname-和-subdomain-字段&#34;&gt;Pod 的 &lt;code&gt;hostname&lt;/code&gt; 和 &lt;code&gt;subdomain&lt;/code&gt; 字段&lt;/h3&gt;
&lt;p&gt;目前，当一个 Pod 创建后， 它的主机名是 Pod 的 &lt;code&gt;metadata.name&lt;/code&gt; 字段的值。&lt;/p&gt;
&lt;p&gt;Pod 的定义中有一个可选字段 &lt;code&gt;hostname&lt;/code&gt;， 可以用来指定 Pod 的主机名。 当设置了主机名时，它的优先
级是高于 Pod 名称作为 Pod 的主机名的。 例如， 设置一个 Pod 的 &lt;code&gt;hostname&lt;/code&gt; 为 &lt;code&gt;my-host&lt;/code&gt;，
这个 Pod 的主机名就会设置为 &lt;code&gt;my-host&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pod 定义中还有一个可选择字段 &lt;code&gt;subdomain&lt;/code&gt;， 可以用来指定它的子域名。 例如， 一个 Pod 的
&lt;code&gt;hostname&lt;/code&gt; 设置为 &lt;code&gt;foo&lt;/code&gt;， &lt;code&gt;subdomain&lt;/code&gt; 设置为 &lt;code&gt;bar&lt;/code&gt;， 处理 &lt;code&gt;my-namespace&lt;/code&gt; 命名空间。
它的全限定名(FQDN) 就是 &lt;code&gt;foo.bar.my-namespace.svc.cluster-domain.example&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-subdomain&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;clusterIP&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;None&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo # 实际上是不需要端口的&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1234&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1234&lt;/span&gt;
---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;hostname&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox-1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;subdomain&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-subdomain&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox:1.28&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;sleep&lt;/span&gt;
      - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3600&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox2&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;hostname&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox-2&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;subdomain&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-subdomain&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox:1.28&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;sleep&lt;/span&gt;
      - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3600&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果存在这样一个无头(headless) 的 Service, 它与另一个 Pod 在同一个命名空间，且 Pod 的
&lt;code&gt;subdomain&lt;/code&gt; 与这个 Service 的名称是一样的。 集群 DNS 服务一样会返回 Pod 全限制名的 A/AAAA 记录。
例如，假定一个 Pod 的 &lt;code&gt;hostname&lt;/code&gt; 设置为 &lt;code&gt;busybox-1&lt;/code&gt;， &lt;code&gt;subdomain&lt;/code&gt; 设置为 &lt;code&gt;default-subdomain&lt;/code&gt;，
一个无头(headless) 的 Service 名字是 &lt;code&gt;default-subdomain&lt;/code&gt;， 它们在同一个命名空间。
Pod 就可以看到它自己的全限定名(FQDN)为
&lt;code&gt;busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example&lt;/code&gt;。
DNS 服务为这个名称提供一个 A/AAAA 记录， 指向该 Pod 的 IP。 &lt;code&gt;busybox1&lt;/code&gt; 和 &lt;code&gt;busybox2&lt;/code&gt;
都能有它们各自的 A/AAAA 记录。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;这里需要配个完整的例子&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;Endpoint 对象可以将任意端点地址和IP 设置为 &lt;code&gt;hostname&lt;/code&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;这里不理解，并且需要一个例子&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 因为 A/AAAA 记录不是为 Pod 名称创建的， &lt;code&gt;hostname&lt;/code&gt; 是 Pod A/AAAA 记录创建所必须的。
一个没有 &lt;code&gt;hostname&lt;/code&gt; 字段，但是有 &lt;code&gt;subdomain&lt;/code&gt; 字段的 Pod 只会为无头(headless)Service
创建 A/AAAA 记录(&lt;code&gt;default-subdomain.my-namespace.svc.cluster-domain.example&lt;/code&gt;)
该记录指向的是该 Pod 的 IP 地址。还有，如果 Service 上没有设置 &lt;code&gt;publishNotReadyAddresses=True&lt;/code&gt;
则 Pod 状态变为就绪后才的 DNS 记录&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Pod&#39;s setHostnameAsFQDN field {#pod-sethostnameasfqdn-field}






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [alpha]&lt;/code&gt;
&lt;/div&gt;



**Prerequisites**: The `SetHostnameAsFQDN` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
must be enabled for the
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API Server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt;

When a Pod is configured to have fully qualified domain name (FQDN), its hostname is the short hostname. For example, if you have a Pod with the fully qualified domain name `busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example`, then by default the `hostname` command inside that Pod returns `busybox-1` and  the `hostname --fqdn` command returns the FQDN.

When you set `setHostnameAsFQDN: true` in the Pod spec, the kubelet writes the Pod&#39;s FQDN into the hostname for that Pod&#39;s namespace. In this case, both `hostname` and `hostname --fqdn` return the Pod&#39;s FQDN.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;In Linux, the hostname field of the kernel (the &lt;code&gt;nodename&lt;/code&gt; field of &lt;code&gt;struct utsname&lt;/code&gt;) is limited to 64 characters.&lt;/p&gt;
&lt;p&gt;If a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start. The Pod will remain in &lt;code&gt;Pending&lt;/code&gt; status (&lt;code&gt;ContainerCreating&lt;/code&gt; as seen by &lt;code&gt;kubectl&lt;/code&gt;) generating error events, such as Failed to construct FQDN from pod hostname and cluster domain, FQDN &lt;code&gt;long-FDQN&lt;/code&gt; is too long (64 characters is the max, 70 characters requested). One way of improving user experience for this scenario is to create an &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks&#34;&gt;admission webhook controller&lt;/a&gt; to control FQDN size when users create top level objects, for example, Deployment.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;pod-sethostnameasfqdn-field&#34;&gt;Pod 的 setHostnameAsFQDN 字段&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;前提条件&lt;/strong&gt;:
需要在 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API Server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 启用
&lt;code&gt;SetHostnameAsFQDN&lt;/code&gt; &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;当一个 Pod 配置了全限定名(FQDN)时，它的主机名是短主机名。 例如， 如果有一个全限定名为
&lt;code&gt;busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example&lt;/code&gt; 的 Pod，
在 Pod 内使用默认的 &lt;code&gt;hostname&lt;/code&gt; 命令时返回的是 &lt;code&gt;busybox-1&lt;/code&gt;， 而 &lt;code&gt;hostname --fqdn&lt;/code&gt; 命令
返回的是全限定名(FQDN)。&lt;/p&gt;
&lt;p&gt;当在 Pod 的定义中设置 &lt;code&gt;setHostnameAsFQDN: true&lt;/code&gt; 时， kubelet 会将 Pod 的 全限定名(FQDN)
写到那个 Pod 命名空间的 hostname. 在这种情况下 &lt;code&gt;hostname&lt;/code&gt; 和 &lt;code&gt;hostname --fqdn&lt;/code&gt; 两个命令
返回的都是全限定名。&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 在 Linux 中， 内核中的 &lt;code&gt;hostname&lt;/code&gt; 字段(&lt;code&gt;struct utsname&lt;/code&gt; 的 &lt;code&gt;nodename&lt;/code&gt; 字段)限制最多只能有 64 个字符。&lt;/div&gt;
&lt;/blockquote&gt;

如果一个 Pod 开启了该特性，并且它的 全限定名(FQDN) 长度大于 64 个字符，就会启动失败。
Pod 会一停在 &lt;code&gt;Pending&lt;/code&gt; 状态(通过 &lt;code&gt;kubectl&lt;/code&gt; 看到的是 &lt;code&gt;ContainerCreating&lt;/code&gt;)，最终会产生一个
错误事件，错误信息类似基于 Pod 主机名和集群域构建全限定名(FQDN)失败， &lt;code&gt;long-FDQN&lt;/code&gt;  FQDN 太长了
(最长只能有 64 个字符，但实际有 70 个字符)。 在这种情况下改善用户体验的一种方式是创建一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks&#34;&gt;admission webhook controller&lt;/a&gt;
来在创建顶级对象(如，Deployment)时控制全限定名(FQDN)的长度&lt;/p&gt;
&lt;!--
### Pod&#39;s DNS Policy

DNS policies can be set on a per-pod basis. Currently Kubernetes supports the
following pod-specific DNS policies. These policies are specified in the
`dnsPolicy` field of a Pod Spec.

- &#34;`Default`&#34;: The Pod inherits the name resolution configuration from the node
  that the pods run on.
  See [related discussion](/docs/tasks/administer-cluster/dns-custom-nameservers/#inheriting-dns-from-the-node)
  for more details.
- &#34;`ClusterFirst`&#34;: Any DNS query that does not match the configured cluster
  domain suffix, such as &#34;`www.kubernetes.io`&#34;, is forwarded to the upstream
  nameserver inherited from the node. Cluster administrators may have extra
  stub-domain and upstream DNS servers configured.
  See [related discussion](/docs/tasks/administer-cluster/dns-custom-nameservers/#effects-on-pods)
  for details on how DNS queries are handled in those cases.
- &#34;`ClusterFirstWithHostNet`&#34;: For Pods running with hostNetwork, you should
  explicitly set its DNS policy &#34;`ClusterFirstWithHostNet`&#34;.
- &#34;`None`&#34;: It allows a Pod to ignore DNS settings from the Kubernetes
  environment. All DNS settings are supposed to be provided using the
  `dnsConfig` field in the Pod Spec.
  See [Pod&#39;s DNS config](#pod-dns-config) subsection below.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &amp;ldquo;Default&amp;rdquo; is not the default DNS policy. If &lt;code&gt;dnsPolicy&lt;/code&gt; is not
explicitly specified, then &amp;ldquo;ClusterFirst&amp;rdquo; is used.&lt;/div&gt;
&lt;/blockquote&gt;



The example below shows a Pod with its DNS policy set to
&#34;`ClusterFirstWithHostNet`&#34; because it has `hostNetwork` set to `true`.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox:1.28
    command:
      - sleep
      - &#34;3600&#34;
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always
  hostNetwork: true
  dnsPolicy: ClusterFirstWithHostNet
```
 --&gt;
&lt;h3 id=&#34;pod-的-dns-策略&#34;&gt;Pod 的 DNS 策略&lt;/h3&gt;
&lt;p&gt;DNS policies can be set on a per-pod basis. Currently Kubernetes supports the
following pod-specific DNS policies. These policies are specified in the
&lt;code&gt;dnsPolicy&lt;/code&gt; field of a Pod Spec.
DNS 策略可以在 Pod 级别设置， 目前 k8s 支持以下的 Pod 级别 DNS 策略。 这个策略通过 Pod
定义的 &lt;code&gt;dnsPolicy&lt;/code&gt; 字段指定。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;&lt;code&gt;Default&lt;/code&gt;&amp;quot;: Pod 从它自己运行的节点上继承域名解析配置。更多信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/administer-cluster/dns-custom-nameservers/#inheriting-dns-from-the-node&#34;&gt;相关讨论&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;&lt;code&gt;ClusterFirst&lt;/code&gt;&amp;quot;: 任何与配置的集群域后缀匹配的 DNS 查询，如   &amp;ldquo;&lt;code&gt;www.kubernetes.io&lt;/code&gt;&amp;quot;，
会被转发到由节点继承的上游域名服务器。 集群管理员可以配置额外的 存根域(stub-domain) 和上游
DNS 服务器。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;&lt;code&gt;ClusterFirstWithHostNet&lt;/code&gt;&amp;quot;: 以 &lt;code&gt;hostNetwork&lt;/code&gt; 运行的 Pod，需要显式的设置它的 DNS
策略为 &amp;ldquo;&lt;code&gt;ClusterFirstWithHostNet&lt;/code&gt;&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;&lt;code&gt;None&lt;/code&gt;&amp;quot;: 这种策略允许 Pod 无视 k8s 环境的 DNS 配置。 所有的 DNS 配置都应该由 Pod 定义中
的 &lt;code&gt;dnsConfig&lt;/code&gt; 字段提供。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &amp;ldquo;Default&amp;rdquo; 并不是默认的 DNS 策略， 如果没有显式的设置 &lt;code&gt;dnsPolicy&lt;/code&gt;，则使用 &amp;ldquo;ClusterFirst&amp;rdquo;&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以面的例子中的 Pod 的 DNS 策略被设置为 &amp;ldquo;&lt;code&gt;ClusterFirstWithHostNet&lt;/code&gt;&amp;rdquo; 因为它的 &lt;code&gt;hostNetwork&lt;/code&gt;
被设置为了 &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox:1.28&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;sleep&lt;/span&gt;
      - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3600&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;imagePullPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IfNotPresent&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Always&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;hostNetwork&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dnsPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ClusterFirstWithHostNet&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Pod&#39;s DNS Config {#pod-dns-config}

Pod&#39;s DNS Config allows users more control on the DNS settings for a Pod.

The `dnsConfig` field is optional and it can work with any `dnsPolicy` settings.
However, when a Pod&#39;s `dnsPolicy` is set to &#34;`None`&#34;, the `dnsConfig` field has
to be specified.

Below are the properties a user can specify in the `dnsConfig` field:

- `nameservers`: a list of IP addresses that will be used as DNS servers for the
  Pod. There can be at most 3 IP addresses specified. When the Pod&#39;s `dnsPolicy`
  is set to &#34;`None`&#34;, the list must contain at least one IP address, otherwise
  this property is optional.
  The servers listed will be combined to the base nameservers generated from the
  specified DNS policy with duplicate addresses removed.
- `searches`: a list of DNS search domains for hostname lookup in the Pod.
  This property is optional. When specified, the provided list will be merged
  into the base search domain names generated from the chosen DNS policy.
  Duplicate domain names are removed.
  Kubernetes allows for at most 6 search domains.
- `options`: an optional list of objects where each object may have a `name`
  property (required) and a `value` property (optional). The contents in this
  property will be merged to the options generated from the specified DNS policy.
  Duplicate entries are removed.

The following is an example Pod with custom DNS settings:



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingcustom-dnsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/custom-dns.yaml&#34; download=&#34;service/networking/custom-dns.yaml&#34;&gt;
                    &lt;code&gt;service/networking/custom-dns.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingcustom-dnsyaml&#39;)&#34; title=&#34;Copy service/networking/custom-dns.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;dns-example&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dnsPolicy&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;None&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dnsConfig&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nameservers&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;1.2.3.4&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;searches&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;ns1.svc.cluster-domain.example&lt;/span&gt;
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;my.dns.search.suffix&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;options&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ndots&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2&amp;#34;&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;edns0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



When the Pod above is created, the container `test` gets the following contents
in its `/etc/resolv.conf` file:

```
nameserver 1.2.3.4
search ns1.svc.cluster-domain.example my.dns.search.suffix
options ndots:2 edns0
```

For IPv6 setup, search path and name server should be setup like this:

```shell
kubectl exec -it dns-example -- cat /etc/resolv.conf
```
The output is similar to this:
```shell
nameserver fd00:79:30::a
search default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.example
options ndots:5
```
 --&gt;
&lt;h3 id=&#34;pod-dns-config&#34;&gt;Pod 的 DNS 配置&lt;/h3&gt;
&lt;p&gt;Pod&amp;rsquo;s DNS Config allows users more control on the DNS settings for a Pod.
Pod 的 DNS 配置让用户可以更多地控制 Pod 上的 DNS 配置。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;dnsConfig&lt;/code&gt; 是一个可选字段， 它可以与 &lt;code&gt;dnsPolicy&lt;/code&gt; 配置配合使用。 但是当一个 Pod 的 &lt;code&gt;dnsPolicy&lt;/code&gt;
设置为 &amp;ldquo;&lt;code&gt;None&lt;/code&gt;&amp;ldquo;时，就必须要设置 &lt;code&gt;dnsConfig&lt;/code&gt; 字段。
以下是 &lt;code&gt;dnsConfig&lt;/code&gt; 字段中用户可以配置的字段:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;nameservers&lt;/code&gt;: Pod 用作 DNS 服务的一个 IP 地址列表。 最多可以指定三个 IP 地址。
当 Pod &lt;code&gt;dnsPolicy&lt;/code&gt; 设置为 &amp;ldquo;&lt;code&gt;None&lt;/code&gt;&amp;quot;， 这个列表中至少包含一个 IP 地址，其它情况下这个字段为可选。
这个 DNS 列表会与 DNS 策略配置产生的基础 DNS 服务器合并，重复的会被删除。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;searches&lt;/code&gt;: 一个 DNS 检索列表，用于 Pod 中的主机名查找。 这个属性为可选。当设置这个字段时，
这个列表会合并到 DNS 策略配置产生的DNS 检索域名中，重复的条目会被删除。 k8s 允许最多 6 个检索域名。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;options&lt;/code&gt;: 一个可选的对象列表， 其中的每个对象有一个必要的 &lt;code&gt;name&lt;/code&gt; 属性和一个可选的 &lt;code&gt;value&lt;/code&gt; 属性。
这些属性会被合并到配置的 DNS 策略生成的选项中&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下来一个包含自定义 DNS 配置的 Pod 的示例:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingcustom-dnsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/custom-dns.yaml&#34; download=&#34;service/networking/custom-dns.yaml&#34;&gt;
                    &lt;code&gt;service/networking/custom-dns.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingcustom-dnsyaml&#39;)&#34; title=&#34;Copy service/networking/custom-dns.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;dns-example&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dnsPolicy&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;None&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dnsConfig&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nameservers&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;1.2.3.4&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;searches&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;ns1.svc.cluster-domain.example&lt;/span&gt;
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;my.dns.search.suffix&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;options&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ndots&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2&amp;#34;&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;edns0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;当上面这个 Pod 被创建后，这个叫  &lt;code&gt;test&lt;/code&gt; 容器中的 &lt;code&gt;/etc/resolv.conf&lt;/code&gt; 就会有下面的这些内容:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nameserver 1.2.3.4
search ns1.svc.cluster-domain.example my.dns.search.suffix
options ndots:2 edns0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;For IPv6 setup, search path and name server should be setup like this:
如果设置了 IPv6， 检索路径和DNS服务应该这么配置:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl exec -it dns-example -- cat /etc/resolv.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nameserver fd00:79:30::a
search default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.example
options ndots:5
&lt;/code&gt;&lt;/pre&gt;&lt;!--
### Feature availability

The availability of Pod DNS Config and DNS Policy &#34;`None`&#34; is shown as below.

| k8s version | Feature support |
| :---------: |:-----------:|
| 1.14 | Stable |
| 1.10 | Beta (on by default)|
| 1.9 | Alpha |
 --&gt;
&lt;h3 id=&#34;feature-可用性&#34;&gt;Feature 可用性&lt;/h3&gt;
&lt;p&gt;Pod DNS 配置 和  DNS 策略的 &amp;ldquo;&lt;code&gt;None&lt;/code&gt;&amp;rdquo; 的可用性如下:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;k8s 版本&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;特性可用性&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.14&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Stable&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.10&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Beta (默认启用)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1.9&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Alpha&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
For guidance on administering DNS configurations, check
[Configure DNS Service](/docs/tasks/administer-cluster/dns-custom-nameservers/)
 --&gt;
&lt;p&gt;DNS 配置的管理指导见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/dns-custom-nameservers/&#34;&gt;Configure DNS Service&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Secret</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/secret/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/secret/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- mikedanese
title: Secrets
content_type: concept
feature:
  title: Secret and configuration management
  description: &gt;
    Deploy and update secrets and application configuration without rebuilding your image and without exposing secrets in your stack configuration.
weight: 30
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
Kubernetes Secrets let you store and manage sensitive information, such
as passwords, OAuth tokens, and ssh keys. Storing confidential information in a Secret
is safer and more flexible than putting it verbatim in a
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; definition or in a
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-image&#39; target=&#39;_blank&#39;&gt;container image&lt;span class=&#39;tooltip-text&#39;&gt;一个容器的存储实例，其中包含一系列运行应用所需要的软件。&lt;/span&gt;
&lt;/a&gt;.
See [Secrets design document](https://git.k8s.io/community/contributors/design-proposals/auth/secrets.md) for more information.

A Secret is an object that contains a small amount of sensitive data such as
a password, a token, or a key. Such information might otherwise be put in a
Pod specification or in an image. Users can create Secrets and the system
also creates some Secrets.
 --&gt;
&lt;p&gt;k8s Secret 可以让用户存储和管理敏感信息，如密码, OAuth token, ssh 密钥。将私密信息放在
Secret 中更安全，并且比直接放在
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
的配置定义中或
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-image&#39; target=&#39;_blank&#39;&gt;镜像(Image)&lt;span class=&#39;tooltip-text&#39;&gt;一个容器的存储实例，其中包含一系列运行应用所需要的软件。&lt;/span&gt;
&lt;/a&gt;
中更加灵活。
更多信息见
&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/auth/secrets.md&#34;&gt;Secrets 设计文稿&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Secret 一个包含少量敏感信息如 如密码, OAuth token, ssh 密钥的对象。 这些信息也能被放在
Pod 的配置定义或镜像中。 用户可以创建 Secret，系统也会创建一些 Secret。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Overview of Secrets

To use a Secret, a Pod needs to reference the Secret.
A Secret can be used with a Pod in three ways:

- As [files](#using-secrets-as-files-from-a-pod) in a
  &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;volume&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt; mounted on one or more of
  its containers.
- As [container environment variable](#using-secrets-as-environment-variables).
- By the [kubelet when pulling images](#using-imagepullsecrets) for the Pod.

The name of a Secret object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
You can specify the `data` and/or the `stringData` field when creating a
configuration file for a Secret.  The `data` and the `stringData` fields are optional.
The values for all keys in the `data` field have to be base64-encoded strings.
If the conversion to base64 string is not desirable, you can choose to specify
the `stringData` field instead, which accepts arbitrary strings as values.

The keys of `data` and `stringData` must consist of alphanumeric characters,
`-`, `_` or `.`. All key-value pairs in the `stringData` field are internally
merged into the `data` field. If a key appears in both the `data` and the
`stringData` field, the value specified in the `stringData` field takes
precedence.
 --&gt;
&lt;h2 id=&#34;overview-of-secrets&#34;&gt;Secret 概览&lt;/h2&gt;
&lt;p&gt;要使用 Secret 需要在 Pod 中引用这个 Secret。 Pod 可以以下三种方式 Secret:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;作为
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;volume&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;
中的
&lt;a href=&#34;#using-secrets-as-files-from-a-pod&#34;&gt;文件&lt;/a&gt;
挂载到一个或多个容器中。&lt;/li&gt;
&lt;li&gt;作为 &lt;a href=&#34;#using-secrets-as-environment-variables&#34;&gt;容器中的环境变量&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;通过 &lt;a href=&#34;#using-imagepullsecrets&#34;&gt;kubelet 拉取镜像&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Secret 对象的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.
在为 Secret 创建配置文件时可以指定 &lt;code&gt;data&lt;/code&gt; 和/或 &lt;code&gt;stringData&lt;/code&gt; 字段。 &lt;code&gt;data&lt;/code&gt; 和 &lt;code&gt;stringData&lt;/code&gt;
字段都是可选的。 &lt;code&gt;data&lt;/code&gt; 字段下所有键的值都得是 base64 编码的字符串。如果不想转化为 base64
的编码字符串，则可以选择 &lt;code&gt;stringData&lt;/code&gt; 字段代替，它可以接受任意字符串作为值。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;data&lt;/code&gt; 和 &lt;code&gt;stringData&lt;/code&gt; 下面的键必须由 字母，数字， &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;_&lt;/code&gt; 或 &lt;code&gt;.&lt;/code&gt; 组成。 &lt;code&gt;stringData&lt;/code&gt;
字段下面的所有键值对都会内部地合并到 &lt;code&gt;data&lt;/code&gt; 字段。 如果一个键同时出现在 &lt;code&gt;data&lt;/code&gt; 和 &lt;code&gt;stringData&lt;/code&gt;
字段下面， 则 &lt;code&gt;stringData&lt;/code&gt; 字段下面指定的值有更高的优先级。&lt;/p&gt;
&lt;!--
## Types of Secret {#secret-types}

When creating a Secret, you can specify its type using the `type` field of
the [`Secret`](/docs/reference/generated/kubernetes-api/v1.19/#secret-v1-core)
resource, or certain equivalent `kubectl` command line flags (if available).
The Secret type is used to facilitate programmatic handling of the Secret data.

Kubernetes provides several builtin types for some common usage scenarios.
These types vary in terms of the validations performed and the constraints
Kubernetes imposes on them.

| Builtin Type | Usage |
|--------------|-------|
| `Opaque`     |  arbitrary user-defined data |
| `kubernetes.io/service-account-token` | service account token |
| `kubernetes.io/dockercfg` | serialized `~/.dockercfg` file |
| `kubernetes.io/dockerconfigjson` | serialized `~/.docker/config.json` file |
| `kubernetes.io/basic-auth` | credentials for basic authentication |
| `kubernetes.io/ssh-auth` | credentials for SSH authentication |
| `kubernetes.io/tls` | data for a TLS client or server |
| `bootstrap.kubernetes.io/token` | bootstrap token data |

You can define and use your own Secret type by assigning a non-empty string as the
`type` value for a Secret object. An empty string is treated as an `Opaque` type.
Kubernetes doesn&#39;t impose any constraints on the type name. However, if you
are using one of the builtin types, you must meet all the requirements defined
for that type.
 --&gt;
&lt;h2 id=&#34;secret-types&#34;&gt;Secret 的类别&lt;/h2&gt;
&lt;p&gt;在创建 Secret 时，用户通过设置
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#secret-v1-core&#34;&gt;&lt;code&gt;Secret&lt;/code&gt;&lt;/a&gt;
资源的 &lt;code&gt;type&lt;/code&gt; 字段指定它的类型， 或使用等同的 &lt;code&gt;kubectl&lt;/code&gt; 命令参数(如果存在)。 Secret 类型用于
帮助程式化地处理 Secret 数据。&lt;/p&gt;
&lt;p&gt;k8s 提供了几种内置的类型用于一些常见的使用场景。 这些类型不同对应不同的验证操作和对其施加不同的
约束。
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;内置类型&lt;/th&gt;
&lt;th&gt;Usage&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;Opaque&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;任意用户定义数据&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubernetes.io/service-account-token&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;service account token&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubernetes.io/dockercfg&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;序列化的 &lt;code&gt;~/.dockercfg&lt;/code&gt; 文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubernetes.io/dockerconfigjson&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;序列化的 &lt;code&gt;~/.docker/config.json&lt;/code&gt; 文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubernetes.io/basic-auth&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;基础认证的凭据&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubernetes.io/ssh-auth&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;SSH 认证的凭据&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;kubernetes.io/tls&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;用户 TLS 客户端或服务端的数据&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;bootstrap.kubernetes.io/token&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;引导 token 数据&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;用户可以通过为 Secret 对象的 &lt;code&gt;type&lt;/code&gt; 值设置为一个非空字符串的方式定义并使用自己的 Secret 类型。
一个空字符串被认作是 &lt;code&gt;Opaque&lt;/code&gt; 类型。k8s 没有对类型的名称作任何限制。 但如果使用的是内置类型，
必须要满足这个类型的定义需求。&lt;/p&gt;
&lt;!--

### Opaque secrets

`Opaque` is the default Secret type if omitted from a Secret configuration file.
When you create a Secret using `kubectl`, you will use the `generic`
subcommand to indicate an `Opaque` Secret type. For example, the following
command creates an empty Secret of type `Opaque`.

```shell
kubectl create secret generic empty-secret
kubectl get secret empty-secret
```

The output looks like:

```
NAME           TYPE     DATA   AGE
empty-secret   Opaque   0      2m6s
```

The `DATA` column shows the number of data items stored in the Secret.
In this case, `0` means we have just created an empty Secret.
 --&gt;
&lt;h3 id=&#34;opaque-secrets&#34;&gt;Opaque 类别 Secret&lt;/h3&gt;
&lt;p&gt;如果 Secret 配置文件中没有定义 Secret 类型则 &lt;code&gt;Opaque&lt;/code&gt; 就是默认类型。 当使用 &lt;code&gt;kubectl&lt;/code&gt; 创建
Secret, 可以使用 &lt;code&gt;generic&lt;/code&gt; 子命令来表示 &lt;code&gt;Opaque&lt;/code&gt; Secret 类型。 例如，以下命令会创建一个
类型为 &lt;code&gt;Opaque&lt;/code&gt; 空 Secret 。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret generic empty-secret
kubectl get secret empty-secret
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME           TYPE     DATA   AGE
empty-secret   Opaque   0      2m6s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;DATA&lt;/code&gt; 列显示的是 Secret 中存储的数据条目数。 在这种情况下， &lt;code&gt;0&lt;/code&gt; 表示这里创建的是一个空的 Secret&lt;/p&gt;
&lt;!--
###  Service account token Secrets

A `kubernetes.io/service-account-token` type of Secret is used to store a
token that identifies a service account. When using this Secret type, you need
to ensure that the `kubernetes.io/service-account.name` annotation is set to an
existing service account name. An Kubernetes controller fills in some other
fields such as the `kubernetes.io/service-account.uid` annotation and the
`token` key in the `data` field set to actual token content.

The following example configuration declares a service account token Secret:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: secret-sa-sample
  annotations:
    kubernetes.io/service-account.name: &#34;sa-name&#34;
type: kubernetes.io/service-account-token
data:
  # You can include additional key value pairs as you do with Opaque Secrets
  extra: YmFyCg==
```

When creating a `Pod`, Kubernetes automatically creates a service account Secret
and automatically modifies your Pod to use this Secret. The service account token
Secret contains credentials for accessing the API.

The automatic creation and use of API credentials can be disabled or
overridden if desired. However, if all you need to do is securely access the
API server, this is the recommended workflow.

See the [ServiceAccount](/docs/tasks/configure-pod-container/configure-service-account/)
documentation for more information on how service accounts work.
You can also check the `automountServiceAccountToken` field and the
`serviceAccountName` field of the
[`Pod`](/docs/reference/generated/kubernetes-api/v1.19/#pod-v1-core)
for information on referencing service account from Pods.
 --&gt;
&lt;h3 id=&#34;service-account-token-secrets&#34;&gt;服务账号(Service account) token 类别 Secret&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubernetes.io/service-account-token&lt;/code&gt; 类型的 Secret 用于存储一个鉴定服务账号(service account)的 token.
当使用这个 Secret 类别时， 需要保证 &lt;code&gt;kubernetes.io/service-account.name&lt;/code&gt; 注解设置为一
个现有的服务账号(service account)名称. 一个 k8s 控制会填充其它字段，如
&lt;code&gt;kubernetes.io/service-account.uid&lt;/code&gt; 注解， &lt;code&gt;data&lt;/code&gt; 字段中的 &lt;code&gt;token&lt;/code&gt; 键设置实际 token
的内容。&lt;/p&gt;
&lt;p&gt;The following example configuration declares a service account token Secret:
下面的例子中定义了一个 服务账号(service account) token 类别的 Secret:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-sa-sample&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;kubernetes.io/service-account.name&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sa-name&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/service-account-token&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#75715e&#34;&gt;# 可以与 Opaque 类别的 Secret 一样添加更多的键值对&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;extra&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;YmFyCg==&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;当创建一个 &lt;code&gt;Pod&lt;/code&gt; 时， k8s 会自动地创建一个服务账号 Secret 再自动修改 Pod 来使用这个 Secret.
这个 服务账号(service account) token 中包含了访问 API 的凭据。&lt;/p&gt;
&lt;p&gt;如果需要可以禁用或覆盖这种自动创建和使用 API 凭据的行为。 但是，如果仅需要安全地访问 API 服务，
这种是推荐的工作方式。&lt;/p&gt;
&lt;p&gt;更多关于服务账号是工件的信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/configure-pod-container/configure-service-account/&#34;&gt;ServiceAccount&lt;/a&gt;。
也可以查看
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubernetes-api/v1.19/#pod-v1-core&#34;&gt;&lt;code&gt;Pod&lt;/code&gt;&lt;/a&gt;
中的 &lt;code&gt;automountServiceAccountToken&lt;/code&gt; 和 &lt;code&gt;serviceAccountName&lt;/code&gt; 字段了解 Pod 中引用的
服务账号的信息。&lt;/p&gt;
&lt;!--
### Docker config Secrets

You can use one of the following `type` values to create a Secret to
store the credentials for accessing a Docker registry for images.

- `kubernetes.io/dockercfg`
- `kubernetes.io/dockerconfigjson`

The `kubernetes.io/dockercfg` type is reserved to store a serialized
`~/.dockercfg` which is the legacy format for configuring Docker command line.
When using this Secret type, you have to ensure the Secret `data` field
contains a `.dockercfg` key whose value is content of a `~/.dockercfg` file
encoded in the base64 format.

The `kubernetes.io/dockerconfigjson` type is designed for storing a serialized
JSON that follows the same format rules as the `~/.docker/config.json` file
which is a new format for `~/.dockercfg`.
When using this Secret type, the `data` field of the Secret object must
contain a `.dockerconfigjson` key, in which the content for the
`~/.docker/config.json` file is provided as a base64 encoded string.

Below is an example for a `kubernetes.io/dockercfg` type of Secret:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: secret-dockercfg
type: kubernetes.io/dockercfg
data:
  .dockercfg: |
    &#34;&lt;base64 encoded ~/.dockercfg file&gt;&#34;
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If you do not want to perform the base64 encoding, you can choose to use the
&lt;code&gt;stringData&lt;/code&gt; field instead.&lt;/div&gt;
&lt;/blockquote&gt;


When you create these types of Secrets using a manifest, the API
server checks whether the expected key does exists in the `data` field, and
it verifies if the value provided can be parsed as a valid JSON. The API
server doesn&#39;t validate if the JSON actually is a Docker config file.

When you do not have a Docker config file, or you want to use `kubectl`
to create a Docker registry Secret, you can do:

```shell
kubectl create secret docker-registry secret-tiger-docker \
  --docker-username=tiger \
  --docker-password=pass113 \
  --docker-email=tiger@acme.com
```

This command creates a Secret of type `kubernetes.io/dockerconfigjson`.
If you dump the `.dockerconfigjson` content from the `data` field, you will
get the following JSON content which is a valid Docker configuration created
on the fly:

```json
{
  &#34;auths&#34;: {
    &#34;https://index.docker.io/v1/&#34;: {
      &#34;username&#34;: &#34;tiger&#34;,
      &#34;password&#34;: &#34;pass113&#34;,
      &#34;email&#34;: &#34;tiger@acme.com&#34;,
      &#34;auth&#34;: &#34;dGlnZXI6cGFzczExMw==&#34;
    }
  }
}
```
 --&gt;
&lt;h3 id=&#34;docker-config-Secrets&#34;&gt;Docker 配置 Secret&lt;/h3&gt;
&lt;p&gt;用户可以使用以下 &lt;code&gt;type&lt;/code&gt; 值来创建存储访问 Docker 镜像仓库的凭据。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kubernetes.io/dockercfg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kubernetes.io/dockerconfigjson&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;kubernetes.io/dockercfg&lt;/code&gt; 类别是存储序列化 &lt;code&gt;~/.dockercfg&lt;/code&gt; 文件的保留类别， 其中存放的
是配置 Docker 命令行的经典格式。 当使用这个 Secret 类别时， 需要确保 &lt;code&gt;data&lt;/code&gt; 字段中包含
一个 &lt;code&gt;.dockercfg&lt;/code&gt; 键，并且它的值就是一个 &lt;code&gt;~/.dockercfg&lt;/code&gt; 内容的 base64 编码格式。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubernetes.io/dockerconfigjson&lt;/code&gt;  类别被设计用来存储序列化 JSON, 其格式规则与
&lt;code&gt;~/.docker/config.json&lt;/code&gt; 文件一样, 也就是 &lt;code&gt;~/.dockercfg&lt;/code&gt; 的新格式。
当使用这个 Secret 类别时， Secret 对象的 &lt;code&gt;data&lt;/code&gt; 字段必须包含一个 &lt;code&gt;.dockerconfigjson&lt;/code&gt; 键
，其中的内容是 &lt;code&gt;~/.docker/config.json&lt;/code&gt; 文件内容 base64 编码字符串&lt;/p&gt;
&lt;p&gt;Below is an example for a &lt;code&gt;kubernetes.io/dockercfg&lt;/code&gt; type of Secret:
下面是一个 &lt;code&gt;kubernetes.io/dockercfg&lt;/code&gt; 类别 Secret 的示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-dockercfg&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/dockercfg&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;.dockercfg&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;|
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    &amp;#34;&amp;lt;base64 encoded ~/.dockercfg file&amp;gt;&amp;#34;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果不想要执行 base64 编码，则可以使用 &lt;code&gt;stringData&lt;/code&gt; 字段，而不是 &lt;code&gt;data&lt;/code&gt; 字段&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;在创建这些类别的 Secret 时，API 服务会检查其中的键是否是 &lt;code&gt;data&lt;/code&gt; 字段中存在。并且会验证提供
的值是否能解析为有效的 JSON. 但 API 服务不会验证这个 JSON 实际上是不是一个 Docker 配置文件。&lt;/p&gt;
&lt;p&gt;当在没有 Docker 配置文件时，可以使用 &lt;code&gt;kubectl&lt;/code&gt; 创建一个 Docker 镜像仓库凭据 Secret， 例如:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret docker-registry secret-tiger-docker &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  --docker-username&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tiger &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  --docker-password&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;pass113 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  --docker-email&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tiger@acme.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个命令会创建一个 &lt;code&gt;kubernetes.io/dockerconfigjson&lt;/code&gt; 类别的 Secret。如果将 &lt;code&gt;.dockerconfigjson&lt;/code&gt;
&lt;code&gt;data&lt;/code&gt; 字段转存，就会得到新创建的一个 JSON 格式的有效的 Docker 配置:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;{
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;auths&amp;#34;&lt;/span&gt;: {
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;https://index.docker.io/v1/&amp;#34;&lt;/span&gt;: {
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;username&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tiger&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;password&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pass113&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;email&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tiger@acme.com&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;auth&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;dGlnZXI6cGFzczExMw==&amp;#34;&lt;/span&gt;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Basic authentication Secret

The `kubernetes.io/basic-auth` type is provided for storing credentials needed
for basic authentication. When using this Secret type, the `data` field of the
Secret must contain the following two keys:

- `username`: the user name for authentication;
- `password`: the password or token for authentication.

Both values for the above two keys are base64 encoded strings. You can, of
course, provide the clear text content using the `stringData` for Secret
creation.

The following YAML is an example config for a basic authentication Secret:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: secret-basic-auth
type: kubernetes.io/basic-auth
stringData:
  username: admin
  password: t0p-Secret
```

The basic authentication Secret type is provided only for user&#39;s convenience.
You can create an `Opaque` for credentials used for basic authentication.
However, using the builtin Secret type helps unify the formats of your credentials
and the API server does verify if the required keys are provided in a Secret
configuration.
 --&gt;
&lt;h3 id=&#34;基础认证-secret&#34;&gt;基础认证 Secret&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubernetes.io/basic-auth&lt;/code&gt; 类别的 Secret 是用来存储基础认证所需的凭据的。 当使用这个
Secret 类别时， Secret 的 &lt;code&gt;data&lt;/code&gt; 字段必须要包含以下两个键:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;username&lt;/code&gt;: 认证的用户名;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;password&lt;/code&gt;: 认证的密码或 token.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下两个键的值都需要进行 base64 编码。 也可以在创建 Secret 使用 &lt;code&gt;stringData&lt;/code&gt; 这样就可以在
直接使用明文的键值。&lt;/p&gt;
&lt;p&gt;以下 YAML 就是一个基础认证 Secret 的示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-basic-auth&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/basic-auth&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;stringData&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;username&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;admin&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;password&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;t0p-Secret&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;提供基础认证类型的 Secret 只是为了用户方便。 也可以创建一个 &lt;code&gt;Opaque&lt;/code&gt; 类别的 Secret 来存储
基础认证的凭据。 但使用内置的 Secret 类别有助于统一凭据格式并且 API 服务也会验证 Secret 配置
中是否提供了需要的键。&lt;/p&gt;
&lt;!--
### SSH authentication secrets

The builtin type `kubernetes.io/ssh-auth` is provided for storing data used in
SSH authentication. When using this Secret type, you will have to specify a
`ssh-privatekey` key-value pair in the `data` (or `stringData`) field
as the SSH credential to use.

The following YAML is an example config for a SSH authentication Secret:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: secret-ssh-auth
type: kubernetes.io/ssh-auth
data:
  # the data is abbreviated in this example
  ssh-privatekey: |
     MIIEpQIBAAKCAQEAulqb/Y ...
```

The SSH authentication Secret type is provided only for user&#39;s convenience.
You can create an `Opaque` for credentials used for SSH authentication.
However, using the builtin Secret type helps unify the formats of your credentials
and the API server does verify if the required keys are provided in a Secret
configuration.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; SSH private keys do not establish trusted communication between an SSH client and
host server on their own. A secondary means of establishing trust is needed to
mitigate &amp;ldquo;man in the middle&amp;rdquo; attacks, such as a &lt;code&gt;known_hosts&lt;/code&gt; file added to a
ConfigMap.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;!--
### SSH authentication secrets

The builtin type `kubernetes.io/ssh-auth` is provided for storing data used in
SSH authentication. When using this Secret type, you will have to specify a
`ssh-privatekey` key-value pair in the `data` (or `stringData`) field
as the SSH credential to use.

The following YAML is an example config for a SSH authentication Secret:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: secret-ssh-auth
type: kubernetes.io/ssh-auth
data:
  # the data is abbreviated in this example
  ssh-privatekey: |
     MIIEpQIBAAKCAQEAulqb/Y ...
```

The SSH authentication Secret type is provided only for user&#39;s convenience.
You can create an `Opaque` for credentials used for SSH authentication.
However, using the builtin Secret type helps unify the formats of your credentials
and the API server does verify if the required keys are provided in a Secret
configuration.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; SSH private keys do not establish trusted communication between an SSH client and
host server on their own. A secondary means of establishing trust is needed to
mitigate &amp;ldquo;man in the middle&amp;rdquo; attacks, such as a &lt;code&gt;known_hosts&lt;/code&gt; file added to a
ConfigMap.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;ssh-authentication-secrets&#34;&gt;SSH 认证 Secret&lt;/h3&gt;
&lt;p&gt;内置的 &lt;code&gt;kubernetes.io/ssh-auth&lt;/code&gt; 类别的 Secret 是用于存储 SSH 认证数据的。 当使用该类别
Secret 时， 需要在 &lt;code&gt;data&lt;/code&gt; (或 &lt;code&gt;stringData&lt;/code&gt;)指定用于 SSH 认证的键值对，其中键为 &lt;code&gt;ssh-privatekey&lt;/code&gt;
值为私钥的内容&lt;/p&gt;
&lt;p&gt;下面的 YAML 就是一个 SSH 认证 Secret 的配置示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-ssh-auth&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/ssh-auth&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#75715e&#34;&gt;# 示例中省略了部分数据&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ssh-privatekey&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;|
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;     MIIEpQIBAAKCAQEAulqb/Y ...
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;提供 SSH 认证 Secret 只是为了用户方便。 也可以创建一个 &lt;code&gt;Opaque&lt;/code&gt; 类别的 Secret 来存储
SSH 认证的凭据。 但使用内置的 Secret 类别有助于统一凭据格式并且 API 服务也会验证 Secret 配置
中是否提供了需要的键。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; SSH 私钥并不能独立在 SSH 客户端和服务端之间建议安全连接。 一种建立安全连接辅助方式能降低&amp;quot;中间人&amp;quot;攻击的可能，
例如将 &lt;code&gt;known_hosts&lt;/code&gt; 文件添加到 ConfigMap&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### TLS secrets

Kubernetes provides a builtin Secret type `kubernetes.io/tls` for to storing
a certificate and its associated key that are typically used for TLS . This
data is primarily used with TLS termination of the Ingress resource, but may
be used with other resources or directly by a workload.
When using this type of Secret, the `tls.key` and the `tls.crt` key must be provided
in the `data` (or `stringData`) field of the Secret configuration, although the API
server doesn&#39;t actually validate the values for each key.

The following YAML contains an example config for a TLS Secret:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: secret-tls
type: kubernetes.io/tls
data:
  # the data is abbreviated in this example
  tls.crt: |
    MIIC2DCCAcCgAwIBAgIBATANBgkqh ...
  tls.key: |
    MIIEpgIBAAKCAQEA7yn3bRHQ5FHMQ ...
```

The TLS Secret type is provided for user&#39;s convenience. You can create an `Opaque`
for credentials used for TLS server and/or client. However, using the builtin Secret
type helps ensure the consistency of Secret format in your project; the API server
does verify if the required keys are provided in a Secret configuration.

When creating a TLS Secret using `kubectl`, you can use the `tls` subcommand
as shown in the following example:

```shell
kubectl create secret tls my-tls-secret \
  --cert=path/to/cert/file \
  --key=path/to/key/file
```

The public/private key pair must exist before hand. The public key certificate
for `--cert` must be .PEM encoded (Base64-encoded DER format), and match the
given private key for `--key`.
The private key must be in what is commonly called PEM private key format,
unencrypted. In both cases, the initial and the last lines from PEM (for
example, `--------BEGIN CERTIFICATE-----` and `-------END CERTIFICATE----` for
a cetificate) are *not* included.
 --&gt;
&lt;h3 id=&#34;tls-secrets&#34;&gt;TLS Secret&lt;/h3&gt;
&lt;p&gt;k8s 提供了内置的 &lt;code&gt;kubernetes.io/tls&lt;/code&gt; Secret 类别用在存储 TLS 通常使用的证书和对应的 key.
这些数据主要用于 Ingress 资源的 TLS 终结，但可能被其它资源使用或直接被工作负载使用。 当使用
该类另的 Secret 时，在 Secret 配置的 &lt;code&gt;data&lt;/code&gt; (或 &lt;code&gt;stringData&lt;/code&gt;) 字段中必须要提供 &lt;code&gt;tls.key&lt;/code&gt;
和 &lt;code&gt;tls.crt&lt;/code&gt; 这两个键，尽管 API 服务实际上是不验证这些键的。&lt;/p&gt;
&lt;p&gt;The following YAML contains an example config for a TLS Secret:
下面的 YAML 中包含了一个 TLS Secret 的示例配置:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-tls&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/tls&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#75715e&#34;&gt;# 下面的示例数据部分省略&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;tls.crt&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;|
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    MIIC2DCCAcCgAwIBAgIBATANBgkqh ...
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  tls.key: |
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    MIIEpgIBAAKCAQEA7yn3bRHQ5FHMQ ...
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;提供 TLS Secret 只是为了用户方便。 也可以创建一个 &lt;code&gt;Opaque&lt;/code&gt; 类别的 Secret 来存储
TLS 服务端和/或客户端的凭据。 但使用内置的 Secret 类别有助于统一凭据格式并且 API 服务也会
验证 Secret 配置中是否提供了需要的键。&lt;/p&gt;
&lt;p&gt;在使用 &lt;code&gt;kubectl&lt;/code&gt; 创建 TLS Secret，可以使用 &lt;code&gt;tls&lt;/code&gt; 子命令，见下面的例子:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret tls my-tls-secret &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  --cert&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;path/to/cert/file &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  --key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;path/to/key/file
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;公钥/私钥对必须要先准备好。 &lt;code&gt;--cert&lt;/code&gt; 使用的公钥证书必要是 .PEM 编码(Base64-encoded DER 格式)，
并且与 &lt;code&gt;--key&lt;/code&gt; 中提供的私钥匹配。 私钥也必须以常见的 PEM 私钥格式提供， 不加密。
对于两个键， PEM 的首尾行(就如证书的 &lt;code&gt;--------BEGIN CERTIFICATE-----&lt;/code&gt; 和
&lt;code&gt;-------END CERTIFICATE----&lt;/code&gt;)是 &lt;em&gt;不&lt;/em&gt; 包含在内的。&lt;/p&gt;
&lt;!--
### Bootstrap token Secrets

A bootstrap token Secret can be created by explicitly specifying the Secret
`type` to `bootstrap.kubernetes.io/token`. This type of Secret is designed for
tokens used during the node bootstrap process. It stores tokens used to sign
well known ConfigMaps.

A bootstrap token Secret is usually created in the `kube-system` namespace and
named in the form `bootstrap-token-&lt;token-id&gt;` where `&lt;token-id&gt;` is a 6 character
string of the token ID.

As a Kubernetes manifest, a bootstrap token Secret might look like the
following:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: bootstrap-token-5emitj
  namespace: kube-system
type: bootstrap.kubernetes.io/token
data:
  auth-extra-groups: c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=
  expiration: MjAyMC0wOS0xM1QwNDozOToxMFo=
  token-id: NWVtaXRq
  token-secret: a3E0Z2lodnN6emduMXAwcg==
  usage-bootstrap-authentication: dHJ1ZQ==
  usage-bootstrap-signing: dHJ1ZQ==
```

A bootstrap type Secret has the following keys specified under `data`:

- `token_id`: A random 6 character string as the token identifier. Required.
- `token-secret`: A random 16 character string as the actual token secret. Required.
- `description`: A human-readable string that describes what the token is
  used for. Optional.
- `expiration`: An absolute UTC time using RFC3339 specifying when the token
  should be expired. Optional.
- `usage-bootstrap-&lt;usage&gt;`: A boolean flag indicating additional usage for
  the bootstrap token.
- `auth-extra-groups`: A comma-separated list of group names that will be
  authenticated as in addition to the `system:bootstrappers` group.

The above YAML may look confusing because the values are all in base64 encoded
strings. In fact, you can create an identical Secret using the following YAML:

```yaml
apiVersion: v1
kind: Secret
metadata:
  # Note how the Secret is named
  name: bootstrap-token-5emitj
  # A bootstrap token Secret usually resides in the kube-system namespace
  namespace: kube-system
type: bootstrap.kubernetes.io/token
stringData:
  auth-extra-groups: &#34;system:bootstrappers:kubeadm:default-node-token&#34;
  expiration: &#34;2020-09-13T04:39:10Z&#34;
  # This token ID is used in the name
  token-id: &#34;5emitj&#34;
  token-secret: &#34;kq4gihvszzgn1p0r&#34;
  # This token can be used for authentication
  usage-bootstrap-authentication: &#34;true&#34;
  # and it can be used for signing
  usage-bootstrap-signing: &#34;true&#34;
```
 --&gt;
&lt;h3 id=&#34;bootstrap-token-secrets&#34;&gt;引导令牌 Secret&lt;/h3&gt;
&lt;p&gt;引导令牌 Secret 可以通过显示地将 Secret 的 &lt;code&gt;type&lt;/code&gt; 设置为 &lt;code&gt;bootstrap.kubernetes.io/token&lt;/code&gt;
来创建. 这个 Secret 类别是设计来存储节点引导进程所使用的令牌的。 它存储的令牌是用来签发认可
的 ConfigMap 的。&lt;/p&gt;
&lt;p&gt;引导令牌 Secret 通常是创建在 &lt;code&gt;kube-system&lt;/code&gt; 命名空间中， 命名格式为 &lt;code&gt;bootstrap-token-&amp;lt;token-id&amp;gt;&lt;/code&gt;
其中 &lt;code&gt;&amp;lt;token-id&amp;gt;&lt;/code&gt; 是一个 6 字符的令牌 ID。&lt;/p&gt;
&lt;p&gt;一个引导令牌 Secret 可能就长成下面的这个梯子:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bootstrap-token-5emitj&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube-system&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bootstrap.kubernetes.io/token&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;auth-extra-groups&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;expiration&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MjAyMC0wOS0xM1QwNDozOToxMFo=&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;token-id&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NWVtaXRq&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;token-secret&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;a3E0Z2lodnN6emduMXAwcg==&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;usage-bootstrap-authentication&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;dHJ1ZQ==&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;usage-bootstrap-signing&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;dHJ1ZQ==&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;一个引导令牌 Secret 的 &lt;code&gt;data&lt;/code&gt;字段下有如下字段:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;token-id&lt;/code&gt;: 一个 6 字符随机字符串作为令牌 ID. 必要.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;token-secret&lt;/code&gt;: 一个 16 字符随机字符串，作为真正的令牌秘文. 必要.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;description&lt;/code&gt;: 一个人类可读的字符串，描述令牌是用来做啥的。 可选&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;expiration&lt;/code&gt;: 一个绝对 UTF 时间，使用 RFC3339， 指定令牌啥时候过期。可选&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;usage-bootstrap-&amp;lt;usage&amp;gt;&lt;/code&gt;: 一个布尔标示，用来指示这个引导令牌附加使用信息&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;auth-extra-groups&lt;/code&gt;: 一个逗号分隔的组名称列表，用来认证 &lt;code&gt;system:bootstrappers&lt;/code&gt; 打头的组&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上面的 YAML 因为值都是 base64 编码的，看起来有点晕。实际上可以使用以下 YAML 创建一个与它一样
的 Secret:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#75715e&#34;&gt;# 注意 Secret 是怎么命名的&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bootstrap-token-5emitj&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;# 引导令牌 Secret 通常都是在 kube-system 命名空间中&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube-system&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bootstrap.kubernetes.io/token&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;stringData&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;auth-extra-groups&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;system:bootstrappers:kubeadm:default-node-token&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;expiration&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2020-09-13T04:39:10Z&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;# 这个 令牌 ID 被用在名称上&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;token-id&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;5emitj&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;token-secret&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kq4gihvszzgn1p0r&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;# 这个令牌可以被用作认证&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;usage-bootstrap-authentication&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;# 也可以被用来签名&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;usage-bootstrap-signing&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Creating a Secret

There are several options to create a Secret:

- [create Secret using `kubectl` command](/docs/tasks/configmap-secret/managing-secret-using-kubectl/)
- [create Secret from config file](/docs/tasks/configmap-secret/managing-secret-using-config-file/)
- [create Secret using kustomize](/docs/tasks/configmap-secret/managing-secret-using-kustomize/)
 --&gt;
&lt;h2 id=&#34;creating-a-secret&#34;&gt;创建 Secret&lt;/h2&gt;
&lt;p&gt;There are several options to create a Secret:
有几种创建 Secret 方式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/configmap-secret/managing-secret-using-kubectl/&#34;&gt;使用 &lt;code&gt;kubectl&lt;/code&gt; 命令创建 Secret&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/configmap-secret/managing-secret-using-config-file/&#34;&gt;通过配置文件创建 Secret &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/configmap-secret/managing-secret-using-kustomize/&#34;&gt;使用 kustomize 创建 Secret&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Editing a Secret

An existing Secret may be edited with the following command:

```shell
kubectl edit secrets mysecret
```

This will open the default configured editor and allow for updating the base64 encoded Secret values in the `data` field:

```yaml
# Please edit the object below. Lines beginning with a &#39;#&#39; will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  username: YWRtaW4=
  password: MWYyZDFlMmU2N2Rm
kind: Secret
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: { ... }
  creationTimestamp: 2016-01-22T18:41:56Z
  name: mysecret
  namespace: default
  resourceVersion: &#34;164619&#34;
  uid: cfee02d6-c137-11e5-8d73-42010af00002
type: Opaque
```
 --&gt;
&lt;h2 id=&#34;editing-a-secret&#34;&gt;编辑 Secret&lt;/h2&gt;
&lt;p&gt;An existing Secret may be edited with the following command:
可以通过以下命令修改一个现有的 Secret:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl edit secrets mysecret
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will open the default configured editor and allow for updating the base64 encoded Secret values in the &lt;code&gt;data&lt;/code&gt; field:
这会打开默认编辑器，并允许对 &lt;code&gt;data&lt;/code&gt; 字段下面  base64 编辑的 Secret 进行修改:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 请绑架下面的对象，以 &amp;#39;#&amp;#39; 开头的行会被忽略，&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 一个空文件会中止编辑。 如果保存时发生错误就会响应相应的失败信息&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;username&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;YWRtaW4=&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;password&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MWYyZDFlMmU2N2Rm&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;kubectl.kubernetes.io/last-applied-configuration&lt;/span&gt;: { &lt;span style=&#34;color:#ae81ff&#34;&gt;... }&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;creationTimestamp&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;2016-01-22T18:41:56Z&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysecret&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resourceVersion&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;164619&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;uid&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cfee02d6-c137-11e5-8d73-42010af00002&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Opaque&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Using Secrets

Secrets can be mounted as data volumes or exposed as
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/containers/container-environment/&#39; target=&#39;_blank&#39;&gt;environment variables&lt;span class=&#39;tooltip-text&#39;&gt;容器环境变量是 name=value 对，用来给 Pod 中运行的容器提供有用的信息&lt;/span&gt;
&lt;/a&gt;
to be used by a container in a Pod. Secrets can also be used by other parts of the
system, without being directly exposed to the Pod. For example, Secrets can hold
credentials that other parts of the system should use to interact with external
systems on your behalf.
 --&gt;
&lt;h2 id=&#34;using-secrets&#34;&gt;使用 Secret&lt;/h2&gt;
&lt;p&gt;Secret 可以挂载为数据卷或暴露为
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/containers/container-environment/&#39; target=&#39;_blank&#39;&gt;容器环境变量&lt;span class=&#39;tooltip-text&#39;&gt;容器环境变量是 name=value 对，用来给 Pod 中运行的容器提供有用的信息&lt;/span&gt;
&lt;/a&gt;
以供 Pod 中的容器使用。 Secret 也可以被系统的其它部分使用，而不需要直接暴露给 Pod。 例如，
Secret 可以存储系统其它部分与外部系统交互时需要用到的凭据。&lt;/p&gt;
&lt;!--
### Using Secrets as files from a Pod

To consume a Secret in a volume in a Pod:

1. Create a secret or use an existing one. Multiple Pods can reference the same secret.
1. Modify your Pod definition to add a volume under `.spec.volumes[]`. Name the volume anything, and have a `.spec.volumes[].secret.secretName` field equal to the name of the Secret object.
1. Add a `.spec.containers[].volumeMounts[]` to each container that needs the secret. Specify `.spec.containers[].volumeMounts[].readOnly = true` and `.spec.containers[].volumeMounts[].mountPath` to an unused directory name where you would like the secrets to appear.
1. Modify your image or command line so that the program looks for files in that directory. Each key in the secret `data` map becomes the filename under `mountPath`.

This is an example of a Pod that mounts a Secret in a volume:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: &#34;/etc/foo&#34;
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: mysecret
```

Each Secret you want to use needs to be referred to in `.spec.volumes`.

If there are multiple containers in the Pod, then each container needs its
own `volumeMounts` block, but only one `.spec.volumes` is needed per Secret.

You can package many files into one secret, or use many secrets, whichever is convenient.
 --&gt;
&lt;h3 id=&#34;using-secrets-as-files-from-a-pod&#34;&gt;将 Secret 用作 Pod 中的文件&lt;/h3&gt;
&lt;p&gt;在 Pod 中以卷的方式使用 Secret:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;创建或使用一个现有的 Secret. 多个 Pod 可以引用同一个 Secret.&lt;/li&gt;
&lt;li&gt;修改 Pod 定义， 在 &lt;code&gt;.spec.volumes[]&lt;/code&gt; 下面添加一个卷。 卷名随便起，但其中的
&lt;code&gt;.spec.volumes[].secret.secretName&lt;/code&gt; 字段的值需要是 Secret 对象的名称。&lt;/li&gt;
&lt;li&gt;在每个需要 Secret 的容器中添加一个 &lt;code&gt;.spec.containers[].volumeMounts[]&lt;/code&gt;。 指定
&lt;code&gt;.spec.containers[].volumeMounts[].readOnly = true&lt;/code&gt;
和
&lt;code&gt;.spec.containers[].volumeMounts[].mountPath&lt;/code&gt; 指向一个期望的未使用的位置&lt;/li&gt;
&lt;li&gt;修改镜像或命令行，让程序查找目录中的文件。 Secret &lt;code&gt;data&lt;/code&gt; 字段下的每一个键都会投射为
&lt;code&gt;mountPath&lt;/code&gt; 目录中的一个文件名。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下面是一个将 Secret 挂载为卷的 Pod 的示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/etc/foo&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysecret&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;每一个想要使用的 Secret 都需要在 &lt;code&gt;.spec.volumes&lt;/code&gt; 中引用。&lt;/p&gt;
&lt;p&gt;如果 Pod 中有多个容器， 则每个容器需要各自的 &lt;code&gt;volumeMounts&lt;/code&gt; 块， 但每个 Secret 只需要一个
&lt;code&gt;.spec.volumes&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;用户可以将多个文件放在一个 Secret 中，或使用多个 Secret, 哪个方便用哪个。&lt;/p&gt;
&lt;!--
#### Projection of Secret keys to specific paths

You can also control the paths within the volume where Secret keys are projected.
You can use the `.spec.volumes[].secret.items` field to change the target path of each key:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: &#34;/etc/foo&#34;
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: mysecret
      items:
      - key: username
        path: my-group/my-username
```

What will happen:

* `username` secret is stored under `/etc/foo/my-group/my-username` file instead of `/etc/foo/username`.
* `password` secret is not projected.

If `.spec.volumes[].secret.items` is used, only keys specified in `items` are projected.
To consume all keys from the secret, all of them must be listed in the `items` field.
All listed keys must exist in the corresponding secret. Otherwise, the volume is not created.
 --&gt;
&lt;h4 id=&#34;projection-of-secret-keys-to-specific-paths&#34;&gt;将 Secret 的键投射到指定目录&lt;/h4&gt;
&lt;p&gt;用户可以控制 Secret 键投射到卷中的哪个目录。
通过 &lt;code&gt;.spec.volumes[].secret.items&lt;/code&gt; 字段就可以修改每个键的目标路径:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/etc/foo&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysecret&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;items&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;username&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-group/my-username&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;结果表现为:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;username&lt;/code&gt; 秘文就是存储在 &lt;code&gt;/etc/foo/my-group/my-username&lt;/code&gt; 而不是 &lt;code&gt;/etc/foo/username&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;password&lt;/code&gt; 秘文则不会投射&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果使用了 &lt;code&gt;.spec.volumes[].secret.items&lt;/code&gt;，则只有在 &lt;code&gt;items&lt;/code&gt; 中指定的键才会被投射。
要使用 Secret 中所有键，则需要在  &lt;code&gt;items&lt;/code&gt; 字段把它们全部列举。
所有列表的键都必须要在对应的 Secret 存在，否则，卷不能被创建。&lt;/p&gt;
&lt;!--
#### Secret files permissions

You can set the file access permission bits for a single Secret key.
If you don&#39;t specify any permissions, `0644` is used by default.
You can also set a default mode for the entire Secret volume and override per key if needed.

For example, you can specify a default mode like this:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: &#34;/etc/foo&#34;
  volumes:
  - name: foo
    secret:
      secretName: mysecret
      defaultMode: 0400
```

Then, the secret will be mounted on `/etc/foo` and all the files created by the
secret volume mount will have permission `0400`.

Note that the JSON spec doesn&#39;t support octal notation, so use the value 256 for
0400 permissions. If you use YAML instead of JSON for the Pod, you can use octal
notation to specify permissions in a more natural way.

Note if you `kubectl exec` into the Pod, you need to follow the symlink to find
the expected file mode. For example,

Check the secrets file mode on the pod.
```
kubectl exec mypod -it sh

cd /etc/foo
ls -l
```

The output is similar to this:
```
total 0
lrwxrwxrwx 1 root root 15 May 18 00:18 password -&gt; ..data/password
lrwxrwxrwx 1 root root 15 May 18 00:18 username -&gt; ..data/username
```

Follow the symlink to find the correct file mode.

```
cd /etc/foo/..data
ls -l
```

The output is similar to this:
```
total 8
-r-------- 1 root root 12 May 18 00:18 password
-r-------- 1 root root  5 May 18 00:18 username
```

You can also use mapping, as in the previous example, and specify different
permissions for different files like this:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: &#34;/etc/foo&#34;
  volumes:
  - name: foo
    secret:
      secretName: mysecret
      items:
      - key: username
        path: my-group/my-username
        mode: 0777
```

In this case, the file resulting in `/etc/foo/my-group/my-username` will have
permission value of `0777`. If you use JSON, owing to JSON limitations, you
must specify the mode in decimal notation, `511`.

Note that this permission value might be displayed in decimal notation if you
read it later.
--&gt;
&lt;h4 id=&#34;secret-files-permissions&#34;&gt;Secret 文件权限&lt;/h4&gt;
&lt;p&gt;用户可以为 Secret 的每个键投射的文件设置权限。 如果不设置任何权限，则默认使用 &lt;code&gt;0644&lt;/code&gt;。
也可以为整个 Secret 卷设置默认权限，如果需要也可以覆盖每个键的权限。&lt;/p&gt;
&lt;p&gt;例如，指定默认模式可以如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/etc/foo&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysecret&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;defaultMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;0400&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在上面的例子中， Secret 会被挂载到 &lt;code&gt;/etc/foo&lt;/code&gt; 所有在 Secret 中创建的所有的文件权限就是 &lt;code&gt;0400&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;要注意 JSON 配置不支持八进制写法，所以 &lt;code&gt;0400&lt;/code&gt; 权限要用 256. 如果使用 YAML 而不是 JSON 定义
Pod， 则可以直接使用常用的八进制写法来设置权限&lt;/p&gt;
&lt;p&gt;注意，如果通过 &lt;code&gt;kubectl exec&lt;/code&gt; 进入 Pod 内部， 需要通过软连接来找到文件的权限模式。例如，
检查 Pod 中的 Secret 文件的权限.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl exec mypod -it sh

cd /etc/foo
ls -l
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;total 0
lrwxrwxrwx 1 root root 15 May 18 00:18 password -&amp;gt; ..data/password
lrwxrwxrwx 1 root root 15 May 18 00:18 username -&amp;gt; ..data/username
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;权所软连接找到真正的文件权限模式&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /etc/foo/..data
ls -l
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;total 8
-r-------- 1 root root 12 May 18 00:18 password
-r-------- 1 root root  5 May 18 00:18 username
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用于也可以在使用映射时为不同文件指定不同权限，例如:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/etc/foo&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysecret&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;items&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;username&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-group/my-username&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;mode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;0777&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在上面的例子中，最终生成的 &lt;code&gt;/etc/foo/my-group/my-username&lt;/code&gt; 的权限是 &lt;code&gt;0777&lt;/code&gt;。 如果使用的
是 JSON， 就会受 JSON 的限制，就需要以十进制写法来设置权限模式为 &lt;code&gt;511&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;要注意如果在之后的读取中，这个权限值可以会以十进制来显示。&lt;/p&gt;
&lt;!--
#### Consuming Secret values from volumes

Inside the container that mounts a secret volume, the secret keys appear as
files and the secret values are base64 decoded and stored inside these files.
This is the result of commands executed inside the container from the example above:

```shell
ls /etc/foo/
```

The output is similar to:

```
username
password
```

```shell
cat /etc/foo/username
```

The output is similar to:

```
admin
```

```shell
cat /etc/foo/password
```

The output is similar to:

```
1f2d1e2e67df
```

The program in a container is responsible for reading the secrets from the
files.
--&gt;
&lt;h4 id=&#34;consuming-secret-values-from-volumes&#34;&gt;使用投射到卷中的 Secret 值&lt;/h4&gt;
&lt;p&gt;在挂载 Secret 卷的容器中，Secret 键会以文件名，Secret 的值在 base64 解码后存入文件中。
下面是在上面示例中的容器中执行的命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;ls /etc/foo/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;username
password
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;cat /etc/foo/username
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;admin
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;cat /etc/foo/password
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1f2d1e2e67df
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;容器中的的程序要负责读取 Secret 投射出来的文件。&lt;/p&gt;
&lt;!--
#### Mounted Secrets are updated automatically

When a secret currently consumed in a volume is updated, projected keys are eventually updated as well.
The kubelet checks whether the mounted secret is fresh on every periodic sync.
However, the kubelet uses its local cache for getting the current value of the Secret.
The type of the cache is configurable using the `ConfigMapAndSecretChangeDetectionStrategy` field in
the [KubeletConfiguration struct](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go).
A Secret can be either propagated by watch (default), ttl-based, or simply redirecting
all requests directly to the API server.
As a result, the total delay from the moment when the Secret is updated to the moment
when new keys are projected to the Pod can be as long as the kubelet sync period + cache
propagation delay, where the cache propagation delay depends on the chosen cache type
(it equals to watch propagation delay, ttl of cache, or zero correspondingly).

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A container using a Secret as a
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes#using-subpath&#34;&gt;subPath&lt;/a&gt; volume mount will not receive
Secret updates.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h4 id=&#34;mounted-secrets-are-updated-automatically&#34;&gt;挂载 Secret 的自动更新&lt;/h4&gt;
&lt;p&gt;当一个正在被以卷方式使用的 Secret 更新时，与其相映射的键最终也会更新。kubelet 会在每个
同步周期检查挂载的 Secret 是否更新。kubelet 会使用本地缓存来获取 Secret 的当前值。
缓存的类型可以通过
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubelet/config/v1beta1/types.go&#34;&gt;KubeletConfiguration struct&lt;/a&gt;.
中的 &lt;code&gt;ConfigMapAndSecretChangeDetectionStrategy&lt;/code&gt; 字段来配置。 Secret 的传播方式有
监视(默认)，基于 ttl, 或简单地将所有请求直接重定向给 API server. 最终， 从 Secret 更新
到新的键被投射到 Pod 中的总延时就是 kubelet 同时间隔时长 + 缓存传播延时， 而其中缓存传播延时
又基于缓存的类型(相应地它可能等于 监视传播延时，缓存的 TTL, 或零)。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 容器使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes#using-subpath&#34;&gt;子目录&lt;/a&gt;
方式挂载 Secret 的卷不会接收到 Secret 的更新。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Using Secrets as environment variables

To use a secret in an &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/containers/container-environment/&#39; target=&#39;_blank&#39;&gt;environment variable&lt;span class=&#39;tooltip-text&#39;&gt;容器环境变量是 name=value 对，用来给 Pod 中运行的容器提供有用的信息&lt;/span&gt;
&lt;/a&gt;
in a Pod:

1. Create a secret or use an existing one.  Multiple Pods can reference the same secret.
1. Modify your Pod definition in each container that you wish to consume the value of a secret key to add an environment variable for each secret key you wish to consume. The environment variable that consumes the secret key should populate the secret&#39;s name and key in `env[].valueFrom.secretKeyRef`.
1. Modify your image and/or command line so that the program looks for values in the specified environment variables.

This is an example of a Pod that uses secrets from environment variables:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
  - name: mycontainer
    image: redis
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: username
      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: password
  restartPolicy: Never
```
 --&gt;
&lt;h3 id=&#34;using-secrets-as-environment-variables&#34;&gt;将 Secret 用作环境变量&lt;/h3&gt;
&lt;p&gt;要将 Secret 用作 Pod 中的
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/containers/container-environment/&#39; target=&#39;_blank&#39;&gt;容器环境变量&lt;span class=&#39;tooltip-text&#39;&gt;容器环境变量是 name=value 对，用来给 Pod 中运行的容器提供有用的信息&lt;/span&gt;
&lt;/a&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;创建或使用一个现有的 Secret. 多个 Pod 可以引用同一个 Secret.&lt;/li&gt;
&lt;li&gt;修改 Pod 中每个想要将 Secret 的键作为环境变量的容器的定义配置， 每个使用 Secret 键的环境
变量都需要添加 &lt;code&gt;env[].valueFrom.secretKeyRef&lt;/code&gt; 将指向使用 Secret 键&lt;/li&gt;
&lt;li&gt;修改镜像或命令以便让程序查看指定的环境变量&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下面是一个使用 Secret 作为环境变量的 Pod 示例配置:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-env-pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mycontainer&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;SECRET_USERNAME&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;secretKeyRef&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysecret&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;username&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;SECRET_PASSWORD&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;secretKeyRef&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysecret&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;password&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
#### Consuming Secret Values from environment variables

Inside a container that consumes a secret in an environment variables, the secret keys appear as
normal environment variables containing the base64 decoded values of the secret data.
This is the result of commands executed inside the container from the example above:

```shell
echo $SECRET_USERNAME
```

The output is similar to:

```
admin
```

```shell
echo $SECRET_PASSWORD
```

The output is similar to:

```
1f2d1e2e67df
```
 --&gt;
&lt;h4 id=&#34;consuming-secret-values-from-environment-variables&#34;&gt;使用 Secret 投射出来的环境变量&lt;/h4&gt;
&lt;p&gt;Inside a container that consumes a secret in an environment variables, the secret keys appear as
normal environment variables containing the base64 decoded values of the secret data.
This is the result of commands executed inside the container from the example above:
在使用 Secret 投射出来的环境变量的容器中，最终环境变量引用的 Secret 键的环境变量的值是 Secret
数据 base64 解码的值。 下面是在容器中执行的命令示例；&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;echo $SECRET_USERNAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;admin
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;echo $SECRET_PASSWORD
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1f2d1e2e67df
&lt;/code&gt;&lt;/pre&gt;&lt;!--
#### Environment variables are not updated after a secret update

If a container already consumes a Secret in an environment variable, a Secret update will not be seen by the container unless it is restarted.
There are third party solutions for triggering restarts when secrets change.
 --&gt;
&lt;h4 id=&#34;environment-variables-are-not-updated-after-a-secret-update&#34;&gt;环境变量不会在 Secret 更新后更新&lt;/h4&gt;
&lt;p&gt;如果一个容器已经在以环境变量的方式使用一个 Secret, 容器在重启之前是看不到 Secret 的更新的。
有第三方解决方案可以在 Secret 发生变更时来触发重启&lt;/p&gt;
&lt;!--
## Immutable Secrets {#secret-immutable}






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;



The Kubernetes beta feature _Immutable Secrets and ConfigMaps_ provides an option to set
individual Secrets and ConfigMaps as immutable. For clusters that extensively use Secrets
(at least tens of thousands of unique Secret to Pod mounts), preventing changes to their
data has the following advantages:

- protects you from accidental (or unwanted) updates that could cause applications outages
- improves performance of your cluster by significantly reducing load on kube-apiserver, by
closing watches for secrets marked as immutable.

This feature is controlled by the `ImmutableEphemeralVolumes` [feature
gate](/docs/reference/command-line-tools-reference/feature-gates/),
which is enabled by default since v1.19. You can create an immutable
Secret by setting the `immutable` field to `true`. For example,
```yaml
apiVersion: v1
kind: Secret
metadata:
  ...
data:
  ...
immutable: true
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Once a Secret or ConfigMap is marked as immutable, it is &lt;em&gt;not&lt;/em&gt; possible to revert this change
nor to mutate the contents of the &lt;code&gt;data&lt;/code&gt; field. You can only delete and recreate the Secret.
Existing Pods maintain a mount point to the deleted Secret - it is recommended to recreate
these pods.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;secret-immutable&#34;&gt;不可变 Secret&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;这个 k8s 的 bata 特性 &lt;em&gt;不可变 Secret 和 ConfigMap&lt;/em&gt; 提供了一个可选项，可以让一个 Secret
和 ConfigMap 变为不可变。 对于那些广泛使用 Secret (一个 Secret 至少被 10k Pod 挂载)，
防止修改它们中的数据有以下好处:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;防止误操作(或不想要)的更新可能引发的应用事故&lt;/li&gt;
&lt;li&gt;当 Secret 标记为不可变时会关闭监视，这样能极大地减少 kube-apiserver 的负载，从而改善
集群性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个特性通过设置 &lt;code&gt;ImmutableEphemeralVolumes&lt;/code&gt;
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;
控制。 从 k8s v1.19 开始该特性默认开启的。
用户可以在 Secret 中通过设置 &lt;code&gt;immutable&lt;/code&gt; 字段为 &lt;code&gt;true&lt;/code&gt; 让其成功不可变 Secret&lt;/p&gt;
&lt;p&gt;示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;immutable&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 当一个 Secret 或 ConfigMap 被标记为不可变后，它就 &lt;em&gt;不&lt;/em&gt; 可能再被变会普通， 也不可能再修改其
&lt;code&gt;data&lt;/code&gt; 字段的内容。 只能删除或重建 Secret . 现有的 Pod 会维持对已经删除的 Secret 挂载指向，
推荐对这些 Pod 也进行重建。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Using imagePullSecrets

The `imagePullSecrets` field is a list of references to secrets in the same namespace.
You can use an `imagePullSecrets` to pass a secret that contains a Docker (or other) image registry
password to the kubelet. The kubelet uses this information to pull a private image on behalf of your Pod.
See the [PodSpec API](/docs/reference/generated/kubernetes-api/v1.19/#podspec-v1-core) for more information about the `imagePullSecrets` field.
 --&gt;
&lt;h3 id=&#34;using-imagepullsecrets&#34;&gt;使用 &lt;code&gt;imagePullSecrets&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;imagePullSecrets&lt;/code&gt; 字段值是一个引用同一个命名空间中的 Secret 的列表。用户可以使用
&lt;code&gt;imagePullSecrets&lt;/code&gt; 向 Docker (或其它) 镜像仓库传递一个包含密码的 Secret 给 kubelet.
kubelet 使用这些信息来为 Pod 从私有镜像仓库拉取镜像。更多关于 &lt;code&gt;imagePullSecrets&lt;/code&gt; 字段的信息见
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#podspec-v1-core&#34;&gt;PodSpec API&lt;/a&gt;&lt;/p&gt;
&lt;!--
#### Manually specifying an imagePullSecret

You can learn how to specify `ImagePullSecrets` from the [container images documentation](/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod).
 --&gt;
&lt;h4 id=&#34;manually-specifying-an-imagepullsecret&#34;&gt;手动指定一个 imagePullSecret&lt;/h4&gt;
&lt;p&gt;用户可以通过
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod&#34;&gt;容器镜像文档&lt;/a&gt;
学习怎么指定 &lt;code&gt;ImagePullSecrets&lt;/code&gt;&lt;/p&gt;
&lt;!--
### Arranging for imagePullSecrets to be automatically attached

You can manually create `imagePullSecrets`, and reference it from
a ServiceAccount. Any Pods created with that ServiceAccount
or created with that ServiceAccount by default, will get their `imagePullSecrets`
field set to that of the service account.
See [Add ImagePullSecrets to a service account](/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account)
 for a detailed explanation of that process.
 --&gt;
&lt;h3 id=&#34;arranging-for-imagepullsecrets-to-be-automatically-attached&#34;&gt;设置 imagePullSecrets 自动加载&lt;/h3&gt;
&lt;p&gt;用户可以手动创建 &lt;code&gt;imagePullSecrets&lt;/code&gt;，并在 ServiceAccount 中引用它。 任意以这个 ServiceAccount
创建或默认以这个 ServiceAccount 创建的 Pod 都会获得它们的 &lt;code&gt;imagePullSecrets&lt;/code&gt; 字段为这个
ServiceAccount。 更多对这个过程的详细解释见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account&#34;&gt;将 ImagePullSecrets 添加到 ServiceAccount&lt;/a&gt;&lt;/p&gt;
&lt;!--
### Automatic mounting of manually created Secrets

Manually created secrets (for example, one containing a token for accessing a GitHub account)
can be automatically attached to pods based on their service account.
See [Injecting Information into Pods Using a PodPreset](/docs/tasks/inject-data-application/podpreset/) for a detailed explanation of that process.
 --&gt;
&lt;h3 id=&#34;automatic-mounting-of-manually-created-secrets&#34;&gt;自动挂载手动创建的 Secret&lt;/h3&gt;
&lt;p&gt;Manually created secrets (for example, one containing a token for accessing a GitHub account)
can be automatically attached to pods based on their service account.
See &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/inject-data-application/podpreset/&#34;&gt;Injecting Information into Pods Using a PodPreset&lt;/a&gt; for a detailed explanation of that process.
手动创建的 Secret (例如，一个包含 GitHub 账号)可以通过 ServiceAccount 自动挂载到 Pod。
更多关于这个过程的解释见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/inject-data-application/podpreset/&#34;&gt;使用 PodPreset 向 Pod 中注入信息&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Details

### Restrictions

Secret volume sources are validated to ensure that the specified object
reference actually points to an object of type Secret. Therefore, a secret
needs to be created before any Pods that depend on it.

Secret resources reside in a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/namespaces&#39; target=&#39;_blank&#39;&gt;namespace&lt;span class=&#39;tooltip-text&#39;&gt;一个用于在同一个物理集群中支持多个虚拟集群的抽象概念&lt;/span&gt;
&lt;/a&gt;.
Secrets can only be referenced by Pods in that same namespace.

Individual secrets are limited to 1MiB in size. This is to discourage creation
of very large secrets which would exhaust the API server and kubelet memory.
However, creation of many smaller secrets could also exhaust memory. More
comprehensive limits on memory usage due to secrets is a planned feature.

The kubelet only supports the use of secrets for Pods where the secrets
are obtained from the API server.
This includes any Pods created using `kubectl`, or indirectly via a replication
controller. It does not include Pods created as a result of the kubelet
`--manifest-url` flag, its `--config` flag, or its REST API (these are
not common ways to create Pods.)

Secrets must be created before they are consumed in Pods as environment
variables unless they are marked as optional. References to secrets that do
not exist will prevent the Pod from starting.

References (`secretKeyRef` field) to keys that do not exist in a named Secret
will prevent the Pod from starting.

Secrets used to populate environment variables by the `envFrom` field that have keys
that are considered invalid environment variable names will have those keys
skipped. The Pod will be allowed to start. There will be an event whose
reason is `InvalidVariableNames` and the message will contain the list of
invalid keys that were skipped. The example shows a pod which refers to the
default/mysecret that contains 2 invalid keys: `1badkey` and `2alsobad`.

```shell
kubectl get events
```

The output is similar to:

```
LASTSEEN   FIRSTSEEN   COUNT     NAME            KIND      SUBOBJECT                         TYPE      REASON
0s         0s          1         dapi-test-pod   Pod                                         Warning   InvalidEnvironmentVariableNames   kubelet, 127.0.0.1      Keys [1badkey, 2alsobad] from the EnvFrom secret default/mysecret were skipped since they are considered invalid environment variable names.
```
 --&gt;
&lt;h2 id=&#34;details&#34;&gt;一些细节&lt;/h2&gt;
&lt;h3 id=&#34;限制&#34;&gt;限制&lt;/h3&gt;
&lt;p&gt;Secret 卷的源会被验证这个对象引用的确实是一个 Secret 类型的对象。 因此，Secret 需要在任意依赖
它的 Pod 之前创建。&lt;/p&gt;
&lt;p&gt;Secret 资源受限于
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/namespaces&#39; target=&#39;_blank&#39;&gt;命名空间(namespace)&lt;span class=&#39;tooltip-text&#39;&gt;一个用于在同一个物理集群中支持多个虚拟集群的抽象概念&lt;/span&gt;
&lt;/a&gt;.
Secret 只能被同一个命名空间中的 Pod 引用。&lt;/p&gt;
&lt;p&gt;单个 Secret 的空间仅限于 1MiB. 非常不建议创建体积很大的 Secret 因为它可能会导致 API 服务
和 kubelet 内存耗尽。但是创建许多小点的 Secret 也可能耗尽内存。 更多全面限制 Secret 使用
的内存是一个计划的特性。&lt;/p&gt;
&lt;p&gt;kubelet 只支持为 Pod 使用那些通过 API 服务获取的 Secret. 这包含任意通过 &lt;code&gt;kubectl&lt;/code&gt; 的 Pod，
或通过副本控制器间接创建的。 但不支持通过 kubelet 的 &lt;code&gt;--manifest-url&lt;/code&gt;， &lt;code&gt;--config&lt;/code&gt;
创建的 Pod， 或通过 kubelet REST API 创建的 Pod(这些都不创建 Pod 的常用方式)&lt;/p&gt;
&lt;p&gt;Secret 必须在使用它作为环境变量的 Pod 之前先创建好，不然环境变量就只能标记为可选。 如果 Pod
引用的 Secret 不存储就会阻止 Pod 启动。&lt;/p&gt;
&lt;p&gt;引用(通过 &lt;code&gt;secretKeyRef&lt;/code&gt; 字段)的键在指定 Secret 中不存在也会阻止 Pod 启动。&lt;/p&gt;
&lt;p&gt;通过 &lt;code&gt;envFrom&lt;/code&gt; 字段使用 Secret 作为环境变量时，如果其中的键被认为不是一个有效的环境变量名称
则这些键就会被跳过。 Pod 会被允许启动。 这会产生一个事件，其中的原因是 &lt;code&gt;InvalidVariableNames&lt;/code&gt;
信息中会包含被跳过的无效的键的列表。下面的示例中，显示一个引用 default/mysecret 的 Pod。 这个
Secret 中包含两个无效的键: &lt;code&gt;1badkey&lt;/code&gt; 和 &lt;code&gt;2alsobad&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get events
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类别如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;LASTSEEN   FIRSTSEEN   COUNT     NAME            KIND      SUBOBJECT                         TYPE      REASON
0s         0s          1         dapi-test-pod   Pod                                         Warning   InvalidEnvironmentVariableNames   kubelet, 127.0.0.1      Keys [1badkey, 2alsobad] from the EnvFrom secret default/mysecret were skipped since they are considered invalid environment variable names.
&lt;/code&gt;&lt;/pre&gt;&lt;!--
### Secret and Pod lifetime interaction

When a Pod is created by calling the Kubernetes API, there is no check if a referenced
secret exists. Once a Pod is scheduled, the kubelet will try to fetch the
secret value. If the secret cannot be fetched because it does not exist or
because of a temporary lack of connection to the API server, the kubelet will
periodically retry. It will report an event about the Pod explaining the
reason it is not started yet. Once the secret is fetched, the kubelet will
create and mount a volume containing it. None of the Pod&#39;s containers will
start until all the Pod&#39;s volumes are mounted.
--&gt;
&lt;h3 id=&#34;secret-and-pod-lifetime-interaction&#34;&gt;Secret 和 Pod 生命周期交互&lt;/h3&gt;
&lt;p&gt;当通过调用 k8s API 创建一个 Pod 时，并不会检查它引用的 Secret 是不是存储。 当 Pod 被调度时
kubelet 会尝试获取 Secret 的值。 如果因为 Secret 不存在或暂时无法连接到 API 服务而导致
无法获取 Secret, kubelet 会定期重试。 它会报告一个解释为啥 Pod 还没有启动的原因的事件。
当 Secret 获取后， kubelet 会创建并挂载一个卷来存储它。 在 Pod 所有的卷挂载之前没有容器会
启动。&lt;/p&gt;
&lt;!--
## Use cases

### Use-Case: As container environment variables

Create a secret
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  USER_NAME: YWRtaW4=
  PASSWORD: MWYyZDFlMmU2N2Rm
```

Create the Secret:
```shell
kubectl apply -f mysecret.yaml
```

Use `envFrom` to define all of the Secret&#39;s data as container environment variables. The key from the Secret becomes the environment variable name in the Pod.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secret-test-pod
spec:
  containers:
    - name: test-container
      image: k8s.gcr.io/busybox
      command: [ &#34;/bin/sh&#34;, &#34;-c&#34;, &#34;env&#34; ]
      envFrom:
      - secretRef:
          name: mysecret
  restartPolicy: Never
```
 --&gt;
&lt;h2 id=&#34;use-cases&#34;&gt;使用场景&lt;/h2&gt;
&lt;h3 id=&#34;使用场景-作为容器环境变量&#34;&gt;使用场景: 作为容器环境变量&lt;/h3&gt;
&lt;p&gt;Secret 定义&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysecret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Opaque&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;USER_NAME&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;YWRtaW4=&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;PASSWORD&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MWYyZDFlMmU2N2Rm&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;创建 Secret:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f mysecret.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 &lt;code&gt;envFrom&lt;/code&gt; 将 Secret 中的所有数据作为容器环境变量。 键会作为 Pod 中环境变量的名称。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-test-pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-container&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/busybox&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/bin/sh&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;env&amp;#34;&lt;/span&gt; ]
      &lt;span style=&#34;color:#f92672&#34;&gt;envFrom&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;secretRef&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysecret&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;使用场景-pod-中使用-ssh-密钥&#34;&gt;使用场景: Pod 中使用 SSH 密钥&lt;/h3&gt;
&lt;p&gt;创建一个包含 SSH 密钥的 Secret:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret generic ssh-key-secret --from-file&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;ssh-privatekey&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/path/to/.ssh/id_rsa --from-file&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;ssh-publickey&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/path/to/.ssh/id_rsa.pub
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;命令输出类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;secret &amp;quot;ssh-key-secret&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用户也可以使用一个包含 &lt;code&gt;secretGenerator&lt;/code&gt; 的 &lt;code&gt;kustomization.yaml&lt;/code&gt; 来装 SSH 密钥。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在上传 SSH 密钥之前要仔细考虑: 集群中的其他用户也可能访问这个 Secret. 使用一个 ServiceAccount
来让集群中的部分用户可以访问这个 Secret，如果有用户发生泄漏可以废除这个 ServiceAccount.&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;现在就可以创建一个 Pod 并在其中通过卷使用 Secret 中的 SSH 密钥:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-test-pod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ssh-key-secret&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ssh-test-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mySshImage&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/etc/secret-volume&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;当 容器运行时，SSH 密钥就是在:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/etc/secret-volume/ssh-publickey
/etc/secret-volume/ssh-privatekey
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这时容器就可以使用 Secret 中的数据来建立 SSH 连接。&lt;/p&gt;
&lt;!--
### Use-Case: Pods with prod / test credentials

This example illustrates a Pod which consumes a secret containing production
credentials and another Pod which consumes a secret with test environment
credentials.

You can create a `kustomization.yaml` with a `secretGenerator` field or run
`kubectl create secret`.

```shell
kubectl create secret generic prod-db-secret --from-literal=username=produser --from-literal=password=Y4nys7f11
```

The output is similar to:

```
secret &#34;prod-db-secret&#34; created
```

You can also create a secret for test environment credentials.

```shell
kubectl create secret generic test-db-secret --from-literal=username=testuser --from-literal=password=iluvtests
```

The output is similar to:

```
secret &#34;test-db-secret&#34; created
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;Special characters such as &lt;code&gt;$&lt;/code&gt;, &lt;code&gt;\&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;=&lt;/code&gt;, and &lt;code&gt;!&lt;/code&gt; will be interpreted by your &lt;a href=&#34;https://en.wikipedia.org/wiki/Shell_(computing)&#34;&gt;shell&lt;/a&gt; and require escaping.
In most shells, the easiest way to escape the password is to surround it with single quotes (&lt;code&gt;&#39;&lt;/code&gt;).
For example, if your actual password is &lt;code&gt;S!B\*d$zDsb=&lt;/code&gt;, you should execute the command this way:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret generic dev-db-secret --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;username&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;devuser --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;password&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;S!B\*d$zDsb=&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You do not need to escape special characters in passwords from files (&lt;code&gt;--from-file&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;


Now make the Pods:

```shell
cat &lt;&lt;EOF &gt; pod.yaml
apiVersion: v1
kind: List
items:
- kind: Pod
  apiVersion: v1
  metadata:
    name: prod-db-client-pod
    labels:
      name: prod-db-client
  spec:
    volumes:
    - name: secret-volume
      secret:
        secretName: prod-db-secret
    containers:
    - name: db-client-container
      image: myClientImage
      volumeMounts:
      - name: secret-volume
        readOnly: true
        mountPath: &#34;/etc/secret-volume&#34;
- kind: Pod
  apiVersion: v1
  metadata:
    name: test-db-client-pod
    labels:
      name: test-db-client
  spec:
    volumes:
    - name: secret-volume
      secret:
        secretName: test-db-secret
    containers:
    - name: db-client-container
      image: myClientImage
      volumeMounts:
      - name: secret-volume
        readOnly: true
        mountPath: &#34;/etc/secret-volume&#34;
EOF
```

Add the pods to the same kustomization.yaml:

```shell
cat &lt;&lt;EOF &gt;&gt; kustomization.yaml
resources:
- pod.yaml
EOF
```

Apply all those objects on the API server by running:

```shell
kubectl apply -k .
```

Both containers will have the following files present on their filesystems with the values for each container&#39;s environment:

```
/etc/secret-volume/username
/etc/secret-volume/password
```

Note how the specs for the two Pods differ only in one field; this facilitates
creating Pods with different capabilities from a common Pod template.

You could further simplify the base Pod specification by using two service accounts:

1. `prod-user` with the `prod-db-secret`
1. `test-user` with the `test-db-secret`

The Pod specification is shortened to:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: prod-db-client-pod
  labels:
    name: prod-db-client
spec:
  serviceAccount: prod-db-client
  containers:
  - name: db-client-container
    image: myClientImage
```
 --&gt;
&lt;h3 id=&#34;应用场景-使用生产测试凭据的-pod&#34;&gt;应用场景: 使用生产/测试凭据的 Pod&lt;/h3&gt;
&lt;p&gt;本次示例演示的是一个使用包含生产环境凭据的 Secret 的 Pod 和另一个使用包含测试环境凭据的 Pod&lt;/p&gt;
&lt;p&gt;用户可以通过对一个包含 &lt;code&gt;secretGenerator&lt;/code&gt; 字段的 &lt;code&gt;kustomization.yaml&lt;/code&gt; 执行
&lt;code&gt;kubectl create secret&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret generic prod-db-secret --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;username&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;produser --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;password&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Y4nys7f11
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;secret &amp;quot;prod-db-secret&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;同样的方式为测试环境创建 Secret&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret generic test-db-secret --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;username&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;testuser --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;password&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;iluvtests
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;secret &amp;quot;test-db-secret&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;特殊字符，如 &lt;code&gt;$&lt;/code&gt;, &lt;code&gt;\&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;=&lt;/code&gt;, 和 &lt;code&gt;!&lt;/code&gt;，会被
&lt;a href=&#34;https://en.wikipedia.org/wiki/Shell_(computing)&#34;&gt;shell&lt;/a&gt;
解释，所以需要转义.  在大多数 SHELL 中， 对密码进行转义最简单的方式就是用单引号(&lt;code&gt;&#39;&lt;/code&gt;)包起来。
例如，如果实际的密码是 &lt;code&gt;S!B\*d$zDsb=&lt;/code&gt;, 可以执行下面的命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret generic dev-db-secret --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;username&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;devuser --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;password&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;S!B\*d$zDsb=&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;不需要对文件中的特殊字符进行转义 (&lt;code&gt;--from-file&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;添加 Pod 配置:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt;EOF &amp;gt; pod.yaml
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;apiVersion: v1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kind: List
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;items:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;- kind: Pod
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  apiVersion: v1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  metadata:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    name: prod-db-client-pod
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    labels:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      name: prod-db-client
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  spec:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    volumes:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    - name: secret-volume
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      secret:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        secretName: prod-db-secret
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    containers:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    - name: db-client-container
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      image: myClientImage
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      volumeMounts:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      - name: secret-volume
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        readOnly: true
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mountPath: &amp;#34;/etc/secret-volume&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;- kind: Pod
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  apiVersion: v1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  metadata:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    name: test-db-client-pod
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    labels:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      name: test-db-client
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  spec:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    volumes:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    - name: secret-volume
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      secret:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        secretName: test-db-secret
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    containers:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    - name: db-client-container
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      image: myClientImage
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      volumeMounts:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      - name: secret-volume
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        readOnly: true
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        mountPath: &amp;#34;/etc/secret-volume&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;将这些 Pod 添加到同一个 kustomization.yaml:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; kustomization.yaml
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;resources:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;- pod.yaml
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;通过下面的命令将所有的对象提交到 API 服务:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -k .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;两个容器中都会在它们的文件存在以下文件，其中包含的是各自环境对应的值:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/etc/secret-volume/username
/etc/secret-volume/password
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;两个 Pod 定义中的区别只有一个字段，这使得通过同一个 Pod 模板创建不同功能的 Pod 变得更容易。&lt;/p&gt;
&lt;p&gt;还可以通过以下两个 ServiceAccount 进一步简化基础 Pod 定义:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;包含 &lt;code&gt;prod-db-secret&lt;/code&gt; 的 &lt;code&gt;prod-user&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;包含 &lt;code&gt;test-db-secret&lt;/code&gt; 的 &lt;code&gt;test-user&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Pod 定义就简化为:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prod-db-client-pod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prod-db-client&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;serviceAccount&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;prod-db-client&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;db-client-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myClientImage&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Use-case: dotfiles in a secret volume

You can make your data &#34;hidden&#34; by defining a key that begins with a dot.
This key represents a dotfile or &#34;hidden&#34; file. For example, when the following secret
is mounted into a volume, `secret-volume`:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: dotfile-secret
data:
  .secret-file: dmFsdWUtMg0KDQo=
---
apiVersion: v1
kind: Pod
metadata:
  name: secret-dotfiles-pod
spec:
  volumes:
  - name: secret-volume
    secret:
      secretName: dotfile-secret
  containers:
  - name: dotfile-test-container
    image: k8s.gcr.io/busybox
    command:
    - ls
    - &#34;-l&#34;
    - &#34;/etc/secret-volume&#34;
    volumeMounts:
    - name: secret-volume
      readOnly: true
      mountPath: &#34;/etc/secret-volume&#34;
```

The volume will contain a single file, called `.secret-file`, and
the `dotfile-test-container` will have this file present at the path
`/etc/secret-volume/.secret-file`.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Files beginning with dot characters are hidden from the output of  &lt;code&gt;ls -l&lt;/code&gt;;
you must use &lt;code&gt;ls -la&lt;/code&gt; to see them when listing directory contents.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;use-case-dotfiles-in-a-secret-volume&#34;&gt;应用场景: Secret 卷中的点文件(隐藏文件)&lt;/h3&gt;
&lt;p&gt;用户可以在 Secret 的 data 下面的键设置为以点开头的格式，这样投射出的文件名就是以点开头，也就是隐藏文件
例如， 下面的 Secret 在挂载到 &lt;code&gt;secret-volume&lt;/code&gt; 卷:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;dotfile-secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;.secret-file&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;dmFsdWUtMg0KDQo=&lt;/span&gt;
---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-dotfiles-pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;dotfile-secret&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;dotfile-test-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/busybox&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ls&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-l&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/etc/secret-volume&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/etc/secret-volume&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;最终 卷中就会包含一个文件，文件名为 &lt;code&gt;.secret-file&lt;/code&gt;， 而在 &lt;code&gt;dotfile-test-container&lt;/code&gt; 中
这个文件的路径就是 &lt;code&gt;/etc/secret-volume/.secret-file&lt;/code&gt;.&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;code&gt;ls -l&lt;/code&gt; 命令的输出结果中是没有以点开头的文件的；要查看这些文件需要使用命令 &lt;code&gt;ls -la&lt;/code&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Use-case: Secret visible to one container in a Pod

Consider a program that needs to handle HTTP requests, do some complex business
logic, and then sign some messages with an HMAC. Because it has complex
application logic, there might be an unnoticed remote file reading exploit in
the server, which could expose the private key to an attacker.

This could be divided into two processes in two containers: a frontend container
which handles user interaction and business logic, but which cannot see the
private key; and a signer container that can see the private key, and responds
to simple signing requests from the frontend (for example, over localhost networking).

With this partitioned approach, an attacker now has to trick the application
server into doing something rather arbitrary, which may be harder than getting
it to read a file.
 --&gt;
&lt;!-- TODO: explain how to do this while still using automation. --&gt;
&lt;h3 id=&#34;use-case-secret-visible-to-one-container-in-a-pod&#34;&gt;应用场景: 让 Secret 只在 Pod 中的一个容器中可见&lt;/h3&gt;
&lt;p&gt;假设有一个程序，需要处理 HTTP 请求，完成一些复杂的业务逻辑，最后使用 HMAC 对一些消息做签名。
因为其中有些复杂的业务逻辑，其中可以包含一个没有发现的远程文件读取， 这可能导致将私钥暴露给攻击者。&lt;/p&gt;
&lt;p&gt;这时就可以把这个程序拆分成两个进程运行在两个容器中: 一个前端容器，用来处理用户交互和业务逻辑，
但是它不能看到私钥； 另一个签发容器，其中可以看到私钥，只提供为前端容器一个简单签发请求功能
(例如， 通过本地(localhost)网络).&lt;/p&gt;
&lt;p&gt;通过这种拆分方式， 攻击都现在只能在前端容器中，而不是整个应用，这样可能增加读取文件的难度。&lt;/p&gt;
&lt;!-- TODO: explain how to do this while still using automation. --&gt;
&lt;!--
## Best practices

### Clients that use the Secret API

When deploying applications that interact with the Secret API, you should
limit access using [authorization policies](
/docs/reference/access-authn-authz/authorization/) such as [RBAC](
/docs/reference/access-authn-authz/rbac/).

Secrets often hold values that span a spectrum of importance, many of which can
cause escalations within Kubernetes (e.g. service account tokens) and to
external systems. Even if an individual app can reason about the power of the
secrets it expects to interact with, other apps within the same namespace can
render those assumptions invalid.

For these reasons `watch` and `list` requests for secrets within a namespace are
extremely powerful capabilities and should be avoided, since listing secrets allows
the clients to inspect the values of all secrets that are in that namespace. The ability to
`watch` and `list` all secrets in a cluster should be reserved for only the most
privileged, system-level components.

Applications that need to access the Secret API should perform `get` requests on
the secrets they need. This lets administrators restrict access to all secrets
while [white-listing access to individual instances](
/docs/reference/access-authn-authz/rbac/#referring-to-resources) that
the app needs.

For improved performance over a looping `get`, clients can design resources that
reference a secret then `watch` the resource, re-requesting the secret when the
reference changes. Additionally, a [&#34;bulk watch&#34; API](
https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/bulk_watch.md)
to let clients `watch` individual resources has also been proposed, and will likely
be available in future releases of Kubernetes.
 --&gt;
&lt;h2 id=&#34;best-practices&#34;&gt;最佳实践&lt;/h2&gt;
&lt;h3 id=&#34;clients-that-use-the-secret-api&#34;&gt;使用 Secret API 的那些客户端&lt;/h3&gt;
&lt;p&gt;在部署与 Secret API 交互的应用时，用户需要使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/access-authn-authz/authorization/&#34;&gt;授权策略&lt;/a&gt;
如
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/access-authn-authz/rbac/&#34;&gt;RBAC&lt;/a&gt;
来限制访问&lt;/p&gt;
&lt;p&gt;Secret 经常包含的值都是很重要的，它们从集群内(如 服务账号令牌) 和外部系统。 即便有天大的理由
让一个应该可以访问这些 Secret, 同一个命名空间的其它应用也会推翻这些理由。
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;因为这些原因在一个命名空间中对 Secret 的 &lt;code&gt;watch&lt;/code&gt; 和 &lt;code&gt;list&lt;/code&gt; 的请求就是相当强大的能力，并应该被避免，
因为列举 Secret 可以让客户端可以查看命名空间中的所有 Secret 的值。 对所有 Secret 使用
&lt;code&gt;watch&lt;/code&gt; 和 &lt;code&gt;list&lt;/code&gt; 的能力在集群中应该只提供给最高权限，系统级别的组件。&lt;/p&gt;
&lt;p&gt;需要访问 Secret API 的应用对其需要的 Secret 执行 &lt;code&gt;get&lt;/code&gt; 请求。 这让管理员通过
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/access-authn-authz/rbac/#referring-to-resources&#34;&gt;访问独立实例的白名单&lt;/a&gt;
限制所有的 Secret 只被需要的应用访问。&lt;/p&gt;
&lt;p&gt;为了改善使用 &lt;code&gt;get&lt;/code&gt; 轮询的性能， 客户端可以设计资源，这些资源可以引用一个 Secret 然后 &lt;code&gt;watch&lt;/code&gt;
这个资源， 在引用变量时，重新请求 Secret. 另外，
&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/bulk_watch.md&#34;&gt;&amp;ldquo;bulk watch&amp;rdquo; API&lt;/a&gt;
让客户端 &lt;code&gt;watch&lt;/code&gt; 独立资源的提议已经有了， 可以在未来版本中的 k8s 中就有了。&lt;/p&gt;
&lt;!--
## Security properties

### Protections

Because secrets can be created independently of the Pods that use
them, there is less risk of the secret being exposed during the workflow of
creating, viewing, and editing Pods. The system can also take additional
precautions with Secrets, such as avoiding writing them to disk where
possible.

A secret is only sent to a node if a Pod on that node requires it.
The kubelet stores the secret into a `tmpfs` so that the secret is not written
to disk storage. Once the Pod that depends on the secret is deleted, the kubelet
will delete its local copy of the secret data as well.

There may be secrets for several Pods on the same node. However, only the
secrets that a Pod requests are potentially visible within its containers.
Therefore, one Pod does not have access to the secrets of another Pod.

There may be several containers in a Pod. However, each container in a Pod has
to request the secret volume in its `volumeMounts` for it to be visible within
the container. This can be used to construct useful [security partitions at the
Pod level](#use-case-secret-visible-to-one-container-in-a-pod).

On most Kubernetes distributions, communication between users
and the API server, and from the API server to the kubelets, is protected by SSL/TLS.
Secrets are protected when transmitted over these channels.






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.13 [beta]&lt;/code&gt;
&lt;/div&gt;



You can enable [encryption at rest](/docs/tasks/administer-cluster/encrypt-data/)
for secret data, so that the secrets are not stored in the clear into &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/administer-cluster/configure-upgrade-etcd/&#39; target=&#39;_blank&#39;&gt;etcd&lt;span class=&#39;tooltip-text&#39;&gt;用来存储 k8s 所有集群数据的一致性和高可用键值存储&lt;/span&gt;
&lt;/a&gt;.
 --&gt;
&lt;h2 id=&#34;security-properties&#34;&gt;安全属性&lt;/h2&gt;
&lt;h3 id=&#34;protections&#34;&gt;保护&lt;/h3&gt;
&lt;p&gt;因为 Secret 可以独立于使用它们的 Pod 外创建，这就使得 Secret 在创建，查看，和编辑 Pod 时，
暴露的风险减少。 系统还可以对 Secret 添加额外的保护， 比如，尽量避免将它们写入磁盘。&lt;/p&gt;
&lt;p&gt;Secret 只会在节点上有 Pod 需要它时才会发到这个节点。 kubelet 会将 Secret 存储在 &lt;code&gt;tmpfs&lt;/code&gt;，
这样 Secret 就不会被写入到磁盘存储。 当信赖这个 Secret 的 Pod 被删除时， kubelet 就会删除
本地的 Secret 数据备份。&lt;/p&gt;
&lt;p&gt;当一个节点上有多个 Secret 被多个 Pod 使用时。 只有被 Pod 请求的 Secret 才会在它的容器中可见。
因此， 一个 Pod 没有访问另一个 Pod 中的 Secret 的权限。&lt;/p&gt;
&lt;p&gt;Pod 中可能有多个容器，每个容器都可以在其 &lt;code&gt;volumeMounts&lt;/code&gt; 请求 Secret 卷，这样它就能在容器中。
这些可以用来构建有用的
&lt;a href=&#34;#use-case-secret-visible-to-one-container-in-a-pod&#34;&gt;Pod 级别的安全分隔&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;在大多数 k8s 发行版本中， 用户与 API 服务之间的通信， 和从 API 服务到 kubelet 通信, 是由 SSL/TLS
保护。 Secret 通过这些通道的传输是受保护的。&lt;/p&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.13 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;用户也可以启用对 Secret 数据
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/administer-cluster/encrypt-data/&#34;&gt;加密&lt;/a&gt;，
，这样 Secret 不会以明文的形式存入
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/administer-cluster/configure-upgrade-etcd/&#39; target=&#39;_blank&#39;&gt;etcd&lt;span class=&#39;tooltip-text&#39;&gt;用来存储 k8s 所有集群数据的一致性和高可用键值存储&lt;/span&gt;
&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### Risks

 - In the API server, secret data is stored in &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/administer-cluster/configure-upgrade-etcd/&#39; target=&#39;_blank&#39;&gt;etcd&lt;span class=&#39;tooltip-text&#39;&gt;用来存储 k8s 所有集群数据的一致性和高可用键值存储&lt;/span&gt;
&lt;/a&gt;;
   therefore:
   - Administrators should enable encryption at rest for cluster data (requires v1.13 or later).
   - Administrators should limit access to etcd to admin users.
   - Administrators may want to wipe/shred disks used by etcd when no longer in use.
   - If running etcd in a cluster, administrators should make sure to use SSL/TLS
     for etcd peer-to-peer communication.
 - If you configure the secret through a manifest (JSON or YAML) file which has
   the secret data encoded as base64, sharing this file or checking it in to a
   source repository means the secret is compromised. Base64 encoding is _not_ an
   encryption method and is considered the same as plain text.
 - Applications still need to protect the value of secret after reading it from the volume,
   such as not accidentally logging it or transmitting it to an untrusted party.
 - A user who can create a Pod that uses a secret can also see the value of that secret. Even
   if the API server policy does not allow that user to read the Secret, the user could
   run a Pod which exposes the secret.
 - Currently, anyone with root permission on any node can read _any_ secret from the API server,
   by impersonating the kubelet. It is a planned feature to only send secrets to
   nodes that actually require them, to restrict the impact of a root exploit on a
   single node.
 --&gt;
&lt;h3 id=&#34;risks&#34;&gt;风险&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在 API 服务中，Secret 的数据被存储在
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/administer-cluster/configure-upgrade-etcd/&#39; target=&#39;_blank&#39;&gt;etcd&lt;span class=&#39;tooltip-text&#39;&gt;用来存储 k8s 所有集群数据的一致性和高可用键值存储&lt;/span&gt;
&lt;/a&gt;;
因此:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;管理员应该启动对集群数据的静态加密(需要 k8s v1.13+)&lt;/li&gt;
&lt;li&gt;管理员应该限制只有管理员能访问 etcd&lt;/li&gt;
&lt;li&gt;管理员可能希望对 etcd 不再使用的存储磁盘，进行清除/粉碎&lt;/li&gt;
&lt;li&gt;如果在集群中运行 etcd, 管理员应该保证在 etcd 的点对点通信之间使用 SSL/TLS&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果用户通过配置(JSON 或 YAML)文件来管理 Secret, Secret 中数据是进行 base64， 分享这些
文件或将其提供到源代码系统就意味着 Secret 已经泄漏。 Base64 编码并 &lt;em&gt;不&lt;/em&gt; 是一种加密方式，
其实和明文没啥区别。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;应用在从卷中读取到 Secret 的值后还是要注意保护，例如不要意外地写入日志或传输到一个不受信的部分。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果一个用户可以创建一个使用 Secret 的 Pod 就代表他们可以看到 Secret 的值。即便 API 服务
的策略不允许这个用户读取 Secret, 用户也可以通过运行一个可以暴露 Secret 的 Pod&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;目前，任何在任意节点上有 root 权限的用户，可以冒充 kubelet 从 API 服务读取 &lt;em&gt;任意&lt;/em&gt; Secret,
有一个计划的特性将只允许将 Secret 发送给那些真的需要它的节点，来避免单个节点上被其它 root
权限用户访问。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/configmap-secret/managing-secret-using-kubectl/&#34;&gt;使用 &lt;code&gt;kubectl&lt;/code&gt; 管理 Secret&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/configmap-secret/managing-secret-using-config-file/&#34;&gt;使用配置文件管理 Secret &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/configmap-secret/managing-secret-using-kustomize/&#34;&gt;使用 kustomize 管理 Secret&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 卷快照</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volume-snapshots/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volume-snapshots/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- saad-ali
- thockin
- msau42
- jingxu97
- xing-yang
- yuxiangqian
title: Volume Snapshots
content_type: concept
weight: 20
---
--&gt;
&lt;!-- overview --&gt;
&lt;!--





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;


In Kubernetes, a _VolumeSnapshot_ represents a snapshot of a volume on a storage system. This document assumes that you are already familiar with Kubernetes [persistent volumes](/docs/concepts/storage/persistent-volumes/).
 --&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;在 k8s 中， &lt;em&gt;VolumeSnapshot&lt;/em&gt; 代表存储系统中一个卷的快照。 在阅读此文之间需要熟悉 k8s
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/&#34;&gt;持久化卷(PV)&lt;/a&gt;.&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Introduction

Similar to how API resources `PersistentVolume` and `PersistentVolumeClaim` are used to provision volumes for users and administrators, `VolumeSnapshotContent` and `VolumeSnapshot` API resources are provided to create volume snapshots for users and administrators.

A `VolumeSnapshotContent` is a snapshot taken from a volume in the cluster that has been provisioned by an administrator. It is a resource in the cluster just like a PersistentVolume is a cluster resource.

A `VolumeSnapshot` is a request for snapshot of a volume by a user. It is similar to a PersistentVolumeClaim.

`VolumeSnapshotClass` allows you to specify different attributes belonging to a `VolumeSnapshot`. These attributes may differ among snapshots taken from the same volume on the storage system and therefore cannot be expressed by using the same `StorageClass` of a `PersistentVolumeClaim`.

Volume snapshots provide Kubernetes users with a standardized way to copy a volume&#39;s contents at a particular point in time without creating an entirely new volume. This functionality enables, for example, database administrators to backup databases before performing edit or delete modifications.

Users need to be aware of the following when using this feature:

* API Objects `VolumeSnapshot`, `VolumeSnapshotContent`, and `VolumeSnapshotClass` are &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/&#39; target=&#39;_blank&#39;&gt;CRDs&lt;span class=&#39;tooltip-text&#39;&gt;Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server.&lt;/span&gt;
&lt;/a&gt;, not part of the core API.
* `VolumeSnapshot` support is only available for CSI drivers.
* As part of the deployment process in the beta version of `VolumeSnapshot`, the Kubernetes team provides a snapshot controller to be deployed into the control plane, and a sidecar helper container called csi-snapshotter to be deployed together with the CSI driver.  The snapshot controller watches `VolumeSnapshot` and `VolumeSnapshotContent` objects and is responsible for the creation and deletion of `VolumeSnapshotContent` object in dynamic provisioning.  The sidecar csi-snapshotter watches `VolumeSnapshotContent` objects and triggers `CreateSnapshot` and `DeleteSnapshot` operations against a CSI endpoint.
* CSI drivers may or may not have implemented the volume snapshot functionality. The CSI drivers that have provided support for volume snapshot will likely use the csi-snapshotter. See [CSI Driver documentation](https://kubernetes-csi.github.io/docs/) for details.
* The CRDs and snapshot controller installations are the responsibility of the Kubernetes distribution.
 --&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;与 &lt;code&gt;PersistentVolume&lt;/code&gt; 和 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; API 资源用来为用户和管理员管理卷相似，
&lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 和 &lt;code&gt;VolumeSnapshot&lt;/code&gt; API 资源是用来为用户和管理员创建卷快照的。&lt;/p&gt;
&lt;p&gt;一个 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 是一个由管理员管理的从集群中一个卷创建的快照。
它是一个与  &lt;code&gt;PersistentVolume&lt;/code&gt; 类似的集群资源。&lt;/p&gt;
&lt;p&gt;一个 &lt;code&gt;VolumeSnapshot&lt;/code&gt; 是由用户申请的对一个卷的快照。 与 PVC 类似。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;VolumeSnapshotClass&lt;/code&gt; 可以指定属于 &lt;code&gt;VolumeSnapshot&lt;/code&gt; 的许多属性。 这些属性可能与其它来自
同一个卷打的快照不一样， 因此不能由使用同样 &lt;code&gt;StorageClass&lt;/code&gt; 的 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 表现。&lt;/p&gt;
&lt;p&gt;卷快照为 k8s 用户提供了一种拷贝特定时间点的卷内容，而不用创建新的卷的标准方式。 这个功能可以
用于数据库管理员在做变更或删除操作之前备份数据库。&lt;/p&gt;
&lt;p&gt;用户在使用该特性时要注意以下几点:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;VolumeSnapshot&lt;/code&gt;, &lt;code&gt;VolumeSnapshotContent&lt;/code&gt;, &lt;code&gt;VolumeSnapshotClass&lt;/code&gt; API 对象是
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/&#39; target=&#39;_blank&#39;&gt;CRDs&lt;span class=&#39;tooltip-text&#39;&gt;Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server.&lt;/span&gt;
&lt;/a&gt;，
不是核心 API 的对象。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VolumeSnapshot&lt;/code&gt; 只支持 CSI 驱动。&lt;/li&gt;
&lt;li&gt;作为 &lt;code&gt;VolumeSnapshot&lt;/code&gt; beta 版本的部署里程， k8s 团队提供了一个部署到控制中心的快照控制器，
和一个叫做 &lt;code&gt;csi-snapshotter&lt;/code&gt; 的边车工具容器和 CSI 驱动部署在一起。 快照控制器监测
&lt;code&gt;VolumeSnapshot&lt;/code&gt; 和 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 对象，并负责动态管理的 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt;
对象的创建和删除。 csi-snapshotter 边车，则监测 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 对象并触发
到 CSI 接口的 &lt;code&gt;CreateSnapshot&lt;/code&gt; 和 &lt;code&gt;DeleteSnapshot&lt;/code&gt; 操作。&lt;/li&gt;
&lt;li&gt;CSI 驱动有可能实现也有可能没有实现快照功能。 提供了卷快照支持的 CSI 驱动通常会使用 &lt;code&gt;csi-snapshotter&lt;/code&gt;。
详细信息见 &lt;a href=&#34;https://kubernetes-csi.github.io/docs/&#34;&gt;CSI 驱动文档&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CRD 和 快照控制器的安装由 k8s 发行版本负责&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Lifecycle of a volume snapshot and volume snapshot content

`VolumeSnapshotContents` are resources in the cluster. `VolumeSnapshots` are requests for those resources. The interaction between `VolumeSnapshotContents` and `VolumeSnapshots` follow this lifecycle:
 --&gt;
&lt;h2 id=&#34;volumesnapshot-和-volumesnapshotcontent-的生命周期&#34;&gt;&lt;code&gt;VolumeSnapshot&lt;/code&gt; 和 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 的生命周期&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;VolumeSnapshotContents&lt;/code&gt; 是集群中的资源。 &lt;code&gt;VolumeSnapshots&lt;/code&gt; 是对这些资源的申请。
&lt;code&gt;VolumeSnapshotContents&lt;/code&gt; 与 &lt;code&gt;VolumeSnapshots&lt;/code&gt; 交互遵循以下周期:&lt;/p&gt;
&lt;!--
### Provisioning Volume Snapshot

There are two ways snapshots may be provisioned: pre-provisioned or dynamically provisioned.
 --&gt;
&lt;h3 id=&#34;卷快照管理&#34;&gt;卷快照管理&lt;/h3&gt;
&lt;p&gt;卷快照管理有两种方式: 静态管理 或 动态管理&lt;/p&gt;
&lt;!--
#### Pre-provisioned {#static}
A cluster administrator creates a number of `VolumeSnapshotContents`. They carry the details of the real volume snapshot on the storage system which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.
 --&gt;
&lt;h4 id=&#34;static&#34;&gt;静态管理&lt;/h4&gt;
&lt;p&gt;由集群管理员创建一定数据的 &lt;code&gt;VolumeSnapshotContents&lt;/code&gt;。 其中包含存储系统中可以被集群用户使用的
真实卷快照的详细信息。 它们存在于 k8s API 并可被消费。&lt;/p&gt;
&lt;!--
#### Dynamic
Instead of using a pre-existing snapshot, you can request that a snapshot to be dynamically taken from a PersistentVolumeClaim. The [VolumeSnapshotClass](/docs/concepts/storage/volume-snapshot-classes/) specifies storage provider-specific parameters to use when taking a snapshot.
 --&gt;
&lt;h4 id=&#34;动态管理&#34;&gt;动态管理&lt;/h4&gt;
&lt;p&gt;与使用静态快照管理不同，可以申请从 PVC 动态创建快照。
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volume-snapshot-classes/&#34;&gt;VolumeSnapshotClass&lt;/a&gt; 可以在
做快照时指定存储提供者相关的参数&lt;/p&gt;
&lt;!--
### Binding

The snapshot controller handles the binding of a `VolumeSnapshot` object with an appropriate `VolumeSnapshotContent` object, in both pre-provisioned and dynamically provisioned scenarios. The binding is a one-to-one mapping.

In the case of pre-provisioned binding, the VolumeSnapshot will remain unbound until the requested VolumeSnapshotContent object is created.
 --&gt;
&lt;h3 id=&#34;绑定&#34;&gt;绑定&lt;/h3&gt;
&lt;p&gt;在静态管理和动态管理的情况下，快照控制器会处理 &lt;code&gt;VolumeSnapshot&lt;/code&gt; 对象与恰当的 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt;
的绑定。 这种绑定关系是一对一的。&lt;/p&gt;
&lt;p&gt;在静态管理绑定时， &lt;code&gt;VolumeSnapshot&lt;/code&gt; 在 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 创建之前都是未绑定状态&lt;/p&gt;
&lt;!--
### Persistent Volume Claim as Snapshot Source Protection

The purpose of this protection is to ensure that in-use
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/persistent-volumes/&#39; target=&#39;_blank&#39;&gt;PersistentVolumeClaim&lt;span class=&#39;tooltip-text&#39;&gt;索要定义在 PersistentVolume 中的存储资源，然后就可以将其以卷的方式挂载到容器中。&lt;/span&gt;
&lt;/a&gt;
API objects are not removed from the system while a snapshot is being taken from it (as this may result in data loss).

While a snapshot is being taken of a PersistentVolumeClaim, that PersistentVolumeClaim is in-use. If you delete a PersistentVolumeClaim API object in active use as a snapshot source, the PersistentVolumeClaim object is not removed immediately. Instead, removal of the PersistentVolumeClaim object is postponed until the snapshot is readyToUse or aborted.
 --&gt;
&lt;h3 id=&#34;当-pvc-作为快照源的保护机制&#34;&gt;当 PVC 作为快照源的保护机制&lt;/h3&gt;
&lt;p&gt;这个保护机制的目的是确保使用中的
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/persistent-volumes/&#39; target=&#39;_blank&#39;&gt;PersistentVolumeClaim&lt;span class=&#39;tooltip-text&#39;&gt;索要定义在 PersistentVolume 中的存储资源，然后就可以将其以卷的方式挂载到容器中。&lt;/span&gt;
&lt;/a&gt;
API 对象在做快照的过程中不会被从系统中删除(因为这可能会导致数据丢失)。&lt;/p&gt;
&lt;p&gt;当一个 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 在用于做快照，则它就在使用中状态。 如果删除了一个正在作为快照
源的 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 对象，它不会马上被删除。 这个删除行为会被延迟到快照的状态
变为 &lt;code&gt;readyToUse&lt;/code&gt; 或 中止(&lt;code&gt;aborted&lt;/code&gt;) 之后。&lt;/p&gt;
&lt;!--
### Delete

Deletion is triggered by deleting the `VolumeSnapshot` object, and the `DeletionPolicy` will be followed. If the `DeletionPolicy` is `Delete`, then the underlying storage snapshot will be deleted along with the `VolumeSnapshotContent` object. If the `DeletionPolicy` is `Retain`, then both the underlying snapshot and `VolumeSnapshotContent` remain.
 --&gt;
&lt;h3 id=&#34;删除&#34;&gt;删除&lt;/h3&gt;
&lt;p&gt;删除由删除 &lt;code&gt;VolumeSnapshot&lt;/code&gt; 对象触发， 会依照 &lt;code&gt;DeletionPolicy&lt;/code&gt; 进行。 如果 &lt;code&gt;DeletionPolicy&lt;/code&gt;
是删除(&lt;code&gt;Delete&lt;/code&gt;)， 则底层的存储快照会连同 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 对象一起删除。 如果
&lt;code&gt;DeletionPolicy&lt;/code&gt; 是保留(&lt;code&gt;Retain&lt;/code&gt;), 则 底层的存储快照 和 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 对象都保留&lt;/p&gt;
&lt;!--
## VolumeSnapshots

Each VolumeSnapshot contains a spec and a status.

```yaml
apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshot
metadata:
  name: new-snapshot-test
spec:
  volumeSnapshotClassName: csi-hostpath-snapclass
  source:
    persistentVolumeClaimName: pvc-test
```

`persistentVolumeClaimName` is the name of the PersistentVolumeClaim data source for the snapshot. This field is required for dynamically provisioning a snapshot.

A volume snapshot can request a particular class by specifying the name of a
[VolumeSnapshotClass](/docs/concepts/storage/volume-snapshot-classes/)
using the attribute `volumeSnapshotClassName`. If nothing is set, then the default class is used if available.

For pre-provisioned snapshots, you need to specify a `volumeSnapshotContentName` as the source for the snapshot as shown in the following example. The `volumeSnapshotContentName` source field is required for pre-provisioned snapshots.

```yaml
apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshot
metadata:
  name: test-snapshot
spec:
  source:
    volumeSnapshotContentName: test-content
```
 --&gt;
&lt;h2 id=&#34;volumesnapshot&#34;&gt;&lt;code&gt;VolumeSnapshot&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;每个 &lt;code&gt;VolumeSnapshot&lt;/code&gt; 包含一个 &lt;code&gt;spec&lt;/code&gt; 和一个 &lt;code&gt;status&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshot&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;new-snapshot-test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeSnapshotClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;csi-hostpath-snapclass&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;source&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pvc-test&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;persistentVolumeClaimName&lt;/code&gt; is the name of the PersistentVolumeClaim data source for the snapshot. This field is required for dynamically provisioning a snapshot.
&lt;code&gt;persistentVolumeClaimName&lt;/code&gt; 是作为快照数据源的 PVC 的名称。 这个字段在动态管理快照时是必要的。&lt;/p&gt;
&lt;p&gt;A volume snapshot can request a particular class by specifying the name of a
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/docs/concepts/storage/volume-snapshot-classes/&#34;&gt;VolumeSnapshotClass&lt;/a&gt;
using the attribute &lt;code&gt;volumeSnapshotClassName&lt;/code&gt;. If nothing is set, then the default class is used if available.
卷快照时可以通过 &lt;code&gt;volumeSnapshotClassName&lt;/code&gt; 设置一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volume-snapshot-classes/&#34;&gt;VolumeSnapshotClass&lt;/a&gt;
名称来指定特定的类别。
如果没有设置，在有默认的类别时就使用默认的&lt;/p&gt;
&lt;p&gt;对于静态管理的快照，需要向下面例子中所示的指定一个 &lt;code&gt;volumeSnapshotContentName&lt;/code&gt; 作为快照源。
在静态管理快照时 &lt;code&gt;volumeSnapshotContentName&lt;/code&gt; 源字段是必要的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshot&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-snapshot&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;source&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeSnapshotContentName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-content&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Volume Snapshot Contents

Each VolumeSnapshotContent contains a spec and status. In dynamic provisioning, the snapshot common controller creates `VolumeSnapshotContent` objects. Here is an example:

```yaml
apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshotContent
metadata:
  name: snapcontent-72d9a349-aacd-42d2-a240-d775650d2455
spec:
  deletionPolicy: Delete
  driver: hostpath.csi.k8s.io
  source:
    volumeHandle: ee0cfb94-f8d4-11e9-b2d8-0242ac110002
  volumeSnapshotClassName: csi-hostpath-snapclass
  volumeSnapshotRef:
    name: new-snapshot-test
    namespace: default
    uid: 72d9a349-aacd-42d2-a240-d775650d2455
```

`volumeHandle` is the unique identifier of the volume created on the storage backend and returned by the CSI driver during the volume creation. This field is required for dynamically provisioning a snapshot. It specifies the volume source of the snapshot.

For pre-provisioned snapshots, you (as cluster administrator) are responsible for creating the `VolumeSnapshotContent` object as follows.

```yaml
apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshotContent
metadata:
  name: new-snapshot-content-test
spec:
  deletionPolicy: Delete
  driver: hostpath.csi.k8s.io
  source:
    snapshotHandle: 7bdd0de3-aaeb-11e8-9aae-0242ac110002
  volumeSnapshotRef:
    name: new-snapshot-test
    namespace: default
```

`snapshotHandle` is the unique identifier of the volume snapshot created on the storage backend. This field is required for the pre-provisioned snapshots. It specifies the CSI snapshot id on the storage system that this `VolumeSnapshotContent` represents.
 --&gt;
&lt;h2 id=&#34;volumesnapshotcontent&#34;&gt;&lt;code&gt;VolumeSnapshotContent&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;每个 VolumeSnapshotContent 都包含一个 &lt;code&gt;spec&lt;/code&gt; 和一个 &lt;code&gt;status&lt;/code&gt;， 在动态管理时，
快照控制器创建 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 对象，下面是一个示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshotContent&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapcontent-72d9a349-aacd-42d2-a240-d775650d2455&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;deletionPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Delete&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;driver&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hostpath.csi.k8s.io&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;source&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeHandle&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ee0cfb94-f8d4-11e9-b2d8-0242ac110002&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeSnapshotClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;csi-hostpath-snapclass&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeSnapshotRef&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;new-snapshot-test&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;uid&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;72d9a349-aacd-42d2-a240-d775650d2455&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;volumeHandle&lt;/code&gt; 是卷在存储后台上的唯一标识，由 CSI 创建卷时返回。 这个字段在动态管理快照时是必要的。
它指定了快照的源(卷)&lt;/p&gt;
&lt;p&gt;对于静态管理的乜是， 集群管理员负责创建如下 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 对象。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshotContent&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;new-snapshot-content-test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;deletionPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Delete&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;driver&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hostpath.csi.k8s.io&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;source&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;snapshotHandle&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;7bdd0de3-aaeb-11e8-9aae-0242ac110002&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeSnapshotRef&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;new-snapshot-test&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;volumeHandle&lt;/code&gt; 是卷在存储后台上的唯一标识， 这个字段在静态管理快照时是必要的。 它指定这个
&lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 在存储系统所代表的 CSI 快照的ID&lt;/p&gt;
&lt;!--
## Provisioning Volumes from Snapshots

You can provision a new volume, pre-populated with data from a snapshot, by using
the *dataSource* field in the `PersistentVolumeClaim` object.

For more details, see
[Volume Snapshot and Restore Volume from Snapshot](/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support).
 --&gt;
&lt;h2 id=&#34;基于快照创建卷&#34;&gt;基于快照创建卷&lt;/h2&gt;
&lt;p&gt;You can provision a new volume, pre-populated with data from a snapshot, by using
the &lt;em&gt;dataSource&lt;/em&gt; field in the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; object.
可以通过 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 对象的 &lt;em&gt;dataSource&lt;/em&gt; 指定一个快照，将快照的数据预先添加
到新创建的卷中。&lt;/p&gt;
&lt;p&gt;更多详细信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support&#34;&gt;卷快照和从快照中恢复卷&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 通过 Service 连接应用</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/connect-applications-service/</link>
      <pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/connect-applications-service/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- caesarxuchao
- lavalamp
- thockin
title: Connecting Applications with Services
content_type: concept
weight: 30
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
## The Kubernetes model for connecting containers

Now that you have a continuously running, replicated application you can expose it on a network. Before discussing the Kubernetes approach to networking, it is worthwhile to contrast it with the &#34;normal&#34; way networking works with Docker.

By default, Docker uses host-private networking, so containers can talk to other containers only if they are on the same machine. In order for Docker containers to communicate across nodes, there must be allocated ports on the machine&#39;s own IP address, which are then forwarded or proxied to the containers. This obviously means that containers must either coordinate which ports they use very carefully or ports must be allocated dynamically.

Coordinating port allocations across multiple developers or teams that provide containers is very difficult to do at scale, and exposes users to cluster-level issues outside of their control. Kubernetes assumes that pods can communicate with other pods, regardless of which host they land on. Kubernetes gives every pod its own cluster-private IP address, so you do not need to explicitly create links between pods or map container ports to host ports. This means that containers within a Pod can all reach each other&#39;s ports on localhost, and all pods in a cluster can see each other without NAT. The rest of this document elaborates on how you can run reliable services on such a networking model.

This guide uses a simple nginx server to demonstrate proof of concept.
 --&gt;
&lt;h2 id=&#34;连接到容器的-k8s-模型&#34;&gt;连接到容器的 k8s 模型&lt;/h2&gt;
&lt;p&gt;到目前为止我们有一个持续运行的多副本应用，我们可以把它暴露到一个网络中。 在讨论 k8s 的网络实现前，
值得花点时间来看看 Docker 中普通的网络是怎么工作的。&lt;/p&gt;
&lt;p&gt;默认情况下， Docker 使用私有网络，因此只有同一个主机上的容器之间可以相互通信。要使不同主机之间
的容器能够通信，必须要通过主机自己的 IP 地址加上分配端口以转发或代理的方式连接到容器。
通过这种方式很明显的一个问题就是不同容器必须要十分小心地协调怎么分配端口，或者必须动态地分配端口。&lt;/p&gt;
&lt;p&gt;大规模在多个开发者或项目组之间为容器协调端口分配是十分困难的， 并且也直接将用户暴露在无法控制的
集群级问题中。 k8s 确保 Pod 无论他们在哪个主机上，其相互之间都可以通信。k8s 会为每个 Pod 分配
集群内的私有 IP 地址，因此不需要用户显示地创建 Pod 之间的连接，也不需要做容器端口与主机端口的映射关系。
这就是说在一个 Pod 中的容器可以通过本地回环(localhost)以端口实现相互之间的通信，并且集群中
所有的 Pod 都可以在没有 NAT 的情况互相通信。 本文接下来的部分将说结阐怎么通过这个网络模型运行
可靠的服务。&lt;/p&gt;
&lt;p&gt;本文使用一个简单的 nginx 服务来演示证明这个观点。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Exposing pods to the cluster

We did this in a previous example, but let&#39;s do it once again and focus on the networking perspective.
Create an nginx Pod, and note that it has a container port specification:



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingrun-my-nginxyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/run-my-nginx.yaml&#34; download=&#34;service/networking/run-my-nginx.yaml&#34;&gt;
                    &lt;code&gt;service/networking/run-my-nginx.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingrun-my-nginxyaml&#39;)&#34; title=&#34;Copy service/networking/run-my-nginx.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



This makes it accessible from any node in your cluster. Check the nodes the Pod is running on:

```shell
kubectl apply -f ./run-my-nginx.yaml
kubectl get pods -l run=my-nginx -o wide
```
```
NAME                        READY     STATUS    RESTARTS   AGE       IP            NODE
my-nginx-3800858182-jr4a2   1/1       Running   0          13s       10.244.3.4    kubernetes-minion-905m
my-nginx-3800858182-kna2y   1/1       Running   0          13s       10.244.2.5    kubernetes-minion-ljyd
```

Check your pods&#39; IPs:

```shell
kubectl get pods -l run=my-nginx -o yaml | grep podIP
    podIP: 10.244.3.4
    podIP: 10.244.2.5
```

You should be able to ssh into any node in your cluster and curl both IPs. Note that the containers are *not* using port 80 on the node, nor are there any special NAT rules to route traffic to the pod. This means you can run multiple nginx pods on the same node all using the same containerPort and access them from any other pod or node in your cluster using IP. Like Docker, ports can still be published to the host node&#39;s interfaces, but the need for this is radically diminished because of the networking model.

You can read more about [how we achieve this](/docs/concepts/cluster-administration/networking/#how-to-achieve-this) if you&#39;re curious.
 --&gt;
&lt;h2 id=&#34;将-pod-暴露到集群中&#34;&gt;将 Pod 暴露到集群中&lt;/h2&gt;
&lt;p&gt;这是我们之间用过的一个盒子，让我们再做一次，但这次专注于网络的角度。 创建一个 nginx 的 Pod，
注意这个定义中有关于端口的定义:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingrun-my-nginxyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/run-my-nginx.yaml&#34; download=&#34;service/networking/run-my-nginx.yaml&#34;&gt;
                    &lt;code&gt;service/networking/run-my-nginx.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingrun-my-nginxyaml&#39;)&#34; title=&#34;Copy service/networking/run-my-nginx.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f ./run-my-nginx.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这使用它可以从集群中的任意节点访问。通过以下命令查看 Pod 运行在哪个节点上&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods -l run&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;my-nginx -o wide
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME                        READY     STATUS    RESTARTS   AGE       IP            NODE
my-nginx-3800858182-jr4a2   1/1       Running   0          13s       10.244.3.4    kubernetes-minion-905m
my-nginx-3800858182-kna2y   1/1       Running   0          13s       10.244.2.5    kubernetes-minion-ljyd
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看 Pod 的 IP 地址:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods -l run&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;my-nginx -o yaml | grep podIP
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;podIP: 10.244.3.4
podIP: 10.244.2.5
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用户可以通过 ssh 连接到集群中的任意节点，并通过 curl 命令访问以上两个 IP。 要注意容器并 &lt;em&gt;没有&lt;/em&gt;
使用节点上的 80 端口，也没有任何特殊的 NAT 规则将流量路由到 Pod。 也就是说用户可以在同一个节点
上使用一样的 &lt;code&gt;containerPort&lt;/code&gt; 运行多个 nginx 的 Pod， 并且可以从集群中任意 Pod 或 节点使用
它的 IP 访问它。 与 Docker 一样， 端口也可以发布到主机的网卡上， 因为网络模型的原因，应该尽量减少这
种方式的使用。&lt;/p&gt;
&lt;p&gt;如果用户比较好奇这个是怎么实现的见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/cluster-administration/networking/#how-to-achieve-this&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Creating a Service

So we have pods running nginx in a flat, cluster wide, address space. In theory, you could talk to these pods directly, but what happens when a node dies? The pods die with it, and the Deployment will create new ones, with different IPs. This is the problem a Service solves.

A Kubernetes Service is an abstraction which defines a logical set of Pods running somewhere in your cluster, that all provide the same functionality. When created, each Service is assigned a unique IP address (also called clusterIP). This address is tied to the lifespan of the Service, and will not change while the Service is alive. Pods can be configured to talk to the Service, and know that communication to the Service will be automatically load-balanced out to some pod that is a member of the Service.

You can create a Service for your 2 nginx replicas with `kubectl expose`:

```shell
kubectl expose deployment/my-nginx
```
```
service/my-nginx exposed
```

This is equivalent to `kubectl apply -f` the following yaml:



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnginx-svcyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/nginx-svc.yaml&#34; download=&#34;service/networking/nginx-svc.yaml&#34;&gt;
                    &lt;code&gt;service/networking/nginx-svc.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnginx-svcyaml&#39;)&#34; title=&#34;Copy service/networking/nginx-svc.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



This specification will create a Service which targets TCP port 80 on any Pod
with the `run: my-nginx` label, and expose it on an abstracted Service port
(`targetPort`: is the port the container accepts traffic on, `port`: is the
abstracted Service port, which can be any port other pods use to access the
Service).
View [Service](/docs/reference/generated/kubernetes-api/v1.19/#service-v1-core)
API object to see the list of supported fields in service definition.
Check your Service:

```shell
kubectl get svc my-nginx
```
```
NAME       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
my-nginx   ClusterIP   10.0.162.149   &lt;none&gt;        80/TCP    21s
```

As mentioned previously, a Service is backed by a group of Pods. These Pods are
exposed through `endpoints`. The Service&#39;s selector will be evaluated continuously
and the results will be POSTed to an Endpoints object also named `my-nginx`.
When a Pod dies, it is automatically removed from the endpoints, and new Pods
matching the Service&#39;s selector will automatically get added to the endpoints.
Check the endpoints, and note that the IPs are the same as the Pods created in
the first step:

```shell
kubectl describe svc my-nginx
```
```
Name:                my-nginx
Namespace:           default
Labels:              run=my-nginx
Annotations:         &lt;none&gt;
Selector:            run=my-nginx
Type:                ClusterIP
IP:                  10.0.162.149
Port:                &lt;unset&gt; 80/TCP
Endpoints:           10.244.2.5:80,10.244.3.4:80
Session Affinity:    None
Events:              &lt;none&gt;
```
```shell
kubectl get ep my-nginx
```
```
NAME       ENDPOINTS                     AGE
my-nginx   10.244.2.5:80,10.244.3.4:80   1m
```

You should now be able to curl the nginx Service on `&lt;CLUSTER-IP&gt;:&lt;PORT&gt;` from
any node in your cluster. Note that the Service IP is completely virtual, it
never hits the wire. If you&#39;re curious about how this works you can read more
about the [service proxy](/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies).
 --&gt;
&lt;h2 id=&#34;创建一个-service&#34;&gt;创建一个 Service&lt;/h2&gt;
&lt;p&gt;这样我们就一个集群范围以平铺方式地址空间运行 nginx 的 Pod. 理论上，用户可以直接与这些 Pod 通信，
但是要是一个节点挂了会发生什么呢? 节点上面的 Pod 也会跟着一起挂掉， 然后 Deployment 会创建
新的 Pod， 但这些 Pod 的 IP 已经不是之前的了。 这就是 Service 要解决的问题。&lt;/p&gt;
&lt;p&gt;一个 k8s 的 Service 就是对定义了一个集群中运行的提供同样功能的 Pod 的逻辑集合的抽象。每个
Service 被创建了以后都会被分配一个唯一的 IP 地址(也被称为集群IP(clusterIP)). 这个地址与该
Service 一生相伴，在 Service 的整个生命期内都不会改变。 Pod 也可以被配置为与 Service 通信，
访问 Service 的请求会被负载均衡到该 Service 所属的一个 Pod 上。&lt;/p&gt;
&lt;p&gt;可以通过如下 &lt;code&gt;kubectl expose&lt;/code&gt; 为上面的两个 nginx 副本创建一个 Service:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl expose deployment/my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;service/my-nginx exposed
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个命令与 &lt;code&gt;kubectl apply -f&lt;/code&gt; 下面这个 yaml 文件等效:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnginx-svcyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/nginx-svc.yaml&#34; download=&#34;service/networking/nginx-svc.yaml&#34;&gt;
                    &lt;code&gt;service/networking/nginx-svc.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnginx-svcyaml&#39;)&#34; title=&#34;Copy service/networking/nginx-svc.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;这个配置文件中定义会创建一个 Service, 这个 Service 的目标为任意一个标签为 &lt;code&gt;run: my-nginx&lt;/code&gt;
的 Pod 的 TCP 80 端口，并将这些端口暴露在一个抽象的 Service 端口(&lt;code&gt;targetPort&lt;/code&gt;: 是容器
接收流量的端口，&lt;code&gt;port&lt;/code&gt;: 是抽象 Service 的端口，可以是任意端口，用于其它的 Pod 访问该 Service)
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#service-v1-core&#34;&gt;Service&lt;/a&gt;
的 API 对象中有所有 Service 定义支持的字段列表。&lt;/p&gt;
&lt;p&gt;查看创建的 Service:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get svc my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
my-nginx   ClusterIP   10.0.162.149   &amp;lt;none&amp;gt;        80/TCP    21s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;就如之前提到的，一个 Service 背后都有一组(个) Pod. 这些 Pod 是通过 &lt;code&gt;Endpoint&lt;/code&gt; 来暴露的。
Service 会持续执行然后将结果通过 POST 请求到一个也叫 &lt;code&gt;my-nginx&lt;/code&gt; 的 Endpoint 对象。当有一个
Pod 挂掉时，该 Pod 的端点会自动从 Endpoint 上移除，当有一个新的 Pod 匹配 Service 的选择器
时也会自动添加到 Endpoint 上。 检查 Endpoint， 可以看到上面的 IP 地址与第一步 Pod 创建时的 IP 地址
是一样的&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe svc my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Name:                my-nginx
Namespace:           default
Labels:              run=my-nginx
Annotations:         &amp;lt;none&amp;gt;
Selector:            run=my-nginx
Type:                ClusterIP
IP:                  10.0.162.149
Port:                &amp;lt;unset&amp;gt; 80/TCP
Endpoints:           10.244.2.5:80,10.244.3.4:80
Session Affinity:    None
Events:              &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get ep my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME       ENDPOINTS                     AGE
my-nginx   10.244.2.5:80,10.244.3.4:80   1m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个时候就能在集群中的任意一个 Pod 中通过 curl 访问 &lt;code&gt;&amp;lt;CLUSTER-IP&amp;gt;:&amp;lt;PORT&amp;gt;&lt;/code&gt; 来访问这个 nginx
的 Service.  要注意 Service 的 IP 完全是虚拟的。 如果用户对此有兴趣可以看看
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies&#34;&gt;service proxy&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Accessing the Service

Kubernetes supports 2 primary modes of finding a Service - environment variables
and DNS. The former works out of the box while the latter requires the
[CoreDNS cluster addon](https://releases.k8s.io/master/cluster/addons/dns/coredns).
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If the service environment variables are not desired (because possible clashing with expected program ones,
too many variables to process, only using DNS, etc) you can disable this mode by setting the &lt;code&gt;enableServiceLinks&lt;/code&gt;
flag to &lt;code&gt;false&lt;/code&gt; on the &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubernetes-api/v1.19/#pod-v1-core&#34;&gt;pod spec&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;访问这个-service&#34;&gt;访问这个 Service&lt;/h2&gt;
&lt;p&gt;k8s 主要有两种发现 Service 的模式 - 环境变量和 DNS。 前一个是直接可以用的(要注意顺序问题)
后一种则需要
&lt;a href=&#34;https://releases.k8s.io/master/cluster/addons/dns/coredns&#34;&gt;CoreDNS 集群插件&lt;/a&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果不需要 Service 的环境变量(因为可能与程序有冲突，需要处理的变量太多，只使用 DNS 等)， 可以
通过在
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#pod-v1-core&#34;&gt;Pod 定义中&lt;/a&gt;
将 &lt;code&gt;enableServiceLinks&lt;/code&gt; 字段的值设置为 &lt;code&gt;false&lt;/code&gt; 来禁用这种模式&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;!--
### Environment Variables

When a Pod runs on a Node, the kubelet adds a set of environment variables for
each active Service. This introduces an ordering problem. To see why, inspect
the environment of your running nginx Pods (your Pod name will be different):

```shell
kubectl exec my-nginx-3800858182-jr4a2 -- printenv | grep SERVICE
```
```
KUBERNETES_SERVICE_HOST=10.0.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
```

Note there&#39;s no mention of your Service. This is because you created the replicas
before the Service. Another disadvantage of doing this is that the scheduler might
put both Pods on the same machine, which will take your entire Service down if
it dies. We can do this the right way by killing the 2 Pods and waiting for the
Deployment to recreate them. This time around the Service exists *before* the
replicas. This will give you scheduler-level Service spreading of your Pods
(provided all your nodes have equal capacity), as well as the right environment
variables:

```shell
kubectl scale deployment my-nginx --replicas=0; kubectl scale deployment my-nginx --replicas=2;

kubectl get pods -l run=my-nginx -o wide
```
```
NAME                        READY     STATUS    RESTARTS   AGE     IP            NODE
my-nginx-3800858182-e9ihh   1/1       Running   0          5s      10.244.2.7    kubernetes-minion-ljyd
my-nginx-3800858182-j4rm4   1/1       Running   0          5s      10.244.3.8    kubernetes-minion-905m
```

You may notice that the pods have different names, since they are killed and recreated.

```shell
kubectl exec my-nginx-3800858182-e9ihh -- printenv | grep SERVICE
```
```
KUBERNETES_SERVICE_PORT=443
MY_NGINX_SERVICE_HOST=10.0.162.149
KUBERNETES_SERVICE_HOST=10.0.0.1
MY_NGINX_SERVICE_PORT=80
KUBERNETES_SERVICE_PORT_HTTPS=443
```
 --&gt;
&lt;h3 id=&#34;环境变量&#34;&gt;环境变量&lt;/h3&gt;
&lt;p&gt;当一个 Pod 在一个节点上运行时， kubelet 为会每个活跃的 Service 添加一系列环境变量。这会引入
一个顺序问题。为啥呢，先看看现在运行的 nginx Pod 的环境变量(Pod 名称根据实际会不同)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl exec my-nginx-3800858182-jr4a2 -- printenv | grep SERVICE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;KUBERNETES_SERVICE_HOST=10.0.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以看到没有 nginx Service 的信息。 这是因为这些 Pod 副本是在 Service 之前创建的。另一个缺点
是调度器可能将两个 Pod 都丢到一个节点点上，如果这个节点挂了，则整个 Service 也就挂了。这时候
先杀掉这两个 Pod 然后等 Deployment 重建。 这样 Service 就在这些 Pod 创建 &lt;em&gt;之前&lt;/em&gt; 就存在了.
这样就会让 Pod 在 Service 的调度级别(所有节点均匀分布)，同时环境变量也有了:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl scale deployment my-nginx --replicas&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0; kubectl scale deployment my-nginx --replicas&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;2;

kubectl get pods -l run&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;my-nginx -o wide
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME                        READY     STATUS    RESTARTS   AGE     IP            NODE
my-nginx-3800858182-e9ihh   1/1       Running   0          5s      10.244.2.7    kubernetes-minion-ljyd
my-nginx-3800858182-j4rm4   1/1       Running   0          5s      10.244.3.8    kubernetes-minion-905m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这时候可以看到 Pod 的名称也变了，因为它们被干掉然后重建了。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl exec my-nginx-3800858182-e9ihh -- printenv | grep SERVICE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;KUBERNETES_SERVICE_PORT=443
MY_NGINX_SERVICE_HOST=10.0.162.149
KUBERNETES_SERVICE_HOST=10.0.0.1
MY_NGINX_SERVICE_PORT=80
KUBERNETES_SERVICE_PORT_HTTPS=443
&lt;/code&gt;&lt;/pre&gt;&lt;!--
### DNS

Kubernetes offers a DNS cluster addon Service that automatically assigns dns names to other Services. You can check if it&#39;s running on your cluster:

```shell
kubectl get services kube-dns --namespace=kube-system
```
```
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   10.0.0.10    &lt;none&gt;        53/UDP,53/TCP   8m
```

The rest of this section will assume you have a Service with a long lived IP
(my-nginx), and a DNS server that has assigned a name to that IP. Here we use the CoreDNS cluster addon (application name `kube-dns`), so you can talk to the Service from any pod in your cluster using standard methods (e.g. `gethostbyname()`). If CoreDNS isn&#39;t running, you can enable it referring to the [CoreDNS README](https://github.com/coredns/deployment/tree/master/kubernetes) or [Installing CoreDNS](/docs/tasks/administer-cluster/coredns/#installing-coredns). Let&#39;s run another curl application to test this:

```shell
kubectl run curl --image=radial/busyboxplus:curl -i --tty
```
```
Waiting for pod default/curl-131556218-9fnch to be running, status is Pending, pod ready: false
Hit enter for command prompt
```

Then, hit enter and run `nslookup my-nginx`:

```shell
[ root@curl-131556218-9fnch:/ ]$ nslookup my-nginx
Server:    10.0.0.10
Address 1: 10.0.0.10

Name:      my-nginx
Address 1: 10.0.162.149
```
 --&gt;
&lt;h3 id=&#34;dns&#34;&gt;DNS&lt;/h3&gt;
&lt;p&gt;k8s 提供了一个 DNS 集群插件的 Service, 它会自动地为其它的 Service 分配 DNS 名称。
可以通过以下命令查看集群中是否运行了该服务:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get services kube-dns --namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   10.0.0.10    &amp;lt;none&amp;gt;        53/UDP,53/TCP   8m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;本节接下来部分假设有一个拥有长期存在 IP 的 Service (my-nginx), 并且 DNS 服务为这个 IP
分配了一个名称。 这里我们使用的是 CoreDNS 集群插件(应用名称为 &lt;code&gt;kube-dns&lt;/code&gt;)， 因此可以在集群中
的任意一个 Pod 通过标准方式(例如，&lt;code&gt;gethostbyname()&lt;/code&gt; ) 与这个 Service 通信。 如果没有 CoreDNS，
可以参考
&lt;a href=&#34;https://github.com/coredns/deployment/tree/master/kubernetes&#34;&gt;CoreDNS README&lt;/a&gt;
或
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/administer-cluster/coredns/#installing-coredns&#34;&gt;安装 CoreDNS&lt;/a&gt;.
这时候再运行 curl 应用来验证:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl run curl --image&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;radial/busyboxplus:curl -i --tty
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Waiting for pod default/curl-131556218-9fnch to be running, status is Pending, pod ready: false
Hit enter for command prompt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;看到以上提供，再次敲回车键，然后运行 &lt;code&gt;nslookup my-nginx&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt; root@curl-131556218-9fnch:/ &lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;$ nslookup my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Server:    10.0.0.10
Address 1: 10.0.0.10

Name:      my-nginx
Address 1: 10.0.162.149
&lt;/code&gt;&lt;/pre&gt;&lt;!--
## Securing the Service

Till now we have only accessed the nginx server from within the cluster. Before exposing the Service to the internet, you want to make sure the communication channel is secure. For this, you will need:

* Self signed certificates for https (unless you already have an identity certificate)
* An nginx server configured to use the certificates
* A [secret](/docs/concepts/configuration/secret/) that makes the certificates accessible to pods

You can acquire all these from the [nginx https example](https://github.com/kubernetes/examples/tree/master/staging/https-nginx/). This requires having go and make tools installed. If you don&#39;t want to install those, then follow the manual steps later. In short:

```shell
make keys KEY=/tmp/nginx.key CERT=/tmp/nginx.crt
kubectl create secret tls nginxsecret --key /tmp/nginx.key --cert /tmp/nginx.crt
```
```
secret/nginxsecret created
```
```shell
kubectl get secrets
```
```
NAME                  TYPE                                  DATA      AGE
default-token-il9rc   kubernetes.io/service-account-token   1         1d
nginxsecret           kubernetes.io/tls                     2         1m
```
And also the configmap:
```shell
kubectl create configmap nginxconfigmap --from-file=default.conf
```
```
configmap/nginxconfigmap created
```
```shell
kubectl get configmaps
```
```
NAME             DATA   AGE
nginxconfigmap   1      114s
```
Following are the manual steps to follow in case you run into problems running make (on windows for example):

```shell
# Create a public private key pair
openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /d/tmp/nginx.key -out /d/tmp/nginx.crt -subj &#34;/CN=my-nginx/O=my-nginx&#34;
# Convert the keys to base64 encoding
cat /d/tmp/nginx.crt | base64
cat /d/tmp/nginx.key | base64
```
Use the output from the previous commands to create a yaml file as follows. The base64 encoded value should all be on a single line.

```yaml
apiVersion: &#34;v1&#34;
kind: &#34;Secret&#34;
metadata:
  name: &#34;nginxsecret&#34;
  namespace: &#34;default&#34;
type: kubernetes.io/tls
data:
  tls.crt: &#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURIekNDQWdlZ0F3SUJBZ0lKQUp5M3lQK0pzMlpJTUEwR0NTcUdTSWIzRFFFQkJRVUFNQ1l4RVRBUEJnTlYKQkFNVENHNW5hVzU0YzNaak1SRXdEd1lEVlFRS0V3aHVaMmx1ZUhOMll6QWVGdzB4TnpFd01qWXdOekEzTVRKYQpGdzB4T0RFd01qWXdOekEzTVRKYU1DWXhFVEFQQmdOVkJBTVRDRzVuYVc1NGMzWmpNUkV3RHdZRFZRUUtFd2h1CloybHVlSE4yWXpDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBSjFxSU1SOVdWM0IKMlZIQlRMRmtobDRONXljMEJxYUhIQktMSnJMcy8vdzZhU3hRS29GbHlJSU94NGUrMlN5ajBFcndCLzlYTnBwbQppeW1CL3JkRldkOXg5UWhBQUxCZkVaTmNiV3NsTVFVcnhBZW50VWt1dk1vLzgvMHRpbGhjc3paenJEYVJ4NEo5Ci82UVRtVVI3a0ZTWUpOWTVQZkR3cGc3dlVvaDZmZ1Voam92VG42eHNVR0M2QURVODBpNXFlZWhNeVI1N2lmU2YKNHZpaXdIY3hnL3lZR1JBRS9mRTRqakxCdmdONjc2SU90S01rZXV3R0ljNDFhd05tNnNTSzRqYUNGeGpYSnZaZQp2by9kTlEybHhHWCtKT2l3SEhXbXNhdGp4WTRaNVk3R1ZoK0QrWnYvcW1mMFgvbVY0Rmo1NzV3ajFMWVBocWtsCmdhSXZYRyt4U1FVQ0F3RUFBYU5RTUU0d0hRWURWUjBPQkJZRUZPNG9OWkI3YXc1OUlsYkROMzhIYkduYnhFVjcKTUI4R0ExVWRJd1FZTUJhQUZPNG9OWkI3YXc1OUlsYkROMzhIYkduYnhFVjdNQXdHQTFVZEV3UUZNQU1CQWY4dwpEUVlKS29aSWh2Y05BUUVGQlFBRGdnRUJBRVhTMW9FU0lFaXdyMDhWcVA0K2NwTHI3TW5FMTducDBvMm14alFvCjRGb0RvRjdRZnZqeE04Tzd2TjB0clcxb2pGSW0vWDE4ZnZaL3k4ZzVaWG40Vm8zc3hKVmRBcStNZC9jTStzUGEKNmJjTkNUekZqeFpUV0UrKzE5NS9zb2dmOUZ3VDVDK3U2Q3B5N0M3MTZvUXRUakViV05VdEt4cXI0Nk1OZWNCMApwRFhWZmdWQTRadkR4NFo3S2RiZDY5eXM3OVFHYmg5ZW1PZ05NZFlsSUswSGt0ejF5WU4vbVpmK3FqTkJqbWZjCkNnMnlwbGQ0Wi8rUUNQZjl3SkoybFIrY2FnT0R4elBWcGxNSEcybzgvTHFDdnh6elZPUDUxeXdLZEtxaUMwSVEKQ0I5T2wwWW5scE9UNEh1b2hSUzBPOStlMm9KdFZsNUIyczRpbDlhZ3RTVXFxUlU9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K&#34;
  tls.key: &#34;LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk1JSUV2UUlCQURBTkJna3Foa2lHOXcwQkFRRUZBQVNDQktjd2dnU2pBZ0VBQW9JQkFRQ2RhaURFZlZsZHdkbFIKd1V5eFpJWmVEZWNuTkFhbWh4d1NpeWF5N1AvOE9ta3NVQ3FCWmNpQ0RzZUh2dGtzbzlCSzhBZi9WemFhWm9zcApnZjYzUlZuZmNmVUlRQUN3WHhHVFhHMXJKVEVGSzhRSHA3VkpMcnpLUC9QOUxZcFlYTE0yYzZ3MmtjZUNmZitrCkU1bEVlNUJVbUNUV09UM3c4S1lPNzFLSWVuNEZJWTZMMDUrc2JGQmd1Z0ExUE5JdWFubm9UTWtlZTRuMG4rTDQKb3NCM01ZUDhtQmtRQlAzeE9JNHl3YjREZXUraURyU2pKSHJzQmlIT05Xc0RadXJFaXVJMmdoY1kxeWIyWHI2UAozVFVOcGNSbC9pVG9zQngxcHJHclk4V09HZVdPeGxZZmcvbWIvNnBuOUYvNWxlQlkrZStjSTlTMkQ0YXBKWUdpCkwxeHZzVWtGQWdNQkFBRUNnZ0VBZFhCK0xkbk8ySElOTGo5bWRsb25IUGlHWWVzZ294RGQwci9hQ1Zkank4dlEKTjIwL3FQWkUxek1yall6Ry9kVGhTMmMwc0QxaTBXSjdwR1lGb0xtdXlWTjltY0FXUTM5SjM0VHZaU2FFSWZWNgo5TE1jUHhNTmFsNjRLMFRVbUFQZytGam9QSFlhUUxLOERLOUtnNXNrSE5pOWNzMlY5ckd6VWlVZWtBL0RBUlBTClI3L2ZjUFBacDRuRWVBZmI3WTk1R1llb1p5V21SU3VKdlNyblBESGtUdW1vVlVWdkxMRHRzaG9reUxiTWVtN3oKMmJzVmpwSW1GTHJqbGtmQXlpNHg0WjJrV3YyMFRrdWtsZU1jaVlMbjk4QWxiRi9DSmRLM3QraTRoMTVlR2ZQegpoTnh3bk9QdlVTaDR2Q0o3c2Q5TmtEUGJvS2JneVVHOXBYamZhRGR2UVFLQmdRRFFLM01nUkhkQ1pKNVFqZWFKClFGdXF4cHdnNzhZTjQyL1NwenlUYmtGcVFoQWtyczJxWGx1MDZBRzhrZzIzQkswaHkzaE9zSGgxcXRVK3NHZVAKOWRERHBsUWV0ODZsY2FlR3hoc0V0L1R6cEdtNGFKSm5oNzVVaTVGZk9QTDhPTm1FZ3MxMVRhUldhNzZxelRyMgphRlpjQ2pWV1g0YnRSTHVwSkgrMjZnY0FhUUtCZ1FEQmxVSUUzTnNVOFBBZEYvL25sQVB5VWs1T3lDdWc3dmVyClUycXlrdXFzYnBkSi9hODViT1JhM05IVmpVM25uRGpHVHBWaE9JeXg5TEFrc2RwZEFjVmxvcG9HODhXYk9lMTAKMUdqbnkySmdDK3JVWUZiRGtpUGx1K09IYnRnOXFYcGJMSHBzUVpsMGhucDBYSFNYVm9CMUliQndnMGEyOFVadApCbFBtWmc2d1BRS0JnRHVIUVV2SDZHYTNDVUsxNFdmOFhIcFFnMU16M2VvWTBPQm5iSDRvZUZKZmcraEppSXlnCm9RN3hqWldVR3BIc3AyblRtcHErQWlSNzdyRVhsdlhtOElVU2FsbkNiRGlKY01Pc29RdFBZNS9NczJMRm5LQTQKaENmL0pWb2FtZm1nZEN0ZGtFMXNINE9MR2lJVHdEbTRpb0dWZGIwMllnbzFyb2htNUpLMUI3MkpBb0dBUW01UQpHNDhXOTVhL0w1eSt5dCsyZ3YvUHM2VnBvMjZlTzRNQ3lJazJVem9ZWE9IYnNkODJkaC8xT2sybGdHZlI2K3VuCnc1YytZUXRSTHlhQmd3MUtpbGhFZDBKTWU3cGpUSVpnQWJ0LzVPbnlDak9OVXN2aDJjS2lrQ1Z2dTZsZlBjNkQKckliT2ZIaHhxV0RZK2Q1TGN1YSt2NzJ0RkxhenJsSlBsRzlOZHhrQ2dZRUF5elIzT3UyMDNRVVV6bUlCRkwzZAp4Wm5XZ0JLSEo3TnNxcGFWb2RjL0d5aGVycjFDZzE2MmJaSjJDV2RsZkI0VEdtUjZZdmxTZEFOOFRwUWhFbUtKCnFBLzVzdHdxNWd0WGVLOVJmMWxXK29xNThRNTBxMmk1NVdUTThoSDZhTjlaMTltZ0FGdE5VdGNqQUx2dFYxdEYKWSs4WFJkSHJaRnBIWll2NWkwVW1VbGc9Ci0tLS0tRU5EIFBSSVZBVEUgS0VZLS0tLS0K&#34;
```
Now create the secrets using the file:

```shell
kubectl apply -f nginxsecrets.yaml
kubectl get secrets
```
```
NAME                  TYPE                                  DATA      AGE
default-token-il9rc   kubernetes.io/service-account-token   1         1d
nginxsecret           kubernetes.io/tls                     2         1m
```

Now modify your nginx replicas to start an https server using the certificate in the secret, and the Service, to expose both ports (80 and 443):



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnginx-secure-appyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/nginx-secure-app.yaml&#34; download=&#34;service/networking/nginx-secure-app.yaml&#34;&gt;
                    &lt;code&gt;service/networking/nginx-secure-app.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnginx-secure-appyaml&#39;)&#34; title=&#34;Copy service/networking/nginx-secure-app.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NodePort&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;443&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginxsecret&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;configmap-volume&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;configMap&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginxconfigmap&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginxhttps&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bprashanth/nginxhttps:1.0&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;443&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/etc/nginx/ssl&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/etc/nginx/conf.d&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;configmap-volume&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



Noteworthy points about the nginx-secure-app manifest:

- It contains both Deployment and Service specification in the same file.
- The [nginx server](https://github.com/kubernetes/examples/tree/master/staging/https-nginx/default.conf)
  serves HTTP traffic on port 80 and HTTPS traffic on 443, and nginx Service
  exposes both ports.
- Each container has access to the keys through a volume mounted at `/etc/nginx/ssl`.
  This is setup *before* the nginx server is started.

```shell
kubectl delete deployments,svc my-nginx; kubectl create -f ./nginx-secure-app.yaml
```

At this point you can reach the nginx server from any node.

```shell
kubectl get pods -o yaml | grep -i podip
    podIP: 10.244.3.5
node $ curl -k https://10.244.3.5
...
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
```

Note how we supplied the `-k` parameter to curl in the last step, this is because we don&#39;t know anything about the pods running nginx at certificate generation time,
so we have to tell curl to ignore the CName mismatch. By creating a Service we linked the CName used in the certificate with the actual DNS name used by pods during Service lookup.
Let&#39;s test this from a pod (the same secret is being reused for simplicity, the pod only needs nginx.crt to access the Service):



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingcurlpodyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/curlpod.yaml&#34; download=&#34;service/networking/curlpod.yaml&#34;&gt;
                    &lt;code&gt;service/networking/curlpod.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingcurlpodyaml&#39;)&#34; title=&#34;Copy service/networking/curlpod.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;curl-deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;curlpod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;curlpod&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginxsecret&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;curlpod&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;:
        - &lt;span style=&#34;color:#ae81ff&#34;&gt;sh&lt;/span&gt;
        - -&lt;span style=&#34;color:#ae81ff&#34;&gt;c&lt;/span&gt;
        - &lt;span style=&#34;color:#ae81ff&#34;&gt;while true; do sleep 1; done&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;radial/busyboxplus:curl&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/etc/nginx/ssl&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



```shell
kubectl apply -f ./curlpod.yaml
kubectl get pods -l app=curlpod
```
```
NAME                               READY     STATUS    RESTARTS   AGE
curl-deployment-1515033274-1410r   1/1       Running   0          1m
```
```shell
kubectl exec curl-deployment-1515033274-1410r -- curl https://my-nginx --cacert /etc/nginx/ssl/tls.crt
...
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
...
```
 --&gt;
&lt;h2 id=&#34;为-service-加安全层&#34;&gt;为 Service 加安全层&lt;/h2&gt;
&lt;p&gt;到目前为止我们都是在集群内访问这个 nginx 服务。 在将这个 Service 暴露到互联网之前，需要确保
连接是安全。 因此，必须要:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Self signed certificates for https (unless you already have an identity certificate)&lt;/li&gt;
&lt;li&gt;An nginx server configured to use the certificates&lt;/li&gt;
&lt;li&gt;A &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/secret/&#34;&gt;secret&lt;/a&gt; that makes the certificates accessible to pods&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;创建一个自签的 https 证书(除非已经有对应的证书)&lt;/li&gt;
&lt;li&gt;配置一个使用该证书的 nginx 服务&lt;/li&gt;
&lt;li&gt;创建一个 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/secret/&#34;&gt;Secret&lt;/a&gt;， 让证书在 Pod 中可用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有的这些都可以参考
&lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/https-nginx&#34;&gt;nginx https 示例&lt;/a&gt;
这个需要有 go 环境和安装打包工具。如果不想安装这些， 则跟着下面步骤手动操作， 简单说:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;make keys KEY&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/tmp/nginx.key CERT&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/tmp/nginx.crt
kubectl create secret tls nginxsecret --key /tmp/nginx.key --cert /tmp/nginx.crt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;secret/nginxsecret created
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get secrets
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME                  TYPE                                  DATA      AGE
default-token-il9rc   kubernetes.io/service-account-token   1         1d
nginxsecret           kubernetes.io/tls                     2         1m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;再创建 Configmap:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create configmap nginxconfigmap --from-file&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;default.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;configmap/nginxconfigmap created
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get configmaps
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME             DATA   AGE
nginxconfigmap   1      114s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果运行 make 有问题，则跟随下面的手动步骤操作(比如在 windows 上):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Create a public private key pair&lt;/span&gt;
openssl req -x509 -nodes -days &lt;span style=&#34;color:#ae81ff&#34;&gt;365&lt;/span&gt; -newkey rsa:2048 -keyout /d/tmp/nginx.key -out /d/tmp/nginx.crt -subj &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/CN=my-nginx/O=my-nginx&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Convert the keys to base64 encoding&lt;/span&gt;
cat /d/tmp/nginx.crt | base64
cat /d/tmp/nginx.key | base64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Use the output from the previous commands to create a yaml file as follows. The base64 encoded value should all be on a single line.
使用上面的输出创建以下的 yaml 配置文件。 base64 编码的内容应该只有一行。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;v1&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Secret&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nginxsecret&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/tls&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;tls.crt&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURIekNDQWdlZ0F3SUJBZ0lKQUp5M3lQK0pzMlpJTUEwR0NTcUdTSWIzRFFFQkJRVUFNQ1l4RVRBUEJnTlYKQkFNVENHNW5hVzU0YzNaak1SRXdEd1lEVlFRS0V3aHVaMmx1ZUhOMll6QWVGdzB4TnpFd01qWXdOekEzTVRKYQpGdzB4T0RFd01qWXdOekEzTVRKYU1DWXhFVEFQQmdOVkJBTVRDRzVuYVc1NGMzWmpNUkV3RHdZRFZRUUtFd2h1CloybHVlSE4yWXpDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBSjFxSU1SOVdWM0IKMlZIQlRMRmtobDRONXljMEJxYUhIQktMSnJMcy8vdzZhU3hRS29GbHlJSU94NGUrMlN5ajBFcndCLzlYTnBwbQppeW1CL3JkRldkOXg5UWhBQUxCZkVaTmNiV3NsTVFVcnhBZW50VWt1dk1vLzgvMHRpbGhjc3paenJEYVJ4NEo5Ci82UVRtVVI3a0ZTWUpOWTVQZkR3cGc3dlVvaDZmZ1Voam92VG42eHNVR0M2QURVODBpNXFlZWhNeVI1N2lmU2YKNHZpaXdIY3hnL3lZR1JBRS9mRTRqakxCdmdONjc2SU90S01rZXV3R0ljNDFhd05tNnNTSzRqYUNGeGpYSnZaZQp2by9kTlEybHhHWCtKT2l3SEhXbXNhdGp4WTRaNVk3R1ZoK0QrWnYvcW1mMFgvbVY0Rmo1NzV3ajFMWVBocWtsCmdhSXZYRyt4U1FVQ0F3RUFBYU5RTUU0d0hRWURWUjBPQkJZRUZPNG9OWkI3YXc1OUlsYkROMzhIYkduYnhFVjcKTUI4R0ExVWRJd1FZTUJhQUZPNG9OWkI3YXc1OUlsYkROMzhIYkduYnhFVjdNQXdHQTFVZEV3UUZNQU1CQWY4dwpEUVlKS29aSWh2Y05BUUVGQlFBRGdnRUJBRVhTMW9FU0lFaXdyMDhWcVA0K2NwTHI3TW5FMTducDBvMm14alFvCjRGb0RvRjdRZnZqeE04Tzd2TjB0clcxb2pGSW0vWDE4ZnZaL3k4ZzVaWG40Vm8zc3hKVmRBcStNZC9jTStzUGEKNmJjTkNUekZqeFpUV0UrKzE5NS9zb2dmOUZ3VDVDK3U2Q3B5N0M3MTZvUXRUakViV05VdEt4cXI0Nk1OZWNCMApwRFhWZmdWQTRadkR4NFo3S2RiZDY5eXM3OVFHYmg5ZW1PZ05NZFlsSUswSGt0ejF5WU4vbVpmK3FqTkJqbWZjCkNnMnlwbGQ0Wi8rUUNQZjl3SkoybFIrY2FnT0R4elBWcGxNSEcybzgvTHFDdnh6elZPUDUxeXdLZEtxaUMwSVEKQ0I5T2wwWW5scE9UNEh1b2hSUzBPOStlMm9KdFZsNUIyczRpbDlhZ3RTVXFxUlU9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;tls.key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;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&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;然后使用这引文件创建 Secret：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f nginxsecrets.yaml
kubectl get secrets
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME                  TYPE                                  DATA      AGE
default-token-il9rc   kubernetes.io/service-account-token   1         1d
nginxsecret           kubernetes.io/tls                     2         1m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这时候修改 nginx 副本使用 Secret 中的证书启动一个 https 服务。 Service 也需要同时暴露 80 和 443 端口:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnginx-secure-appyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/nginx-secure-app.yaml&#34; download=&#34;service/networking/nginx-secure-app.yaml&#34;&gt;
                    &lt;code&gt;service/networking/nginx-secure-app.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnginx-secure-appyaml&#39;)&#34; title=&#34;Copy service/networking/nginx-secure-app.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NodePort&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;443&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginxsecret&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;configmap-volume&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;configMap&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginxconfigmap&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginxhttps&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bprashanth/nginxhttps:1.0&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;443&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/etc/nginx/ssl&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/etc/nginx/conf.d&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;configmap-volume&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;需要重点注意 nginx-secure-app 的地方:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在同一个文件中包含了 Deployment 和 Service 的定义配置。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/https-nginx/default.conf&#34;&gt;nginx 服务器&lt;/a&gt;
同时在 80 端口提供 HTTP 流量和 443 端口提供 HTTPS 流量， nginx Service 同时暴露了这两个端口。&lt;/li&gt;
&lt;li&gt;每个容器都通过挂载在  &lt;code&gt;/etc/nginx/ssl&lt;/code&gt; 的数据卷来访问证书
因此需要在 nginx 服务启动 &lt;em&gt;之前&lt;/em&gt; 配置好&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl delete deployments,svc my-nginx; kubectl create -f ./nginx-secure-app.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这时候就可以在任意一个节点上访问这个 nginx 服务&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods -o yaml | grep -i podip
    podIP: 10.244.3.5
node $ curl -k https://10.244.3.5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;...
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;注意这里最后一步中的 curl 使用用了 &lt;code&gt;-k&lt;/code&gt;， 因为在创建证书的时候不会知道运行 nginx 的 Pod的
任何信息， 因此要让 curl 忽略 CName 不匹配的问题。通过创建一个 Service, 就可以把证书中的
CName 和 Pod 在对 Service 解析使用的 DNS 名称统一起来。
在一个 Pod 中再测试一下(为了简单使用同一个 Secret, 这个 Pod 访问 Service 只需要 nginx.crt)&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingcurlpodyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/curlpod.yaml&#34; download=&#34;service/networking/curlpod.yaml&#34;&gt;
                    &lt;code&gt;service/networking/curlpod.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingcurlpodyaml&#39;)&#34; title=&#34;Copy service/networking/curlpod.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;curl-deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;curlpod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;curlpod&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginxsecret&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;curlpod&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;:
        - &lt;span style=&#34;color:#ae81ff&#34;&gt;sh&lt;/span&gt;
        - -&lt;span style=&#34;color:#ae81ff&#34;&gt;c&lt;/span&gt;
        - &lt;span style=&#34;color:#ae81ff&#34;&gt;while true; do sleep 1; done&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;radial/busyboxplus:curl&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/etc/nginx/ssl&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;secret-volume&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f ./curlpod.yaml
kubectl get pods -l app&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;curlpod
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME                               READY     STATUS    RESTARTS   AGE
curl-deployment-1515033274-1410r   1/1       Running   0          1m
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl exec curl-deployment-1515033274-1410r -- curl https://my-nginx --cacert /etc/nginx/ssl/tls.crt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;...
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;!--
## Exposing the Service

For some parts of your applications you may want to expose a Service onto an
external IP address. Kubernetes supports two ways of doing this: NodePorts and
LoadBalancers. The Service created in the last section already used `NodePort`,
so your nginx HTTPS replica is ready to serve traffic on the internet if your
node has a public IP.

```shell
kubectl get svc my-nginx -o yaml | grep nodePort -C 5
  uid: 07191fb3-f61a-11e5-8ae5-42010af00002
spec:
  clusterIP: 10.0.162.149
  ports:
  - name: http
    nodePort: 31704
    port: 8080
    protocol: TCP
    targetPort: 80
  - name: https
    nodePort: 32453
    port: 443
    protocol: TCP
    targetPort: 443
  selector:
    run: my-nginx
```
```shell
kubectl get nodes -o yaml | grep ExternalIP -C 1
    - address: 104.197.41.11
      type: ExternalIP
    allocatable:
--
    - address: 23.251.152.56
      type: ExternalIP
    allocatable:
...

$ curl https://&lt;EXTERNAL-IP&gt;:&lt;NODE-PORT&gt; -k
...
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
```

Let&#39;s now recreate the Service to use a cloud load balancer, just change the `Type` of `my-nginx` Service from `NodePort` to `LoadBalancer`:

```shell
kubectl edit svc my-nginx
kubectl get svc my-nginx
```
```
NAME       TYPE           CLUSTER-IP     EXTERNAL-IP        PORT(S)               AGE
my-nginx   LoadBalancer   10.0.162.149     xx.xxx.xxx.xxx     8080:30163/TCP        21s
```
```
curl https://&lt;EXTERNAL-IP&gt; -k
...
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
```

The IP address in the `EXTERNAL-IP` column is the one that is available on the public internet.  The `CLUSTER-IP` is only available inside your
cluster/private cloud network.

Note that on AWS, type `LoadBalancer` creates an ELB, which uses a (long)
hostname, not an IP.  It&#39;s too long to fit in the standard `kubectl get svc`
output, in fact, so you&#39;ll need to do `kubectl describe service my-nginx` to
see it.  You&#39;ll see something like this:

```shell
kubectl describe service my-nginx
...
LoadBalancer Ingress:   a320587ffd19711e5a37606cf4a74574-1142138393.us-east-1.elb.amazonaws.com
...
```
 --&gt;
&lt;h2 id=&#34;暴露这个-service&#34;&gt;暴露这个 Service&lt;/h2&gt;
&lt;p&gt;For some parts of your applications you may want to expose a Service onto an
external IP address. Kubernetes supports two ways of doing this: NodePorts and
LoadBalancers. The Service created in the last section already used &lt;code&gt;NodePort&lt;/code&gt;,
so your nginx HTTPS replica is ready to serve traffic on the internet if your
node has a public IP.&lt;/p&gt;
&lt;p&gt;对于应用中的一部分，用户可能希望通过一个 Service 将其暴露到一个公网 IP 地址上。 k8s 支持两种方式:
NodePort 和 负载均衡器。 最后创建的这个 Service 已经使用了 &lt;code&gt;NodePort&lt;/code&gt;， 所以如果集群中的
节点有公网IP，这个 nginx 的 HTTPS 就可以通过公网访问。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get svc my-nginx -o yaml | grep nodePort -C &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;uid: 07191fb3-f61a-11e5-8ae5-42010af00002
spec:
clusterIP: 10.0.162.149
ports:
- name: http
  nodePort: 31704
  port: 8080
  protocol: TCP
  targetPort: 80
- name: https
  nodePort: 32453
  port: 443
  protocol: TCP
  targetPort: 443
selector:
  run: my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get nodes -o yaml | grep ExternalIP -C &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;- address: 104.197.41.11
  type: ExternalIP
allocatable:
--
- address: 23.251.152.56
  type: ExternalIP
allocatable:
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ curl https://&amp;lt;EXTERNAL-IP&amp;gt;:&amp;lt;NODE-PORT&amp;gt; -k
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;...
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;现在修改 Service 使它使用云负载均衡器，只需要将 &lt;code&gt;my-nginx&lt;/code&gt; Service 的 &lt;code&gt;Type&lt;/code&gt; 由 &lt;code&gt;NodePort&lt;/code&gt; 改为 &lt;code&gt;LoadBalancer&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl edit svc my-nginx
kubectl get svc my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME       TYPE           CLUSTER-IP     EXTERNAL-IP        PORT(S)               AGE
my-nginx   LoadBalancer   10.0.162.149     xx.xxx.xxx.xxx     8080:30163/TCP        21s
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl https://&amp;lt;EXTERNAL-IP&amp;gt; -k
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;...
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The IP address in the &lt;code&gt;EXTERNAL-IP&lt;/code&gt; column is the one that is available on the public internet.  The &lt;code&gt;CLUSTER-IP&lt;/code&gt; is only available inside your
cluster/private cloud network.
&lt;code&gt;EXTERNAL-IP&lt;/code&gt; 列中的 IP 地址就是一个公网可用的地址。 &lt;code&gt;CLUSTER-IP&lt;/code&gt; 只能在集群内部使用。
Note that on AWS, type &lt;code&gt;LoadBalancer&lt;/code&gt; creates an ELB, which uses a (long)
hostname, not an IP.  It&amp;rsquo;s too long to fit in the standard &lt;code&gt;kubectl get svc&lt;/code&gt;
output, in fact, so you&amp;rsquo;ll need to do &lt;code&gt;kubectl describe service my-nginx&lt;/code&gt; to
see it.  You&amp;rsquo;ll see something like this:&lt;/p&gt;
&lt;p&gt;要注意在 AWS 上， &lt;code&gt;LoadBalancer&lt;/code&gt; 类型的 Service 会创建一个 ELB, 它会使用一个(很长的)主机名，
而不是一个 IP 地址。 因为这个主机名太长而不适配标准的 &lt;code&gt;kubectl get svc&lt;/code&gt; 输出，实际上需要使用
&lt;code&gt;kubectl describe service my-nginx&lt;/code&gt; 命令才能看当输出的这个主机名，样子就类似下面的:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe service my-nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;...
LoadBalancer Ingress:   a320587ffd19711e5a37606cf4a74574-1142138393.us-east-1.elb.amazonaws.com
...
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/access-application-cluster/service-access-application-cluster/&#34;&gt;使用 Service 让集群中的一个应用可以访问&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/access-application-cluster/connecting-frontend-backend/&#34;&gt;使用 Service 连接前后端应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/access-application-cluster/create-external-load-balancer/&#34;&gt;创建一个外部负载均衡器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: CSI 卷克隆</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volume-pvc-datasource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volume-pvc-datasource/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jsafrane
- saad-ali
- thockin
- msau42
title: CSI Volume Cloning
content_type: concept
weight: 30
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This document describes the concept of cloning existing CSI Volumes in Kubernetes.  Familiarity with [Volumes](/docs/concepts/storage/volumes) is suggested.
 --&gt;
&lt;p&gt;本文主要介绍在 k8s 中克隆已有的 CSI 卷的概念. 建议先熟悉
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes&#34;&gt;Volumes&lt;/a&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Introduction

The &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;CSI&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt; Volume Cloning feature adds support for specifying existing &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/persistent-volumes/&#39; target=&#39;_blank&#39;&gt;PVC&lt;span class=&#39;tooltip-text&#39;&gt;索要定义在 PersistentVolume 中的存储资源，然后就可以将其以卷的方式挂载到容器中。&lt;/span&gt;
&lt;/a&gt;s in the `dataSource` field to indicate a user would like to clone a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;卷(Volume)&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;.

A Clone is defined as a duplicate of an existing Kubernetes Volume that can be consumed as any standard Volume would be.  The only difference is that upon provisioning, rather than creating a &#34;new&#34; empty Volume, the back end device creates an exact duplicate of the specified Volume.

The implementation of cloning, from the perspective of the Kubernetes API, simply adds the ability to specify an existing PVC as a dataSource during new PVC creation. The source PVC must be bound and available (not in use).

Users need to be aware of the following when using this feature:

* Cloning support (`VolumePVCDataSource`) is only available for CSI drivers.
* Cloning support is only available for dynamic provisioners.
* CSI drivers may or may not have implemented the volume cloning functionality.
* You can only clone a PVC when it exists in the same namespace as the destination PVC (source and destination must be in the same namespace).
* Cloning is only supported within the same Storage Class.
    - Destination volume must be the same storage class as the source
    - Default storage class can be used and storageClassName omitted in the spec
* Cloning can only be performed between two volumes that use the same VolumeMode setting (if you request a block mode volume, the source MUST also be block mode)
 --&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;CSI&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt; 卷克隆的特性。它支持在 &lt;code&gt;dataSource&lt;/code&gt;
字段指定现有的
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/persistent-volumes/&#39; target=&#39;_blank&#39;&gt;PVC&lt;span class=&#39;tooltip-text&#39;&gt;索要定义在 PersistentVolume 中的存储资源，然后就可以将其以卷的方式挂载到容器中。&lt;/span&gt;
&lt;/a&gt;
表示用户希望克隆一个
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;卷(Volume)&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这个克隆体是对现有的 k8s 卷的复制，可以用于任意标准卷可用的地方。 区别只有提供时不是创建一个新
的空卷，而对源卷底层存储设备完全复制的卷&lt;/p&gt;
&lt;p&gt;以 k8s API 的角度来看，克隆的实现就是，在创建新的 PVC 时，可以在 &lt;code&gt;dataSource&lt;/code&gt; 字段指定一个
现有的 PVC 作为数据源。 源 PVC 必须绑定且可用(不能在使用中)&lt;/p&gt;
&lt;p&gt;用户在使用该特性时需要注意以下几点:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只有 CSI 驱动才支持克隆 (&lt;code&gt;VolumePVCDataSource&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;只支持在动态供应中使用克隆&lt;/li&gt;
&lt;li&gt;CSI 驱动可能实现或可能没实现克隆功能&lt;/li&gt;
&lt;li&gt;只有源 PVC 必须与目标 PVC 在同一个命名空间才能克隆(源和目标必须在一个命名空间)&lt;/li&gt;
&lt;li&gt;克隆只能发生一同一个存储类别中(StorageClass)
&lt;ul&gt;
&lt;li&gt;目标卷的 存储类别必须与源相同&lt;/li&gt;
&lt;li&gt;当定义中没有指定 &lt;code&gt;storageClassName&lt;/code&gt; 时使用默认存储类别(StorageClass)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;克隆只能在两个相同卷类别(VolumeMode)的卷之间操作(如果请求的是一个块设备卷，源就必须是一个块设备卷)&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Provisioning

Clones are provisioned just like any other PVC with the exception of adding a dataSource that references an existing PVC in the same namespace.

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
    name: clone-of-pvc-1
    namespace: myns
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: cloning
  resources:
    requests:
      storage: 5Gi
  dataSource:
    kind: PersistentVolumeClaim
    name: pvc-1
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You must specify a capacity value for &lt;code&gt;spec.resources.requests.storage&lt;/code&gt;, and the value you specify must be the same or larger than the capacity of the source volume.&lt;/div&gt;
&lt;/blockquote&gt;


The result is a new PVC with the name `clone-of-pvc-1` that has the exact same content as the specified source `pvc-1`.
 --&gt;
&lt;h2 id=&#34;供应&#34;&gt;供应&lt;/h2&gt;
&lt;p&gt;克隆体除了在 &lt;code&gt;dataSource&lt;/code&gt; 指定一个同一命名空间的 PVC 外与任意其它的 PVC 的供应完全相同，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;clone-of-pvc-1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myns&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cloning&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dataSource&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pvc-1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 必须要为 &lt;code&gt;spec.resources.requests.storage&lt;/code&gt; 指定一个值，并且这个值必须大于或等于源卷的值&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;最终结果就是新创建一个叫  &lt;code&gt;clone-of-pvc-1&lt;/code&gt; 的PVC 其内容与它的源 &lt;code&gt;pvc-1&lt;/code&gt; 完全一样。&lt;/p&gt;
&lt;!--
## Usage

Upon availability of the new PVC, the cloned PVC is consumed the same as other PVC.  It&#39;s also expected at this point that the newly created PVC is an independent object.  It can be consumed, cloned, snapshotted, or deleted independently and without consideration for it&#39;s original dataSource PVC.  This also implies that the source is not linked in any way to the newly created clone, it may also be modified or deleted without affecting the newly created clone.
 --&gt;
&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;
&lt;p&gt;在可用性上，克隆体的 PVC 与其它的 PVC 一样。 新创建的 PVC 是一个独立的对象。 它可以消费，克隆，创建快照，
并在不影响它源 PVC 的情况下独立的删除。 这就是说克隆的源和目标之间没有任意形式的链接，对源的
修改或删除也不会影响它克隆体&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 管理容器资源</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/manage-resources-containers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/manage-resources-containers/</guid>
      <description>
        
        
        &lt;!--
---
title: Managing Resources for Containers
content_type: concept
weight: 40
feature:
  title: Automatic bin packing
  description: &gt;
    Automatically places containers based on their resource requirements and other constraints, while not sacrificing availability. Mix critical and best-effort workloads in order to drive up utilization and save even more resources.
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
When you specify a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;, you can optionally specify how
much of each resource a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/what-is-kubernetes/#why-containers&#39; target=&#39;_blank&#39;&gt;Container&lt;span class=&#39;tooltip-text&#39;&gt;一个包含应用及其所有信赖的轻量的可移植的可运行的镜像&lt;/span&gt;
&lt;/a&gt; needs.
The most common resources to specify are CPU and memory (RAM); there are others.

When you specify the resource _request_ for Containers in a Pod, the scheduler uses this
information to decide which node to place the Pod on. When you specify a resource _limit_
for a Container, the kubelet enforces those limits so that the running container is not
allowed to use more of that resource than the limit you set. The kubelet also reserves
at least the _request_ amount of that system resource specifically for that container
to use.
 --&gt;
&lt;p&gt;在配置一个
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
时，用户可以选择对每个
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/what-is-kubernetes/#why-containers&#39; target=&#39;_blank&#39;&gt;Container&lt;span class=&#39;tooltip-text&#39;&gt;一个包含应用及其所有信赖的轻量的可移植的可运行的镜像&lt;/span&gt;
&lt;/a&gt;
设置其所使用的资源。
最常设置的资源是 CPU 和 内存 (RAM); 但还是有其他的。&lt;/p&gt;
&lt;p&gt;当用户为一个 Pod 中的容器设置资源 &lt;em&gt;请求&lt;/em&gt; ，调度器会使用这些信息来决定将 Pod 放到哪个节点上。
当为容器设置资源 &lt;em&gt;限制&lt;/em&gt; 时， kubelet 会执行这些限制以保证容器在运行时使用的资源不会超过这个设置
的资源限制。 kubelet 也会为这些容器保留资源 &lt;em&gt;请求&lt;/em&gt; 所设置的的资源的数量给予容器使用。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Requests and limits

If the node where a Pod is running has enough of a resource available, it&#39;s possible (and
allowed) for a container to use more resource than its `request` for that resource specifies.
However, a container is not allowed to use more than its resource `limit`.

For example, if you set a `memory` request of 256 MiB for a container, and that container is in
a Pod scheduled to a Node with 8GiB of memory and no other Pods, then the container can try to use
more RAM.

If you set a `memory` limit of 4GiB for that Container, the kubelet (and
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;container runtime&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt;) enforce the limit.
The runtime prevents the container from using more than the configured resource limit. For example:
when a process in the container tries to consume more than the allowed amount of memory,
the system kernel terminates the process that attempted the allocation, with an out of memory
(OOM) error.

Limits can be implemented either reactively (the system intervenes once it sees a violation)
or by enforcement (the system prevents the container from ever exceeding the limit). Different
runtimes can have different ways to implement the same restrictions.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If a Container specifies its own memory limit, but does not specify a memory request, Kubernetes
automatically assigns a memory request that matches the limit. Similarly, if a Container specifies its own
CPU limit, but does not specify a CPU request, Kubernetes automatically assigns a CPU request that matches
the limit.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;requests-and-limits&#34;&gt;请求与限制&lt;/h2&gt;
&lt;p&gt;如果 Pod 运行的节点上的某种资源足够， 这就使得这个容器可能(也允许)使用比 &lt;code&gt;request&lt;/code&gt; 所设置的资源
更多的资源，但容器不能使用比 &lt;code&gt;limit&lt;/code&gt; 设置的资源的限制。&lt;/p&gt;
&lt;p&gt;例如， 如果设置容器的 &lt;code&gt;memory&lt;/code&gt; 资源请求为 256 MiB，然后容器所在的 Pod 被调度到一个有 8GiB
内存的节点上并且这个节点上没有其它的 Pod， 这时容器就可以常用使用更多的 RAM.&lt;/p&gt;
&lt;p&gt;如果设置了容器 &lt;code&gt;memory&lt;/code&gt; 限制为 4GiB， kubelet
(和 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;容器运行环境&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt;)
会执行这个限制。容器运行环境会防止容器使用超过容器资源限制所配置的资源数额。 例如，当容器中的一
个进行尝试使用超过允许的内存数量，系统内核就会终止这个尝试申请的进程，错误信息的内存不足(OOM).&lt;/p&gt;
&lt;p&gt;这些限制的实现可以是反应式的(当发现起限时系统干涉)或通过强制(系统防止容器使用超过限制资源)。
不同容器运行环境可能以不同的方式实现同样的限制。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果一个容器设置内存限制(&lt;code&gt;limit&lt;/code&gt;)，但没有设置内存请求(&lt;code&gt;request&lt;/code&gt;)，k8s 会自动为其分配一个与限制数额相同的请求。类似地
如果容器指定 CPU 限制(&lt;code&gt;limit&lt;/code&gt;)，但没设置 CPU 请求(&lt;code&gt;request&lt;/code&gt;)， k8s 会自动为其分配一个与限制数额相同的请求。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Resource types

*CPU* and *memory* are each a *resource type*. A resource type has a base unit.
CPU represents compute processing and is specified in units of [Kubernetes CPUs](#meaning-of-cpu).
Memory is specified in units of bytes.
If you&#39;re using Kubernetes v1.14 or newer, you can specify _huge page_ resources.
Huge pages are a Linux-specific feature where the node kernel allocates blocks of memory
that are much larger than the default page size.

For example, on a system where the default page size is 4KiB, you could specify a limit,
`hugepages-2Mi: 80Mi`. If the container tries allocating over 40 2MiB huge pages (a
total of 80 MiB), that allocation fails.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You cannot overcommit &lt;code&gt;hugepages-*&lt;/code&gt; resources.
This is different from the &lt;code&gt;memory&lt;/code&gt; and &lt;code&gt;cpu&lt;/code&gt; resources.&lt;/div&gt;
&lt;/blockquote&gt;


CPU and memory are collectively referred to as *compute resources*, or just
*resources*. Compute
resources are measurable quantities that can be requested, allocated, and
consumed. They are distinct from
[API resources](/docs/concepts/overview/kubernetes-api/). API resources, such as Pods and
[Services](/docs/concepts/services-networking/service/) are objects that can be read and modified
through the Kubernetes API server.
 --&gt;
&lt;h2 id=&#34;resource-types&#34;&gt;资源类型&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;CPU&lt;/em&gt; 和 &lt;em&gt;memory&lt;/em&gt; 都是 &lt;em&gt;资源类型&lt;/em&gt; 的一种。 每种资源类型都有一个基础单位。CPU 代表计算处理
并以
&lt;a href=&#34;#meaning-of-cpu&#34;&gt;Kubernetes CPUs&lt;/a&gt;
为设置的基础单位。 内存是以字节为单位来设置的。 如果使用的是 k8s v1.14+, 可以设置 &lt;em&gt;huge page&lt;/em&gt; 资源。
Huge page 是一个 Linux 的特性，当节点内存分配内存块时可以多默认的 page size 大很多&lt;/p&gt;
&lt;p&gt;例如，在一个系统中默认的 page size 是 4KiB, 设置了一个限制为 &lt;code&gt;hugepages-2Mi: 80Mi&lt;/code&gt;.
如果容器尝试分配了超出 40 个 2Mib 的 huge page(总共就是 80 Mib), 这个分配就会失败。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 用户并不能过量使用 &lt;code&gt;hugepages-*&lt;/code&gt; 资源。 这与 &lt;code&gt;memory&lt;/code&gt; 与 &lt;code&gt;cpu&lt;/code&gt; 资源不同。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;CPU 和 内存都可以被认为是 &lt;em&gt;计算资源&lt;/em&gt;, 或者直接称为 &lt;em&gt;资源&lt;/em&gt;。 计算资源作为可以请求，分配，和使用
的可量化资源。 他们与
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/kubernetes-api/&#34;&gt;API 资源&lt;/a&gt; 不同的。 API 资源，如
Pod 和
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/service/&#34;&gt;Services&lt;/a&gt;
是可以通过 k8s API 服务读取和修改的对象。&lt;/p&gt;
&lt;!--
## Resource requests and limits of Pod and Container

Each Container of a Pod can specify one or more of the following:

* `spec.containers[].resources.limits.cpu`
* `spec.containers[].resources.limits.memory`
* `spec.containers[].resources.limits.hugepages-&lt;size&gt;`
* `spec.containers[].resources.requests.cpu`
* `spec.containers[].resources.requests.memory`
* `spec.containers[].resources.requests.hugepages-&lt;size&gt;`

Although requests and limits can only be specified on individual Containers, it
is convenient to talk about Pod resource requests and limits. A
*Pod resource request/limit* for a particular resource type is the sum of the
resource requests/limits of that type for each Container in the Pod.
--&gt;
&lt;h2 id=&#34;resource-requests-and-limits-of-pod-and-container&#34;&gt;Pod 和 容器对资源的请求和限制&lt;/h2&gt;
&lt;p&gt;一个 Pod 中的每个容器都可以指定以下配置中的一个或多个:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;spec.containers[].resources.limits.cpu&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spec.containers[].resources.limits.memory&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spec.containers[].resources.limits.hugepages-&amp;lt;size&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spec.containers[].resources.requests.cpu&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spec.containers[].resources.requests.memory&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spec.containers[].resources.requests.hugepages-&amp;lt;size&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;尽管资源请求和限制只能被设置在独立的容器上，对 Pod 资源的请求和限制也是很方便的。
对于一个特定类别的 &lt;em&gt;Pod 资源请求/限制&lt;/em&gt;就是 Pod 中所有容器该类型的资源 请求/限制 的总和。&lt;/p&gt;
&lt;!--
## Resource units in Kubernetes

### Meaning of CPU

Limits and requests for CPU resources are measured in *cpu* units.
One cpu, in Kubernetes, is equivalent to **1 vCPU/Core** for cloud providers and **1 hyperthread** on bare-metal Intel processors.

Fractional requests are allowed. A Container with
`spec.containers[].resources.requests.cpu` of `0.5` is guaranteed half as much
CPU as one that asks for 1 CPU. The expression `0.1` is equivalent to the
expression `100m`, which can be read as &#34;one hundred millicpu&#34;. Some people say
&#34;one hundred millicores&#34;, and this is understood to mean the same thing. A
request with a decimal point, like `0.1`, is converted to `100m` by the API, and
precision finer than `1m` is not allowed. For this reason, the form `100m` might
be preferred.

CPU is always requested as an absolute quantity, never as a relative quantity;
0.1 is the same amount of CPU on a single-core, dual-core, or 48-core machine.
 --&gt;
&lt;h2 id=&#34;resource-units-in-kubernetes&#34;&gt;k8s 中的资源单元&lt;/h2&gt;
&lt;h3 id=&#34;meaning-of-cpu&#34;&gt;CPU 的含义&lt;/h3&gt;
&lt;p&gt;对 CPU 资源的请求和限制是以 &lt;em&gt;cpu&lt;/em&gt; 的单元来计量的。在 k8s 中 1 个单位的 CPU， 与其等效的是
云提供商的 &lt;strong&gt;1 个核心(vCPU/Core)&lt;/strong&gt; 和 是裸金属的 Intel 处理器的 &lt;strong&gt;1 个超线程(hyperthread)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;允许使用小数的请求。 一个容器中如果设置 &lt;code&gt;spec.containers[].resources.requests.cpu&lt;/code&gt; 为
&lt;code&gt;0.5&lt;/code&gt; 就表示它至少要保证提供给容器二分之一个 CPU 的资源。 &lt;code&gt;0.1&lt;/code&gt; 等同与 &lt;code&gt;100m&lt;/code&gt;， 可以被读作
&amp;ldquo;一百微 CPU&amp;rdquo;. 也有人说的是 &amp;ldquo;一百微核心&amp;rdquo;，只要知道这说的是一个意思就行。 配置中如果设置为像
&lt;code&gt;0.1&lt;/code&gt; 这样的小数，会被 API 转化为 &lt;code&gt;100m&lt;/code&gt;，不能设置比 &lt;code&gt;1m&lt;/code&gt; 更小的粒度。所以 &lt;code&gt;100m&lt;/code&gt; 这种格式
更合用。&lt;/p&gt;
&lt;p&gt;CPU 始终是以绝对数量请求的，绝不是相对数量； 0.1 在单核，双核，48 核的机器表示是一样的数量。&lt;/p&gt;
&lt;!--
### Meaning of memory

Limits and requests for `memory` are measured in bytes. You can express memory as
a plain integer or as a fixed-point number using one of these suffixes:
E, P, T, G, M, K. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,
Mi, Ki. For example, the following represent roughly the same value:

```shell
128974848, 129e6, 129M, 123Mi
```

Here&#39;s an example.
The following Pod has two Containers. Each Container has a request of 0.25 cpu
and 64MiB (2&lt;sup&gt;26&lt;/sup&gt; bytes) of memory. Each Container has a limit of 0.5
cpu and 128MiB of memory. You can say the Pod has a request of 0.5 cpu and 128
MiB of memory, and a limit of 1 cpu and 256MiB of memory.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        memory: &#34;64Mi&#34;
        cpu: &#34;250m&#34;
      limits:
        memory: &#34;128Mi&#34;
        cpu: &#34;500m&#34;
  - name: log-aggregator
    image: images.my-company.example/log-aggregator:v6
    resources:
      requests:
        memory: &#34;64Mi&#34;
        cpu: &#34;250m&#34;
      limits:
        memory: &#34;128Mi&#34;
        cpu: &#34;500m&#34;
```
 --&gt;
&lt;h3 id=&#34;meaning-of-memory&#34;&gt;内存的含义&lt;/h3&gt;
&lt;p&gt;对 &lt;code&gt;memory&lt;/code&gt; 的请求和限制是字节来计量的。 可以直接以整数或小数加以下后缀中的一个: E, P, T, G, M, K.
也可以使用 2 的幂的计量单位: Ei, Pi, Ti, Gi, Mi, Ki. 例如，下面几个值的值大约是相等的:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;128974848, 129e6, 129M, 123Mi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;下面是一个例子。下面这个 Pod 中有两个容器。 每个容器请求 0.25 CPU 和 64MiB (2&lt;sup&gt;26&lt;/sup&gt; 字节)内存。
每个容器限制 0.5 CPU 和 128MiB 内存。 这样就可以说这个 Pod 请求了 0.5 CPU 和 128 MiB 内存，
限制为 1 CPU 和 256MiB 内存。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;app&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;images.my-company.example/app:v4&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;memory&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;64Mi&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;cpu&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;250m&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;limits&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;memory&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;128Mi&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;cpu&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;500m&amp;#34;&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;log-aggregator&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;images.my-company.example/log-aggregator:v6&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;memory&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;64Mi&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;cpu&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;250m&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;limits&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;memory&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;128Mi&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;cpu&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;500m&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## How Pods with resource requests are scheduled

When you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum capacity for each of the resource types: the
amount of CPU and memory it can provide for Pods. The scheduler ensures that,
for each resource type, the sum of the resource requests of the scheduled
Containers is less than the capacity of the node. Note that although actual memory
or CPU resource usage on nodes is very low, the scheduler still refuses to place
a Pod on a node if the capacity check fails. This protects against a resource
shortage on a node when resource usage later increases, for example, during a
daily peak in request rate.
 --&gt;
&lt;h2 id=&#34;how-pods-with-resource-requests-are-scheduled&#34;&gt;带有资源请求的 Pod 是怎么调度的&lt;/h2&gt;
&lt;p&gt;当用户创建一个 Pod 时， k8s 调度器会为 Pod 选择一个节点让它在上面运行。 每个节点都有对每个资源
类型的最大容量: 可以供给 Pod 运行的 CPU 的数量和内存数量。调度器会确保被调度的容器所请求的
各种资源的总和要小于节点对应资源的容量。 即便节点上实际内存或 CPU 资源都很低，调度器依然会在容量
检查失败后拒绝将 Pod 放在这个节点上。 这是为了防止后续资源使用增加而导致资源短缺，例如，每天的
请求峰值。&lt;/p&gt;
&lt;!--
## How Pods with resource limits are run

When the kubelet starts a Container of a Pod, it passes the CPU and memory limits
to the container runtime.

When using Docker:

- The `spec.containers[].resources.requests.cpu` is converted to its core value,
  which is potentially fractional, and multiplied by 1024. The greater of this number
  or 2 is used as the value of the
  [`--cpu-shares`](https://docs.docker.com/engine/reference/run/#cpu-share-constraint)
  flag in the `docker run` command.

- The `spec.containers[].resources.limits.cpu` is converted to its millicore value and
  multiplied by 100. The resulting value is the total amount of CPU time that a container can use
  every 100ms. A container cannot use more than its share of CPU time during this interval.

  &lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The default quota period is 100ms. The minimum resolution of CPU quota is 1ms.&lt;/div&gt;
&lt;/blockquote&gt;


- The `spec.containers[].resources.limits.memory` is converted to an integer, and
  used as the value of the
  [`--memory`](https://docs.docker.com/engine/reference/run/#/user-memory-constraints)
  flag in the `docker run` command.

If a Container exceeds its memory limit, it might be terminated. If it is
restartable, the kubelet will restart it, as with any other type of runtime
failure.

If a Container exceeds its memory request, it is likely that its Pod will
be evicted whenever the node runs out of memory.

A Container might or might not be allowed to exceed its CPU limit for extended
periods of time. However, it will not be killed for excessive CPU usage.

To determine whether a Container cannot be scheduled or is being killed due to
resource limits, see the
[Troubleshooting](#troubleshooting) section.
 --&gt;
&lt;h2 id=&#34;how-pods-with-resource-limits-are-run&#34;&gt;带有资源限制的 Pod 是怎么运行的&lt;/h2&gt;
&lt;p&gt;当 kubelet 为一个 Pod 启动一个容器时，它会给容器运行时传递 CPU 和 内存的限制。&lt;/p&gt;
&lt;p&gt;在使用 Docker 时:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;spec.containers[].resources.requests.cpu&lt;/code&gt; 会被转化为核心值，一般来说它很可能是个小数，
并乘上 1024，得出的结果与 2 相比比较大的一个会作为 &lt;code&gt;docker run&lt;/code&gt; 命令的
&lt;a href=&#34;https://docs.docker.com/engine/reference/run/#cpu-share-constraint&#34;&gt;&lt;code&gt;--cpu-shares&lt;/code&gt;&lt;/a&gt;
的值。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;spec.containers[].resources.limits.cpu&lt;/code&gt; 会被转化为微核心值并乘以 100. 得出的结果就是
每 100ms 中这个容器可以使用的 CPU 时间。容器在每个时间段不能使用超出其限制的 CPU 时间&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 默认的配额时间段是 100ms， CPU 配置的最小粒度是 1ms&lt;/div&gt;
&lt;/blockquote&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;spec.containers[].resources.limits.memory&lt;/code&gt; 会被转化为一个整数，并作为 &lt;code&gt;docker run&lt;/code&gt;
命令的
&lt;a href=&#34;https://docs.docker.com/engine/reference/run/#/user-memory-constraints&#34;&gt;&lt;code&gt;--memory&lt;/code&gt;&lt;/a&gt;
标记的值。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果容器超出了内存限制，它就可能被终止。 如果容器是可以重启的，kubelet 就会把它重启了，就像任意
其它类型的运行失败一样。&lt;/p&gt;
&lt;p&gt;如果一个容器超出了其请求的内存，它会在节点内存耗尽时被踢出去。&lt;/p&gt;
&lt;p&gt;一个容器可能允许也可能不允许超出 CPU 使用时间限制。 但是它不会因为 CPU 使用超限而被杀掉。&lt;/p&gt;
&lt;p&gt;决定容器不能被调度或因资源限制而被杀掉的因素见 &lt;a href=&#34;#troubleshooting&#34;&gt;故障检查&lt;/a&gt; 章节&lt;/p&gt;
&lt;!--
### Monitoring compute &amp; memory resource usage

The resource usage of a Pod is reported as part of the Pod status.

If optional [tools for monitoring](/docs/tasks/debug-application-cluster/resource-usage-monitoring/)
are available in your cluster, then Pod resource usage can be retrieved either
from the [Metrics API](/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#the-metrics-api)
directly or from your monitoring tools.
 --&gt;
&lt;h3 id=&#34;monitoring-compute-memory-resource-usage&#34;&gt;监控计算和内存资源使用&lt;/h3&gt;
&lt;p&gt;Pod 所使用的资源会作为 Pod 状态报告的一部分出现。&lt;/p&gt;
&lt;p&gt;如果集群中有可选的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/debug-application-cluster/resource-usage-monitoring/&#34;&gt;监控工具&lt;/a&gt;，
Pod 资源使用可以通过
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#the-metrics-api&#34;&gt;Metrics API&lt;/a&gt;
或直接通过监控工作中的一种中获取。&lt;/p&gt;
&lt;!-- feature gate LocalStorageCapacityIsolation --&gt;
&lt;!--
## Local ephemeral storage







&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.10 [beta]&lt;/code&gt;
&lt;/div&gt;



Nodes have local ephemeral storage, backed by
locally-attached writeable devices or, sometimes, by RAM.
&#34;Ephemeral&#34; means that there is no long-term guarantee about durability.

Pods use ephemeral local storage for scratch space, caching, and for logs.
The kubelet can provide scratch space to Pods using local ephemeral storage to
mount [`emptyDir`](/docs/concepts/storage/volumes/#emptydir)
 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;volumes&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt; into containers.

The kubelet also uses this kind of storage to hold
[node-level container logs](/docs/concepts/cluster-administration/logging/#logging-at-the-node-level),
container images, and the writable layers of running containers.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; If a node fails, the data in its ephemeral storage can be lost.&lt;br&gt;
Your applications cannot expect any performance SLAs (disk IOPS for example)
from local ephemeral storage.&lt;/div&gt;
&lt;/blockquote&gt;


As a beta feature, Kubernetes lets you track, reserve and limit the amount
of ephemeral local storage a Pod can consume.
 --&gt;
&lt;h2 id=&#34;local-ephemeral-storage&#34;&gt;本地临时存储&lt;/h2&gt;
&lt;!-- feature gate LocalStorageCapacityIsolation --&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.10 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;节点有本地临时存储，这些存储由本地挂载可写设备或有时候是 RAM 来提供。&amp;ldquo;临时&amp;rdquo; 的意思就是对
数据的持久性没有长期保证。&lt;/p&gt;
&lt;p&gt;Pod 可以使用临时本地存储作为暂存空间，缓存或日志。 kubelet 可以使用临时本地存储提供暂存空间
来挂载
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/#emptydir&#34;&gt;&lt;code&gt;emptyDir&lt;/code&gt;&lt;/a&gt;
到容器中的
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;卷(Volume)&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;kubelet 还可以使用这种类型的存储来放置
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/cluster-administration/logging/#logging-at-the-node-level&#34;&gt;节点级别的容器日志&lt;/a&gt;,
容器镜像，运行容器的可写层。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 如果节点挂了，临时存储中的数据就可能丢失。并且应用并不能对本地临时存储有任何性能 SLA(例如,磁盘 IOPS)
有任何要求&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;作为一个 beta 特性， k8s 可以让用户跟踪，保留，限制一个 Pod 可以使用的临时本地存储。&lt;/p&gt;
&lt;h3 id=&#34;configurations-for-local-ephemeral-storage&#34;&gt;本地临时存储的配置&lt;/h3&gt;
&lt;p&gt;k8s 支持两种在节点上配置本地临时存储的方式:&lt;/p&gt;
&lt;p&gt;&lt;ul class=&#34;nav nav-tabs&#34; id=&#34;local_storage_configurations&#34; role=&#34;tablist&#34;&gt;&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link active&#34; href=&#34;#local_storage_configurations-0&#34; role=&#34;tab&#34; aria-controls=&#34;local_storage_configurations-0&#34; aria-selected=&#34;true&#34;&gt;单文件系统&lt;/a&gt;&lt;/li&gt;
	  
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#local_storage_configurations-1&#34; role=&#34;tab&#34; aria-controls=&#34;local_storage_configurations-1&#34;&gt;双文件系统&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;div class=&#34;tab-content&#34; id=&#34;local_storage_configurations&#34;&gt;&lt;div id=&#34;local_storage_configurations-0&#34; class=&#34;tab-pane show active&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;local_storage_configurations-0&#34;&gt;

&lt;p&gt;&lt;p&gt;在这种配置中， 用户可以将所有不同类型的临时本地数据(&lt;code&gt;emptyDir&lt;/code&gt; 卷，可写层，容器镜像，日志)
都放到一个文件系统中。 最有效配置 kubelet 的含义就是将这个文件系统用于 k8s (kubelet)的数据。&lt;/p&gt;
&lt;p&gt;kubelet 还会将
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/cluster-administration/logging/#logging-at-the-node-level&#34;&gt;节点级别的容器日志&lt;/a&gt;
并将其类似临时本地存储处理。&lt;/p&gt;
&lt;p&gt;kubelet 会将日志文件写入它配置的日志目录(默认: &lt;code&gt;/var/log&lt;/code&gt;); 还有一个用于其它本地数据存储的
基础目录(默认为 &lt;code&gt;/var/lib/kubelet&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;通常情况下， &lt;code&gt;/var/lib/kubelet&lt;/code&gt; 和 &lt;code&gt;/var/log&lt;/code&gt; 都是在系统根文件系统上，kubelet 在设置上
也就是这个结构的。&lt;/p&gt;
&lt;p&gt;节点上可以有任意数量的其它文件系统，不是给 k8s 用的，随便怎么用都可以。&lt;/p&gt;
&lt;/div&gt;
  &lt;div id=&#34;local_storage_configurations-1&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;local_storage_configurations-1&#34;&gt;

&lt;p&gt;&lt;p&gt;在节点上有一个文件系统被用来入那些来自运行 Pod 的: 日志，&lt;code&gt;emptyDir&lt;/code&gt; 卷的临时数据。也可以使用
这个文件系统来存其它数据(例如: 与 k8s 不相关的系统日志)；甚至也可以是根文件系统。&lt;/p&gt;
&lt;p&gt;kubelet 也会将
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/cluster-administration/logging/#logging-at-the-node-level&#34;&gt;节点级别的容器日志&lt;/a&gt;
写入到第一个文件系统，并将其当作和临时本地存储差不多的东西。&lt;/p&gt;
&lt;p&gt;也可以使用另一个后端为另一个逻辑存储设备的文件系统。 在这个配置中， 给 kubelet 放置容器镜像层
和可写层的目录都是在第二个文件系统上。&lt;/p&gt;
&lt;p&gt;第一个文件系统不包含任何镜像层和可写层。&lt;/p&gt;
&lt;p&gt;节点上可以有任意数量的其它文件系统，不是给 k8s 用的，随便怎么用都可以。&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;kubelet 可以测量它自己用了多少本地存储。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;启用了 &lt;code&gt;LocalStorageCapacityIsolation&lt;/code&gt;
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;
(该特性默认是开启的)，&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;需要设置节点使用一种支持的本地临时存储配置。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果有不同的配置， kubelet 就不能对临时本地存储执行资源限制。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; kubelet 将 &lt;code&gt;tmpfs&lt;/code&gt; &lt;code&gt;emptyDir&lt;/code&gt; 卷作为容器使用的内存来跟踪，而不是被当作本地临时存储。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;setting-requests-and-limits-for-local-ephemeral-storage&#34;&gt;为本地临时存储设置请求和限制&lt;/h3&gt;
&lt;p&gt;用户可以使用 &lt;em&gt;临时存储(ephemeral-storage)&lt;/em&gt; 来管理本地临时存储。 Pod 中的每个容器都可以指定
以下配置中的一个或多个:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;spec.containers[].resources.limits.ephemeral-storage&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spec.containers[].resources.requests.ephemeral-storage&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;ephemeral-storage&lt;/code&gt; 的请求和限制是以字节来计量的。 可以直接以整数或小数加以下后缀中的一个来表达数量:
E, P, T, G, M, K. 也可以使用2次幂的约等单位:Ei, Pi, Ti, Gi, Mi, Ki. 例如，下面这个值
大约都是相等的:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;128974848, 129e6, 129M, 123Mi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在下面的示例中， 这个 Pod 中有两个容器。 每个容器请求了 2GiB 本地临时存储。 每个容器还有一个
4GiB 本地临时存储限制。 因此， Pod 就有一个 4 4GiB 的本地临时存储的请求和一个 8GiB 的本地临时存储限制。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;app&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;images.my-company.example/app:v4&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;ephemeral-storage&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2Gi&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;limits&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;ephemeral-storage&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;4Gi&amp;#34;&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;log-aggregator&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;images.my-company.example/log-aggregator:v6&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;ephemeral-storage&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2Gi&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;limits&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;ephemeral-storage&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;4Gi&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### How Pods with ephemeral-storage requests are scheduled

When you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum amount of local ephemeral storage it can provide for Pods. For more information, see [Node Allocatable](/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable).

The scheduler ensures that the sum of the resource requests of the scheduled Containers is less than the capacity of the node.
 --&gt;
&lt;h3 id=&#34;how-pods-with-ephemeral-storage-requests-are-scheduled&#34;&gt;包含临时存储请求的 Pod 是怎么调度的&lt;/h3&gt;
&lt;p&gt;当创建一个 Pod 时，k8s 调度器会为 Pod 选择一个节点让它在上面运行。 每个节点上都有一个他可以
为 Pod 提供的最大数量的本地临时存储。 更多信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable&#34;&gt;节点可分配量&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;调度器确保其调度的容器所请求的资源是小于节点对应资源的容量。&lt;/p&gt;
&lt;!--
### Ephemeral storage consumption management {#resource-emphemeralstorage-consumption}

If the kubelet is managing local ephemeral storage as a resource, then the
kubelet measures storage use in:

- `emptyDir` volumes, except _tmpfs_ `emptyDir` volumes
- directories holding node-level logs
- writeable container layers

If a Pod is using more ephemeral storage than you allow it to, the kubelet
sets an eviction signal that triggers Pod eviction.

For container-level isolation, if a Container&#39;s writable layer and log
usage exceeds its storage limit, the kubelet marks the Pod for eviction.

For pod-level isolation the kubelet works out an overall Pod storage limit by
summing the limits for the containers in that Pod. In this case, if the sum of
the local ephemeral storage usage from all containers and also the Pod&#39;s `emptyDir`
volumes exceeds the overall Pod storage limit, then the kubelet also marks the Pod
for eviction.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; &lt;p&gt;If the kubelet is not measuring local ephemeral storage, then a Pod
that exceeds its local storage limit will not be evicted for breaching
local storage resource limits.&lt;/p&gt;
&lt;p&gt;However, if the filesystem space for writeable container layers, node-level logs,
or &lt;code&gt;emptyDir&lt;/code&gt; volumes falls low, the node
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/&#39; target=&#39;_blank&#39;&gt;taints&lt;span class=&#39;tooltip-text&#39;&gt;A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups.&lt;/span&gt;
&lt;/a&gt; itself as short on local storage
and this taint triggers eviction for any Pods that don&amp;rsquo;t specifically tolerate the taint.&lt;/p&gt;
&lt;p&gt;See the supported &lt;a href=&#34;#configurations-for-local-ephemeral-storage&#34;&gt;configurations&lt;/a&gt;
for ephemeral local storage.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;


The kubelet supports different ways to measure Pod storage use:

&lt;ul class=&#34;nav nav-tabs&#34; id=&#34;resource-emphemeralstorage-measurement&#34; role=&#34;tablist&#34;&gt;&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link active&#34; href=&#34;#resource-emphemeralstorage-measurement-0&#34; role=&#34;tab&#34; aria-controls=&#34;resource-emphemeralstorage-measurement-0&#34; aria-selected=&#34;true&#34;&gt;Periodic scanning&lt;/a&gt;&lt;/li&gt;
	  
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#resource-emphemeralstorage-measurement-1&#34; role=&#34;tab&#34; aria-controls=&#34;resource-emphemeralstorage-measurement-1&#34;&gt;Filesystem project quota&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;div class=&#34;tab-content&#34; id=&#34;resource-emphemeralstorage-measurement&#34;&gt;&lt;div id=&#34;resource-emphemeralstorage-measurement-0&#34; class=&#34;tab-pane show active&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;resource-emphemeralstorage-measurement-0&#34;&gt;

&lt;p&gt;&lt;p&gt;The kubelet performs regular, scheduled checks that scan each
&lt;code&gt;emptyDir&lt;/code&gt; volume, container log directory, and writeable container layer.&lt;/p&gt;
&lt;p&gt;The scan measures how much space is used.&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;In this mode, the kubelet does not track open file descriptors
for deleted files.&lt;/p&gt;
&lt;p&gt;If you (or a container) create a file inside an &lt;code&gt;emptyDir&lt;/code&gt; volume,
something then opens that file, and you delete the file while it is
still open, then the inode for the deleted file stays until you close
that file but the kubelet does not categorize the space as in use.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
  &lt;div id=&#34;resource-emphemeralstorage-measurement-1&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;resource-emphemeralstorage-measurement-1&#34;&gt;

&lt;p&gt;&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.15 [alpha]&lt;/code&gt;
&lt;/div&gt;
&lt;p&gt;Project quotas are an operating-system level feature for managing
storage use on filesystems. With Kubernetes, you can enable project
quotas for monitoring storage use. Make sure that the filesystem
backing the &lt;code&gt;emptyDir&lt;/code&gt; volumes, on the node, provides project quota support.
For example, XFS and ext4fs offer project quotas.&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Project quotas let you monitor storage use; they do not enforce limits.&lt;/div&gt;
&lt;/blockquote&gt;
&lt;p&gt;Kubernetes uses project IDs starting from &lt;code&gt;1048576&lt;/code&gt;. The IDs in use are
registered in &lt;code&gt;/etc/projects&lt;/code&gt; and &lt;code&gt;/etc/projid&lt;/code&gt;. If project IDs in
this range are used for other purposes on the system, those project
IDs must be registered in &lt;code&gt;/etc/projects&lt;/code&gt; and &lt;code&gt;/etc/projid&lt;/code&gt; so that
Kubernetes does not use them.&lt;/p&gt;
&lt;p&gt;Quotas are faster and more accurate than directory scanning. When a
directory is assigned to a project, all files created under a
directory are created in that project, and the kernel merely has to
keep track of how many blocks are in use by files in that project.&lt;br&gt;
If a file is created and deleted, but has an open file descriptor,
it continues to consume space. Quota tracking records that space accurately
whereas directory scans overlook the storage used by deleted files.&lt;/p&gt;
&lt;p&gt;If you want to use project quotas, you should:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Enable the &lt;code&gt;LocalStorageCapacityIsolationFSQuotaMonitoring=true&lt;/code&gt;
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;feature gate&lt;/a&gt;
in the kubelet configuration.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ensure that the root filesystem (or optional runtime filesystem)
has project quotas enabled. All XFS filesystems support project quotas.
For ext4 filesystems, you need to enable the project quota tracking feature
while the filesystem is not mounted.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# For ext4, with /dev/block-device not mounted&lt;/span&gt;
sudo tune2fs -O project -Q prjquota /dev/block-device
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ensure that the root filesystem (or optional runtime filesystem) is
mounted with project quotas enabled. For both XFS and ext4fs, the
mount option is named &lt;code&gt;prjquota&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;/div&gt;

 --&gt;
&lt;h3 id=&#34;resource-emphemeralstorage-consumption&#34;&gt;临时存储用量管理&lt;/h3&gt;
&lt;p&gt;如果使用 kubelet 管理本地临时存储作为资源，则 kubelet 使用下面这些的计算存储量:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;除了 &lt;em&gt;tmpfs&lt;/em&gt; 外的 &lt;code&gt;emptyDir&lt;/code&gt; 卷&lt;/li&gt;
&lt;li&gt;存放节点级别日志的目录&lt;/li&gt;
&lt;li&gt;容器可写层&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果 Pod 使用了比允许更多多的临时存储， kubelet 会设置一个驱逐信号来触发 Pod 的驱逐。&lt;/p&gt;
&lt;p&gt;对于容器级别隔离， 如果一个容器的可写层和日志使用超过它的存储限制，kubelet 会将该 Pod 标记为
驱逐状态。&lt;/p&gt;
&lt;p&gt;对于 Pod 级别隔离， kubelet 会将 Pod 中所以容器的限额总数作为 Pod 的存储限额。 在这种情况下
，如果所有容器使用的本地临时存储和 Pod 的 &lt;code&gt;emptyDir&lt;/code&gt; 卷超出 Pod 存储总限制，则 kubelet 也
将该 Pod 标记为驱逐状态。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; &lt;p&gt;如果 kubelet 没有测量本地临时存储，则那些超出它本地存储限制的 Pod 不会因为违反本地存储资源限制
而被驱逐&lt;/p&gt;
&lt;p&gt;但是， 如果文件系统中用于可写容器层，节点级别日志或 &lt;code&gt;emptyDir&lt;/code&gt; 卷空间不足， 节点上也有本地存储的
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/&#39; target=&#39;_blank&#39;&gt;Taint&lt;span class=&#39;tooltip-text&#39;&gt;A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups.&lt;/span&gt;
&lt;/a&gt;
这个毒点(Taint)会触发对所有没指定能够忍耐该毒点(taint) Pod 驱逐。&lt;/p&gt;
&lt;p&gt;临时本地存储支持的配置，见
&lt;a href=&#34;#configurations-for-local-ephemeral-storage&#34;&gt;配置&lt;/a&gt;章节&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;kubelet 支持不同的方式测量 Pod 存储用量:
&lt;ul class=&#34;nav nav-tabs&#34; id=&#34;resource-emphemeralstorage-measurement&#34; role=&#34;tablist&#34;&gt;&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link active&#34; href=&#34;#resource-emphemeralstorage-measurement-0&#34; role=&#34;tab&#34; aria-controls=&#34;resource-emphemeralstorage-measurement-0&#34; aria-selected=&#34;true&#34;&gt;定期扫描&lt;/a&gt;&lt;/li&gt;
	  
		&lt;li class=&#34;nav-item&#34;&gt;&lt;a data-toggle=&#34;tab&#34; class=&#34;nav-link&#34; href=&#34;#resource-emphemeralstorage-measurement-1&#34; role=&#34;tab&#34; aria-controls=&#34;resource-emphemeralstorage-measurement-1&#34;&gt;文件系统项目配额&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;div class=&#34;tab-content&#34; id=&#34;resource-emphemeralstorage-measurement&#34;&gt;&lt;div id=&#34;resource-emphemeralstorage-measurement-0&#34; class=&#34;tab-pane show active&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;resource-emphemeralstorage-measurement-0&#34;&gt;

&lt;p&gt;&lt;p&gt;kubelet 执行常规，定时检查每个 &lt;code&gt;emptyDir&lt;/code&gt; 卷，容器日志目录，可写容器层。&lt;/p&gt;
&lt;p&gt;这种扫描测量使用了多少空间。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;在这种模式下， kubelet 不会跟踪删除文件的打开文件描述符。&lt;/p&gt;
&lt;p&gt;如果用户(或一个容器)在一个 &lt;code&gt;emptyDir&lt;/code&gt; 卷中创建了一个文件，再有其它程序再打开了这个文件，又在
它还在打开状态时把它删除了，在它被关闭之前这个被删除文件的索引节点还在但 kubelet 不会计算这个
文件使用的空间。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
  &lt;div id=&#34;resource-emphemeralstorage-measurement-1&#34; class=&#34;tab-pane&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;resource-emphemeralstorage-measurement-1&#34;&gt;

&lt;p&gt;&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.15 [alpha]&lt;/code&gt;
&lt;/div&gt;
&lt;p&gt;项目配额是一个操作系统级别的特性，用来管理文件系统上在存储使用。 通过 k8s 用户可以启用项目配额
来监控存储用量。 要确保在节点上以 &lt;code&gt;emptyDir&lt;/code&gt; 卷为后端的文件系统支持提供项目配额。例如， XFS
和 ext4fs 提供项目配额。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 项目配额让用户监控存储用量；但不会强制限制用量。&lt;/div&gt;
&lt;/blockquote&gt;
&lt;p&gt;k8s 使用的项目 ID 从 &lt;code&gt;1048576&lt;/code&gt; 开始。 被使用的 ID 被注册在  &lt;code&gt;/etc/projects&lt;/code&gt; 和 &lt;code&gt;/etc/projid&lt;/code&gt;.
如果这个范围的项目 ID 被用于系统其它目的， 这些项目 ID 必要注册到 &lt;code&gt;/etc/projects&lt;/code&gt; 和 &lt;code&gt;/etc/projid&lt;/code&gt;
这样 k8s 才不会使用它们。&lt;/p&gt;
&lt;p&gt;配额比目录扫描更快和更精确。当一个目录被分配给一个项目时，所以在那个目录中创建的文件就是在那个
项目中创建，内核只需要保持跟踪有多个块被这个项目中的文件所使用。 如果有一个文件创建然后删除，
但有一个打开文件描述符， 它继续消耗空间。 配置跟踪记录精确空间，通过被删除文件所使用的存储。&lt;/p&gt;
&lt;p&gt;如果想要使用项目配置，可以:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在 kubelet 配置中启用 &lt;code&gt;LocalStorageCapacityIsolationFSQuotaMonitoring=true&lt;/code&gt;
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;要确保根文件系统(或可选运行时文件系统)中启动了项目配额。所有 XFS 文件系统都支持项目配额。
对于 ext4 文件系统，需要在文件系统挂载之前启用项目配额跟踪特性。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# For ext4, with /dev/block-device not mounted&lt;/span&gt;
sudo tune2fs -O project -Q prjquota /dev/block-device
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;要确保根文件系统(或可选运行时文件系统)以启用项目配置挂载。 对于 XFS 和 ext4fs 挂载选项名
都叫 &lt;code&gt;prjquota&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;/p&gt;
&lt;!--
## Extended resources

Extended resources are fully-qualified resource names outside the
`kubernetes.io` domain. They allow cluster operators to advertise and users to
consume the non-Kubernetes-built-in resources.

There are two steps required to use Extended Resources. First, the cluster
operator must advertise an Extended Resource. Second, users must request the
Extended Resource in Pods.
 --&gt;
&lt;h2 id=&#34;extended-resources&#34;&gt;扩展资源&lt;/h2&gt;
&lt;p&gt;扩展资源是 &lt;code&gt;kubernetes.io&lt;/code&gt; 域名外的全限定资源名。 它们允许集群管理员配置并且用户消费非 k8s
内置资源。&lt;/p&gt;
&lt;p&gt;要使用扩展资源需要以下两个步骤。 第一步， 集群管理必须指定一个扩展资源。第二步，用户必须在 Pod
中请求扩展资源。&lt;/p&gt;
&lt;!--
### Managing extended resources

#### Node-level extended resources

Node-level extended resources are tied to nodes.
 --&gt;
&lt;h3 id=&#34;managing-extended-resources&#34;&gt;管理扩展资源&lt;/h3&gt;
&lt;h4 id=&#34;node-level-extended-resources&#34;&gt;节点级别扩展资源&lt;/h4&gt;
&lt;p&gt;节点级别扩展资源与节点是绑在一起的&lt;/p&gt;
&lt;!--
##### Device plugin managed resources
See [Device
Plugin](/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)
for how to advertise device plugin managed resources on each node.
 --&gt;
&lt;h5 id=&#34;device-plugin-managed-resources&#34;&gt;设备插件管理的资源&lt;/h5&gt;
&lt;p&gt;怎么在每个节点上指定设备插件来管理资源，见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/&#34;&gt;设备插件&lt;/a&gt;&lt;/p&gt;
&lt;!--
##### Other resources
To advertise a new node-level extended resource, the cluster operator can
submit a `PATCH` HTTP request to the API server to specify the available
quantity in the `status.capacity` for a node in the cluster. After this
operation, the node&#39;s `status.capacity` will include a new resource. The
`status.allocatable` field is updated automatically with the new resource
asynchronously by the kubelet. Note that because the scheduler uses the	node
`status.allocatable` value when evaluating Pod fitness, there may be a short
delay between patching the node capacity with a new resource and the first Pod
that requests the resource to be scheduled on that node.

**Example:**

Here is an example showing how to use `curl` to form an HTTP request that
advertises five &#34;example.com/foo&#34; resources on node `k8s-node-1` whose master
is `k8s-master`.

```shell
curl --header &#34;Content-Type: application/json-patch+json&#34; \
--request PATCH \
--data &#39;[{&#34;op&#34;: &#34;add&#34;, &#34;path&#34;: &#34;/status/capacity/example.com~1foo&#34;, &#34;value&#34;: &#34;5&#34;}]&#39; \
http://k8s-master:8080/api/v1/nodes/k8s-node-1/status
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; In the preceding request, &lt;code&gt;~1&lt;/code&gt; is the encoding for the character &lt;code&gt;/&lt;/code&gt;
in the patch path. The operation path value in JSON-Patch is interpreted as a
JSON-Pointer. For more details, see
&lt;a href=&#34;https://tools.ietf.org/html/rfc6901#section-3&#34;&gt;IETF RFC 6901, section 3&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h5 id=&#34;other-resources&#34;&gt;其它资源&lt;/h5&gt;
&lt;p&gt;为添加一个新的节点级别的扩展资源， 集群管理可以通过提交一个 &lt;code&gt;PATCH&lt;/code&gt; HTTP 请求到 API 服务来
指定集群中这个节点上 &lt;code&gt;status.capacity&lt;/code&gt; 的可用数量。 在这个操作之后， 节点的 &lt;code&gt;status.capacity&lt;/code&gt;
会包含一个新的资源。 &lt;code&gt;status.allocatable&lt;/code&gt; 字段会被 kubelet 在包含新资源异步更新。 因为
调度使用节点上 &lt;code&gt;status.allocatable&lt;/code&gt; 值来检测 Pod 适配性， 在因新资源而变更的节点容量和
调度到这个节点上请求这个资源的第一个 Pod 之间会有一个短暂的延迟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;示例:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;下面的这个示例中显示怎么使用 &lt;code&gt;curl&lt;/code&gt; 来执行一个 HTTP 请求在主控节点为 &lt;code&gt;k8s-master&lt;/code&gt; 名叫
&lt;code&gt;k8s-node-1&lt;/code&gt; 的节点上创建五个 &amp;ldquo;example.com/foo&amp;rdquo; 资源&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl --header &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Content-Type: application/json-patch+json&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--request PATCH &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--data &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;[{&amp;#34;op&amp;#34;: &amp;#34;add&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/status/capacity/example.com~1foo&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;5&amp;#34;}]&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;http://k8s-master:8080/api/v1/nodes/k8s-node-1/status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 在上面的请求中，在 &lt;code&gt;PATCH&lt;/code&gt; 请求路径中 &lt;code&gt;~1&lt;/code&gt; 是对 &lt;code&gt;/&lt;/code&gt; 字符的编码。 JSON-Patch 中的操作路径值
会被翻译为 JSON-Pointer. 详细信息，见
&lt;a href=&#34;https://tools.ietf.org/html/rfc6901#section-3&#34;&gt;IETF RFC 6901, section 3&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
#### Cluster-level extended resources

Cluster-level extended resources are not tied to nodes. They are usually managed
by scheduler extenders, which handle the resource consumption and resource quota.

You can specify the extended resources that are handled by scheduler extenders
in [scheduler policy
configuration](https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/scheduler/api/v1/types.go#L31).

**Example:**

The following configuration for a scheduler policy indicates that the
cluster-level extended resource &#34;example.com/foo&#34; is handled by the scheduler
extender.

- The scheduler sends a Pod to the scheduler extender only if the Pod requests
     &#34;example.com/foo&#34;.
- The `ignoredByScheduler` field specifies that the scheduler does not check
     the &#34;example.com/foo&#34; resource in its `PodFitsResources` predicate.

```json
{
  &#34;kind&#34;: &#34;Policy&#34;,
  &#34;apiVersion&#34;: &#34;v1&#34;,
  &#34;extenders&#34;: [
    {
      &#34;urlPrefix&#34;:&#34;&lt;extender-endpoint&gt;&#34;,
      &#34;bindVerb&#34;: &#34;bind&#34;,
      &#34;managedResources&#34;: [
        {
          &#34;name&#34;: &#34;example.com/foo&#34;,
          &#34;ignoredByScheduler&#34;: true
        }
      ]
    }
  ]
}
```
 --&gt;
&lt;h4 id=&#34;cluster-level-extended-resources&#34;&gt;集群级别的扩展资源&lt;/h4&gt;
&lt;p&gt;集群级别的扩展资源是没有与节点绑定的。 它们通常是被调度扩展来管理的，调度扩展处理资源消耗和资源配额。&lt;/p&gt;
&lt;p&gt;用户可以指定在
&lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/scheduler/api/v1/types.go#L31&#34;&gt;调度策略配置&lt;/a&gt;.
中由调度扩展处理的扩展资源
&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;下面的调度策略配置指示的是由调度扩展处理的集群级别扩展资源 &amp;ldquo;example.com/foo&amp;rdquo;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只有 Pod 请求 &amp;ldquo;example.com/foo&amp;rdquo; 时，调度器才会发送一个 Pod 给调度扩展器&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ignoredByScheduler&lt;/code&gt; 字段不胜感激调度器不检查 &lt;code&gt;PodFitsResources&lt;/code&gt; 中的 &amp;ldquo;example.com/foo&amp;rdquo; 资源。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;{
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;kind&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Policy&amp;#34;&lt;/span&gt;,
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;apiVersion&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;v1&amp;#34;&lt;/span&gt;,
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;extenders&amp;#34;&lt;/span&gt;: [
    {
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;urlPrefix&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;lt;extender-endpoint&amp;gt;&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;bindVerb&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bind&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;managedResources&amp;#34;&lt;/span&gt;: [
        {
          &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;example.com/foo&amp;#34;&lt;/span&gt;,
          &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;ignoredByScheduler&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
        }
      ]
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Consuming extended resources

Users can consume extended resources in Pod specs just like CPU and memory.
The scheduler takes care of the resource accounting so that no more than the
available amount is simultaneously allocated to Pods.

The API server restricts quantities of extended resources to whole numbers.
Examples of _valid_ quantities are `3`, `3000m` and `3Ki`. Examples of
_invalid_ quantities are `0.5` and `1500m`.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Extended resources replace Opaque Integer Resources.
Users can use any domain name prefix other than &lt;code&gt;kubernetes.io&lt;/code&gt; which is reserved.&lt;/div&gt;
&lt;/blockquote&gt;


To consume an extended resource in a Pod, include the resource name as a key
in the `spec.containers[].resources.limits` map in the container spec.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Extended resources cannot be overcommitted, so request and limit
must be equal if both are present in a container spec.&lt;/div&gt;
&lt;/blockquote&gt;


A Pod is scheduled only if all of the resource requests are satisfied, including
CPU, memory and any extended resources. The Pod remains in the `PENDING` state
as long as the resource request cannot be satisfied.

**Example:**

The Pod below requests 2 CPUs and 1 &#34;example.com/foo&#34; (an extended resource).

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: myimage
    resources:
      requests:
        cpu: 2
        example.com/foo: 1
      limits:
        example.com/foo: 1
```
 --&gt;
&lt;h3 id=&#34;consuming-extended-resources&#34;&gt;使用扩展资源&lt;/h3&gt;
&lt;p&gt;用户可以在 Pod 中像 CPU 和 内存一样消费扩展资源。调度器会处理好资源计数，所以不会有超出可用数量的
资源会分配给 Pod.&lt;/p&gt;
&lt;p&gt;API 服务限制扩展资源的数量只能是整数。 &lt;em&gt;有效的&lt;/em&gt; 数量示例有 &lt;code&gt;3&lt;/code&gt;, &lt;code&gt;3000m&lt;/code&gt; 和 &lt;code&gt;3Ki&lt;/code&gt;。 &lt;em&gt;无效的&lt;/em&gt;
数量示例有  &lt;code&gt;0.5&lt;/code&gt;， &lt;code&gt;1500m&lt;/code&gt;。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 扩展资源替换了模糊的整数资源。 用户可以使用被保留的 &lt;code&gt;kubernetes.io&lt;/code&gt; 外的任意域名作为前缀。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;在一个 Pod 中使用一个扩展资源，就是在容器配置的 &lt;code&gt;spec.containers[].resources.limits&lt;/code&gt;
字典中包含资源名称作为健名。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 扩展资源不能超限，所以请求和限制如果在同一个容器中同时配置了，则必须相同。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;一个 Pod 只有在所有的资源请求都满足时才会调度， 包括 CPU，内存和任意扩展资源。 在资源请求被
满足之前 Pod 会卡在 &lt;code&gt;PENDING&lt;/code&gt; 状态。
&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;下面的 Pod 中请求的 2 CPU 和 1 &amp;ldquo;example.com/foo&amp;rdquo;(一个扩展资源)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myimage&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;cpu&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;example.com/foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;limits&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;example.com/foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## PID limiting

Process ID (PID) limits allow for the configuration of a kubelet to limit the number of PIDs that a given Pod can consume. See [Pid Limiting](/docs/concepts/policy/pid-limiting/) for information.
 --&gt;
&lt;h2 id=&#34;pid-limiting&#34;&gt;PID 限制&lt;/h2&gt;
&lt;p&gt;进程 ID (PID) 限制允许对 kubelet 配置让它限制可以给予 Pod 消费的 PID 的数量。 更多信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/policy/pid-limiting/&#34;&gt;PID 限制&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Troubleshooting

### My Pods are pending with event message failedScheduling

If the scheduler cannot find any node where a Pod can fit, the Pod remains
unscheduled until a place can be found. An event is produced each time the
scheduler fails to find a place for the Pod, like this:

```shell
kubectl describe pod frontend | grep -A 3 Events
```
```
Events:
  FirstSeen LastSeen   Count  From          Subobject   PathReason      Message
  36s   5s     6      {scheduler }              FailedScheduling  Failed for reason PodExceedsFreeCPU and possibly others
```

In the preceding example, the Pod named &#34;frontend&#34; fails to be scheduled due to
insufficient CPU resource on the node. Similar error messages can also suggest
failure due to insufficient memory (PodExceedsFreeMemory). In general, if a Pod
is pending with a message of this type, there are several things to try:

- Add more nodes to the cluster.
- Terminate unneeded Pods to make room for pending Pods.
- Check that the Pod is not larger than all the nodes. For example, if all the
  nodes have a capacity of `cpu: 1`, then a Pod with a request of `cpu: 1.1` will
  never be scheduled.

You can check node capacities and amounts allocated with the
`kubectl describe nodes` command. For example:

```shell
kubectl describe nodes e2e-test-node-pool-4lw4
```
```
Name:            e2e-test-node-pool-4lw4
[ ... lines removed for clarity ...]
Capacity:
 cpu:                               2
 memory:                            7679792Ki
 pods:                              110
Allocatable:
 cpu:                               1800m
 memory:                            7474992Ki
 pods:                              110
[ ... lines removed for clarity ...]
Non-terminated Pods:        (5 in total)
  Namespace    Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------    ----                                  ------------  ----------  ---------------  -------------
  kube-system  fluentd-gcp-v1.38-28bv1               100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)
  kube-system  kube-dns-3297075139-61lj3             260m (13%)    0 (0%)      100Mi (1%)       170Mi (2%)
  kube-system  kube-proxy-e2e-test-...               100m (5%)     0 (0%)      0 (0%)           0 (0%)
  kube-system  monitoring-influxdb-grafana-v4-z1m12  200m (10%)    200m (10%)  600Mi (8%)       600Mi (8%)
  kube-system  node-problem-detector-v0.1-fj7m3      20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests    CPU Limits    Memory Requests    Memory Limits
  ------------    ----------    ---------------    -------------
  680m (34%)      400m (20%)    920Mi (11%)        1070Mi (13%)
```

In the preceding output, you can see that if a Pod requests more than 1120m
CPUs or 6.23Gi of memory, it will not fit on the node.

By looking at the `Pods` section, you can see which Pods are taking up space on
the node.

The amount of resources available to Pods is less than the node capacity, because
system daemons use a portion of the available resources. The `allocatable` field
[NodeStatus](/docs/reference/generated/kubernetes-api/v1.19/#nodestatus-v1-core)
gives the amount of resources that are available to Pods. For more information, see
[Node Allocatable Resources](https://git.k8s.io/community/contributors/design-proposals/node/node-allocatable.md).

The [resource quota](/docs/concepts/policy/resource-quotas/) feature can be configured
to limit the total amount of resources that can be consumed. If used in conjunction
with namespaces, it can prevent one team from hogging all the resources.
 --&gt;
&lt;h2 id=&#34;troubleshooting&#34;&gt;问题排查&lt;/h2&gt;
&lt;h3 id=&#34;my-pods-are-pending-with-event-message-failedscheduling&#34;&gt;Pod 挂起事件信息为 &lt;code&gt;failedScheduling&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;如果调度器不能为 Pod 找到一个适合它的节点， Pod 在找到一个合适的地方之前会保留在未调度状态。
每次调度器在为 Pod 找地方失败都会产生一个事件，就像下面这样:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe pod frontend | grep -A &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; Events
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Events:
  FirstSeen LastSeen   Count  From          Subobject   PathReason      Message
  36s   5s     6      {scheduler }              FailedScheduling  Failed for reason PodExceedsFreeCPU and possibly others
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在上面的例子中， 这个叫 &amp;ldquo;frontend&amp;rdquo; 的 Pod 因为节点上的 CPU 资源不足导致调度失败。 类似的错误
消息还可能是因为内存不足(PodExceedsFreeMemory). 通常，如果一个 Pod 因为这些类型的消息而
挂起，有下面这些选项可以尝试:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;添加更多节点到集群。&lt;/li&gt;
&lt;li&gt;终止不再需要的 Pod 为挂起的 Pod 腾点空间&lt;/li&gt;
&lt;li&gt;检查 Pod 是不是比所有的节点都大。 例如， 如果所有的节点都有一个 &lt;code&gt;cpu: 1&lt;/code&gt; 的容量，然后一个
Pod 请求了 &lt;code&gt;cpu: 1.1&lt;/code&gt;，就永远不会被调度。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用户可以通过 &lt;code&gt;kubectl describe nodes&lt;/code&gt; 命令来检查节点容量和已经被分配的量。 例如:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe nodes e2e-test-node-pool-4lw4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Name:            e2e-test-node-pool-4lw4
[ ... lines removed for clarity ...]
Capacity:
 cpu:                               2
 memory:                            7679792Ki
 pods:                              110
Allocatable:
 cpu:                               1800m
 memory:                            7474992Ki
 pods:                              110
[ ... lines removed for clarity ...]
Non-terminated Pods:        (5 in total)
  Namespace    Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------    ----                                  ------------  ----------  ---------------  -------------
  kube-system  fluentd-gcp-v1.38-28bv1               100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)
  kube-system  kube-dns-3297075139-61lj3             260m (13%)    0 (0%)      100Mi (1%)       170Mi (2%)
  kube-system  kube-proxy-e2e-test-...               100m (5%)     0 (0%)      0 (0%)           0 (0%)
  kube-system  monitoring-influxdb-grafana-v4-z1m12  200m (10%)    200m (10%)  600Mi (8%)       600Mi (8%)
  kube-system  node-problem-detector-v0.1-fj7m3      20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests    CPU Limits    Memory Requests    Memory Limits
  ------------    ----------    ---------------    -------------
  680m (34%)      400m (20%)    920Mi (11%)        1070Mi (13%)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在上面的输出中， 就可以看出来如果有一个 Pod 请求超出了 1120m CPU 或 6.23Gi 内存，那么它就不适合
调度到这个节点上。&lt;/p&gt;
&lt;p&gt;从 &lt;code&gt;Pods&lt;/code&gt; 区域可以看到哪些 Pod 在这个节点上用了多少资源。&lt;/p&gt;
&lt;p&gt;可以用于 Pod 的资源要比节点的容量少些，因为系统守护进行需要使用部分可用资源。
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#nodestatus-v1-core&#34;&gt;NodeStatus&lt;/a&gt;
的 &lt;code&gt;allocatable&lt;/code&gt; 字段显示了可以分配给 Pod 的资源数量。 更多信息见
&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/node/node-allocatable.md&#34;&gt;Node Allocatable Resources&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/policy/resource-quotas/&#34;&gt;资源配额&lt;/a&gt; 特性可以用来配置可以使用的资源总数。
如果与命名空间配置使用，可以防止一个项目组吃完所有的资源。&lt;/p&gt;
&lt;h3 id=&#34;my-container-is-terminated&#34;&gt;容器被干掉了&lt;/h3&gt;
&lt;p&gt;容器可能因为资源不足而被干掉。 要检查一个容器是不是因为达到资源限制而被杀掉，对 Pod 执行
&lt;code&gt;kubectl describe pod&lt;/code&gt; 命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe pod simmemleak-hra99
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Name:                           simmemleak-hra99
Namespace:                      default
Image(s):                       saadali/simmemleak
Node:                           kubernetes-node-tf0f/10.240.216.66
Labels:                         name=simmemleak
Status:                         Running
Reason:
Message:
IP:                             10.244.2.75
Replication Controllers:        simmemleak (1/1 replicas created)
Containers:
  simmemleak:
    Image:  saadali/simmemleak
    Limits:
      cpu:                      100m
      memory:                   50Mi
    State:                      Running
      Started:                  Tue, 07 Jul 2015 12:54:41 -0700
    Last Termination State:     Terminated
      Exit Code:                1
      Started:                  Fri, 07 Jul 2015 12:54:30 -0700
      Finished:                 Fri, 07 Jul 2015 12:54:33 -0700
    Ready:                      False
    Restart Count:              5
Conditions:
  Type      Status
  Ready     False
Events:
  FirstSeen                         LastSeen                         Count  From                              SubobjectPath                       Reason      Message
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {scheduler }                                                          scheduled   Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {kubelet kubernetes-node-tf0f}    implicitly required container POD   pulled      Pod container image &amp;quot;k8s.gcr.io/pause:0.8.0&amp;quot; already present on machine
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {kubelet kubernetes-node-tf0f}    implicitly required container POD   created     Created with docker id 6a41280f516d
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {kubelet kubernetes-node-tf0f}    implicitly required container POD   started     Started with docker id 6a41280f516d
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {kubelet kubernetes-node-tf0f}    spec.containers{simmemleak}         created     Created with docker id 87348f12526a
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在上面的例子中， &lt;code&gt;Restart Count:  5&lt;/code&gt; 表示 Pod 中叫 &lt;code&gt;simmemleak&lt;/code&gt; 容器被干掉和重启了5次。&lt;/p&gt;
&lt;p&gt;用户可以使用命令 &lt;code&gt;kubectl get pod&lt;/code&gt; 加 &lt;code&gt;-o go-template=...&lt;/code&gt; 选项来获取前面被干掉的容器的状态:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pod -o go-template&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{{range.status.containerStatuses}}{{&amp;#34;Container Name: &amp;#34;}}{{.name}}{{&amp;#34;\r\nLastState: &amp;#34;}}{{.lastState}}{{end}}&amp;#39;&lt;/span&gt;  simmemleak-hra99
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Container Name: simmemleak
LastState: map[terminated:map[exitCode:137 reason:OOM Killed startedAt:2015-07-07T20:58:43Z finishedAt:2015-07-07T20:58:43Z containerID:docker://0e4095bba1feccdfe7ef9fb6ebffe972b4b14285d5acdec6f0d3ae8a22fad8b2]]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样就可以看出来这个容器是因为 &lt;code&gt;reason:OOM Killed&lt;/code&gt; 被干掉，而 &lt;code&gt;OOM&lt;/code&gt; 表示内存炸了。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
* Get hands-on experience [assigning Memory resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-memory-resource/).

* Get hands-on experience [assigning CPU resources to Containers and Pods](/docs/tasks/configure-pod-container/assign-cpu-resource/).

* For more details about the difference between requests and limits, see
  [Resource QoS](https://git.k8s.io/community/contributors/design-proposals/node/resource-qos.md).

* Read the [Container](/docs/reference/generated/kubernetes-api/v1.19/#container-v1-core) API reference

* Read the [ResourceRequirements](/docs/reference/generated/kubernetes-api/v1.19/#resourcerequirements-v1-core) API reference

* Read about [project quotas](https://xfs.org/docs/xfsdocs-xml-dev/XFS_User_Guide/tmp/en-US/html/xfs-quotas.html) in XFS
 --&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/configure-pod-container/assign-memory-resource/&#34;&gt;为容器和 Pod 分配内存资源&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/configure-pod-container/assign-cpu-resource/&#34;&gt;为容器和 Pod 分配CPU资源&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;更多关于请求和限制的区别信息见
&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/node/resource-qos.md&#34;&gt;Resource QoS&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;API 文档
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#container-v1-core&#34;&gt;Container&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;API 文档
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#resourcerequirements-v1-core&#34;&gt;ResourceRequirements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;XFS
&lt;a href=&#34;https://xfs.org/docs/xfsdocs-xml-dev/XFS_User_Guide/tmp/en-US/html/xfs-quotas.html&#34;&gt;项目配额&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: EndpointSlice</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/endpoint-slices/</link>
      <pubDate>Thu, 17 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/endpoint-slices/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- freehan
title: EndpointSlices
content_type: concept
weight: 35
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;



_EndpointSlices_ provide a simple way to track network endpoints within a
Kubernetes cluster. They offer a more scalable and extensible alternative to
Endpoints.
 --&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;em&gt;EndpointSlice&lt;/em&gt; 提供了一种简单的方式来在 k8s 集群跟踪网络端点。 它提供了比 Endpoint 拥有更
好的伸缩性和扩展性的替代方案&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Motivation

The Endpoints API has provided a simple and straightforward way of
tracking network endpoints in Kubernetes. Unfortunately as Kubernetes clusters
and &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Services&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; have grown to handle and
send more traffic to more backend Pods, limitations of that original API became
more visible.
Most notably, those included challenges with scaling to larger numbers of
network endpoints.

Since all network endpoints for a Service were stored in a single Endpoints
resource, those resources could get quite large. That affected the performance
of Kubernetes components (notably the master control plane) and resulted in
significant amounts of network traffic and processing when Endpoints changed.
EndpointSlices help you mitigate those issues as well as provide an extensible
platform for additional features such as topological routing.
 --&gt;
&lt;h2 id=&#34;动机&#34;&gt;动机&lt;/h2&gt;
&lt;p&gt;Endpoint 的 API 提供了一个简单的直接的方式来跟踪 k8s 中的网络端点。不幸的是当 k8s 集群和
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 处理和发送更多的流量到更多的后端 Pod 时，
这个 API 的限制就越来越明显了.  特别是当集群中扩充到很大数量的网络端点时，这些挑战就越发明显。&lt;/p&gt;
&lt;p&gt;当一个 Service 所有的网络端点都被存储在一个 Endpoint 资源时，这个资源就会变得很大。这会影响
到 k8s 组件(特别是控制中心)的性能， 并且的 Endpoint 发生变化时导致大量的网络流量和相关处理业务。
EndpointSlice 缓解这些问题的同时提供了一个可扩展的平台，这个平台上还有包括拓扑路由等特性。&lt;/p&gt;
&lt;!--
## EndpointSlice resources {#endpointslice-resource}

In Kubernetes, an EndpointSlice contains references to a set of network
endpoints. The control plane automatically creates EndpointSlices
for any Kubernetes Service that has a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/&#39; target=&#39;_blank&#39;&gt;selector&lt;span class=&#39;tooltip-text&#39;&gt;允许用户基于标签过滤资源列表&lt;/span&gt;
&lt;/a&gt; specified. These EndpointSlices include
references to all the Pods that match the Service selector. EndpointSlices group
network endpoints together by unique combinations of protocol, port number, and
Service name.  
The name of a EndpointSlice object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

As an example, here&#39;s a sample EndpointSlice resource for the `example`
Kubernetes Service.

```yaml
apiVersion: discovery.k8s.io/v1beta1
kind: EndpointSlice
metadata:
  name: example-abc
  labels:
    kubernetes.io/service-name: example
addressType: IPv4
ports:
  - name: http
    protocol: TCP
    port: 80
endpoints:
  - addresses:
      - &#34;10.1.2.3&#34;
    conditions:
      ready: true
    hostname: pod-1
    topology:
      kubernetes.io/hostname: node-1
      topology.kubernetes.io/zone: us-west2-a
```

By default, the control plane creates and manages EndpointSlices to have no
more than 100 endpoints each. You can configure this with the
`--max-endpoints-per-slice`
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/kube-controller-manager/&#39; target=&#39;_blank&#39;&gt;kube-controller-manager&lt;span class=&#39;tooltip-text&#39;&gt;Control Plane component that runs controller processes.&lt;/span&gt;
&lt;/a&gt;
flag, up to a maximum of 1000.

EndpointSlices can act as the source of truth for
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/kube-proxy/&#39; target=&#39;_blank&#39;&gt;kube-proxy&lt;span class=&#39;tooltip-text&#39;&gt;kube-proxy is a network proxy that runs on each node in the cluster.&lt;/span&gt;
&lt;/a&gt; when it comes to
how to route internal traffic. When enabled, they should provide a performance
improvement for services with large numbers of endpoints.
 --&gt;
&lt;h2 id=&#34;endpointslice-resource&#34;&gt;EndpointSlice 资源&lt;/h2&gt;
&lt;p&gt;在 k8s 中， 一个 EndpointSlice 包含一组网络端点集合的引用。 控制中心会自动为任意包含
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/&#39; target=&#39;_blank&#39;&gt;选择器&lt;span class=&#39;tooltip-text&#39;&gt;允许用户基于标签过滤资源列表&lt;/span&gt;
&lt;/a&gt;
的 Service 创建 EndpointSlice。 这些 EndpointSlice 包含所有匹配该 Service 的 Pod 的引用。
EndpointSlice 通过协议，端口号，Service 名称的唯一组合将这些网络端点组织在一起。
EndpointSlice 对象的名称必须是一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;下面的例子中，是一个叫 &lt;code&gt;example&lt;/code&gt; 的 Service 的 EndpointSlice 资源的简单示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;discovery.k8s.io/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;EndpointSlice&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;example-abc&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;kubernetes.io/service-name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;example&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;addressType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IPv4&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;endpoints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;addresses&lt;/span&gt;:
      - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10.1.2.3&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;conditions&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;ready&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostname&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod-1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topology&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;kubernetes.io/hostname&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;node-1&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;topology.kubernetes.io/zone&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;us-west2-a&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;默认情况下， 控制中心创建和管理的 EndpointSlice 每个不超过 100 个网络端点。用户可以通过
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/kube-controller-manager/&#39; target=&#39;_blank&#39;&gt;kube-controller-manager&lt;span class=&#39;tooltip-text&#39;&gt;Control Plane component that runs controller processes.&lt;/span&gt;
&lt;/a&gt;
的 &lt;code&gt;--max-endpoints-per-slice&lt;/code&gt; 参数配置，最高可以到 1000。&lt;/p&gt;
&lt;p&gt;EndpointSlices 可以在路由内部流量时被认作
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/kube-proxy/&#39; target=&#39;_blank&#39;&gt;kube-proxy&lt;span class=&#39;tooltip-text&#39;&gt;kube-proxy is a network proxy that runs on each node in the cluster.&lt;/span&gt;
&lt;/a&gt; 的真实的来源。
当启用后，可以改善拥有大量网络端点的 Service 的性能。&lt;/p&gt;
&lt;!--
### Address types

EndpointSlices support three address types:

* IPv4
* IPv6
* FQDN (Fully Qualified Domain Name)
 --&gt;
&lt;h3 id=&#34;地址类型&#34;&gt;地址类型&lt;/h3&gt;
&lt;p&gt;EndpointSlice 支持以下三种类型:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;IPv4&lt;/li&gt;
&lt;li&gt;IPv6&lt;/li&gt;
&lt;li&gt;FQDN (全限定名)&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Topology information {#topology}

Each endpoint within an EndpointSlice can contain relevant topology information.
This is used to indicate where an endpoint is, containing information about the
corresponding Node, zone, and region. When the values are available, the
control plane sets the following Topology labels for EndpointSlices:

* `kubernetes.io/hostname` - The name of the Node this endpoint is on.
* `topology.kubernetes.io/zone` - The zone this endpoint is in.
* `topology.kubernetes.io/region` - The region this endpoint is in.

The values of these labels are derived from resources associated with each
endpoint in a slice. The hostname label represents the value of the NodeName
field on the corresponding Pod. The zone and region labels represent the value
of the labels with the same names on the corresponding Node.
 --&gt;
&lt;h3 id=&#34;topology&#34;&gt;拓扑信息&lt;/h3&gt;
&lt;p&gt;EndpointSlice 中的每一个网络端点都可以有一个有意义的拓扑信息。 用于指示这个网络端点在哪，包含对应的
节点信息，分区信息和地区信息。 当这些值存在时，控制中心会为 EndpointSlice 打上如下标签:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;kubernetes.io/hostname&lt;/code&gt; - 这个网络端点所在的节点名称.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;topology.kubernetes.io/zone&lt;/code&gt; - 这个网络端点所在分区.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;topology.kubernetes.io/region&lt;/code&gt; - 这个网络端点所在地区.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些标签的值源于 EndpointSlice 中的每个网络端点对应的资源。  hostname 标签表示 对应 Pod
的 NodeName 字段的值。 &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;region&lt;/code&gt; 标签与对应节点同一标签一致。&lt;/p&gt;
&lt;!--
### Management

Most often, the control plane (specifically, the endpoint slice
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;controller&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt;) creates and
manages EndpointSlice objects. There are a variety of other use cases for
EndpointSlices, such as service mesh implementations, that could result in other
entities or controllers managing additional sets of EndpointSlices.

To ensure that multiple entities can manage EndpointSlices without interfering
with each other, Kubernetes defines the
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels&#39; target=&#39;_blank&#39;&gt;label&lt;span class=&#39;tooltip-text&#39;&gt;在对象上标上对用户有意义和有关联的属性&lt;/span&gt;
&lt;/a&gt;
`endpointslice.kubernetes.io/managed-by`, which indicates the entity managing
an EndpointSlice.
The endpoint slice controller sets `endpointslice-controller.k8s.io` as the value
for this label on all EndpointSlices it manages. Other entities managing
EndpointSlices should also set a unique value for this label.
 --&gt;
&lt;h3 id=&#34;管理&#34;&gt;管理&lt;/h3&gt;
&lt;p&gt;大多数时候，控制中心(确切的说，EndpointSlice &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt;)
创建和管理 EndpointSlice 对象。 除此之外 EndpointSlice 还有其它多种应用场景， 例如服务网格的实现，
由此可以会导致其它的实体或控制顺管理额外的 EndpointSlice 集合。&lt;/p&gt;
&lt;p&gt;为了能保证多个实体在管理 EndpointSlice 时不会相互影响， k8s 定义了 &lt;code&gt;endpointslice.kubernetes.io/managed-by&lt;/code&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels&#39; target=&#39;_blank&#39;&gt;label&lt;span class=&#39;tooltip-text&#39;&gt;在对象上标上对用户有意义和有关联的属性&lt;/span&gt;
&lt;/a&gt;，
这个标签用于指示是哪个实体在管理这个 EndpointSlice。 EndpointSlice 控制器会为所有它管理的
EndpointSlice 设置 &lt;code&gt;endpointslice-controller.k8s.io&lt;/code&gt; 标签。 其它实体管理 EndpointSlice
也应当为该标签设置唯一的值。&lt;/p&gt;
&lt;!--
### Ownership

In most use cases, EndpointSlices are owned by the Service that the endpoint
slice object tracks endpoints for. This ownership is indicated by an owner
reference on each EndpointSlice as well as a `kubernetes.io/service-name`
label that enables simple lookups of all EndpointSlices belonging to a Service.
 --&gt;
&lt;h3 id=&#34;所有权&#34;&gt;所有权&lt;/h3&gt;
&lt;p&gt;在大多数情况下， EndpointSlice 的所有者是它跟踪的网络端点对应的 Service。 这个所有权会通过
EndpointSlice 上的 &lt;code&gt;kubernetes.io/service-name&lt;/code&gt; 标签显示，标签的值为其所属 Service 的名称&lt;/p&gt;
&lt;!--
### EndpointSlice mirroring

In some cases, applications create custom Endpoints resources. To ensure that
these applications do not need to concurrently write to both Endpoints and
EndpointSlice resources, the cluster&#39;s control plane mirrors most Endpoints
resources to corresponding EndpointSlices.

The control plane mirrors Endpoints resources unless:

* the Endpoints resource has a `endpointslice.kubernetes.io/skip-mirror` label
  set to `true`.
* the Endpoints resource has a `control-plane.alpha.kubernetes.io/leader`
  annotation.
* the corresponding Service resource does not exist.
* the corresponding Service resource has a non-nil selector.

Individual Endpoints resources may translate into multiple EndpointSlices. This
will occur if an Endpoints resource has multiple subsets or includes endpoints
with multiple IP families (IPv4 and IPv6). A maximum of 1000 addresses per
subset will be mirrored to EndpointSlices.
 --&gt;
&lt;h3 id=&#34;endpointslice-镜像&#34;&gt;EndpointSlice 镜像&lt;/h3&gt;
&lt;p&gt;在某些情况下，应用会创建自定义的 Endpoint。 为了保证这些应用不会同时写入到 Endpoint 和 EndpointSlice
资源， 集群控制中心会为大多数 Endpoint 创建对应的 EndpointSlice。&lt;/p&gt;
&lt;p&gt;以下情况控制中心不会镜像 Endpoint:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Endpoint 资源有 &lt;code&gt;endpointslice.kubernetes.io/skip-mirror&lt;/code&gt; 标签，且值为 &lt;code&gt;true&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Endpoint 资源有 &lt;code&gt;control-plane.alpha.kubernetes.io/leader&lt;/code&gt; 注解。&lt;/li&gt;
&lt;li&gt;对应的 Service 资源不存在&lt;/li&gt;
&lt;li&gt;对应的 Service 资源有非空选择器&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个 Endpoint 可能会被转化为多个 EndpointSlice。发生这种情况的原因有可能是 Endpoint 资源
包含多个子网或包含的网络端点同时包含 IPv4 和 IPv6。 每个子网最多可以把 1000 个地址镜像到一个 EndpointSlice&lt;/p&gt;
&lt;!--
### Distribution of EndpointSlices

Each EndpointSlice has a set of ports that applies to all endpoints within the
resource. When named ports are used for a Service, Pods may end up with
different target port numbers for the same named port, requiring different
EndpointSlices. This is similar to the logic behind how subsets are grouped
with Endpoints.

The control plane tries to fill EndpointSlices as full as possible, but does not
actively rebalance them. The logic is fairly straightforward:

1. Iterate through existing EndpointSlices, remove endpoints that are no longer
   desired and update matching endpoints that have changed.
2. Iterate through EndpointSlices that have been modified in the first step and
   fill them up with any new endpoints needed.
3. If there&#39;s still new endpoints left to add, try to fit them into a previously
   unchanged slice and/or create new ones.

Importantly, the third step prioritizes limiting EndpointSlice updates over a
perfectly full distribution of EndpointSlices. As an example, if there are 10
new endpoints to add and 2 EndpointSlices with room for 5 more endpoints each,
this approach will create a new EndpointSlice instead of filling up the 2
existing EndpointSlices. In other words, a single EndpointSlice creation is
preferrable to multiple EndpointSlice updates.

With kube-proxy running on each Node and watching EndpointSlices, every change
to an EndpointSlice becomes relatively expensive since it will be transmitted to
every Node in the cluster. This approach is intended to limit the number of
changes that need to be sent to every Node, even if it may result with multiple
EndpointSlices that are not full.

In practice, this less than ideal distribution should be rare. Most changes
processed by the EndpointSlice controller will be small enough to fit in an
existing EndpointSlice, and if not, a new EndpointSlice is likely going to be
necessary soon anyway. Rolling updates of Deployments also provide a natural
repacking of EndpointSlices with all Pods and their corresponding endpoints
getting replaced.
 --&gt;
&lt;h3 id=&#34;endpointslice-分布&#34;&gt;EndpointSlice 分布&lt;/h3&gt;
&lt;p&gt;每个 EndpointSlice 都有一系列端口，这些端口会被应用到该资源内的所有的网络端点上。当一个
Service 使用命名端口时， Pod 最终可以会有同一个名称的端口的目标端口不一样的情况，这样就需要不
同的 EndpointSlice。 这与子网是怎么对 Endpoint 分组的逻辑是类似的。&lt;/p&gt;
&lt;p&gt;控制中心会尽量将 EndpointSlice 装满，但不会主动地重新平衡它们。 其中逻辑是相同简单的:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;迭代所有存在的 EndpointSlice，称除不需要的网络端点，更新发生变化的网络端点&lt;/li&gt;
&lt;li&gt;迭代每一步中被修改的 EndpointSlice ，添加所有需要的新的网络端点&lt;/li&gt;
&lt;li&gt;如果还有新的网络端点需要添加，尝试添加到之间没变更的 EndpointSlice 或者(同时)创建一个新的。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;重要的是， 在第三步的优先级会限制 EndpointSlice 更新到一个完美分布。 例如，如果有 10 个新增的
网络端点要添加到两个 EndpointSlice，并且这两个 EndpointSlice 都还有 5 个空位，这种方式会
创建一个新的 EndpointSlice， 而不是装满这两个已经存在的 EndpointSlice。 换句话说，创建一个新的
EndpointSlice 优先于更新两个 EndpointSlice。&lt;/p&gt;
&lt;p&gt;当 kube-proxy 运行在每个节点上并监听 EndpointSlice， 每一次对 EndpointSlice 修改都会变得
相对来多代价比较高的，因为每个更新都会传达到信念中的每一个节点上。这种方式旨在限制发送到每个节点
的变更次数，尽管它会导致产生多个没有被装满的 EndpointSlice&lt;/p&gt;
&lt;p&gt;在实践中，这种不太理想分配应该是比较少见的。 大多数由 EndpointSlice 处理的变量应该都足够小到适应
已经存在的 EndpointSlice， 如果没，也就是创建一个很快就也必须要创建的新的 EndpointSlice。
Deployment 的滚动更新也提供了一个自然的  EndpointSlice 重新打包，因为所有的 Pod 和它们对
应的网络端点也是被替换。&lt;/p&gt;
&lt;!--
### Duplicate endpoints

Due to the nature of EndpointSlice changes, endpoints may be represented in more
than one EndpointSlice at the same time. This naturally occurs as changes to
different EndpointSlice objects can arrive at the Kubernetes client watch/cache
at different times. Implementations using EndpointSlice must be able to have the
endpoint appear in more than one slice. A reference implementation of how to
perform endpoint deduplication can be found in the `EndpointSliceCache`
implementation in `kube-proxy`.
 --&gt;
&lt;h3 id=&#34;重复的网络端点&#34;&gt;重复的网络端点&lt;/h3&gt;
&lt;p&gt;由于 EndpointSlice 的自然变更，网络端点可能同时存在于不同的 EndpointSlice 中。 这些自然地
发生在对不同 EndpointSlice 对象的变更，可能在不同的时间到到 k8s 客户端的监听或缓存。使用
EndpointSlice 的实现必须要能处理这种同一个网络端点出现在多个 EndpointSlice 的情况。
怎么处理重复网络端点的实现参考可以在 &lt;code&gt;kube-proxy&lt;/code&gt; 中的 &lt;code&gt;EndpointSliceCache&lt;/code&gt; 实现中找到。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/enabling-endpointslices&#34;&gt;开启 EndpointSlices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/connect-applications-service/&#34;&gt;通过 Service 连接应用&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: StorageClass</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-classes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-classes/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jsafrane
- saad-ali
- thockin
- msau42
title: Storage Classes
content_type: concept
weight: 30
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This document describes the concept of a StorageClass in Kubernetes. Familiarity
with [volumes](/docs/concepts/storage/volumes/) and
[persistent volumes](/docs/concepts/storage/persistent-volumes) is suggested.
 --&gt;
&lt;p&gt;本文介绍 k8s 中的 StorageClass 这个概念。 建议先熟悉
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#34;&gt;卷(Volume)&lt;/a&gt;
和
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes&#34;&gt;持久化卷(PV)&lt;/a&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Introduction

A StorageClass provides a way for administrators to describe the &#34;classes&#34; of
storage they offer. Different classes might map to quality-of-service levels,
or to backup policies, or to arbitrary policies determined by the cluster
administrators. Kubernetes itself is unopinionated about what classes
represent. This concept is sometimes called &#34;profiles&#34; in other storage
systems.
--&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;StorageClass 为管理员提供了一种描述不同存储类别的方式. 不同的类别与 服务质量(QoS)级别，备份策略，
或其它由管理决定的任意策略关联。k8s 本身没不固化出现的类别。 这个概念在某些存储系统中被称为 &amp;ldquo;profiles&amp;rdquo;&lt;/p&gt;
&lt;!--
## The StorageClass Resource

Each StorageClass contains the fields `provisioner`, `parameters`, and
`reclaimPolicy`, which are used when a PersistentVolume belonging to the
class needs to be dynamically provisioned.

The name of a StorageClass object is significant, and is how users can
request a particular class. Administrators set the name and other parameters
of a class when first creating StorageClass objects, and the objects cannot
be updated once they are created.

Administrators can specify a default StorageClass just for PVCs that don&#39;t
request any particular class to bind to: see the
[PersistentVolumeClaim section](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims)
for details.

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
reclaimPolicy: Retain
allowVolumeExpansion: true
mountOptions:
  - debug
volumeBindingMode: Immediate
```
 --&gt;
&lt;h2 id=&#34;storageclass-资源&#34;&gt;&lt;code&gt;StorageClass&lt;/code&gt; 资源&lt;/h2&gt;
&lt;p&gt;每个 &lt;code&gt;StorageClass&lt;/code&gt; 对象包含 &lt;code&gt;provisioner&lt;/code&gt;, &lt;code&gt;parameters&lt;/code&gt;, &lt;code&gt;reclaimPolicy&lt;/code&gt; 字段，
当有被该类别的 PV 需要被动态供给时会用到。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;StorageClass&lt;/code&gt; 对象的名称是有意义的，它会被用户在申请该类别存储时用到。 管理员在第一次创建
&lt;code&gt;StorageClass&lt;/code&gt; 对象时设置名称和其它的参数， 这些对象在创建后将不可修改。&lt;/p&gt;
&lt;p&gt;管理员可以指定一个默认的 &lt;code&gt;StorageClass&lt;/code&gt;，那些没有指定类别的 PVC 就会使用这个默认的类别:
详细信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims&#34;&gt;PersistentVolumeClaim 章节&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;standard&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/aws-ebs&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gp2&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;reclaimPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Retain&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;allowVolumeExpansion&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;mountOptions&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;debug&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;volumeBindingMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Immediate&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Provisioner

Each StorageClass has a provisioner that determines what volume plugin is used
for provisioning PVs. This field must be specified.

| Volume Plugin        | Internal Provisioner| Config Example                       |
| :---                 |     :---:           |    :---:                             |
| AWSElasticBlockStore | &amp;#x2713;            | [AWS EBS](#aws-ebs)                          |
| AzureFile            | &amp;#x2713;            | [Azure File](#azure-file)            |
| AzureDisk            | &amp;#x2713;            | [Azure Disk](#azure-disk)            |
| CephFS               | -                   | -                                    |
| Cinder               | &amp;#x2713;            | [OpenStack Cinder](#openstack-cinder)|
| FC                   | -                   | -                                    |
| FlexVolume           | -                   | -                                    |
| Flocker              | &amp;#x2713;            | -                                    |
| GCEPersistentDisk    | &amp;#x2713;            | [GCE PD](#gce-pd)                          |
| Glusterfs            | &amp;#x2713;            | [Glusterfs](#glusterfs)              |
| iSCSI                | -                   | -                                    |
| Quobyte              | &amp;#x2713;            | [Quobyte](#quobyte)                  |
| NFS                  | -                   | -                                    |
| RBD                  | &amp;#x2713;            | [Ceph RBD](#ceph-rbd)                |
| VsphereVolume        | &amp;#x2713;            | [vSphere](#vsphere)                  |
| PortworxVolume       | &amp;#x2713;            | [Portworx Volume](#portworx-volume)  |
| ScaleIO              | &amp;#x2713;            | [ScaleIO](#scaleio)                  |
| StorageOS            | &amp;#x2713;            | [StorageOS](#storageos)              |
| Local                | -                   | [Local](#local)              |

You are not restricted to specifying the &#34;internal&#34; provisioners
listed here (whose names are prefixed with &#34;kubernetes.io&#34; and shipped
alongside Kubernetes). You can also run and specify external provisioners,
which are independent programs that follow a [specification](https://git.k8s.io/community/contributors/design-proposals/storage/volume-provisioning.md)
defined by Kubernetes. Authors of external provisioners have full discretion
over where their code lives, how the provisioner is shipped, how it needs to be
run, what volume plugin it uses (including Flex), etc. The repository
[kubernetes-sigs/sig-storage-lib-external-provisioner](https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner)
houses a library for writing external provisioners that implements the bulk of
the specification. Some external provisioners are listed under the repository
[kubernetes-sigs/sig-storage-lib-external-provisioner](https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner).

For example, NFS doesn&#39;t provide an internal provisioner, but an external
provisioner can be used. There are also cases when 3rd party storage
vendors provide their own external provisioner.
 --&gt;
&lt;h3 id=&#34;供应者provisioner&#34;&gt;供应者(Provisioner)&lt;/h3&gt;
&lt;p&gt;每个 &lt;code&gt;StorageClass&lt;/code&gt; 都有一个供应者，这个供应者决定供给 PV 的卷插件。 这个字段必须指定。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Volume Plugin&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Internal Provisioner&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Config Example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AWSElasticBlockStore&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#aws-ebs&#34;&gt;AWS EBS&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AzureFile&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#azure-file&#34;&gt;Azure File&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AzureDisk&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#azure-disk&#34;&gt;Azure Disk&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CephFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Cinder&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#openstack-cinder&#34;&gt;OpenStack Cinder&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FC&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FlexVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Flocker&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;GCEPersistentDisk&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#gce-pd&#34;&gt;GCE PD&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Glusterfs&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#glusterfs&#34;&gt;Glusterfs&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;iSCSI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Quobyte&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#quobyte&#34;&gt;Quobyte&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;RBD&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#ceph-rbd&#34;&gt;Ceph RBD&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;VsphereVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#vsphere&#34;&gt;vSphere&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PortworxVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#portworx-volume&#34;&gt;Portworx Volume&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ScaleIO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#scaleio&#34;&gt;ScaleIO&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;StorageOS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#storageos&#34;&gt;StorageOS&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Local&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#local&#34;&gt;Local&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;用户并不仅限于上面列举的&amp;quot;内部&amp;quot;供应者(这些名称以 &amp;ldquo;kubernetes.io&amp;rdquo; 前缀的是随同 k8s 发行一起的)。
也可以运行和指定外部的供应者，这些是依照由 k8s 定义的
&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/storage/volume-provisioning.md&#34;&gt;specification&lt;/a&gt;
独立程序。 外部供应者的开发者可以完全自主地决定代码存入在哪， 供应者程序是什么发布的， 运行需要什么，
使用什么卷插件(包括 Flex)，等等。
&lt;a href=&#34;https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner&#34;&gt;kubernetes-sigs/sig-storage-lib-external-provisioner&lt;/a&gt;
仓库中包含了编写外部供应者需要实现的一系列规格说明。 一些外部提供也列举在这个仓库中&lt;/p&gt;
&lt;p&gt;例如， NFS 没有提供内部的供应者，就可以使用一个外部的供应都。 还有种情况是第三方存储提供都
也会提供自己的外部供应者。&lt;/p&gt;
&lt;!--
### Reclaim Policy

PersistentVolumes that are dynamically created by a StorageClass will have the
reclaim policy specified in the `reclaimPolicy` field of the class, which can be
either `Delete` or `Retain`. If no `reclaimPolicy` is specified when a
StorageClass object is created, it will default to `Delete`.

PersistentVolumes that are created manually and managed via a StorageClass will have
whatever reclaim policy they were assigned at creation.
 --&gt;
&lt;h3 id=&#34;回收策略&#34;&gt;回收策略&lt;/h3&gt;
&lt;p&gt;由 &lt;code&gt;StorageClass&lt;/code&gt; 动态创建的持久化卷(PV)将通过 &lt;code&gt;StorageClass&lt;/code&gt; 的 &lt;code&gt;reclaimPolicy&lt;/code&gt; 字段
设备回收策略，这些策略可以是 &lt;code&gt;Delete&lt;/code&gt; 或 &lt;code&gt;Retain&lt;/code&gt;。 如果在创建 &lt;code&gt;StorageClass&lt;/code&gt; 对象的时候
没有指定 &lt;code&gt;reclaimPolicy&lt;/code&gt;， 默认回收策略为 &lt;code&gt;Delete&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;由手动创建并通过 &lt;code&gt;StorageClass&lt;/code&gt; 管理的 持久化卷(PV) 会在创建的时候指定回收策略&lt;/p&gt;
&lt;!--
### Allow Volume Expansion






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [beta]&lt;/code&gt;
&lt;/div&gt;



PersistentVolumes can be configured to be expandable. This feature when set to `true`,
allows the users to resize the volume by editing the corresponding PVC object.

The following types of volumes support volume expansion, when the underlying
StorageClass has the field `allowVolumeExpansion` set to true.






&lt;table&gt;&lt;caption style=&#34;display: none;&#34;&gt;Table of Volume types and the version of Kubernetes they require&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Volume type&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Required Kubernetes version&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;gcePersistentDisk&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;awsElasticBlockStore&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Cinder&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;glusterfs&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;rbd&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Azure File&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Azure Disk&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Portworx&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FlexVolume&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CSI&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.14 (alpha), 1.16 (beta)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You can only use the volume expansion feature to grow a Volume, not to shrink it.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;允许卷扩容&#34;&gt;允许卷扩容&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;持久化卷(PV) 可以设置为可扩展的。当开启该特性后允许用户通过修改对应 PVC 对象的方式修改卷的容量。&lt;/p&gt;
&lt;p&gt;以下类型的卷在底层 &lt;code&gt;StorageClass&lt;/code&gt; 的 &lt;code&gt;allowVolumeExpansion&lt;/code&gt; 字段设置为 &lt;code&gt;true&lt;/code&gt;,时支持卷扩展。&lt;/p&gt;





&lt;table&gt;&lt;caption style=&#34;display: none;&#34;&gt;Table of Volume types and the version of Kubernetes they require&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;卷类型&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;需要的 k8s 版本&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;gcePersistentDisk&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;awsElasticBlockStore&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Cinder&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;glusterfs&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;rbd&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Azure File&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Azure Disk&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Portworx&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FlexVolume&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CSI&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.14 (alpha), 1.16 (beta)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 只能使用卷扩展特性扩充卷，不能缩小&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Mount Options

PersistentVolumes that are dynamically created by a StorageClass will have the
mount options specified in the `mountOptions` field of the class.

If the volume plugin does not support mount options but mount options are
specified, provisioning will fail. Mount options are not validated on either
the class or PV, so mount of the PV will simply fail if one is invalid.
 --&gt;
&lt;h3 id=&#34;挂载选项&#34;&gt;挂载选项&lt;/h3&gt;
&lt;p&gt;由 &lt;code&gt;StorageClass&lt;/code&gt; 动态创建的持久化卷(PV)会拥有由 &lt;code&gt;StorageClass&lt;/code&gt; &lt;code&gt;mountOptions&lt;/code&gt; 字段指定
的挂载选项。&lt;/p&gt;
&lt;p&gt;如果卷插件不支持挂载选项但又指定了挂载选项，供应就会失败。 &lt;code&gt;StorageClass&lt;/code&gt; 或 PV 挂载选项
不是有效的， 如果其中有一个无效则挂载就会失败。&lt;/p&gt;
&lt;!--
### Volume Binding Mode

The `volumeBindingMode` field controls when [volume binding and dynamic
provisioning](/docs/concepts/storage/persistent-volumes/#provisioning) should occur.

By default, the `Immediate` mode indicates that volume binding and dynamic
provisioning occurs once the PersistentVolumeClaim is created. For storage
backends that are topology-constrained and not globally accessible from all Nodes
in the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod&#39;s scheduling
requirements. This may result in unschedulable Pods.

A cluster administrator can address this issue by specifying the `WaitForFirstConsumer` mode which
will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created.
PersistentVolumes will be selected or provisioned conforming to the topology that is
specified by the Pod&#39;s scheduling constraints. These include, but are not limited to, [resource
requirements](/docs/concepts/configuration/manage-resources-containers/),
[node selectors](/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector),
[pod affinity and
anti-affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity),
and [taints and tolerations](/docs/concepts/scheduling-eviction/taint-and-toleration).

The following plugins support `WaitForFirstConsumer` with dynamic provisioning:

* [AWSElasticBlockStore](#aws-ebs)
* [GCEPersistentDisk](#gce-pd)
* [AzureDisk](#azure-disk)

The following plugins support `WaitForFirstConsumer` with pre-created PersistentVolume binding:

* All of the above
* [Local](#local)






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [stable]&lt;/code&gt;
&lt;/div&gt;


[CSI volumes](/docs/concepts/storage/volumes/#csi) are also supported with dynamic provisioning
and pre-created PVs, but you&#39;ll need to look at the documentation for a specific CSI driver
to see its supported topology keys and examples.
 --&gt;
&lt;h3 id=&#34;volume-binding-mode&#34;&gt;卷绑定模式&lt;/h3&gt;
&lt;p&gt;当
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/#provisioning&#34;&gt;卷绑定和动态供应&lt;/a&gt;
发生时由 &lt;code&gt;volumeBindingMode&lt;/code&gt; 字段控制。&lt;/p&gt;
&lt;p&gt;默认情况下使用的是 &lt;code&gt;Immediate&lt;/code&gt; 模式，这种模式表示在 PersistentVolumeClaim 对象创建后立即
进行卷绑定和动态供应。对于有拓扑限制的存储后台和并不是集群中所有节点都可以访问的存储时，
持久化卷(PV) 会在不知道 Pod 调度要求的情况下绑定或供应。这可能会导致 Pod 不可调度。&lt;/p&gt;
&lt;p&gt;要避免这个问题，管理员可以将卷模式设置为 &lt;code&gt;WaitForFirstConsumer&lt;/code&gt;，这样持久化卷(PV)的绑定和
供应会推迟到使用这个 PVC 的 Pod 创建之后。此时持久化卷(PV)在供应时会确认由 Pod 指定的调度约束。
这些限制包括但不限于
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/manage-resources-containers/&#34;&gt;资源需求&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector&#34;&gt;节点选择器&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity&#34;&gt;Pod 亲和性和反亲和性&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration&#34;&gt;毒点和耐受&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;以下插件支持带动态供应的 &lt;code&gt;WaitForFirstConsumer&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#aws-ebs&#34;&gt;AWSElasticBlockStore&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gce-pd&#34;&gt;GCEPersistentDisk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#azure-disk&#34;&gt;AzureDisk&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下插件支持在预先创建 持久化卷(PV)绑定的 &lt;code&gt;WaitForFirstConsumer&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;上面所有&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#local&#34;&gt;Local&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#34;&gt;CSI 卷&lt;/a&gt; 也支持动态供应和预创建 PV，但需要先
查看对应 CSI 驱动的文档，看看支持的拓扑键和示例。&lt;/p&gt;
&lt;!--
### Allowed Topologies

When a cluster operator specifies the `WaitForFirstConsumer` volume binding mode, it is no longer necessary
to restrict provisioning to specific topologies in most situations. However,
if still required, `allowedTopologies` can be specified.

This example demonstrates how to restrict the topology of provisioned volumes to specific
zones and should be used as a replacement for the `zone` and `zones` parameters for the
supported plugins.

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
volumeBindingMode: WaitForFirstConsumer
allowedTopologies:
- matchLabelExpressions:
  - key: failure-domain.beta.kubernetes.io/zone
    values:
    - us-central1-a
    - us-central1-b
```
 --&gt;
&lt;h3 id=&#34;allowed-topologies&#34;&gt;允许的拓扑&lt;/h3&gt;
&lt;p&gt;当集群中的卷绑定模式被设置为 &lt;code&gt;WaitForFirstConsumer&lt;/code&gt; 时，在大多数情况下就不供应严格受限于
指定拓扑。 但如果仍然需要这些限制，可以通过 &lt;code&gt;allowedTopologies&lt;/code&gt; 指定。&lt;/p&gt;
&lt;p&gt;以下的示例中展示的是怎么通过设置区域来限制供应的卷拓扑，如果插件支持，这些限制会用来替换插件中的 &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 参数&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;standard&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/gce-pd&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pd-standard&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;volumeBindingMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;WaitForFirstConsumer&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;allowedTopologies&lt;/span&gt;:
- &lt;span style=&#34;color:#f92672&#34;&gt;matchLabelExpressions&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;failure-domain.beta.kubernetes.io/zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;us-central1-a&lt;/span&gt;
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;us-central1-b&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Parameters

Storage Classes have parameters that describe volumes belonging to the storage
class. Different parameters may be accepted depending on the `provisioner`. For
 example, the value `io1`, for the parameter `type`, and the parameter
`iopsPerGB` are specific to EBS. When a parameter is omitted, some default is
used.

There can be at most 512 parameters defined for a StorageClass.
The total length of the parameters object including its keys and values cannot
exceed 256 KiB.
 --&gt;
&lt;h2 id=&#34;参数&#34;&gt;参数&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;StorageClass&lt;/code&gt; 有些参数，这些参数描述属于该存储类别的卷。 基于不同的 &lt;code&gt;provisioner&lt;/code&gt; 可以接受
不同的参数。 例如， 对于 &lt;code&gt;type&lt;/code&gt; 的参数值为 &lt;code&gt;io1&lt;/code&gt;， &lt;code&gt;iopsPerGB&lt;/code&gt; 参数的值为 &lt;code&gt;EBS&lt;/code&gt;。 当一个参数
没有设置时，就会使用默认值。&lt;/p&gt;
&lt;p&gt;对于每个 &lt;code&gt;StorageClass&lt;/code&gt; 最多可以定义 &lt;code&gt;512&lt;/code&gt; 个参数。参数对象的总长度，包含其键和值不能超过
256 KiB&lt;/p&gt;
&lt;!--
### AWS EBS

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/aws-ebs
parameters:
  type: io1
  iopsPerGB: &#34;10&#34;
  fsType: ext4
```

* `type`: `io1`, `gp2`, `sc1`, `st1`. See
  [AWS docs](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html)
  for details. Default: `gp2`.
* `zone` (Deprecated): AWS zone. If neither `zone` nor `zones` is specified, volumes are
  generally round-robin-ed across all active zones where Kubernetes cluster
  has a node. `zone` and `zones` parameters must not be used at the same time.
* `zones` (Deprecated): A comma separated list of AWS zone(s). If neither `zone` nor `zones`
  is specified, volumes are generally round-robin-ed across all active zones
  where Kubernetes cluster has a node. `zone` and `zones` parameters must not
  be used at the same time.
* `iopsPerGB`: only for `io1` volumes. I/O operations per second per GiB. AWS
  volume plugin multiplies this with size of requested volume to compute IOPS
  of the volume and caps it at 20 000 IOPS (maximum supported by AWS, see
  [AWS docs](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html).
  A string is expected here, i.e. `&#34;10&#34;`, not `10`.
* `fsType`: fsType that is supported by kubernetes. Default: `&#34;ext4&#34;`.
* `encrypted`: denotes whether the EBS volume should be encrypted or not.
  Valid values are `&#34;true&#34;` or `&#34;false&#34;`. A string is expected here,
  i.e. `&#34;true&#34;`, not `true`.
* `kmsKeyId`: optional. The full Amazon Resource Name of the key to use when
  encrypting the volume. If none is supplied but `encrypted` is true, a key is
  generated by AWS. See AWS docs for valid ARN value.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;code&gt;zone&lt;/code&gt; and &lt;code&gt;zones&lt;/code&gt; parameters are deprecated and replaced with
&lt;a href=&#34;#allowed-topologies&#34;&gt;allowedTopologies&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;aws-ebs&#34;&gt;AWS EBS&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/aws-ebs&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;io1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;iopsPerGB&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;type&lt;/code&gt;: &lt;code&gt;io1&lt;/code&gt;, &lt;code&gt;gp2&lt;/code&gt;, &lt;code&gt;sc1&lt;/code&gt;, &lt;code&gt;st1&lt;/code&gt;. 默认: &lt;code&gt;gp2&lt;/code&gt;
详细信息见 &lt;a href=&#34;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html&#34;&gt;AWS 文档&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;zone&lt;/code&gt; (废弃): AWS 区域。如果 &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 都没有设置， 卷会在 k8s 集群中所有有节点的
活跃区别之间随机调度&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;zones&lt;/code&gt; (废弃): 一个用逗号分隔的 AWS 区域列表。 如果没有设置，卷会在 k8s 集群中所有有节点的
活跃区别之间随机调度。 &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 参数一定不要同时使用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;iopsPerGB&lt;/code&gt;: 仅限 &lt;code&gt;io1&lt;/code&gt; 卷。每秒每GiB I/O 操作数。AWS 会将这个值乘以申请的卷大小得出
卷的 IOPS， 最高为 20 000 IOPS (AWS 支持的最大值, 见
&lt;a href=&#34;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html&#34;&gt;AWS 文档&lt;/a&gt;.
这个的值是一个字段串，也就是这样 &lt;code&gt;&amp;quot;10&amp;quot;&lt;/code&gt;， 而不是 &lt;code&gt;10&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;fsType&lt;/code&gt;: k8s 支持的文件系统类型。 默认: &lt;code&gt;&amp;quot;ext4&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;encrypted&lt;/code&gt;: 表示这个 EBS 卷是否使用加密。 有效的值是 &lt;code&gt;&amp;quot;true&amp;quot;&lt;/code&gt; 或 &lt;code&gt;&amp;quot;false&amp;quot;&lt;/code&gt;
这里的值也是字符串，也就是 &lt;code&gt;&amp;quot;true&amp;quot;&lt;/code&gt;, 而不是 &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;kmsKeyId&lt;/code&gt;: 可选。 在对卷加密时且的亚马逊资源全名的键。 如果这个值没提供但 &lt;code&gt;encrypted&lt;/code&gt;
设置为 &amp;ldquo;true&amp;rdquo;, AWS 就会生成一个键。 关于有效的 ARN 值见 AWS 文档。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 参数已经废弃，被 &lt;a href=&#34;#allowed-topologies&#34;&gt;允许的拓扑&lt;/a&gt; 替换&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### GCE PD

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
  fstype: ext4
  replication-type: none
```

* `type`: `pd-standard` or `pd-ssd`. Default: `pd-standard`
* `zone` (Deprecated): GCE zone. If neither `zone` nor `zones` is specified, volumes are
  generally round-robin-ed across all active zones where Kubernetes cluster has
  a node. `zone` and `zones` parameters must not be used at the same time.
* `zones` (Deprecated): A comma separated list of GCE zone(s). If neither `zone` nor `zones`
  is specified, volumes are generally round-robin-ed across all active zones
  where Kubernetes cluster has a node. `zone` and `zones` parameters must not
  be used at the same time.
* `fstype`: `ext4` or `xfs`. Default: `ext4`. The defined filesystem type must be supported by the host operating system.

* `replication-type`: `none` or `regional-pd`. Default: `none`.

If `replication-type` is set to `none`, a regular (zonal) PD will be provisioned.

If `replication-type` is set to `regional-pd`, a
[Regional Persistent Disk](https://cloud.google.com/compute/docs/disks/#repds)
will be provisioned. It&#39;s highly recommended to have
`volumeBindingMode: WaitForFirstConsumer` set, in which case when you create
a Pod that consumes a PersistentVolumeClaim which uses this StorageClass, a
Regional Persistent Disk is provisioned with two zones. One zone is the same
as the zone that the Pod is scheduled in. The other zone is randomly picked
from the zones available to the cluster. Disk zones can be further constrained
using `allowedTopologies`.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;code&gt;zone&lt;/code&gt; and &lt;code&gt;zones&lt;/code&gt; parameters are deprecated and replaced with
&lt;a href=&#34;#allowed-topologies&#34;&gt;allowedTopologies&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;gce-pd&#34;&gt;GCE PD&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/gce-pd&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pd-standard&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;fstype&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replication-type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;none&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;type&lt;/code&gt;: &lt;code&gt;pd-standard&lt;/code&gt; 或 &lt;code&gt;pd-ssd&lt;/code&gt;. 默认: &lt;code&gt;pd-standard&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;zone&lt;/code&gt; (废弃): GCE 区域. 如果 &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 都没有设置， 卷会在 k8s 集群中所有有节点的
活跃区别之间随机调度， &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 参数一定不要同时使用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;zones&lt;/code&gt; (废弃): 一个用逗号分隔的 GCE 区域列表。 如果没有设置，卷会在 k8s 集群中所有有节点的
活跃区别之间随机调度。 &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 参数一定不要同时使用。&lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 参数一定不要同时使用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;fstype&lt;/code&gt;: &lt;code&gt;ext4&lt;/code&gt; 或 &lt;code&gt;xfs&lt;/code&gt;. 默认: &lt;code&gt;ext4&lt;/code&gt;. 定义的文件系统类型必须被主机操作系统支持&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;replication-type&lt;/code&gt;: &lt;code&gt;none&lt;/code&gt; 或 &lt;code&gt;regional-pd&lt;/code&gt;. 默认: &lt;code&gt;none&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果 &lt;code&gt;replication-type&lt;/code&gt; 设置为 &lt;code&gt;none&lt;/code&gt;， 会供应一个常规的 PD。&lt;/p&gt;
&lt;p&gt;如果 &lt;code&gt;replication-type&lt;/code&gt; 设置为 &lt;code&gt;regional-pd&lt;/code&gt;， 会供应一个
&lt;a href=&#34;https://cloud.google.com/compute/docs/disks/#repds&#34;&gt;Regional Persistent Disk&lt;/a&gt;
强烈推荐同时设置 &lt;code&gt;volumeBindingMode: WaitForFirstConsumer&lt;/code&gt;。 在这种情况下，当创建一个
消费使用该 &lt;code&gt;StorageClass&lt;/code&gt; 的 PVC 时， 会供应两个区域持久化盘。 一个区域与 Pod 调度的区域相同，
另一个则随机到集群中其它的可用区域。 硬盘区域还可以使用  &lt;code&gt;allowedTopologies&lt;/code&gt; 添加更多多限制。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 参数已经废弃，被 &lt;a href=&#34;#allowed-topologies&#34;&gt;允许的拓扑&lt;/a&gt; 替换&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Glusterfs

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: &#34;http://127.0.0.1:8081&#34;
  clusterid: &#34;630372ccdc720a92c681fb928f27b53f&#34;
  restauthenabled: &#34;true&#34;
  restuser: &#34;admin&#34;
  secretNamespace: &#34;default&#34;
  secretName: &#34;heketi-secret&#34;
  gidMin: &#34;40000&#34;
  gidMax: &#34;50000&#34;
  volumetype: &#34;replicate:3&#34;
```

* `resturl`: Gluster REST service/Heketi service url which provision gluster
  volumes on demand. The general format should be `IPaddress:Port` and this is
  a mandatory parameter for GlusterFS dynamic provisioner. If Heketi service is
  exposed as a routable service in openshift/kubernetes setup, this can have a
  format similar to `http://heketi-storage-project.cloudapps.mystorage.com`
  where the fqdn is a resolvable Heketi service url.
* `restauthenabled` : Gluster REST service authentication boolean that enables
  authentication to the REST server. If this value is `&#34;true&#34;`, `restuser` and
  `restuserkey` or `secretNamespace` + `secretName` have to be filled. This
  option is deprecated, authentication is enabled when any of `restuser`,
  `restuserkey`, `secretName` or `secretNamespace` is specified.
* `restuser` : Gluster REST service/Heketi user who has access to create volumes
  in the Gluster Trusted Pool.
* `restuserkey` : Gluster REST service/Heketi user&#39;s password which will be used
  for authentication to the REST server. This parameter is deprecated in favor
  of `secretNamespace` + `secretName`.
* `secretNamespace`, `secretName` : Identification of Secret instance that
  contains user password to use when talking to Gluster REST service. These
  parameters are optional, empty password will be used when both
  `secretNamespace` and `secretName` are omitted. The provided secret must have
  type `&#34;kubernetes.io/glusterfs&#34;`, for example created in this way:

    ```
    kubectl create secret generic heketi-secret \
      --type=&#34;kubernetes.io/glusterfs&#34; --from-literal=key=&#39;opensesame&#39; \
      --namespace=default
    ```

    Example of a secret can be found in
    [glusterfs-provisioning-secret.yaml](https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/glusterfs/glusterfs-secret.yaml).

* `clusterid`: `630372ccdc720a92c681fb928f27b53f` is the ID of the cluster
  which will be used by Heketi when provisioning the volume. It can also be a
  list of clusterids, for example:
  `&#34;8452344e2becec931ece4e33c4674e4e,42982310de6c63381718ccfa6d8cf397&#34;`. This
  is an optional parameter.
* `gidMin`, `gidMax` : The minimum and maximum value of GID range for the
  StorageClass. A unique value (GID) in this range ( gidMin-gidMax ) will be
  used for dynamically provisioned volumes. These are optional values. If not
  specified, the volume will be provisioned with a value between 2000-2147483647
  which are defaults for gidMin and gidMax respectively.
* `volumetype` : The volume type and its parameters can be configured with this
  optional value. If the volume type is not mentioned, it&#39;s up to the provisioner
  to decide the volume type.

    For example:
    * Replica volume: `volumetype: replicate:3` where &#39;3&#39; is replica count.
    * Disperse/EC volume: `volumetype: disperse:4:2` where &#39;4&#39; is data and &#39;2&#39; is the redundancy count.
    * Distribute volume: `volumetype: none`

    For available volume types and administration options, refer to the
    [Administration Guide](https://access.redhat.com/documentation/en-US/Red_Hat_Storage/3.1/html/Administration_Guide/part-Overview.html).

    For further reference information, see
    [How to configure Heketi](https://github.com/heketi/heketi/wiki/Setting-up-the-topology).

    When persistent volumes are dynamically provisioned, the Gluster plugin
    automatically creates an endpoint and a headless service in the name
    `gluster-dynamic-&lt;claimname&gt;`. The dynamic endpoint and service are automatically
    deleted when the persistent volume claim is deleted.
 --&gt;
&lt;h3 id=&#34;glusterfs&#34;&gt;Glusterfs&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/glusterfs&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;resturl&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://127.0.0.1:8081&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;clusterid&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;630372ccdc720a92c681fb928f27b53f&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;restauthenabled&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;restuser&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;admin&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;secretNamespace&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;heketi-secret&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;gidMin&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;40000&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;gidMax&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;50000&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumetype&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;replicate:3&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;resturl&lt;/code&gt;: Gluster REST 服务/Heketi 服务 url, 用来根据需要供应 gluster 卷。 通常格式为
&lt;code&gt;IPaddress:Port&lt;/code&gt; 这是 GlusterFS 动态供应的必要参数。 如果 Heketi 服务是可路由的服务
在 openshift/kubernetes 设置中提供。 这会有一个类似 &lt;code&gt;http://heketi-storage-project.cloudapps.mystorage.com&lt;/code&gt;
格式的地址，其中的 fqdn 是 Heketi 服务的可解析 url.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;restauthenabled&lt;/code&gt; :Gluster REST 服务是否开启认证。 如果这个值是 &lt;code&gt;&amp;quot;true&amp;quot;&lt;/code&gt;， 则必须要提供
&lt;code&gt;restuser&lt;/code&gt; 和 &lt;code&gt;restuserkey&lt;/code&gt; 或 &lt;code&gt;secretNamespace&lt;/code&gt; + &lt;code&gt;secretName&lt;/code&gt;。 这个选项已经废弃
当 &lt;code&gt;restuser&lt;/code&gt;, &lt;code&gt;restuserkey&lt;/code&gt;, &lt;code&gt;secretName&lt;/code&gt; 或 &lt;code&gt;secretNamespace&lt;/code&gt; 有值时，默认就启用&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;restuser&lt;/code&gt; : Gluster REST 服务/Heketi 中可以访问并在 Gluster Trusted Pool 中创建卷的用户。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;restuserkey&lt;/code&gt; : Gluster REST 服务/Heketi 用户的密码，用于 REST 服务认证。 这个参数
已经废弃，推荐使用 &lt;code&gt;secretNamespace&lt;/code&gt; + &lt;code&gt;secretName&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;secretNamespace&lt;/code&gt;, &lt;code&gt;secretName&lt;/code&gt; :  确定在访问 Gluster REST 服务用户密码的 Secret
实例是哪个。这两个参数为可选， 如果它们都没有设置，则会使用空密码。 提供的 Secret 必要是
&lt;code&gt;&amp;quot;kubernetes.io/glusterfs&amp;quot;&lt;/code&gt; 类型的。 创建示例&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create secret generic heketi-secret \
  --type=&amp;quot;kubernetes.io/glusterfs&amp;quot; --from-literal=key=&#39;opensesame&#39; \
  --namespace=default
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;使用 Secret 的示例在这里
&lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/glusterfs/glusterfs-secret.yaml&#34;&gt;glusterfs-provisioning-secret.yaml&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;clusterid&lt;/code&gt;: &lt;code&gt;630372ccdc720a92c681fb928f27b53f&lt;/code&gt; 集群 ID，会在 Heketi 供应卷时用到。
也可以是集群 ID 的列表，例如: &lt;code&gt;&amp;quot;8452344e2becec931ece4e33c4674e4e,42982310de6c63381718ccfa6d8cf397&amp;quot;&lt;/code&gt;
这个参数为可选&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;gidMin&lt;/code&gt;, &lt;code&gt;gidMax&lt;/code&gt; : StorageClass GID 范围的最小值和最大值， 在这个范围(&lt;code&gt;gidMin&lt;/code&gt;-&lt;code&gt;gidMax&lt;/code&gt;)
中的一个唯一值(GID) 会用于动态供应卷。 这两个参数为可选。 如果没有指定，被供应卷的范围值为
&lt;code&gt;2000-2147483647&lt;/code&gt; 也就对应着默认的最小值(&lt;code&gt;gidMin&lt;/code&gt;)和最大值(&lt;code&gt;gidMax&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;volumetype&lt;/code&gt; : 卷类型及其参数可以使用这个参数配置，这是一个可选参数。 如果卷类型没有指定，
则其类型由供应者决定。&lt;/p&gt;
&lt;p&gt;例如:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;复制型卷: &lt;code&gt;volumetype: replicate:3&lt;/code&gt; 其中 3 是副本数&lt;/li&gt;
&lt;li&gt;Disperse/EC 卷 : &lt;code&gt;volumetype: disperse:4:2&lt;/code&gt; 其中 4 是数据 2 是冗余数&lt;/li&gt;
&lt;li&gt;分布式卷: &lt;code&gt;volumetype: none&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于可用的卷类型及其管理选项，见
&lt;a href=&#34;https://access.redhat.com/documentation/en-US/Red_Hat_Storage/3.1/html/Administration_Guide/part-Overview.html&#34;&gt;Administration Guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;更多信息见
&lt;a href=&#34;https://github.com/heketi/heketi/wiki/Setting-up-the-topology&#34;&gt;怎么配置 Heketi&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;当持久化卷是被动态供应时， Gluster 会自动创建一个 Endpoint 和一个无头 Service, 名称都是
&lt;code&gt;gluster-dynamic-&amp;lt;claimname&amp;gt;&lt;/code&gt;. 当 PVC 被删除时，对应的 Endpoint 和 Service 也会
动态自动删除。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### OpenStack Cinder

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gold
provisioner: kubernetes.io/cinder
parameters:
  availability: nova
```

* `availability`: Availability Zone. If not specified, volumes are generally
  round-robin-ed across all active zones where Kubernetes cluster has a node.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [deprecated]&lt;/code&gt;
&lt;/div&gt;
&lt;p&gt;This internal provisioner of OpenStack is deprecated. Please use &lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack&#34;&gt;the external cloud provider for OpenStack&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

--&gt;
&lt;h3 id=&#34;openstack-cinder&#34;&gt;OpenStack Cinder&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gold&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/cinder&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;availability&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nova&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;availability&lt;/code&gt;: 可用区域。 如果没有设置， 卷会在所有有 k8s 节点的区域中随机&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [deprecated]&lt;/code&gt;
&lt;/div&gt;
&lt;p&gt;这个 OpenStack 内部供应都已经废弃。 请使用
&lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack&#34;&gt;OpenStack 外部云提供者&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### vSphere

There are two types of provisioners for vSphere storage classes:

- [CSI provisioner](#csi-provisioner): `csi.vsphere.vmware.com`
- [vCP provisioner](#vcp-provisioner): `kubernetes.io/vsphere-volume`

In-tree provisioners are [deprecated](/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/#why-are-we-migrating-in-tree-plugins-to-csi). For more information on the CSI provisioner, see [Kubernetes vSphere CSI Driver](https://vsphere-csi-driver.sigs.k8s.io/) and [vSphereVolume CSI migration](/docs/concepts/storage/volumes/#csi-migration-5).
 --&gt;
&lt;h3 id=&#34;vsphere&#34;&gt;vSphere&lt;/h3&gt;
&lt;p&gt;vSphere 存储类别有两种类型的供应者:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#vsphere-provisioner-csi&#34;&gt;CSI 供应者&lt;/a&gt;: &lt;code&gt;csi.vsphere.vmware.com&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vcp-provisioner&#34;&gt;vCP 供应者&lt;/a&gt;: &lt;code&gt;kubernetes.io/vsphere-volume&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;内部供应都已经
&lt;a href=&#34;https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/#why-are-we-migrating-in-tree-plugins-to-csi&#34;&gt;废弃&lt;/a&gt;
更多关于 CSI 供应者的信息见
&lt;a href=&#34;https://vsphere-csi-driver.sigs.k8s.io/&#34;&gt;Kubernetes vSphere CSI Driver&lt;/a&gt; and &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi-migration-5&#34;&gt;vSphereVolume CSI migration&lt;/a&gt;.&lt;/p&gt;
&lt;!--
#### CSI Provisioner {#vsphere-provisioner-csi}

The vSphere CSI StorageClass provisioner works with Tanzu Kubernetes clusters. For an example, refer to the [vSphere CSI repository](https://raw.githubusercontent.com/kubernetes-sigs/vsphere-csi-driver/master/example/vanilla-k8s-file-driver/example-sc.yaml).
 --&gt;
&lt;h4 id=&#34;vsphere-provisioner-csi&#34;&gt;CSI 供应者&lt;/h4&gt;
&lt;p&gt;vSphere CSI StorageClass 供应者工作于 Tanzu k8s 集群中。 示例见
&lt;a href=&#34;https://raw.githubusercontent.com/kubernetes-sigs/vsphere-csi-driver/master/example/vanilla-k8s-file-driver/example-sc.yaml&#34;&gt;vSphere CSI 仓库&lt;/a&gt;.&lt;/p&gt;
&lt;!--
#### vCP Provisioner

The following examples use the VMware Cloud Provider (vCP) StorageClass provisioner.  

1. Create a StorageClass with a user specified disk format.

    ```yaml
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: fast
    provisioner: kubernetes.io/vsphere-volume
    parameters:
      diskformat: zeroedthick
    ```

    `diskformat`: `thin`, `zeroedthick` and `eagerzeroedthick`. Default: `&#34;thin&#34;`.

2. Create a StorageClass with a disk format on a user specified datastore.

    ```yaml
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: fast
    provisioner: kubernetes.io/vsphere-volume
    parameters:
        diskformat: zeroedthick
        datastore: VSANDatastore
    ```

    `datastore`: The user can also specify the datastore in the StorageClass.
    The volume will be created on the datastore specified in the StorageClass,
    which in this case is `VSANDatastore`. This field is optional. If the
    datastore is not specified, then the volume will be created on the datastore
    specified in the vSphere config file used to initialize the vSphere Cloud
    Provider.

3. Storage Policy Management inside kubernetes

    * Using existing vCenter SPBM policy

        One of the most important features of vSphere for Storage Management is
        policy based Management. Storage Policy Based Management (SPBM) is a
        storage policy framework that provides a single unified control plane
        across a broad range of data services and storage solutions. SPBM enables
        vSphere administrators to overcome upfront storage provisioning challenges,
        such as capacity planning, differentiated service levels and managing
        capacity headroom.

        The SPBM policies can be specified in the StorageClass using the
        `storagePolicyName` parameter.

    * Virtual SAN policy support inside Kubernetes

        Vsphere Infrastructure (VI) Admins will have the ability to specify custom
        Virtual SAN Storage Capabilities during dynamic volume provisioning. You
        can now define storage requirements, such as performance and availability,
        in the form of storage capabilities during dynamic volume provisioning.
        The storage capability requirements are converted into a Virtual SAN
        policy which are then pushed down to the Virtual SAN layer when a
        persistent volume (virtual disk) is being created. The virtual disk is
        distributed across the Virtual SAN datastore to meet the requirements.

        You can see [Storage Policy Based Management for dynamic provisioning of volumes](https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/policy-based-mgmt.html)
        for more details on how to use storage policies for persistent volumes
        management.

There are few
[vSphere examples](https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere)
which you try out for persistent volume management inside Kubernetes for vSphere.
 --&gt;
&lt;h4 id=&#34;vcp-provisioner&#34;&gt;vCP 供应者&lt;/h4&gt;
&lt;p&gt;The following examples use the VMware Cloud Provider (vCP) StorageClass provisioner.&lt;br&gt;
以下示例使用 VMware Cloud Provider (vCP) StorageClass 应用者。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;创建一个用户指定硬盘模式的 StorageClass&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fast&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/vsphere-volume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;diskformat&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zeroedthick&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;diskformat&lt;/code&gt;: &lt;code&gt;thin&lt;/code&gt;, &lt;code&gt;zeroedthick&lt;/code&gt; 和 &lt;code&gt;eagerzeroedthick&lt;/code&gt;. 默认: &lt;code&gt;&amp;quot;thin&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建一个用于指定数据源和基于该数据库的磁盘模式的 StorageClass&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fast&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/vsphere-volume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;diskformat&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zeroedthick&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;datastore&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VSANDatastore&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;datastore&lt;/code&gt;:  用户也可以在 StorageClass 中指定数据源(datastore). 卷会在 StorageClass
指定的数据源上创建，本例中就是 &lt;code&gt;VSANDatastore&lt;/code&gt;. 该字段为可选。 如果没有指定数据源，
则会使用 vSphere Cloud Provider 初始化时使用的 vSphere 配置文件中指定的数据源来创建卷。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 k8s 中的存储策略管理&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;使用已经存在的 vCenter SPBM 策略&lt;/p&gt;
&lt;p&gt;vSphere 对于存储管理的重要特性之一就是基于策略的管理。 基于策略的存储管理(SPBM)是一个
存储策略框架， 它为大范围的数据服务和存储方案提供一个统一的控制台。 SPBM 让 vSphere
管理员能够应对存储供应的挑战，如 容量计划，细分服务级别，可用空间管理(capacity headroom)&lt;/p&gt;
&lt;p&gt;SPBM 可能通过 StorageClass 中的 &lt;code&gt;storagePolicyName&lt;/code&gt; 来指定。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;k8s 内部支持的虚拟 SAN 策略&lt;/p&gt;
&lt;p&gt;Vsphere 基础设施 (VI) 管理员可以在动态卷供应时指定自定义的虚拟 SAN 存储能力。
这时候可以在动态供应时以存储能力的形式定义存储要求， 如性能和可用性。 在一个持久化卷(虚拟磁盘)
被创建时，存储能力要求会被转换成虚拟 SAN 策略，再被推到虚拟 SAN 层。虚拟磁盘会发布在
满足要求的虚拟 SAN 数据中。&lt;/p&gt;
&lt;p&gt;更多关于怎么使用持久化卷管理的存储策略见
&lt;a href=&#34;https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/policy-based-mgmt.html&#34;&gt;Storage Policy Based Management for dynamic provisioning of volumes&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些
&lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere&#34;&gt;vSphere 示例&lt;/a&gt;
可以用来熟悉用于 vSphere 的 k8s 集群中的持久化卷管理&lt;/p&gt;
&lt;!--
### Ceph RBD

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 10.16.153.105:6789
  adminId: kube
  adminSecretName: ceph-secret
  adminSecretNamespace: kube-system
  pool: kube
  userId: kube
  userSecretName: ceph-secret-user
  userSecretNamespace: default
  fsType: ext4
  imageFormat: &#34;2&#34;
  imageFeatures: &#34;layering&#34;
```

* `monitors`: Ceph monitors, comma delimited. This parameter is required.
* `adminId`: Ceph client ID that is capable of creating images in the pool.
  Default is &#34;admin&#34;.
* `adminSecretName`: Secret Name for `adminId`. This parameter is required.
  The provided secret must have type &#34;kubernetes.io/rbd&#34;.
* `adminSecretNamespace`: The namespace for `adminSecretName`. Default is &#34;default&#34;.
* `pool`: Ceph RBD pool. Default is &#34;rbd&#34;.
* `userId`: Ceph client ID that is used to map the RBD image. Default is the
  same as `adminId`.
* `userSecretName`: The name of Ceph Secret for `userId` to map RBD image. It
  must exist in the same namespace as PVCs. This parameter is required.
  The provided secret must have type &#34;kubernetes.io/rbd&#34;, for example created in this
  way:

    ```shell
    kubectl create secret generic ceph-secret --type=&#34;kubernetes.io/rbd&#34; \
      --from-literal=key=&#39;QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ==&#39; \
      --namespace=kube-system
    ```
* `userSecretNamespace`: The namespace for `userSecretName`.
* `fsType`: fsType that is supported by kubernetes. Default: `&#34;ext4&#34;`.
* `imageFormat`: Ceph RBD image format, &#34;1&#34; or &#34;2&#34;. Default is &#34;2&#34;.
* `imageFeatures`: This parameter is optional and should only be used if you
  set `imageFormat` to &#34;2&#34;. Currently supported features are `layering` only.
  Default is &#34;&#34;, and no features are turned on.
 --&gt;
&lt;h3 id=&#34;ceph-rbd&#34;&gt;Ceph RBD&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fast&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/rbd&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;monitors&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10.16.153.105&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;6789&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;adminId&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;adminSecretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ceph-secret&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;adminSecretNamespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube-system&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;pool&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;userId&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;userSecretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ceph-secret-user&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;userSecretNamespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;imageFormat&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;imageFeatures&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;layering&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;monitors&lt;/code&gt;: Ceph monitor, 逗号分隔，这是个必要参数&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;adminId&lt;/code&gt;: 能够在 pool 中创建镜像的 Ceph 客户 ID， 默认为 &lt;code&gt;&amp;quot;admin&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;adminSecretName&lt;/code&gt;: &lt;code&gt;adminId&lt;/code&gt; 的 &lt;code&gt;Secret&lt;/code&gt; 名称。 这是个必要参数。 这个 &lt;code&gt;Secret&lt;/code&gt;
必须包含 &amp;ldquo;kubernetes.io/rbd&amp;rdquo; 类型&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;adminSecretNamespace&lt;/code&gt;: &lt;code&gt;adminSecretName&lt;/code&gt; 对应的命名空间。 默认为 &lt;code&gt;&amp;quot;default&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;pool&lt;/code&gt;: Ceph RBD pool。 默认是 &lt;code&gt;&amp;quot;rbd&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;userId&lt;/code&gt;: 用来与 RBD 镜像关联的 Ceph 客户 ID。 默认与 &lt;code&gt;adminId&lt;/code&gt; 相同。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;userSecretName&lt;/code&gt;: 与 RBD image 镜像关联的 Ceph 客户 的 &lt;code&gt;Secret&lt;/code&gt;。 这个  &lt;code&gt;Secret&lt;/code&gt; 必须
与 PVC 在同一个命名空间。 这是一个必要参数。这个  &lt;code&gt;Secret&lt;/code&gt;必须包含 &amp;ldquo;kubernetes.io/rbd&amp;rdquo; 类型。
以下为创建示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret generic ceph-secret --type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/rbd&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ==&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  --namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;userSecretNamespace&lt;/code&gt;: &lt;code&gt;userSecretName&lt;/code&gt; 的命名空间&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;fsType&lt;/code&gt;: 受 k8s 支持的文件系统。 默认 &lt;code&gt;&amp;quot;ext4&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;imageFormat&lt;/code&gt;: Ceph RBD image 模式，&amp;ldquo;1&amp;rdquo; or &amp;ldquo;2&amp;rdquo;. 默认为 &amp;ldquo;2&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;imageFeatures&lt;/code&gt;: 这个参数为可选，但只用在 &lt;code&gt;imageFormat&lt;/code&gt; 设置为 &amp;ldquo;2&amp;rdquo; 时。 目前支持的特性
只有 &lt;code&gt;layering&lt;/code&gt;， 默认是 &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;，表示没有开启任何特性。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Quobyte

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: slow
provisioner: kubernetes.io/quobyte
parameters:
    quobyteAPIServer: &#34;http://138.68.74.142:7860&#34;
    registry: &#34;138.68.74.142:7861&#34;
    adminSecretName: &#34;quobyte-admin-secret&#34;
    adminSecretNamespace: &#34;kube-system&#34;
    user: &#34;root&#34;
    group: &#34;root&#34;
    quobyteConfig: &#34;BASE&#34;
    quobyteTenant: &#34;DEFAULT&#34;
```

* `quobyteAPIServer`: API Server of Quobyte in the format
  `&#34;http(s)://api-server:7860&#34;`
* `registry`: Quobyte registry to use to mount the volume. You can specify the
  registry as ``&lt;host&gt;:&lt;port&gt;`` pair or if you want to specify multiple
  registries you just have to put a comma between them e.q.
  ``&lt;host1&gt;:&lt;port&gt;,&lt;host2&gt;:&lt;port&gt;,&lt;host3&gt;:&lt;port&gt;``.
  The host can be an IP address or if you have a working DNS you can also
  provide the DNS names.
* `adminSecretNamespace`: The namespace for `adminSecretName`.
  Default is &#34;default&#34;.
* `adminSecretName`: secret that holds information about the Quobyte user and
  the password to authenticate against the API server. The provided secret
  must have type &#34;kubernetes.io/quobyte&#34; and the keys `user` and `password`,
  for example:

    ```shell
    kubectl create secret generic quobyte-admin-secret \
      --type=&#34;kubernetes.io/quobyte&#34; --from-literal=user=&#39;admin&#39; --from-literal=password=&#39;opensesame&#39; \
      --namespace=kube-system
    ```

* `user`: maps all access to this user. Default is &#34;root&#34;.
* `group`: maps all access to this group. Default is &#34;nfsnobody&#34;.
* `quobyteConfig`: use the specified configuration to create the volume. You
  can create a new configuration or modify an existing one with the Web
  console or the quobyte CLI. Default is &#34;BASE&#34;.
* `quobyteTenant`: use the specified tenant ID to create/delete the volume.
  This Quobyte tenant has to be already present in Quobyte.
  Default is &#34;DEFAULT&#34;.
 --&gt;
&lt;h3 id=&#34;quobyte&#34;&gt;Quobyte&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
   &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/quobyte&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;quobyteAPIServer&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://138.68.74.142:7860&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;registry&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;138.68.74.142:7861&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;adminSecretName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;quobyte-admin-secret&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;adminSecretNamespace&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kube-system&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;user&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;root&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;group&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;root&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;quobyteConfig&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;BASE&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;quobyteTenant&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;DEFAULT&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;quobyteAPIServer&lt;/code&gt;:  &lt;code&gt;&amp;quot;http(s)://api-server:7860&amp;quot;&lt;/code&gt; 格式的 Quobyte API 服务&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;registry&lt;/code&gt;: 用于挂载卷的 Quobyte 注册中心。 注册中心的格式为 &lt;code&gt;&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;&lt;/code&gt;， 如果要
设置多个则用逗号分隔，如 &lt;code&gt;&amp;lt;host1&amp;gt;:&amp;lt;port&amp;gt;,&amp;lt;host2&amp;gt;:&amp;lt;port&amp;gt;,&amp;lt;host3&amp;gt;:&amp;lt;port&amp;gt;&lt;/code&gt;.
其中 host 是一个 IP 地址或如果有 DNS 服务，则可以使用 DNS 名称。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;adminSecretName&lt;/code&gt;: 用于存放 Quobyte API 服务对应用户密码信息的 &lt;code&gt;Secret&lt;/code&gt;. 这个 &lt;code&gt;Secret&lt;/code&gt;
必须包含 &amp;ldquo;kubernetes.io/quobyte&amp;rdquo; 类型和 &lt;code&gt;user&lt;/code&gt; 和 &lt;code&gt;password&lt;/code&gt; 键。
示例创建命令如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret generic quobyte-admin-secret &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  --type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/quobyte&amp;#34;&lt;/span&gt; --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;user&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;admin&amp;#39;&lt;/span&gt; --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;password&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;opensesame&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  --namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;adminSecretNamespace&lt;/code&gt;: &lt;code&gt;adminSecretName&lt;/code&gt; 所属的命名空间，默认为 &lt;code&gt;&amp;quot;default&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;user&lt;/code&gt;: 将所有访问指向这个用户。 默认为 &lt;code&gt;&amp;quot;root&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;group&lt;/code&gt;: 将所有访问指向这个用户组。 默认为 &lt;code&gt;&amp;quot;nfsnobody&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;quobyteConfig&lt;/code&gt;: 使用这个配置创建卷。 可通过控制台 quobyte 命令行或以创建一个新的配置
或修改已经存在的配置。 默认为 &amp;ldquo;BASE&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;quobyteTenant&lt;/code&gt;: 使用指定租户 ID 来创建/删除卷。 这个 Quobyte 租户必须是已经存在于 Quobyte 中。
默认为 &amp;ldquo;DEFAULT&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Azure Disk

#### Azure Unmanaged Disk storage class {#azure-unmanaged-disk-storage-class}

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/azure-disk
parameters:
  skuName: Standard_LRS
  location: eastus
  storageAccount: azure_storage_account_name
```

* `skuName`: Azure storage account Sku tier. Default is empty.
* `location`: Azure storage account location. Default is empty.
* `storageAccount`: Azure storage account name. If a storage account is provided,
  it must reside in the same resource group as the cluster, and `location` is
  ignored. If a storage account is not provided, a new storage account will be
  created in the same resource group as the cluster.
 --&gt;
&lt;h3 id=&#34;azure-disk&#34;&gt;Azure 磁盘&lt;/h3&gt;
&lt;h4 id=&#34;azure-unmanaged-disk-storage-class&#34;&gt;Azure 非托管磁盘 StorageClass&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/azure-disk&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;skuName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Standard_LRS&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;location&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;eastus&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageAccount&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;azure_storage_account_name&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;skuName&lt;/code&gt;: Azure 存储账户 Sku 层。 默认为空&lt;/li&gt;
&lt;li&gt;&lt;code&gt;location&lt;/code&gt;: Azure 存储账户位置。 默认为空&lt;/li&gt;
&lt;li&gt;&lt;code&gt;storageAccount&lt;/code&gt;: Azure 存储账户名称。 如果提供了存储账户，必须与集群在同一个资源组，
同时 &lt;code&gt;location&lt;/code&gt; 会被忽略。 如果没有提供存储账户，会在集群所在资源组创建一个新的存储账户。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;azure-disk-storage-class&#34;&gt;Azure 磁盘 StorageClass (starting from v1.7.2)&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/azure-disk&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;storageaccounttype&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Standard_LRS&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Shared&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;storageaccounttype&lt;/code&gt;: Azure 存储账户类型。 默认值 (原文档有问题，暂不知道默认值是啥)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kind&lt;/code&gt;: 可用值有 &lt;code&gt;shared&lt;/code&gt; (默认), &lt;code&gt;dedicated&lt;/code&gt;, &lt;code&gt;managed&lt;/code&gt;.
当 &lt;code&gt;kind&lt;/code&gt; 是 &lt;code&gt;shared&lt;/code&gt; 时，所有非托管的硬盘会在与集群同一个资源组创建几个分享存储账户。
当 &lt;code&gt;kind&lt;/code&gt; 是 &lt;code&gt;dedicated&lt;/code&gt; 时，在与集群同一个资源组中会为每一个新创建的非托管磁盘创建一个独立
的存储账户。
当 &lt;code&gt;kind&lt;/code&gt; 是 &lt;code&gt;managed&lt;/code&gt; 时，所有托管的磁盘都会创建在与集群同一个资源组&lt;/li&gt;
&lt;li&gt;&lt;code&gt;resourceGroup&lt;/code&gt;: 指定 Azure 磁盘创建的资源组。 必须是一个已经存在的资源组名称。 如果没有指定，
磁盘会放在与当前 k8s 集群所在的这个资源组。&lt;/li&gt;
&lt;li&gt;高级的 VM 可以挂载 Standard_LRS 和 Premium_LRS 磁盘，标准 VM 只可以挂载 Standard_LRS 磁盘。&lt;/li&gt;
&lt;li&gt;托管 VM 只能挂载托管磁盘，非托管 VM 只能挂载 非托管&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Azure File

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: azurefile
provisioner: kubernetes.io/azure-file
parameters:
  skuName: Standard_LRS
  location: eastus
  storageAccount: azure_storage_account_name
```

* `skuName`: Azure storage account Sku tier. Default is empty.
* `location`: Azure storage account location. Default is empty.
* `storageAccount`: Azure storage account name.  Default is empty. If a storage
  account is not provided, all storage accounts associated with the resource
  group are searched to find one that matches `skuName` and `location`. If a
  storage account is provided, it must reside in the same resource group as the
  cluster, and `skuName` and `location` are ignored.
* `secretNamespace`: the namespace of the secret that contains the Azure Storage
  Account Name and Key. Default is the same as the Pod.
* `secretName`: the name of the secret that contains the Azure Storage Account Name and
  Key. Default is `azure-storage-account-&lt;accountName&gt;-secret`
* `readOnly`: a flag indicating whether the storage will be mounted as read only.
  Defaults to false which means a read/write mount. This setting will impact the
  `ReadOnly` setting in VolumeMounts as well.

During storage provisioning, a secret named by `secretName` is created for the
mounting credentials. If the cluster has enabled both
[RBAC](/docs/reference/access-authn-authz/rbac/) and
[Controller Roles](/docs/reference/access-authn-authz/rbac/#controller-roles),
add the `create` permission of resource `secret` for clusterrole
`system:controller:persistent-volume-binder`.

In a multi-tenancy context, it is strongly recommended to set the value for
`secretNamespace` explicitly, otherwise the storage account credentials may
be read by other users.
 --&gt;
&lt;h3 id=&#34;azure-file&#34;&gt;Azure 文件&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;azurefile&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/azure-file&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;skuName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Standard_LRS&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;location&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;eastus&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageAccount&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;azure_storage_account_name&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;skuName&lt;/code&gt;: Azure 存储账户 Sku 层。 默认为空&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;location&lt;/code&gt;: Azure 存储账户位置。 默认为空&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;storageAccount&lt;/code&gt;: Azure 存储账户名称。 默认为空。 如果没有提供存储账户，会搜索与资源组
关联所有的存储账户中查找一个匹配 &lt;code&gt;skuName&lt;/code&gt; 和 &lt;code&gt;location&lt;/code&gt; 的那个。如果提供了存储账户
则必须是与集群在同一个资源组，并且 &lt;code&gt;skuName&lt;/code&gt; 和 &lt;code&gt;location&lt;/code&gt; 会被忽略。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;secretNamespace&lt;/code&gt;: 包含 Azure 存储账户名称的 key 的 Secret 所在的命名空间。 默认与 Pod
相同&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;secretName&lt;/code&gt;: 包含 Azure 存储账户名称的 key 的 Secret 的名称。 默认为
&lt;code&gt;azure-storage-account-&amp;lt;accountName&amp;gt;-secret&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;readOnly&lt;/code&gt;: 一个标记这个存储是否以只读方式挂载的标记。默认为 false 也就是以读写方式挂载。
这个配置会与 VolumeMount 的 &lt;code&gt;ReadOnly&lt;/code&gt; 设置有冲突&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在存储供应时会为挂载凭证创建一个名字为 &lt;code&gt;secretName&lt;/code&gt; 字段值的 Secret。如果集群启用了
&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/rbac/&#34;&gt;RBAC&lt;/a&gt; 和
&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/rbac/#controller-roles&#34;&gt;Controller Roles&lt;/a&gt;,
为要 &lt;code&gt;clusterrole&lt;/code&gt; &lt;code&gt;system:controller:persistent-volume-binder&lt;/code&gt; 中的 &lt;code&gt;secret&lt;/code&gt; 资源添加
&lt;code&gt;create&lt;/code&gt; 权限。&lt;/p&gt;
&lt;p&gt;在一个多租户的上下文下， 强烈建议显示地设置 &lt;code&gt;secretNamespace&lt;/code&gt; 值， 否则存储账户的凭据可能被
其它用户访问。&lt;/p&gt;
&lt;!--
### Portworx Volume

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: portworx-io-priority-high
provisioner: kubernetes.io/portworx-volume
parameters:
  repl: &#34;1&#34;
  snap_interval:   &#34;70&#34;
  priority_io:  &#34;high&#34;

```

* `fs`: filesystem to be laid out: `none/xfs/ext4` (default: `ext4`).
* `block_size`: block size in Kbytes (default: `32`).
* `repl`: number of synchronous replicas to be provided in the form of
  replication factor `1..3` (default: `1`) A string is expected here i.e.
  `&#34;1&#34;` and not `1`.
* `priority_io`: determines whether the volume will be created from higher
  performance or a lower priority storage `high/medium/low` (default: `low`).
* `snap_interval`: clock/time interval in minutes for when to trigger snapshots.
  Snapshots are incremental based on difference with the prior snapshot, 0
  disables snaps (default: `0`). A string is expected here i.e.
  `&#34;70&#34;` and not `70`.
* `aggregation_level`: specifies the number of chunks the volume would be
  distributed into, 0 indicates a non-aggregated volume (default: `0`). A string
  is expected here i.e. `&#34;0&#34;` and not `0`
* `ephemeral`: specifies whether the volume should be cleaned-up after unmount
  or should be persistent. `emptyDir` use case can set this value to true and
  `persistent volumes` use case such as for databases like Cassandra should set
  to false, `true/false` (default `false`). A string is expected here i.e.
  `&#34;true&#34;` and not `true`.
 --&gt;
&lt;h3 id=&#34;portworx-volume&#34;&gt;Portworx 卷&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;portworx-io-priority-high&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/portworx-volume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;repl&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;snap_interval&lt;/span&gt;:   &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;70&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;priority_io&lt;/span&gt;:  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;high&amp;#34;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fs&lt;/code&gt;: 文件系统 &lt;code&gt;none/xfs/ext4&lt;/code&gt; (默认: &lt;code&gt;ext4&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;block_size&lt;/code&gt;: 块大小，单位千字节 (默认: &lt;code&gt;32&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;repl&lt;/code&gt;: 并发副本的数量，副本倍率格式为 &lt;code&gt;1..3&lt;/code&gt; (默认: &lt;code&gt;1&lt;/code&gt;) 这里值是字符串，也就是 &lt;code&gt;&amp;quot;1&amp;quot;&lt;/code&gt; 而不是 &lt;code&gt;1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;priority_io&lt;/code&gt;: 决定卷以什么级别的性能创建。&lt;code&gt;high/medium/low&lt;/code&gt; (默认: &lt;code&gt;low&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;snap_interval&lt;/code&gt;: 触发快照的时间间隔，快照是基于之前的快照增量创建，0 就是禁用快照(默认: &lt;code&gt;0&lt;/code&gt;).
这里值是字符串，也就是 &lt;code&gt;&amp;quot;70&amp;quot;&lt;/code&gt; 而不是 &lt;code&gt;70&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;aggregation_level&lt;/code&gt;: 指定卷分布的块数量， 0 表示非聚合卷(默认为: &lt;code&gt;0&lt;/code&gt;)
这里值是字符串，也就是 &lt;code&gt;&amp;quot;0&amp;quot;&lt;/code&gt; 而不是 &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ephemeral&lt;/code&gt;: 指定这个卷是否在卸载时清查或应该持久存在。 &lt;code&gt;emptyDir&lt;/code&gt; 情况下可以将该值设置为 true
&lt;code&gt;persistent volumes&lt;/code&gt; 情况下，如例如 Cassandra 这些数据库，应该设置为 false,  &lt;code&gt;true/false&lt;/code&gt; (默认为 &lt;code&gt;false&lt;/code&gt;).
这个字段的值是一个字段串，是&lt;code&gt;&amp;quot;true&amp;quot;&lt;/code&gt; 而不是 &lt;code&gt;true&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### ScaleIO

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/scaleio
parameters:
  gateway: https://192.168.99.200:443/api
  system: scaleio
  protectionDomain: pd0
  storagePool: sp1
  storageMode: ThinProvisioned
  secretRef: sio-secret
  readOnly: false
  fsType: xfs
```

* `provisioner`: attribute is set to `kubernetes.io/scaleio`
* `gateway`: address to a ScaleIO API gateway (required)
* `system`: the name of the ScaleIO system (required)
* `protectionDomain`: the name of the ScaleIO protection domain (required)
* `storagePool`: the name of the volume storage pool (required)
* `storageMode`: the storage provision mode: `ThinProvisioned` (default) or
  `ThickProvisioned`
* `secretRef`: reference to a configured Secret object (required)
* `readOnly`: specifies the access mode to the mounted volume (default false)
* `fsType`: the file system to use for the volume (default ext4)

The ScaleIO Kubernetes volume plugin requires a configured Secret object.
The secret must be created with type `kubernetes.io/scaleio` and use the same
namespace value as that of the PVC where it is referenced
as shown in the following command:

```shell
kubectl create secret generic sio-secret --type=&#34;kubernetes.io/scaleio&#34; \
--from-literal=username=sioadmin --from-literal=password=d2NABDNjMA== \
--namespace=default
```
 --&gt;
&lt;h3 id=&#34;scaleio&#34;&gt;ScaleIO&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/scaleio&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;gateway&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://192.168.99.200:443/api&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;system&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;scaleio&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;protectionDomain&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pd0&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storagePool&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;sp1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ThinProvisioned&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;secretRef&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;sio-secret&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;xfs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;provisioner&lt;/code&gt;: 该属性设置为 &lt;code&gt;kubernetes.io/scaleio&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gateway&lt;/code&gt;: ScaleIO API 网关地址(必要)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;system&lt;/code&gt;: ScaleIO 系统名称(必要)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;protectionDomain&lt;/code&gt;: ScaleIO 保护域名名称 (必要)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;storagePool&lt;/code&gt;: 卷存储池名称(必要)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;storageMode&lt;/code&gt;: 存储供应模式 &lt;code&gt;ThinProvisioned&lt;/code&gt; (默认) 或 &lt;code&gt;ThickProvisioned&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;secretRef&lt;/code&gt;: 配置的 Secret 对象(必要)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;readOnly&lt;/code&gt;: 指定挂载卷访问模式(默认为 &lt;code&gt;&amp;quot;false&amp;quot;&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fsType&lt;/code&gt;: 卷使用的文件系统(默认 &lt;code&gt;ext4&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ScaleIO k8s 卷插件需要一个配置 &lt;code&gt;Secret&lt;/code&gt; 对象。 &lt;code&gt;Secret&lt;/code&gt; 必须要以 &lt;code&gt;kubernetes.io/scaleio&lt;/code&gt;
类型创建并且必须与 PVC 在同一个命名空间。 以下为创建示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret generic sio-secret --type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/scaleio&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;username&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;sioadmin --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;password&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;d2NABDNjMA&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;default
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### StorageOS

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/storageos
parameters:
  pool: default
  description: Kubernetes volume
  fsType: ext4
  adminSecretNamespace: default
  adminSecretName: storageos-secret
```

* `pool`: The name of the StorageOS distributed capacity pool to provision the
  volume from.  Uses the `default` pool which is normally present if not specified.
* `description`: The description to assign to volumes that were created dynamically.
  All volume descriptions will be the same for the storage class, but different
  storage classes can be used to allow descriptions for different use cases.
  Defaults to `Kubernetes volume`.
* `fsType`: The default filesystem type to request. Note that user-defined rules
  within StorageOS may override this value.  Defaults to `ext4`.
* `adminSecretNamespace`: The namespace where the API configuration secret is
  located. Required if adminSecretName set.
* `adminSecretName`: The name of the secret to use for obtaining the StorageOS
  API credentials. If not specified, default values will be attempted.

The StorageOS Kubernetes volume plugin can use a Secret object to specify an
endpoint and credentials to access the StorageOS API. This is only required when
the defaults have been changed.
The secret must be created with type `kubernetes.io/storageos` as shown in the
following command:

```shell
kubectl create secret generic storageos-secret \
--type=&#34;kubernetes.io/storageos&#34; \
--from-literal=apiAddress=tcp://localhost:5705 \
--from-literal=apiUsername=storageos \
--from-literal=apiPassword=storageos \
--namespace=default
```

Secrets used for dynamically provisioned volumes may be created in any namespace
and referenced with the `adminSecretNamespace` parameter. Secrets used by
pre-provisioned volumes must be created in the same namespace as the PVC that
references it.
 --&gt;
&lt;h3 id=&#34;storageos&#34;&gt;StorageOS &lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fast&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/storageos&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;pool&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;description&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Kubernetes volume&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;adminSecretNamespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;adminSecretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storageos-secret&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;pool&lt;/code&gt;: 供应卷的 StorageOS 分布式容量池的名称 ?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;description&lt;/code&gt;: 分配给动态供应的卷的描述。 同样 StorageClass 的所有卷的描述都是一样的，
不同的 StorageClass 可以根据不现的应用场景使用不同的描述。 默认为 &lt;code&gt;Kubernetes volume&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;fsType&lt;/code&gt;: 申请的默认文件系统类型。 StorageOS 中的用户定义规则可能会覆盖该值。 默认为 &lt;code&gt;ext4&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;adminSecretNamespace&lt;/code&gt;: API 配置的 Secret 所在的命名空间。 如果设置了 adminSecretName
则需要设置该字段&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;adminSecretName&lt;/code&gt;: 获取 StorageOS API 凭据的 Secret 名称。如果没有设置，则会尝试使用
默认值&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;StorageOS 的 k8s 卷插件可以使用一个 Secret 对象来指定访问StorageOS API 的地址和凭据。
只有在修改默认情况需要配置该值。
Secret 必须要以 &lt;code&gt;kubernetes.io/storageos&lt;/code&gt; 类型创建。创建命令示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret generic storageos-secret &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/storageos&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;apiAddress&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tcp://localhost:5705 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;apiUsername&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;storageos &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;apiPassword&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;storageos &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;default
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;卷动态供应使用的 Secret 可以创建在任意命名空间中，通过 &lt;code&gt;adminSecretNamespace&lt;/code&gt; 参数引用。
预创建卷所使用的 Secret 必须与使用它的 PVC 在同一个命名空间&lt;/p&gt;
&lt;!--
### Local






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [stable]&lt;/code&gt;
&lt;/div&gt;



```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```

Local volumes do not currently support dynamic provisioning, however a StorageClass
should still be created to delay volume binding until Pod scheduling. This is
specified by the `WaitForFirstConsumer` volume binding mode.

Delaying volume binding allows the scheduler to consider all of a Pod&#39;s
scheduling constraints when choosing an appropriate PersistentVolume for a
PersistentVolumeClaim.
 --&gt;
&lt;h3 id=&#34;local&#34;&gt;Local&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;local-storage&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/no-provisioner&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;volumeBindingMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;WaitForFirstConsumer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Local 卷目前不支持动态供应， 但 StorageClass 也可以用来让卷绑定发生在 Pod 调度之后。
可以通过设置卷绑定模式为 &lt;code&gt;WaitForFirstConsumer&lt;/code&gt; 达到这个目的&lt;/p&gt;
&lt;p&gt;延迟绑定可以在为 PVC 选择恰当的 PV 时考虑 Pod 所有约束。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ingress</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/ingress/</link>
      <pubDate>Mon, 21 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/ingress/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- bprashanth
title: Ingress
content_type: concept
weight: 40
--- --&gt;
&lt;!-- overview --&gt;
&lt;p&gt;




&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;一个用于管理外部访问集群内 Service 的 API 对象，通常是 HTTP。&lt;/p&gt;
&lt;p&gt;Ingress 还可以提供负载均衡，SSL 和基于名称的虚拟主机。&lt;/p&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Terminology

For clarity, this guide defines the following terms:

* Node: A worker machine in Kubernetes, part of a cluster.
* Cluster: A set of Nodes that run containerized applications managed by Kubernetes. For this example, and in most common Kubernetes deployments, nodes in the cluster are not part of the public internet.
* Edge router: A router that enforces the firewall policy for your cluster. This could be a gateway managed by a cloud provider or a physical piece of hardware.
* Cluster network: A set of links, logical or physical, that facilitate communication within a cluster according to the Kubernetes [networking model](/docs/concepts/cluster-administration/networking/).
* Service: A Kubernetes &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; that identifies a set of Pods using &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels&#39; target=&#39;_blank&#39;&gt;label&lt;span class=&#39;tooltip-text&#39;&gt;在对象上标上对用户有意义和有关联的属性&lt;/span&gt;
&lt;/a&gt; selectors. Unless mentioned otherwise, Services are assumed to have virtual IPs only routable within the cluster network.
 --&gt;
&lt;h2 id=&#34;术语&#34;&gt;术语&lt;/h2&gt;
&lt;p&gt;为了明确，本文定义了如下术语:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节点(Node): k8s 中的一个工作机，集群的一部分。&lt;/li&gt;
&lt;li&gt;集群(Cluster): 一组由运行由 k8s 管理的容器化应用的节点集合。在本例和大多数常用的 k8s 部署
中，集群中的节点是不在公网上的。&lt;/li&gt;
&lt;li&gt;边缘路由: 一个为集群执行防火墙策略的路由。 可以是由云提供商管理的网关或一个物理硬件。&lt;/li&gt;
&lt;li&gt;集群网络: 一组逻辑的或物理的连接，这些连接根据 k8s
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/cluster-administration/networking/&#34;&gt;网络模型&lt;/a&gt;简化集群内的通信&lt;/li&gt;
&lt;li&gt;Service: 一个 k8s &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 就是使用
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels&#39; target=&#39;_blank&#39;&gt;标签&lt;span class=&#39;tooltip-text&#39;&gt;在对象上标上对用户有意义和有关联的属性&lt;/span&gt;
&lt;/a&gt; 选择器区分一组 Pod 的抽象概念。 如果没有特别
说明， Service 假定是有一个只能在集群内路由的虚拟 IP 的。&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## What is Ingress?

[Ingress](/docs/reference/generated/kubernetes-api/v1.19/#ingress-v1-networking-k8s-io) exposes HTTP and HTTPS routes from outside the cluster to
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service/&#34; target=&#34;_blank&#34;&gt;services&lt;/a&gt; within the cluster.
Traffic routing is controlled by rules defined on the Ingress resource.

```none
    internet
        |
   [ Ingress ]
   --|-----|--
   [ Services ]
```

An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. An [Ingress controller](/docs/concepts/services-networking/ingress-controllers) is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic.

An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically
uses a service of type [Service.Type=NodePort](/docs/concepts/services-networking/service/#nodeport) or
[Service.Type=LoadBalancer](/docs/concepts/services-networking/service/#loadbalancer).
 --&gt;
&lt;h2 id=&#34;ingress-是什么&#34;&gt;Ingress 是什么?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#ingress-v1-networking-k8s-io&#34;&gt;Ingress&lt;/a&gt;
将集群内部的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/service/&#34; target=&#34;_blank&#34;&gt;Service&lt;/a&gt;
通过 HTTP 和 HTTPS 路由暴露到集群外部。
流量路由由 Ingress 资源内部定义的规则控制。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-none&#34; data-lang=&#34;none&#34;&gt;    internet
        |
   [ Ingress ]
   --|-----|--
   [ Services ]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以配置一个 Ingress 为 Service 提供外部可以访问的 URL, 负载均衡，SSL / TLS, 提供基于名称的
虚拟主机名。
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/ingress-controllers&#34;&gt;Ingress 控制器&lt;/a&gt;将负责
实现 Ingress， 通常是通过负载均衡器，也可能是通过配置边缘路由或额外的前端组件来帮助处理流量。&lt;/p&gt;
&lt;p&gt;Ingress 不是暴露任意的端口或协议。 要暴露非 HTTP / HTTPS 的 Service 通常使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/service/#nodeport&#34;&gt;Service.Type=NodePort&lt;/a&gt;
或
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/service/#loadbalancer&#34;&gt;Service.Type=LoadBalancer&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Prerequisites

You must have an [Ingress controller](/docs/concepts/services-networking/ingress-controllers) to satisfy an Ingress. Only creating an Ingress resource has no effect.

You may need to deploy an Ingress controller such as [ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/). You can choose from a number of
[Ingress controllers](/docs/concepts/services-networking/ingress-controllers).

Ideally, all Ingress controllers should fit the reference specification. In reality, the various Ingress
controllers operate slightly differently.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Make sure you review your Ingress controller&amp;rsquo;s documentation to understand the caveats of choosing it.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;前置条件&#34;&gt;前置条件&lt;/h2&gt;
&lt;p&gt;要使用 Ingress 首先得有一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/ingress-controllers&#34;&gt;Ingress 控制器&lt;/a&gt;。
不然只创建一个 Ingress 资源是没有效果的。&lt;/p&gt;
&lt;p&gt;需要部署一个 Ingress 控制器，例如
&lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/deploy/&#34;&gt;ingress-nginx&lt;/a&gt;.
也可以从
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/ingress-controllers&#34;&gt;Ingress 控制器&lt;/a&gt;
中选几个.&lt;/p&gt;
&lt;p&gt;理想情况下，所有的 Ingress 控制器都应该是符合参考规格说明的。 实际上，不同的 Ingress 运转方式
各自都有点不同。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 在选择 Ingress 控制器前请仔细阅读相关文档，确定理解了相关注意事项。&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;!--
## The Ingress resource

A minimal Ingress resource example:



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingminimal-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/minimal-ingress.yaml&#34; download=&#34;service/networking/minimal-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/minimal-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingminimal-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/minimal-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;minimal-ingress&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nginx.ingress.kubernetes.io/rewrite-target&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/testpath&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



As with all other Kubernetes resources, an Ingress needs `apiVersion`, `kind`, and `metadata` fields.
The name of an Ingress object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
For general information about working with config files, see [deploying applications](/docs/tasks/run-application/run-stateless-application-deployment/), [configuring containers](/docs/tasks/configure-pod-container/configure-pod-configmap/), [managing resources](/docs/concepts/cluster-administration/manage-deployment/).
 Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of which
 is the [rewrite-target annotation](https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/rewrite/README.md).
Different [Ingress controller](/docs/concepts/services-networking/ingress-controllers) support different annotations. Review the documentation for
 your choice of Ingress controller to learn which annotations are supported.

The Ingress [spec](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status)
has all the information needed to configure a load balancer or proxy server. Most importantly, it
contains a list of rules matched against all incoming requests. Ingress resource only supports rules
for directing HTTP(S) traffic.
 --&gt;
&lt;h2 id=&#34;ingress-资源&#34;&gt;Ingress 资源&lt;/h2&gt;
&lt;p&gt;一个最小化 Ingress 资源的示例:


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingminimal-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/minimal-ingress.yaml&#34; download=&#34;service/networking/minimal-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/minimal-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingminimal-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/minimal-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;minimal-ingress&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nginx.ingress.kubernetes.io/rewrite-target&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/testpath&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;

&lt;/p&gt;
&lt;p&gt;与所有其它的 k8s 资源一样， 一个 Ingress 的必要字段有 &lt;code&gt;apiVersion&lt;/code&gt;, &lt;code&gt;kind&lt;/code&gt;, &lt;code&gt;metadata&lt;/code&gt;。
Ingress 对象的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.
关于配置文件的通用信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/run-stateless-application-deployment/&#34;&gt;部署应用&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-pod-configmap/&#34;&gt;配置容器&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/cluster-administration/manage-deployment/&#34;&gt;管理资源&lt;/a&gt;.
Ingress 经常使用注解(annotation) 来配置一些基于 Ingress 控制器的可选配置，这里有一个例子
&lt;a href=&#34;https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/rewrite/README.md&#34;&gt;rewrite-target 注解&lt;/a&gt;.
不同的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/ingress-controllers&#34;&gt;Ingress 控制器&lt;/a&gt;
会支持不同的注解。 查看对应的控制文档了解其支持的注解(annotation)。&lt;/p&gt;
&lt;p&gt;Ingress &lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;spec&lt;/a&gt;
中包含了配置负载均衡器或代理服务器的所有信息。 最重要的是它包含了一个匹配进入请求的规则列表。
Ingress 资源只支持对 HTTP(S) 流量的转发。&lt;/p&gt;
&lt;!--
### Ingress rules

Each HTTP rule contains the following information:

* An optional host. In this example, no host is specified, so the rule applies to all inbound
  HTTP traffic through the IP address specified. If a host is provided (for example,
  foo.bar.com), the rules apply to that host.
* A list of paths (for example, `/testpath`), each of which has an associated
  backend defined with a `service.name` and a `service.port.name` or
  `service.port.number`. Both the host and path must match the content of an
  incoming request before the load balancer directs traffic to the referenced
  Service.
* A backend is a combination of Service and port names as described in the
  [Service doc](/docs/concepts/services-networking/service/) or a [custom resource backend](#resource-backend) by way of a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/&#39; target=&#39;_blank&#39;&gt;CRD&lt;span class=&#39;tooltip-text&#39;&gt;Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server.&lt;/span&gt;
&lt;/a&gt;. HTTP (and HTTPS) requests to the
  Ingress that matches the host and path of the rule are sent to the listed backend.

A `defaultBackend` is often configured in an Ingress controller to service any requests that do not
match a path in the spec.
 --&gt;
&lt;h3 id=&#34;ingress-rules&#34;&gt;Ingress 规则&lt;/h3&gt;
&lt;p&gt;每个 HTTP 规则包含以下信息:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个可选的主机名。 在这个例子中就没有配，所以这个规则应用于所有通过指定 IP 的流量。如果主机名
有设置(例如，foo.bar.com), 这个规则就应用于这个主机名。&lt;/li&gt;
&lt;li&gt;一个路径列表(如例子中的 &lt;code&gt;/testpath&lt;/code&gt;)， 每个路径都与一个后台通过一个 &lt;code&gt;service.name&lt;/code&gt; 和
一个 &lt;code&gt;service.port.name&lt;/code&gt; 或 &lt;code&gt;service.port.number&lt;/code&gt; 相关联。 必须要主机名和路径都匹配的
进入请求才会重定向到相应的 Service&lt;/li&gt;
&lt;li&gt;一个后端就是一个 Service 和 端口名称的组合，就如
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service/&#34;&gt;Service&lt;/a&gt;
或
&lt;a href=&#34;#resource-backend&#34;&gt;自定义资源后端&lt;/a&gt;通过
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/&#39; target=&#39;_blank&#39;&gt;CRD&lt;span class=&#39;tooltip-text&#39;&gt;Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server.&lt;/span&gt;
&lt;/a&gt;
方式中所描述的一样。
到达 Ingress 的 HTTP (和 HTTPS) 请求匹配到对应规则的主机名和路径就会发送到列举的后端。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个 &lt;code&gt;defaultBackend&lt;/code&gt; 通常是配置在一个 Ingress 控制器中，用于接收所有没有匹配到任何配置中的
路径的请求。&lt;/p&gt;
&lt;!--
### DefaultBackend {#default-backend}

An Ingress with no rules sends all traffic to a single default backend. The `defaultBackend` is conventionally a configuration option
of the [Ingress controller](/docs/concepts/services-networking/ingress-controllers) and is not specified in your Ingress resources.

If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is
routed to your default backend.
 --&gt;
&lt;h3 id=&#34;default-backend&#34;&gt;默认后端(DefaultBackend)&lt;/h3&gt;
&lt;p&gt;一个没有任何规则的 Ingress 会将所有的流量发送到一个默认的后端。 &lt;code&gt;defaultBackend&lt;/code&gt; 是一个很方便
的配置在  &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/ingress-controllers&#34;&gt;Ingress 控制器&lt;/a&gt;
并不需要在 Ingress 中配置。&lt;/p&gt;
&lt;p&gt;如果没有一个 Ingress 中的主机名或路径匹配到的 HTTP 请求，这些请求就会路由到默认后端&lt;/p&gt;
&lt;!--
### Resource backends {#resource-backend}

A `Resource` backend is an ObjectRef to another Kubernetes resource within the
same namespace as the Ingress object. A `Resource` is a mutually exclusive
setting with Service, and will fail validation if both are specified. A common
usage for a `Resource` backend is to ingress data to an object storage backend
with static assets.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingingress-resource-backendyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/ingress-resource-backend.yaml&#34; download=&#34;service/networking/ingress-resource-backend.yaml&#34;&gt;
                    &lt;code&gt;service/networking/ingress-resource-backend.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingingress-resource-backendyaml&#39;)&#34; title=&#34;Copy service/networking/ingress-resource-backend.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ingress-resource-backend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;defaultBackend&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;resource&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;apiGroup&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.example.com&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageBucket&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;static-assets&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/icons&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ImplementationSpecific&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;resource&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;apiGroup&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.example.com&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageBucket&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;icon-assets&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



After creating the Ingress above, you can view it with the following command:

```bash
kubectl describe ingress ingress-resource-backend
```

```
Name:             ingress-resource-backend
Namespace:        default
Address:
Default backend:  APIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assets
Rules:
  Host        Path  Backends
  ----        ----  --------
  *
              /icons   APIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assets
Annotations:  &lt;none&gt;
Events:       &lt;none&gt;
```
 --&gt;
&lt;h3 id=&#34;resource-backend&#34;&gt;资源(Resource) 后端&lt;/h3&gt;
&lt;p&gt;一个 &lt;code&gt;Resource&lt;/code&gt; 后端是一个与 Ingress 对象在同一个命名空间的另一个 k8s 资源的引用。
&lt;code&gt;Resource&lt;/code&gt; 与 Service 是互斥的，如果同时配置了两者，则不会通过验证。 &lt;code&gt;Resource&lt;/code&gt; 的一个常见
应用场景是进入一个存放静态资源的对象存储后端。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingingress-resource-backendyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/ingress-resource-backend.yaml&#34; download=&#34;service/networking/ingress-resource-backend.yaml&#34;&gt;
                    &lt;code&gt;service/networking/ingress-resource-backend.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingingress-resource-backendyaml&#39;)&#34; title=&#34;Copy service/networking/ingress-resource-backend.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ingress-resource-backend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;defaultBackend&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;resource&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;apiGroup&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.example.com&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageBucket&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;static-assets&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/icons&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ImplementationSpecific&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;resource&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;apiGroup&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.example.com&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageBucket&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;icon-assets&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;在创建以上定义的 Ingress 后，可以通过以下命令查看:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl describe ingress ingress-resource-backend
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Name:             ingress-resource-backend
Namespace:        default
Address:
Default backend:  APIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assets
Rules:
  Host        Path  Backends
  ----        ----  --------
  *
              /icons   APIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assets
Annotations:  &amp;lt;none&amp;gt;
Events:       &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;!--
### Path types

Each path in an Ingress is required to have a corresponding path type. Paths
that do not include an explicit `pathType` will fail validation. There are three
supported path types:

* `ImplementationSpecific`: With this path type, matching is up to the
  IngressClass. Implementations can treat this as a separate `pathType` or treat
  it identically to `Prefix` or `Exact` path types.

* `Exact`: Matches the URL path exactly and with case sensitivity.

* `Prefix`: Matches based on a URL path prefix split by `/`. Matching is case
  sensitive and done on a path element by element basis. A path element refers
  to the list of labels in the path split by the `/` separator. A request is a
  match for path _p_ if every _p_ is an element-wise prefix of _p_ of the
  request path.

  &lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If the last element of the path is a substring of the last
element in request path, it is not a match (for example: &lt;code&gt;/foo/bar&lt;/code&gt;
matches&lt;code&gt;/foo/bar/baz&lt;/code&gt;, but does not match &lt;code&gt;/foo/barbaz&lt;/code&gt;).&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;路径path类型&#34;&gt;路径(Path)类型&lt;/h3&gt;
&lt;p&gt;Ingress 中的每一个路径都需要有一个相应的路径类型。 没有显示设置 &lt;code&gt;pathType&lt;/code&gt; 是没办法通过验证的。
有以下三种支持的路径类型:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;ImplementationSpecific&lt;/code&gt;: 这个路径类型的匹配是由 IngressClass 决定。 具体实现可以将其认为是
独立的 &lt;code&gt;pathType&lt;/code&gt; 或认为是 &lt;code&gt;Prefix&lt;/code&gt; 或 &lt;code&gt;Exact&lt;/code&gt; 路径类型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Exact&lt;/code&gt;:  完全匹配路径，区分大小写。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Prefix&lt;/code&gt;: 基于由 &lt;code&gt;/&lt;/code&gt; 分割的 URL 路径前缀匹配。匹配区分大小写，并且逐个元素匹配。
一个元素就是通过 &lt;code&gt;/&lt;/code&gt; 分割后列表中的每一个元素。 一个请求与路径 &lt;em&gt;p&lt;/em&gt; 相匹配则请求中的元素可以
依次匹配完路径 &lt;em&gt;p&lt;/em&gt; 的每一个元素。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果路径的最后一个元素是请求路径的最后一个元素的子串，则不能匹配(例如：&lt;code&gt;/foo/bar&lt;/code&gt; 可以匹配
请求路径 &lt;code&gt;/foo/bar/baz&lt;/code&gt;，但是不能匹配请求路径 &lt;code&gt;/foo/barbaz&lt;/code&gt; )。&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;!--
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Kind&lt;/th&gt;
&lt;th&gt;Path(s)&lt;/th&gt;
&lt;th&gt;Request path(s)&lt;/th&gt;
&lt;th&gt;Matches?&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;(all paths)&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exact&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exact&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/bar&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exact&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exact&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;, &lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;, &lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes, ignores trailing slash&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes,  matches trailing slash&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb/ccc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes, matches subpath&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbbxyz&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;No, does not match string prefix&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;, &lt;code&gt;/aaa&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/ccc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes, matches &lt;code&gt;/aaa&lt;/code&gt; prefix&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;, &lt;code&gt;/aaa&lt;/code&gt;, &lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes, matches &lt;code&gt;/aaa/bbb&lt;/code&gt; prefix&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;, &lt;code&gt;/aaa&lt;/code&gt;, &lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/ccc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes, matches &lt;code&gt;/&lt;/code&gt; prefix&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/ccc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;No, uses default backend&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mixed&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt; (Prefix), &lt;code&gt;/foo&lt;/code&gt; (Exact)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Yes, prefers Exact&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&amp;ndash;&amp;gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;示例&#34;&gt;示例&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;类型&lt;/th&gt;
&lt;th&gt;路径&lt;/th&gt;
&lt;th&gt;请求路径&lt;/th&gt;
&lt;th&gt;匹配结果&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;(all paths)&lt;/td&gt;
&lt;td&gt;匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exact&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exact&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/bar&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;不匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exact&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;不匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exact&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;不匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;, &lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;, &lt;code&gt;/foo/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;不匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配, 忽略尾部的斜线&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb/&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配, 匹配尾部的斜线&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb/ccc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配, 匹配前缀&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbbxyz&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;不匹配, 不是匹配字符串前缀&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;, &lt;code&gt;/aaa&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/ccc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配, 匹配 &lt;code&gt;/aaa&lt;/code&gt; 前缀&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;, &lt;code&gt;/aaa&lt;/code&gt;, &lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配, 匹配 &lt;code&gt;/aaa/bbb&lt;/code&gt; 前缀&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/&lt;/code&gt;, &lt;code&gt;/aaa&lt;/code&gt;, &lt;code&gt;/aaa/bbb&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/ccc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配, 匹配 &lt;code&gt;/&lt;/code&gt; 前缀&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Prefix&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/aaa&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/ccc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;不匹配, 使用默认后端&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mixed&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt; (Prefix), &lt;code&gt;/foo&lt;/code&gt; (Exact)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/foo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配, 优先完全匹配&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
#### Multiple matches
In some cases, multiple paths within an Ingress will match a request. In those
cases precedence will be given first to the longest matching path. If two paths
are still equally matched, precedence will be given to paths with an exact path
type over prefix path type.
 --&gt;
&lt;h4 id=&#34;多匹配&#34;&gt;多匹配&lt;/h4&gt;
&lt;p&gt;在某些情况下，一个 Ingress 中的多个路径都能与请求路径相匹配。在这种情况下优先级最高的是最长匹配
路径。 如果还有两个路径匹配长度匹配相同，则完全匹配优先级高于前端匹配。&lt;/p&gt;
&lt;!--
## Hostname wildcards
Hosts can be precise matches (for example “`foo.bar.com`”) or a wildcard (for
example “`*.foo.com`”). Precise matches require that the HTTP `host` header
matches the `host` field. Wildcard matches require the HTTP `host` header is
equal to the suffix of the wildcard rule.

| Host        | Host header       | Match?                                            |
| ----------- |-------------------| --------------------------------------------------|
| `*.foo.com` | `bar.foo.com`     | Matches based on shared suffix                    |
| `*.foo.com` | `baz.bar.foo.com` | No match, wildcard only covers a single DNS label |
| `*.foo.com` | `foo.com`         | No match, wildcard only covers a single DNS label |



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingingress-wildcard-hostyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/ingress-wildcard-host.yaml&#34; download=&#34;service/networking/ingress-wildcard-host.yaml&#34;&gt;
                    &lt;code&gt;service/networking/ingress-wildcard-host.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingingress-wildcard-hostyaml&#39;)&#34; title=&#34;Copy service/networking/ingress-wildcard-host.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ingress-wildcard-host&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;foo.bar.com&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/bar&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*.foo.com&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/foo&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


 --&gt;
&lt;h2 id=&#34;主机名通配符&#34;&gt;主机名通配符&lt;/h2&gt;
&lt;p&gt;主机名可以是精确匹配(例如 &lt;code&gt;foo.bar.com&lt;/code&gt;)，也可以有通配符(例如 &lt;code&gt;*.foo.com&lt;/code&gt;)。
精确匹配必须要 HTTP 请求头中的 &lt;code&gt;host&lt;/code&gt; 与规则的 &lt;code&gt;host&lt;/code&gt; 字段完全匹配。
通配符则要 HTTP 请求头中的 &lt;code&gt;host&lt;/code&gt; 与通配符规则的后缀相同。&lt;/p&gt;
&lt;p&gt;例如:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;规则&lt;/th&gt;
&lt;th&gt;请求头&lt;code&gt;host&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;匹配结果&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;*.foo.com&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;bar.foo.com&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;匹配，因为后缀相同&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;*.foo.com&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;baz.bar.foo.com&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;不匹配, 通配符只能包含一级子域名&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;*.foo.com&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;foo.com&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;不匹配, 通配符只能包含一级子域名&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingingress-wildcard-hostyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/ingress-wildcard-host.yaml&#34; download=&#34;service/networking/ingress-wildcard-host.yaml&#34;&gt;
                    &lt;code&gt;service/networking/ingress-wildcard-host.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingingress-wildcard-hostyaml&#39;)&#34; title=&#34;Copy service/networking/ingress-wildcard-host.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ingress-wildcard-host&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;foo.bar.com&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/bar&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*.foo.com&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/foo&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;!--
## Ingress class

Ingresses can be implemented by different controllers, often with different
configuration. Each Ingress should specify a class, a reference to an
IngressClass resource that contains additional configuration including the name
of the controller that should implement the class.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingexternal-lbyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/external-lb.yaml&#34; download=&#34;service/networking/external-lb.yaml&#34;&gt;
                    &lt;code&gt;service/networking/external-lb.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingexternal-lbyaml&#39;)&#34; title=&#34;Copy service/networking/external-lb.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IngressClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;external-lb&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;controller&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;example.com/ingress-controller&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;apiGroup&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.example.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IngressParameters&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;external-lb&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



IngressClass resources contain an optional parameters field. This can be used to
reference additional configuration for this class.
 --&gt;
&lt;h2 id=&#34;ingressclass&#34;&gt;IngressClass&lt;/h2&gt;
&lt;p&gt;Ingress 可以由不同的控制器实现，通常也会有不同的配置。 每个 Ingress 都需要指定一个类型，
它是一个 IngressClass 资源的引用， 其中包含如实现该类型的控制器的名称等额外配置。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingexternal-lbyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/external-lb.yaml&#34; download=&#34;service/networking/external-lb.yaml&#34;&gt;
                    &lt;code&gt;service/networking/external-lb.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingexternal-lbyaml&#39;)&#34; title=&#34;Copy service/networking/external-lb.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IngressClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;external-lb&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;controller&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;example.com/ingress-controller&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;apiGroup&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.example.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IngressParameters&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;external-lb&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;IngressClass 资源中包含一个可选参数字段。 可以用设置该类型额外配置的引用&lt;/p&gt;
&lt;!--

### Deprecated annotation

Before the IngressClass resource and `ingressClassName` field were added in
Kubernetes 1.18, Ingress classes were specified with a
`kubernetes.io/ingress.class` annotation on the Ingress. This annotation was
never formally defined, but was widely supported by Ingress controllers.

The newer `ingressClassName` field on Ingresses is a replacement for that
annotation, but is not a direct equivalent. While the annotation was generally
used to reference the name of the Ingress controller that should implement the
Ingress, the field is a reference to an IngressClass resource that contains
additional Ingress configuration, including the name of the Ingress controller.
 --&gt;
&lt;h3 id=&#34;废弃的注解&#34;&gt;废弃的注解&lt;/h3&gt;
&lt;p&gt;在 k8s 1.18 版本中添加 IngressClass 资源和 &lt;code&gt;ingressClassName&lt;/code&gt; 资源前，Ingress 的类型是
通过其上的 &lt;code&gt;kubernetes.io/ingress.class&lt;/code&gt; 注解指定的。 这个注解从来没有正式的定义过，但它
在 Ingress 控制器中被广泛支持。&lt;/p&gt;
&lt;p&gt;Ingress 中的新字段 &lt;code&gt;ingressClassName&lt;/code&gt; 就是对这个注解的替代，但它们也不是直接等价的。
其中注解通常是用来指定实现该 Ingress 的控制器的名称， 新字段则是指定一个 IngressClass 资源的引用。
这个 IngressClass 资源中包含一额外的 Ingress 配置参数和 Ingress 控制器的名称。&lt;/p&gt;
&lt;!--
### Default IngressClass {#default-ingress-class}

You can mark a particular IngressClass as default for your cluster. Setting the
`ingressclass.kubernetes.io/is-default-class` annotation to `true` on an
IngressClass resource will ensure that new Ingresses without an
`ingressClassName` field specified will be assigned this default IngressClass.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; If you have more than one IngressClass marked as the default for your cluster,
the admission controller prevents creating new Ingress objects that don&amp;rsquo;t have
an &lt;code&gt;ingressClassName&lt;/code&gt; specified. You can resolve this by ensuring that at most 1
IngressClass is marked as default in your cluster.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;default-ingress-class&#34;&gt;默认 IngressClass&lt;/h3&gt;
&lt;p&gt;用户可以将某个 IngressClass 设置为集群的默认 IngressClass。
在 IngressClass 资源上设置 &lt;code&gt;ingressclass.kubernetes.io/is-default-class&lt;/code&gt; 注解值为 &lt;code&gt;true&lt;/code&gt;。
这样新创建 Ingress 如果没有设置 &lt;code&gt;ingressClassName&lt;/code&gt; 字段则会分配这个默认的 IngressClass。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 如果集群中有不止一个 IngressClass 被标记为默认，准入控制器(admission controller) 就会禁止
创建没有设置 &lt;code&gt;ingressClassName&lt;/code&gt; 的 Ingress。 需要用户手动解决，确保集群中只有一个默认
IngressClass。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Types of Ingress

### Ingress backed by a single Service {#single-service-ingress}

There are existing Kubernetes concepts that allow you to expose a single Service
(see [alternatives](#alternatives)). You can also do this with an Ingress by specifying a
*default backend* with no rules.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingtest-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/test-ingress.yaml&#34; download=&#34;service/networking/test-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/test-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingtest-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/test-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;defaultBackend&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



If you create it using `kubectl apply -f` you should be able to view the state
of the Ingress you just added:

```bash
kubectl get ingress test-ingress
```

```
NAME           CLASS         HOSTS   ADDRESS         PORTS   AGE
test-ingress   external-lb   *       203.0.113.123   80      59s
```

Where `203.0.113.123` is the IP allocated by the Ingress controller to satisfy
this Ingress.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Ingress controllers and load balancers may take a minute or two to allocate an IP address.
Until that time, you often see the address listed as &lt;code&gt;&amp;lt;pending&amp;gt;&lt;/code&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;ingress-的类型&#34;&gt;Ingress 的类型&lt;/h2&gt;
&lt;h3 id=&#34;single-service-ingress&#34;&gt;后端只有一个 Service 的 Ingress &lt;/h3&gt;
&lt;p&gt;k8s 存在概念可以让用户暴露单个 Service (见 &lt;a href=&#34;#alternatives&#34;&gt;替代方案&lt;/a&gt;).
也可以通过一个指定 &lt;em&gt;默认后端&lt;/em&gt; 但没有配置规则的 Ingress 达到同样的效果。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingtest-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/test-ingress.yaml&#34; download=&#34;service/networking/test-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/test-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingtest-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/test-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;defaultBackend&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;通过 &lt;code&gt;kubectl apply -f&lt;/code&gt; 创建 Ingress，并通过以下命令查看:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get ingress test-ingress
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME           CLASS         HOSTS   ADDRESS         PORTS   AGE
test-ingress   external-lb   *       203.0.113.123   80      59s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中 &lt;code&gt;203.0.113.123&lt;/code&gt; 是由 Ingress 控制器分配来满足这个 Ingress 的 IP 地址。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Ingress 控制器和负载均衡器可能需要一两分钟为分配一个 IP 地址。在这之前，通常看到地址列的值为 &lt;code&gt;&amp;lt;pending&amp;gt;&lt;/code&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Simple fanout

A fanout configuration routes traffic from a single IP address to more than one Service,
based on the HTTP URI being requested. An Ingress allows you to keep the number of load balancers
down to a minimum. For example, a setup like:

```
foo.bar.com -&gt; 178.91.123.132 -&gt; / foo    service1:4200
                                 / bar    service2:8080
```

would require an Ingress such as:



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingsimple-fanout-exampleyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/simple-fanout-example.yaml&#34; download=&#34;service/networking/simple-fanout-example.yaml&#34;&gt;
                    &lt;code&gt;service/networking/simple-fanout-example.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingsimple-fanout-exampleyaml&#39;)&#34; title=&#34;Copy service/networking/simple-fanout-example.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;simple-fanout-example&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/foo&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4200&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/bar&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



When you create the Ingress with `kubectl apply -f`:

```shell
kubectl describe ingress simple-fanout-example
```

```
Name:             simple-fanout-example
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:4200 (10.8.0.90:4200)
               /bar   service2:8080 (10.8.0.91:8080)
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     22s                loadbalancer-controller  default/test
```

The Ingress controller provisions an implementation-specific load balancer
that satisfies the Ingress, as long as the Services (`service1`, `service2`) exist.
When it has done so, you can see the address of the load balancer at the
Address field.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Depending on the &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/ingress-controllers/&#34;&gt;Ingress controller&lt;/a&gt;
you are using, you may need to create a default-http-backend
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/service/&#34;&gt;Service&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;简单分散fanout&#34;&gt;简单分散(fanout)&lt;/h3&gt;
&lt;p&gt;一个分散就是基于请求的 HTTP URI 配置路由流量从一单个 IP 地址到多个 Service。
一个 Ingress 可以使负载均衡器的数量减少到最低。 例如如下配置:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;foo.bar.com -&amp;gt; 178.91.123.132 -&amp;gt; / foo    service1:4200
                                 / bar    service2:8080
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;需要要一个如下 Ingress:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingsimple-fanout-exampleyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/simple-fanout-example.yaml&#34; download=&#34;service/networking/simple-fanout-example.yaml&#34;&gt;
                    &lt;code&gt;service/networking/simple-fanout-example.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingsimple-fanout-exampleyaml&#39;)&#34; title=&#34;Copy service/networking/simple-fanout-example.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;simple-fanout-example&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/foo&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4200&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/bar&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;在使用 &lt;code&gt;kubectl apply -f&lt;/code&gt; 命令创建 Ingress 后，通过以下命令查看详情:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe ingress simple-fanout-example
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Name:             simple-fanout-example
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:4200 (10.8.0.90:4200)
               /bar   service2:8080 (10.8.0.91:8080)
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     22s                loadbalancer-controller  default/test
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Ingress 控制器会在目标 Service (&lt;code&gt;service1&lt;/code&gt;, &lt;code&gt;service2&lt;/code&gt;) 还存在时，
根据本身实现提供一个满足 Ingress 的负载均衡器直到。
当创建好后，可以在地址字段看到负载均衡器的地址。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 基于使用的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/ingress-controllers/&#34;&gt;Ingress 控制器&lt;/a&gt;
可能需要创建一个 default-http-backend
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/service/&#34;&gt;Service&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Name based virtual hosting

Name-based virtual hosts support routing HTTP traffic to multiple host names at the same IP address.

```none
foo.bar.com --|                 |-&gt; foo.bar.com service1:80
              | 178.91.123.132  |
bar.foo.com --|                 |-&gt; bar.foo.com service2:80
```

The following Ingress tells the backing load balancer to route requests based on
the [Host header](https://tools.ietf.org/html/rfc7230#section-5.4).



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingname-virtual-host-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/name-virtual-host-ingress.yaml&#34; download=&#34;service/networking/name-virtual-host-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/name-virtual-host-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingname-virtual-host-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/name-virtual-host-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;name-virtual-host-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar.foo.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



If you create an Ingress resource without any hosts defined in the rules, then any
web traffic to the IP address of your Ingress controller can be matched without a name based
virtual host being required.

For example, the following Ingress routes traffic
requested for `first.bar.com` to `service1`, `second.foo.com` to `service2`, and any traffic
to the IP address without a hostname defined in request (that is, without a request header being
presented) to `service3`.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingname-virtual-host-ingress-no-third-hostyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/name-virtual-host-ingress-no-third-host.yaml&#34; download=&#34;service/networking/name-virtual-host-ingress-no-third-host.yaml&#34;&gt;
                    &lt;code&gt;service/networking/name-virtual-host-ingress-no-third-host.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingname-virtual-host-ingress-no-third-hostyaml&#39;)&#34; title=&#34;Copy service/networking/name-virtual-host-ingress-no-third-host.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;name-virtual-host-ingress-no-third-host&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;first.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;second.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service3&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


 --&gt;
&lt;h3 id=&#34;基于名称的虚拟主机&#34;&gt;基于名称的虚拟主机&lt;/h3&gt;
&lt;p&gt;基于名称的虚拟主机支持在同一个IP上将 HTTP 流量路由到多个主机名上。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-none&#34; data-lang=&#34;none&#34;&gt;foo.bar.com --|                 |-&amp;gt; foo.bar.com service1:80
              | 178.91.123.132  |
bar.foo.com --|                 |-&amp;gt; bar.foo.com service2:80
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The following Ingress tells the backing load balancer to route requests based on
the &lt;a href=&#34;https://tools.ietf.org/html/rfc7230#section-5.4&#34;&gt;Host header&lt;/a&gt;.
以下 Ingress 告诉后端的负载均衡器基于
&lt;a href=&#34;https://tools.ietf.org/html/rfc7230#section-5.4&#34;&gt;Host 头字段&lt;/a&gt;
路由请求。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingname-virtual-host-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/name-virtual-host-ingress.yaml&#34; download=&#34;service/networking/name-virtual-host-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/name-virtual-host-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingname-virtual-host-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/name-virtual-host-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;name-virtual-host-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar.foo.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;If you create an Ingress resource without any hosts defined in the rules, then any
web traffic to the IP address of your Ingress controller can be matched without a name based
virtual host being required.
如果创建的 Ingress 资源中没有定义任何主机规则，则所有到达这个 Ingress 控制 IP 的的 web 流量
则不需要基于名称的虚拟主机就能匹配。&lt;/p&gt;
&lt;p&gt;例如， 以下 Ingress 路由 &lt;code&gt;first.bar.com&lt;/code&gt; 的请求到 &lt;code&gt;service1&lt;/code&gt;，
&lt;code&gt;second.foo.com&lt;/code&gt; 的请求到 &lt;code&gt;service2&lt;/code&gt;，其它任意到达这个 IP 地址，但在请求中没有包含主机名
(也就是没的请求头(或请求头中没有 Host 字段))的请求到 &lt;code&gt;service3&lt;/code&gt;&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingname-virtual-host-ingress-no-third-hostyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/name-virtual-host-ingress-no-third-host.yaml&#34; download=&#34;service/networking/name-virtual-host-ingress-no-third-host.yaml&#34;&gt;
                    &lt;code&gt;service/networking/name-virtual-host-ingress-no-third-host.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingname-virtual-host-ingress-no-third-hostyaml&#39;)&#34; title=&#34;Copy service/networking/name-virtual-host-ingress-no-third-host.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;name-virtual-host-ingress-no-third-host&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;first.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;second.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service3&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;!--
### TLS

You can secure an Ingress by specifying a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;存放如密码, OAuth, ssh 密钥等敏感信息&lt;/span&gt;
&lt;/a&gt;
that contains a TLS private key and certificate. The Ingress resource only
supports a single TLS port, 443, and assumes TLS termination at the ingress point
(traffic to the Service and its Pods is in plaintext).
If the TLS configuration section in an Ingress specifies different hosts, they are
multiplexed on the same port according to the hostname specified through the
SNI TLS extension (provided the Ingress controller supports SNI). The TLS secret
must contain keys named `tls.crt` and `tls.key` that contain the certificate
and private key to use for TLS. For example:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: testsecret-tls
  namespace: default
data:
  tls.crt: base64 encoded cert
  tls.key: base64 encoded key
type: kubernetes.io/tls
```

Referencing this secret in an Ingress tells the Ingress controller to
secure the channel from the client to the load balancer using TLS. You need to make
sure the TLS secret you created came from a certificate that contains a Common
Name (CN), also known as a Fully Qualified Domain Name (FQDN) for `sslexample.foo.com`.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingtls-example-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/tls-example-ingress.yaml&#34; download=&#34;service/networking/tls-example-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/tls-example-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingtls-example-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/tls-example-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;tls-example-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;tls&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;hosts&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;https-example.foo.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;testsecret-tls&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https-example.foo.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; There is a gap between TLS features supported by various Ingress
controllers. Please refer to documentation on
&lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/tls/&#34;&gt;nginx&lt;/a&gt;,
&lt;a href=&#34;https://git.k8s.io/ingress-gce/README.md#frontend-https&#34;&gt;GCE&lt;/a&gt;, or any other
platform specific Ingress controller to understand how TLS works in your environment.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;tls&#34;&gt;TLS&lt;/h3&gt;
&lt;p&gt;用户可以能过一个包含 TLS 私钥和证书的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;存放如密码, OAuth, ssh 密钥等敏感信息&lt;/span&gt;
&lt;/a&gt; 为 Ingress
添加安全层。 Ingress 只支持一个 TLS 端口，&lt;code&gt;443&lt;/code&gt;， 并且假定 TLS 在 Ingress 终结(也就是
到达 Service 及其 Pod 的流量是明文的)。
如果 Ingress TLS 配置区中包含了多个主机名，那么它们根据 SNI TLS 扩展(需要 Ingress 控制器
支持 SNI)中指定的主机名来区分请求。 TLS &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;存放如密码, OAuth, ssh 密钥等敏感信息&lt;/span&gt;
&lt;/a&gt;
中必须要包含名叫 &lt;code&gt;tls.crt&lt;/code&gt; 和 &lt;code&gt;tls.key&lt;/code&gt; 的键，其它分别存放 TLS 的证书和私钥。 例如:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Secret&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;testsecret-tls&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;data&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;tls.crt&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;base64 encoded cert&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;tls.key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;base64 encoded key&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/tls&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在一个 Ingress 中引用该 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;存放如密码, OAuth, ssh 密钥等敏感信息&lt;/span&gt;
&lt;/a&gt; 保证客户端到负载均衡器之间的
连接是使用 TLS。必须要保证创建 TLS Secret 的证书中包含公用名(CN), 也就是 &lt;code&gt;sslexample.foo.com&lt;/code&gt;
的全限定名(FQDN).&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingtls-example-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/tls-example-ingress.yaml&#34; download=&#34;service/networking/tls-example-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/tls-example-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingtls-example-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/tls-example-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;tls-example-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;tls&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;hosts&lt;/span&gt;:
      - &lt;span style=&#34;color:#ae81ff&#34;&gt;https-example.foo.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;testsecret-tls&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https-example.foo.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 不同的 Ingress 控制器对 TLS 特性的支持有较大差异。 请查阅
&lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/tls/&#34;&gt;nginx&lt;/a&gt;,
&lt;a href=&#34;https://git.k8s.io/ingress-gce/README.md#frontend-https&#34;&gt;GCE&lt;/a&gt;,
或其它平台相应的 Ingress 控制的说明文档，以理解在你的环境中 TLS 是怎么工作的。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Load balancing {#load-balancing}

An Ingress controller is bootstrapped with some load balancing policy settings
that it applies to all Ingress, such as the load balancing algorithm, backend
weight scheme, and others. More advanced load balancing concepts
(e.g. persistent sessions, dynamic weights) are not yet exposed through the
Ingress. You can instead get these features through the load balancer used for
a Service.

It&#39;s also worth noting that even though health checks are not exposed directly
through the Ingress, there exist parallel concepts in Kubernetes such as
[readiness probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)
that allow you to achieve the same end result. Please review the controller
specific documentation to see how they handle health checks (for example:
[nginx](https://git.k8s.io/ingress-nginx/README.md), or
[GCE](https://git.k8s.io/ingress-gce/README.md#health-checks)).
 --&gt;
&lt;h3 id=&#34;load-balancing&#34;&gt;负载均衡&lt;/h3&gt;
&lt;p&gt;Ingress 控制器天生带有一些会应用到所有 Ingress 负载均衡策略设置，如 负载均衡算法，后端权重，等。
更高级的负载均衡概念(如，持久会化，动态权重)目前还没能过 Ingress 实现。 要实现这些特性可以通过
Service 使用的负载均衡器。&lt;/p&gt;
&lt;p&gt;还有值得注意的是健康检查不是直接通过 Ingress 暴露的。 k8s 中存在如
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/&#34;&gt;存活探针&lt;/a&gt;
的平行概念，允许用户达成同样的结果。 请查阅控制的说明文档，看看是怎么处理健康检查的(例如
&lt;a href=&#34;https://git.k8s.io/ingress-nginx/README.md&#34;&gt;nginx&lt;/a&gt;, or
&lt;a href=&#34;https://git.k8s.io/ingress-gce/README.md#health-checks&#34;&gt;GCE&lt;/a&gt;).
)&lt;/p&gt;
&lt;!--
## Updating an Ingress

To update an existing Ingress to add a new Host, you can update it by editing the resource:

```shell
kubectl describe ingress test
```

```
Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     35s                loadbalancer-controller  default/test
```

```shell
kubectl edit ingress test
```

This pops up an editor with the existing configuration in YAML format.
Modify it to include the new Host:

```yaml
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - backend:
          service:
            name: service1
            port:
              number: 80
        path: /foo
        pathType: Prefix
  - host: bar.baz.com
    http:
      paths:
      - backend:
          service:
            name: service2
            port:
              number: 80
        path: /foo
        pathType: Prefix
..
```

After you save your changes, kubectl updates the resource in the API server, which tells the
Ingress controller to reconfigure the load balancer.

Verify this:

```shell
kubectl describe ingress test
```

```
Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
  bar.baz.com
               /foo   service2:80 (10.8.0.91:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     45s                loadbalancer-controller  default/test
```

You can achieve the same outcome by invoking `kubectl replace -f` on a modified Ingress YAML file.
 --&gt;
&lt;h2 id=&#34;更新-ingress&#34;&gt;更新 Ingress&lt;/h2&gt;
&lt;p&gt;要在已经存在的 Ingress 中添加一个新的主机规则，可以通过编辑 Ingress 资源实现:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe ingress test
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     35s                loadbalancer-controller  default/test
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl edit ingress test
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这条命令会用文本编辑器打开已经存在的 Ingress  YAML 格式的配置文件，在其中添加新的主机规则配置:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo.bar.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/foo&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;host&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar.baz.com&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;service2&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/foo&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
&lt;span style=&#34;color:#ae81ff&#34;&gt;..&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在保存修改后， kubectl 就会在 API-server 中更新该资源，这会使得 Ingress 控制器重新配置
负载均衡器&lt;/p&gt;
&lt;p&gt;通过以下命令验证:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe ingress test
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
  bar.baz.com
               /foo   service2:80 (10.8.0.91:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     45s                loadbalancer-controller  default/test
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;也可以通过在外部修改 Ingress YAML 配置后，使用 &lt;code&gt;kubectl replace -f&lt;/code&gt; 命令可以达成相同的效果。&lt;/p&gt;
&lt;!--
## Failing across availability zones

Techniques for spreading traffic across failure domains differ between cloud providers.
Please check the documentation of the relevant [Ingress controller](/docs/concepts/services-networking/ingress-controllers) for details.
 --&gt;
&lt;h2 id=&#34;跨可用区失效&#34;&gt;跨可用区失效&lt;/h2&gt;
&lt;p&gt;在故障域之间传播流量的方法在各家云提供商上的方式都不一样。
请查看 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/ingress-controllers&#34;&gt;Ingress 控制器&lt;/a&gt;文档了解详细信息&lt;/p&gt;
&lt;!--
## Alternatives

You can expose a Service in multiple ways that don&#39;t directly involve the Ingress resource:

* Use [Service.Type=LoadBalancer](/docs/concepts/services-networking/service/#loadbalancer)
* Use [Service.Type=NodePort](/docs/concepts/services-networking/service/#nodeport)
 --&gt;
&lt;h2 id=&#34;alternatives&#34;&gt;替代方案&lt;/h2&gt;
&lt;p&gt;在不直接使用 Ingress 的情况下还有几种暴露 Service 的方法:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/service/#loadbalancer&#34;&gt;Service.Type=LoadBalancer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/service/#nodeport&#34;&gt;Service.Type=NodePort&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;查阅 &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#ingress-v1beta1-networking-k8s-io&#34;&gt;Ingress API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/ingress-controllers/&#34;&gt;Ingress 控制器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/access-application-cluster/ingress-minikube/&#34;&gt;使用 NGINX 控制器在 Minikube 上设置 Ingress&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: VolumeSnapshotClass</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volume-snapshot-classes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volume-snapshot-classes/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- saad-ali
- thockin
- msau42
- jingxu97
- xing-yang
- yuxiangqian
title: Volume Snapshot Classes
content_type: concept
weight: 30
---
--&gt;
&lt;!-- overview --&gt;
&lt;!--
This document describes the concept of VolumeSnapshotClass in Kubernetes. Familiarity
with [volume snapshots](/docs/concepts/storage/volume-snapshots/) and
[storage classes](/docs/concepts/storage/storage-classes) is suggested.
--&gt;
&lt;p&gt;本文介绍 k8s 中的 VolumeSnapshotClass 概念。建议先熟悉
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volume-snapshots/&#34;&gt;卷快照&lt;/a&gt; 和
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes&#34;&gt;StorageClass&lt;/a&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Introduction

Just like StorageClass provides a way for administrators to describe the &#34;classes&#34;
of storage they offer when provisioning a volume, VolumeSnapshotClass provides a
way to describe the &#34;classes&#34; of storage when provisioning a volume snapshot.
 --&gt;
&lt;!--
## Introduction
Just like StorageClass provides a way for administrators to describe the &#34;classes&#34;
of storage they offer when provisioning a volume, VolumeSnapshotClass provides a
way to describe the &#34;classes&#34; of storage when provisioning a volume snapshot.
--&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;与 StorageClass 为管理员提供了一种描述他们供应的存储的类别(class)一样。
VolumeSnapshotClass 提供了一种描述他们供应的卷快照的类别(class)&lt;/p&gt;
&lt;!--
## The VolumeSnapshotClass Resource

Each VolumeSnapshotClass contains the fields `driver`, `deletionPolicy`, and `parameters`,
which are used when a VolumeSnapshot belonging to the class needs to be
dynamically provisioned.

The name of a VolumeSnapshotClass object is significant, and is how users can
request a particular class. Administrators set the name and other parameters
of a class when first creating VolumeSnapshotClass objects, and the objects cannot
be updated once they are created.

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: csi-hostpath-snapclass
driver: hostpath.csi.k8s.io
deletionPolicy: Delete
parameters:
```

Administrators can specify a default VolumeSnapshotClass for VolumeSnapshots
that don&#39;t request any particular class to bind to by adding the
`snapshot.storage.kubernetes.io/is-default-class: &#34;true&#34;` annotation:

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: csi-hostpath-snapclass
  annotations:
    snapshot.storage.kubernetes.io/is-default-class: &#34;true&#34;
driver: hostpath.csi.k8s.io
deletionPolicy: Delete
parameters:
```
 --&gt;
&lt;h2 id=&#34;volumesnapshotclass-资源&#34;&gt;VolumeSnapshotClass 资源&lt;/h2&gt;
&lt;p&gt;每个 &lt;code&gt;VolumeSnapshotClass&lt;/code&gt; 包含的字段有 &lt;code&gt;driver&lt;/code&gt;, &lt;code&gt;deletionPolicy&lt;/code&gt;, 和 &lt;code&gt;parameters&lt;/code&gt;,
这个对象被属于该类别的 VolumeSnapshot 动态供应时用到。&lt;/p&gt;
&lt;p&gt;VolumeSnapshotClass 对象的名称是有意义的，它是用户可以怎么申请一个指定的类别。 管理员在第一次创建
VolumeSnapshotClass 指定该类别的名称和其它参数，这个对象在创建之后将不能被修改。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshotClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;csi-hostpath-snapclass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;driver&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hostpath.csi.k8s.io&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;deletionPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Delete&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;管理员可以能过如下设置注解
&lt;code&gt;snapshot.storage.kubernetes.io/is-default-class: &amp;quot;true&amp;quot;&lt;/code&gt;
为那些没有指定任何类别的 VolumeSnapshot 指定默认的 VolumeSnapshotClass，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshotClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;csi-hostpath-snapclass&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;snapshot.storage.kubernetes.io/is-default-class&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;driver&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hostpath.csi.k8s.io&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;deletionPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Delete&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Driver

Volume snapshot classes have a driver that determines what CSI volume plugin is
used for provisioning VolumeSnapshots. This field must be specified.
 --&gt;
&lt;h3 id=&#34;驱动driver&#34;&gt;驱动(&lt;code&gt;driver&lt;/code&gt;)&lt;/h3&gt;
&lt;p&gt;卷快照类别有一个 driver 字段来决定使用哪个 CSI 卷插件来供应卷快照(VolumeSnapshot).
这是一相必要字段。&lt;/p&gt;
&lt;!--
### DeletionPolicy

Volume snapshot classes have a deletionPolicy. It enables you to configure what happens to a VolumeSnapshotContent when the VolumeSnapshot object it is bound to is to be deleted. The deletionPolicy of a volume snapshot can either be `Retain` or `Delete`. This field must be specified.

If the deletionPolicy is `Delete`, then the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. If the deletionPolicy is `Retain`, then both the underlying snapshot and VolumeSnapshotContent remain.
 --&gt;
&lt;h3 id=&#34;删除策略deletionpolicy&#34;&gt;删除策略(&lt;code&gt;deletionPolicy&lt;/code&gt;)&lt;/h3&gt;
&lt;p&gt;卷快照类别有一个 &lt;code&gt;deletionPolicy&lt;/code&gt;。 它让用户可以配置当与 VolumeSnapshotContent 绑定
VolumeSnapshot 对象被删除时，怎么处理 VolumeSnapshotContent。 卷快照类别的删除策略(&lt;code&gt;deletionPolicy&lt;/code&gt;)
可以是 &lt;code&gt;Retain&lt;/code&gt; 或 &lt;code&gt;Delete&lt;/code&gt;， 这是一个必要字段。&lt;/p&gt;
&lt;p&gt;如果删除策略(&lt;code&gt;deletionPolicy&lt;/code&gt;) 是 &lt;code&gt;Delete&lt;/code&gt;，底层的存储快照会随同 VolumeSnapshotContent
对象的删除一起删除。 如果删除策略(&lt;code&gt;deletionPolicy&lt;/code&gt;) 是 &lt;code&gt;Retain&lt;/code&gt;，
底层的存储快照 和 VolumeSnapshotContent 都会保留。&lt;/p&gt;
&lt;!--
## Parameters

Volume snapshot classes have parameters that describe volume snapshots belonging to
the volume snapshot class. Different parameters may be accepted depending on the
`driver`.
 --&gt;
&lt;h2 id=&#34;其它参数-parameters&#34;&gt;其它参数 (&lt;code&gt;parameters&lt;/code&gt;)&lt;/h2&gt;
&lt;p&gt;卷快照类别有描述属于该类别的卷快照的参数。 可以接受哪些的参数基于不同的驱动 &lt;code&gt;driver&lt;/code&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 使用 kubeconfig 文件组织集群访问</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/organize-cluster-access-kubeconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/organize-cluster-access-kubeconfig/</guid>
      <description>
        
        
        &lt;!--
---
title: Organizing Cluster Access Using kubeconfig Files
content_type: concept
weight: 60
---
 --&gt;
&lt;!-- overview --&gt;
&lt;p&gt;kubeconfig 文件用来组织关于集群，用户，命名空间，和认证机制信息。 &lt;code&gt;kubectl&lt;/code&gt; 命令行工具使用
kubeconfig 文件来查找其用以选择集群和与集群 API 服务通信的信息。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 一个用于配置集群访问的文件就叫做 &lt;em&gt;kubeconfig 文件&lt;/em&gt;。 这是引用配置文件的常用方式。 但并不意味
着有一个叫 &lt;code&gt;kubeconfig&lt;/code&gt; 的文件。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;默认情况下， &lt;code&gt;kubectl&lt;/code&gt; 会在 &lt;code&gt;$HOME/.kube&lt;/code&gt; 目录中寻找一个叫 &lt;code&gt;config&lt;/code&gt; 的文件。 用户可以通过
&lt;code&gt;KUBECONFIG&lt;/code&gt; 环境变量来设置其它的 kubeconfig 文件。&lt;/p&gt;
&lt;p&gt;创建和设置 kubeconfig 文件的详细指南，见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/access-application-cluster/configure-access-multiple-clusters&#34;&gt;配置多集群访问&lt;/a&gt;.&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Supporting multiple clusters, users, and authentication mechanisms

Suppose you have several clusters, and your users and components authenticate
in a variety of ways. For example:

- A running kubelet might authenticate using certificates.
- A user might authenticate using tokens.
- Administrators might have sets of certificates that they provide to individual users.

With kubeconfig files, you can organize your clusters, users, and namespaces.
You can also define contexts to quickly and easily switch between
clusters and namespaces.
 --&gt;
&lt;h2 id=&#34;supporting-multiple-clusters-users-and-authentication-mechanisms&#34;&gt;支持多集群，用户，认证机制&lt;/h2&gt;
&lt;p&gt;假设你有几个集群，并且你的用户和组件有不同的认证方式。 例如:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个运行的 kubelet 可能使用证书来认证。&lt;/li&gt;
&lt;li&gt;一个用户可能使用使用令牌来认证&lt;/li&gt;
&lt;li&gt;管理员可能为每个用户提供认证证书&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过 kubeconfig 文件，就可以组织你的集群，用户， 和命名空间。 你也可以通过定义上下文的方式实现
快速在不同集群和命名空间之间切换。&lt;/p&gt;
&lt;!--
## Context

A *context* element in a kubeconfig file is used to group access parameters
under a convenient name. Each context has three parameters: cluster, namespace, and user.
By default, the `kubectl` command-line tool uses parameters from
the *current context* to communicate with the cluster.

To choose the current context:
```
kubectl config use-context
```
 --&gt;
&lt;h2 id=&#34;context&#34;&gt;上下文&lt;/h2&gt;
&lt;p&gt;在一个 kubeconfig 文件中的一个 &lt;em&gt;上下文&lt;/em&gt; 元素以一个方便的名称的方式来组织访问参数。每个上下文
有三个参数: 集群，命名空间，和用户。 默认情况下， &lt;code&gt;kubectl&lt;/code&gt; 命令行工具使用来自 &lt;em&gt;当前上下文&lt;/em&gt;
中的参数来与集群通信。&lt;/p&gt;
&lt;p&gt;要选择当前上下文,使用下面的命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl config use-context
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## The KUBECONFIG environment variable

The `KUBECONFIG` environment variable holds a list of kubeconfig files.
For Linux and Mac, the list is colon-delimited. For Windows, the list
is semicolon-delimited. The `KUBECONFIG` environment variable is not
required. If the `KUBECONFIG` environment variable doesn&#39;t exist,
`kubectl` uses the default kubeconfig file, `$HOME/.kube/config`.

If the `KUBECONFIG` environment variable does exist, `kubectl` uses
an effective configuration that is the result of merging the files
listed in the `KUBECONFIG` environment variable.
 --&gt;
&lt;h2 id=&#34;the-kubeconfig-environment-variable&#34;&gt;&lt;code&gt;KUBECONFIG&lt;/code&gt; 环境变量&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;KUBECONFIG&lt;/code&gt; 环境变量中包含的是一个 kubeconfig 文件的列表。 对于 Linux 和 Mac，列表是用
冒号(&lt;code&gt;:&lt;/code&gt;)分隔。 对于 Windows， 列表分隔符是分号(&lt;code&gt;;&lt;/code&gt;).  &lt;code&gt;KUBECONFIG&lt;/code&gt; 环境变量不是必要的。
如果 &lt;code&gt;KUBECONFIG&lt;/code&gt; 环境变量不存在， &lt;code&gt;kubectl&lt;/code&gt; 会使用默认的 kubeconfig 文件 &lt;code&gt;$HOME/.kube/config&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;如果 &lt;code&gt;KUBECONFIG&lt;/code&gt; 环境变量是存在的，&lt;code&gt;kubectl&lt;/code&gt; 会使用一个由 &lt;code&gt;KUBECONFIG&lt;/code&gt; 环境变量列举的
文件合并结果作为有效配置。&lt;/p&gt;
&lt;!--
## Merging kubeconfig files

To see your configuration, enter this command:

```shell
kubectl config view
```

As described previously, the output might be from a single kubeconfig file,
or it might be the result of merging several kubeconfig files.

Here are the rules that `kubectl` uses when it merges kubeconfig files:

1. If the `--kubeconfig` flag is set, use only the specified file. Do not merge.
   Only one instance of this flag is allowed.

   Otherwise, if the `KUBECONFIG` environment variable is set, use it as a
   list of files that should be merged.
   Merge the files listed in the `KUBECONFIG` environment variable
   according to these rules:

   * Ignore empty filenames.
   * Produce errors for files with content that cannot be deserialized.
   * The first file to set a particular value or map key wins.
   * Never change the value or map key.
     Example: Preserve the context of the first file to set `current-context`.
     Example: If two files specify a `red-user`, use only values from the first file&#39;s `red-user`.
     Even if the second file has non-conflicting entries under `red-user`, discard them.

   For an example of setting the `KUBECONFIG` environment variable, see
   [Setting the KUBECONFIG environment variable](/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable).

   Otherwise, use the default kubeconfig file, `$HOME/.kube/config`, with no merging.

1. Determine the context to use based on the first hit in this chain:

    1. Use the `--context` command-line flag if it exists.
    1. Use the `current-context` from the merged kubeconfig files.

   An empty context is allowed at this point.

1. Determine the cluster and user. At this point, there might or might not be a context.
   Determine the cluster and user based on the first hit in this chain,
   which is run twice: once for user and once for cluster:

   1. Use a command-line flag if it exists: `--user` or `--cluster`.
   1. If the context is non-empty, take the user or cluster from the context.

   The user and cluster can be empty at this point.

1. Determine the actual cluster information to use. At this point, there might or
   might not be cluster information.
   Build each piece of the cluster information based on this chain; the first hit wins:

   1. Use command line flags if they exist: `--server`, `--certificate-authority`, `--insecure-skip-tls-verify`.
   1. If any cluster information attributes exist from the merged kubeconfig files, use them.
   1. If there is no server location, fail.

1. Determine the actual user information to use. Build user information using the same
   rules as cluster information, except allow only one authentication
   technique per user:

   1. Use command line flags if they exist: `--client-certificate`, `--client-key`, `--username`, `--password`, `--token`.
   1. Use the `user` fields from the merged kubeconfig files.
   1. If there are two conflicting techniques, fail.

1. For any information still missing, use default values and potentially
   prompt for authentication information.
 --&gt;
&lt;h2 id=&#34;merging-kubeconfig-files&#34;&gt;合并 &lt;code&gt;kubeconfig&lt;/code&gt; 文件&lt;/h2&gt;
&lt;p&gt;要看现在的配置，输入下面的命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl config view
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;就如之间所描述的， 输出结果可能来自单个 kubeconfig 文件，也可能是几个 kubeconfig 文件合并的
结果。&lt;/p&gt;
&lt;p&gt;以下为 &lt;code&gt;kubectl&lt;/code&gt; 在合并 kubeconfig 文件使用的规则:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;如果设置 &lt;code&gt;--kubeconfig&lt;/code&gt; 标志，就只使用这个指定的文件。不合并配置文件。这个标志只允许有一个实例。&lt;/p&gt;
&lt;p&gt;否则，如果设置了 &lt;code&gt;KUBECONFIG&lt;/code&gt; 环境变量，就会使用其中列举文件合并的结果。 合并 &lt;code&gt;KUBECONFIG&lt;/code&gt;
环境变量中列举的文件合并会依照如下规则:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;忽略空文件名。&lt;/li&gt;
&lt;li&gt;文件内容不能被反序列化则会生成错误&lt;/li&gt;
&lt;li&gt;排在前面的文件中的键(以及其值)会作为最终结果中的键值&lt;/li&gt;
&lt;li&gt;永远不修改值或字典键
示例: 保留第一个设置 &lt;code&gt;current-context&lt;/code&gt; 文件中的上下文。
示例: 如果有两个文件配置了 &lt;code&gt;red-user&lt;/code&gt;, 使用排在前面的那个文件 &lt;code&gt;red-user&lt;/code&gt; 的值。
即便第二个文件 &lt;code&gt;red-user&lt;/code&gt; 的实体与第一个不冲突，也会把它们整体都丢弃了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里有一个 &lt;code&gt;KUBECONFIG&lt;/code&gt; 环境变量的示例，详见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable&#34;&gt;设置 &lt;code&gt;KUBECONFIG&lt;/code&gt; 环境变量&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;再没有就使用默认的 &lt;code&gt;kubeconfig&lt;/code&gt; 文件，&lt;code&gt;$HOME/.kube/config&lt;/code&gt;， 没得其它合并。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;基于这个链的第一个命中来决定上下文:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如果有则使用 &lt;code&gt;--context&lt;/code&gt; 命令行标志&lt;/li&gt;
&lt;li&gt;使用合并 &lt;code&gt;kubeconfig&lt;/code&gt; 文件中的 &lt;code&gt;current-context&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在这个点允许空的上下文&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;确定集群和用户，在这个点上，可能有也可能没有上下文。 基于下面这个链中的第一个命中的集群和用户，
这个会被运行两次: 一次用于用户和一次用于集群&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如果有命令标志则使用该标志: &lt;code&gt;--user&lt;/code&gt; 或 &lt;code&gt;--cluster&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;如果上下文不是空的，则使用这个上下文中的用户或集群&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在这一级用户和集群可以是空的&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;决定实际使用的集群信息， 在这一级， 可能有也可能没有集群信息。 会依据下面这个链中的片段来构建集群信息；
第一个命中会作为最终结果:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;即便存在，也使用的命令标志: &lt;code&gt;--server&lt;/code&gt;, &lt;code&gt;--certificate-authority&lt;/code&gt;, &lt;code&gt;--insecure-skip-tls-verify&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;如果合并后的 kubeconfig 文件中有任意集群信息的属性存在，这使用它们。&lt;/li&gt;
&lt;li&gt;如果其中没有服务地址信息，则会失败&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;决定实际使用的用户信息。 构建用户信息使用与集群信息相同的规则，只是限制一个用户只能有一个认证机制:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如果存在则使用的标记: &lt;code&gt;--client-certificate&lt;/code&gt;, &lt;code&gt;--client-key&lt;/code&gt;, &lt;code&gt;--username&lt;/code&gt;, &lt;code&gt;--password&lt;/code&gt;, &lt;code&gt;--token&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;使用合并后的 kubeconfig 文件中的 &lt;code&gt;user&lt;/code&gt; 字段。&lt;/li&gt;
&lt;li&gt;如果有两种冲突的认证机制，则会失败&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于到这一步还缺失的信息，使用默认值和潜在的输入提示认证信息。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;!--

## File references

File and path references in a kubeconfig file are relative to the location of the kubeconfig file.
File references on the command line are relative to the current working directory.
In `$HOME/.kube/config`, relative paths are stored relatively, and absolute paths
are stored absolutely.
 --&gt;
&lt;h2 id=&#34;file-references&#34;&gt;文件引用&lt;/h2&gt;
&lt;p&gt;文件和 kubeconfig 文件中的路径引用是相同对 kubeconfig 文件的路径的。 命令行的文件引用是
相对于目前的工作目录的。 在 &lt;code&gt;$HOME/.kube/config&lt;/code&gt; 中, 相对路径以相路径式存储，绝对路径以绝对路径存储。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
* [Configure Access to Multiple Clusters](/docs/tasks/access-application-cluster/configure-access-multiple-clusters/)
* [`kubectl config`](/docs/reference/generated/kubectl/kubectl-commands#config)
--&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/access-application-cluster/configure-access-multiple-clusters/&#34;&gt;配置多集群访问&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#config&#34;&gt;&lt;code&gt;kubectl config&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Ingress 控制器</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/ingress-controllers/</link>
      <pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/ingress-controllers/</guid>
      <description>
        
        
        &lt;!--
---
title: Ingress Controllers
reviewers:
content_type: concept
weight: 40
--- --&gt;
&lt;!-- overview --&gt;
&lt;!--
In order for the Ingress resource to work, the cluster must have an ingress controller running.

Unlike other types of controllers which run as part of the `kube-controller-manager` binary, Ingress controllers
are not started automatically with a cluster. Use this page to choose the ingress controller implementation
that best fits your cluster.

Kubernetes as a project currently supports and maintains [GCE](https://git.k8s.io/ingress-gce/README.md) and
  [nginx](https://git.k8s.io/ingress-nginx/README.md) controllers.
 --&gt;
&lt;p&gt;要使一个 Ingress 工作的前提是集群中必须要有一个 Ingress 控制器在运行。&lt;/p&gt;
&lt;p&gt;与其它类型的控制器作为 &lt;code&gt;kube-controller-manager&lt;/code&gt; 的一部分运行不同， Ingress 不会自动在集群中
运行。 使用本文选择最适合你的集群的 Ingress 控制器实现。&lt;/p&gt;
&lt;p&gt;k8s 项目目前支持和维护
&lt;a href=&#34;https://git.k8s.io/ingress-gce/README.md&#34;&gt;GCE&lt;/a&gt;
和
&lt;a href=&#34;https://git.k8s.io/ingress-nginx/README.md&#34;&gt;nginx&lt;/a&gt;
控制器&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Additional controllers

* [AKS Application Gateway Ingress Controller](https://github.com/Azure/application-gateway-kubernetes-ingress) is an ingress controller that enables ingress to [AKS clusters](https://docs.microsoft.com/azure/aks/kubernetes-walkthrough-portal) using the [Azure Application Gateway](https://docs.microsoft.com/azure/application-gateway/overview).
* [Ambassador](https://www.getambassador.io/) API Gateway is an [Envoy](https://www.envoyproxy.io) based ingress
  controller with [community](https://www.getambassador.io/docs) or
  [commercial](https://www.getambassador.io/pro/) support from [Datawire](https://www.datawire.io/).
* [AppsCode Inc.](https://appscode.com) offers support and maintenance for the most widely used [HAProxy](https://www.haproxy.org/) based ingress controller [Voyager](https://appscode.com/products/voyager).
* [AWS ALB Ingress Controller](https://github.com/kubernetes-sigs/aws-alb-ingress-controller) enables ingress using the [AWS Application Load Balancer](https://aws.amazon.com/elasticloadbalancing/).
* [Contour](https://projectcontour.io/) is an [Envoy](https://www.envoyproxy.io/) based ingress controller
  provided and supported by VMware.
* Citrix provides an [Ingress Controller](https://github.com/citrix/citrix-k8s-ingress-controller) for its hardware (MPX), virtualized (VPX) and [free containerized (CPX) ADC](https://www.citrix.com/products/citrix-adc/cpx-express.html) for [baremetal](https://github.com/citrix/citrix-k8s-ingress-controller/tree/master/deployment/baremetal) and [cloud](https://github.com/citrix/citrix-k8s-ingress-controller/tree/master/deployment) deployments.
* F5 Networks provides [support and maintenance](https://support.f5.com/csp/article/K86859508)
  for the [F5 BIG-IP Container Ingress Services for Kubernetes](https://clouddocs.f5.com/containers/latest/userguide/kubernetes/).
* [Gloo](https://gloo.solo.io) is an open-source ingress controller based on [Envoy](https://www.envoyproxy.io) which offers API Gateway functionality with enterprise support from [solo.io](https://www.solo.io).  
* [HAProxy Ingress](https://haproxy-ingress.github.io) is a highly customizable community-driven ingress controller for HAProxy.
* [HAProxy Technologies](https://www.haproxy.com/) offers support and maintenance for the [HAProxy Ingress Controller for Kubernetes](https://github.com/haproxytech/kubernetes-ingress). See the [official documentation](https://www.haproxy.com/documentation/hapee/1-9r1/traffic-management/kubernetes-ingress-controller/).
* [Istio](https://istio.io/) based ingress controller
  [Control Ingress Traffic](https://istio.io/docs/tasks/traffic-management/ingress/).
* [Kong](https://konghq.com/) offers [community](https://discuss.konghq.com/c/kubernetes) or
  [commercial](https://konghq.com/kong-enterprise/) support and maintenance for the
  [Kong Ingress Controller for Kubernetes](https://github.com/Kong/kubernetes-ingress-controller).
* [NGINX, Inc.](https://www.nginx.com/) offers support and maintenance for the
  [NGINX Ingress Controller for Kubernetes](https://www.nginx.com/products/nginx/kubernetes-ingress-controller).
* [Skipper](https://opensource.zalando.com/skipper/kubernetes/ingress-controller/) HTTP router and reverse proxy for service composition, including use cases like Kubernetes Ingress, designed as a library to build your custom proxy
* [Traefik](https://github.com/containous/traefik) is a fully featured ingress controller
  ([Let&#39;s Encrypt](https://letsencrypt.org), secrets, http2, websocket), and it also comes with commercial
  support by [Containous](https://containo.us/services).
 --&gt;
&lt;h2 id=&#34;其它的控制器&#34;&gt;其它的控制器&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Azure/application-gateway-kubernetes-ingress&#34;&gt;AKS Application Gateway Ingress Controller&lt;/a&gt;
是使用
&lt;a href=&#34;https://docs.microsoft.com/azure/application-gateway/overview&#34;&gt;Azure Application Gateway&lt;/a&gt;
为
&lt;a href=&#34;https://docs.microsoft.com/azure/aks/kubernetes-walkthrough-portal&#34;&gt;AKS 集群&lt;/a&gt;
提供入口的 Ingress 控制器&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.getambassador.io/&#34;&gt;Ambassador&lt;/a&gt; API 网关
是一个基于
&lt;a href=&#34;https://www.envoyproxy.io&#34;&gt;Envoy&lt;/a&gt;
的 Ingress 控制器
有 &lt;a href=&#34;https://www.getambassador.io/docs&#34;&gt;社区&lt;/a&gt; 支持和
来自
&lt;a href=&#34;https://www.datawire.io/&#34;&gt;Datawire&lt;/a&gt;
的
&lt;a href=&#34;https://www.getambassador.io/pro/&#34;&gt;商业&lt;/a&gt; 支持&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://appscode.com&#34;&gt;AppsCode Inc.&lt;/a&gt; 提供了最广泛使用的基于
&lt;a href=&#34;https://www.haproxy.org/&#34;&gt;HAProxy&lt;/a&gt;
的 Ingress 控制器
&lt;a href=&#34;https://appscode.com/products/voyager&#34;&gt;Voyager&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes-sigs/aws-alb-ingress-controller&#34;&gt;AWS ALB Ingress Controller&lt;/a&gt;
使用
&lt;a href=&#34;https://aws.amazon.com/elasticloadbalancing/&#34;&gt;AWS Application Load Balancer&lt;/a&gt;
实现 Ingress&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://projectcontour.io/&#34;&gt;Contour&lt;/a&gt;
由 VMware 提供和支持的基于 &lt;a href=&#34;https://www.envoyproxy.io/&#34;&gt;Envoy&lt;/a&gt; 的 Ingress 控制器&lt;/li&gt;
&lt;li&gt;Citrix 为它的
&lt;a href=&#34;https://github.com/citrix/citrix-k8s-ingress-controller/tree/master/deployment/baremetal&#34;&gt;baremetal&lt;/a&gt;
和
&lt;a href=&#34;https://github.com/citrix/citrix-k8s-ingress-controller/tree/master/deployment&#34;&gt;cloud&lt;/a&gt;
上部署的硬件(MPX),虚拟化(VPX)和
&lt;a href=&#34;https://www.citrix.com/products/citrix-adc/cpx-express.html&#34;&gt;free containerized (CPX) ADC&lt;/a&gt;
提供的 &lt;a href=&#34;https://github.com/citrix/citrix-k8s-ingress-controller&#34;&gt;Ingress 控制器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;F5 Networks&lt;/code&gt; 为
&lt;a href=&#34;https://clouddocs.f5.com/containers/latest/userguide/kubernetes/&#34;&gt;F5 BIG-IP Container Ingress Services for Kubernetes&lt;/a&gt;
提供 &lt;a href=&#34;https://support.f5.com/csp/article/K86859508&#34;&gt;支持和维护&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gloo.solo.io&#34;&gt;Gloo&lt;/a&gt; 是基于
&lt;a href=&#34;https://www.envoyproxy.io&#34;&gt;Envoy&lt;/a&gt; 的开源 Ingress 控制器
提供了来自 &lt;a href=&#34;https://www.solo.io&#34;&gt;solo.io&lt;/a&gt; 企业支持的 API 网络功能&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://haproxy-ingress.github.io&#34;&gt;HAProxy Ingress&lt;/a&gt; 用于 HAProxy 调度可定制的社区驱动的 Ingress 控制器&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.haproxy.com/&#34;&gt;HAProxy Technologies&lt;/a&gt; 为
&lt;a href=&#34;https://github.com/haproxytech/kubernetes-ingress&#34;&gt;HAProxy Ingress Controller for Kubernetes&lt;/a&gt;
提供支持和维护。 详见
&lt;a href=&#34;https://www.haproxy.com/documentation/hapee/1-9r1/traffic-management/kubernetes-ingress-controller/&#34;&gt;官方文档&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;基于 &lt;a href=&#34;https://istio.io/&#34;&gt;Istio&lt;/a&gt; 的
&lt;a href=&#34;https://istio.io/docs/tasks/traffic-management/ingress/&#34;&gt;Control Ingress Traffic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://konghq.com/&#34;&gt;Kong&lt;/a&gt; 为
&lt;a href=&#34;https://github.com/Kong/kubernetes-ingress-controller&#34;&gt;Kong Ingress Controller for Kubernetes&lt;/a&gt;
提供
&lt;a href=&#34;https://discuss.konghq.com/c/kubernetes&#34;&gt;社区&lt;/a&gt;
或
&lt;a href=&#34;https://konghq.com/kong-enterprise/&#34;&gt;商业&lt;/a&gt;的
支持和维护
&lt;a href=&#34;https://www.nginx.com/products/nginx/kubernetes-ingress-controller&#34;&gt;NGINX Ingress Controller for Kubernetes&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nginx.com/&#34;&gt;NGINX, Inc.&lt;/a&gt; 为
&lt;a href=&#34;https://www.nginx.com/products/nginx/kubernetes-ingress-controller&#34;&gt;NGINX Ingress Controller for Kubernetes&lt;/a&gt;
提供支持和维护&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://opensource.zalando.com/skipper/kubernetes/ingress-controller/&#34;&gt;Skipper&lt;/a&gt;
为 Service 提供 HTTP 路由 和反向代理。还包含了类似 k8s Ingress 的使用场景，
设计上可以作为自定义代理的库&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/containous/traefik&#34;&gt;Traefik&lt;/a&gt; 是个功能齐全的 Ingress 控制器
(&lt;a href=&#34;https://letsencrypt.org&#34;&gt;Let&amp;rsquo;s Encrypt&lt;/a&gt;, secrets, http2, websocket)
还有来自 &lt;a href=&#34;https://containo.us/services&#34;&gt;Containous&lt;/a&gt; 的商业支持&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Using multiple Ingress controllers

You may deploy [any number of ingress controllers](https://git.k8s.io/ingress-nginx/docs/user-guide/multiple-ingress.md#multiple-ingress-controllers)
within a cluster. When you create an ingress, you should annotate each ingress with the appropriate
[`ingress.class`](https://git.k8s.io/ingress-gce/docs/faq/README.md#how-do-i-run-multiple-ingress-controllers-in-the-same-cluster)
to indicate which ingress controller should be used if more than one exists within your cluster.

If you do not define a class, your cloud provider may use a default ingress controller.

Ideally, all ingress controllers should fulfill this specification, but the various ingress
controllers operate slightly differently.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Make sure you review your ingress controller&amp;rsquo;s documentation to understand the caveats of choosing it.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;using-multiple-ingress-controllers&#34;&gt;Using multiple Ingress controllers&lt;/h2&gt;
&lt;p&gt;用户可以在集群中部署
&lt;a href=&#34;https://git.k8s.io/ingress-nginx/docs/user-guide/multiple-ingress.md#multiple-ingress-controllers&#34;&gt;任意数量的 Ingress 控制器&lt;/a&gt;
如果集群中有多个 Ingress 控制器，在创建 Ingress 时需要为每个 Ingress 标注恰当的
&lt;a href=&#34;https://git.k8s.io/ingress-gce/docs/faq/README.md#how-do-i-run-multiple-ingress-controllers-in-the-same-cluster&#34;&gt;&lt;code&gt;ingress.class&lt;/code&gt;&lt;/a&gt;
来指示使用哪个 Ingress 控制器。&lt;/p&gt;
&lt;p&gt;如果在 Ingress 中没有定义控制器类型，云提供商可能使用一个默认的 Ingress 控制器。&lt;/p&gt;
&lt;p&gt;理想情况下，所有的 Ingress 控制器都应当满足这个规范，但是很多 Ingress 控制器动作方式都有点不同。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 一定要仔细阅读你选择的 Ingress 控制器的文档，确保理解了相关注意事项。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/ingress/&#34;&gt;Ingress&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/access-application-cluster/ingress-minikube&#34;&gt;使用 NGINX 控制器在 Minikube 上设置 Ingress&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Pod 优先级与抢占</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/pod-priority-preemption/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/pod-priority-preemption/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- davidopp
- wojtek-t
title: Pod Priority and Preemption
content_type: concept
weight: 70
---
 --&gt;
&lt;!-- overview --&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;!--
[Pods](/docs/concepts/workloads/pods/) can have _priority_. Priority indicates the
importance of a Pod relative to other Pods. If a Pod cannot be scheduled, the
scheduler tries to preempt (evict) lower priority Pods to make scheduling of the
pending Pod possible.
 --&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/&#34;&gt;Pod&lt;/a&gt; 是可以有 &lt;em&gt;优先级的&lt;/em&gt;。 优先级表示一个 Pod
与其它 Pod 之间的相对重要程度。 如果一个 Pod 不能被调度， 调度器会尝试抢占(驱逐)一些(个)低优先
级的 Pod， 让这个等待的 Pod 有可能正常调度。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; &lt;!--
In a cluster where not all users are trusted, a malicious user could create Pods
at the highest possible priorities, causing other Pods to be evicted/not get
scheduled.
An administrator can use ResourceQuota to prevent users from creating pods at
high priorities.

See [limit Priority Class consumption by default](/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default)
for details.
 --&gt;
&lt;p&gt;当在一个并不是所有用户都可信的集群中，一个恶意用户可能创建拥有最高可能优先级的 Pod，导致其它的
Pod 被驱逐或无法调度。 管理员可以使用资源配额(ResourceQuota)来防止用户创建高优先级的 Pod.&lt;/p&gt;
&lt;p&gt;详见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default&#34;&gt;对默认使用的 Priority Class进行限制&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## How to use priority and preemption

To use priority and preemption:

1.  Add one or more [PriorityClasses](#priorityclass).

1.  Create Pods with[`priorityClassName`](#pod-priority) set to one of the added
    PriorityClasses. Of course you do not need to create the Pods directly;
    normally you would add `priorityClassName` to the Pod template of a
    collection object like a Deployment.

Keep reading for more information about these steps.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Kubernetes already ships with two PriorityClasses:
&lt;code&gt;system-cluster-critical&lt;/code&gt; and &lt;code&gt;system-node-critical&lt;/code&gt;.
These are common classes and are used to &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/&#34;&gt;ensure that critical components are always scheduled first&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;how-to-use-priority-and-preemption&#34;&gt;怎么使用优先级和抢占&lt;/h2&gt;
&lt;p&gt;要使用优先级和抢占:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;添加一个或多个 &lt;a href=&#34;#priorityclass&#34;&gt;PriorityClasses&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在创建 Pod 时通过 &lt;a href=&#34;#pod-priority&#34;&gt;&lt;code&gt;priorityClassName&lt;/code&gt;&lt;/a&gt; 为它设置一个上一步创建的
PriorityClasses。 当然不需要直接创建 Pod； 通常是将 &lt;code&gt;priorityClassName&lt;/code&gt; 添加到像
Deployment 这一类对象的 Pod 模板中。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下面的章节会更多地介绍这些步骤。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; k8s 已经自带了两个 PriorityClasses: &lt;code&gt;system-cluster-critical&lt;/code&gt; 和 &lt;code&gt;system-node-critical&lt;/code&gt;.
这些是通用类别，被用于
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/&#34;&gt;确保关键组件始终先调度&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## PriorityClass

A PriorityClass is a non-namespaced object that defines a mapping from a
priority class name to the integer value of the priority. The name is specified
in the `name` field of the PriorityClass object&#39;s metadata. The value is
specified in the required `value` field. The higher the value, the higher the
priority.
The name of a PriorityClass object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names),
and it cannot be prefixed with `system-`.

A PriorityClass object can have any 32-bit integer value smaller than or equal
to 1 billion. Larger numbers are reserved for critical system Pods that should
not normally be preempted or evicted. A cluster admin should create one
PriorityClass object for each such mapping that they want.

PriorityClass also has two optional fields: `globalDefault` and `description`.
The `globalDefault` field indicates that the value of this PriorityClass should
be used for Pods without a `priorityClassName`. Only one PriorityClass with
`globalDefault` set to true can exist in the system. If there is no
PriorityClass with `globalDefault` set, the priority of Pods with no
`priorityClassName` is zero.

The `description` field is an arbitrary string. It is meant to tell users of the
cluster when they should use this PriorityClass.
 --&gt;
&lt;h2 id=&#34;priority-class&#34;&gt;PriorityClass&lt;/h2&gt;
&lt;p&gt;PriorityClass 是一个无命名空间字段的对象，它定义了一个优先级类别名称与一个整数值的优先级的映射
关系。 名称定义在 PriorityClass 对象元数据的 &lt;code&gt;name&lt;/code&gt; 字段中。 值则定义在对象的必要字段 &lt;code&gt;value&lt;/code&gt;
中。 这个值越大表示优先级越高。&lt;/p&gt;
&lt;p&gt;PriorityClass 对象可以将任意小于 10 亿的 32 位整数作为其值。 更大的数字则被保留作为系统关键
Pod 使用，这些 Pod 通常不应该被挤掉或驱逐。&lt;/p&gt;
&lt;p&gt;PriorityClass 还有两个可选字段: &lt;code&gt;globalDefault&lt;/code&gt; 和 &lt;code&gt;description&lt;/code&gt;. &lt;code&gt;globalDefault&lt;/code&gt;
字段表示这个 PriorityClass 的值会用作那些没有 &lt;code&gt;priorityClassName&lt;/code&gt; Pod 的优先级。 系统中
只能存在一个 &lt;code&gt;globalDefault&lt;/code&gt; 设置为 true 的 PriorityClass。 如果集群中没有
&lt;code&gt;globalDefault&lt;/code&gt; 设置为 true 的 PriorityClass。 则那些没有 &lt;code&gt;priorityClassName&lt;/code&gt; 的 Pod
的优先级值为 0&lt;/p&gt;
&lt;p&gt;&lt;code&gt;description&lt;/code&gt; 可以是任意字符串。但通常用来告诉集群用户，啥时候应该用这个 PriorityClass.&lt;/p&gt;
&lt;!--
### Notes about PodPriority and existing clusters

-   If you upgrade an existing cluster without this feature, the priority
    of your existing Pods is effectively zero.

-   Addition of a PriorityClass with `globalDefault` set to `true` does not
    change the priorities of existing Pods. The value of such a PriorityClass is
    used only for Pods created after the PriorityClass is added.

-   If you delete a PriorityClass, existing Pods that use the name of the
    deleted PriorityClass remain unchanged, but you cannot create more Pods that
    use the name of the deleted PriorityClass.
 --&gt;
&lt;h3 id=&#34;notes-about-podpriority-and-existing-clusters&#34;&gt;Pod 优先级和已有集群需要注意的一些地方&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;如果从一个没有该特性的集群升级，集群中现在的 Pod 的优先级会被设置为 0&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;添加一个 &lt;code&gt;globalDefault&lt;/code&gt; 设置为 &lt;code&gt;true&lt;/code&gt; 的 PriorityClass， 不会改变已有 Pod 的优先级。
这个 PriorityClass 的优先级值只会被应用到在它之后创建的 Pod 上。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果删除了一个 PriorityClass， 已有的在使用这个 PriorityClass 的 Pod 不会发生变化，
但新创建 Pod 时就不能再使用这个 PriorityClass 了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Example PriorityClass

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: &#34;This priority class should be used for XYZ service pods only.&#34;
```
 --&gt;
&lt;h3 id=&#34;example-priority-class&#34;&gt;&lt;code&gt;PriorityClass&lt;/code&gt; 示例&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;scheduling.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PriorityClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;high-priority&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1000000&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;globalDefault&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;description&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;This priority class should be used for XYZ service pods only.&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Non-preempting PriorityClass {#non-preempting-priority-class}






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;



Pods with `PreemptionPolicy: Never` will be placed in the scheduling queue
ahead of lower-priority pods,
but they cannot preempt other pods.
A non-preempting pod waiting to be scheduled will stay in the scheduling queue,
until sufficient resources are free,
and it can be scheduled.
Non-preempting pods,
like other pods,
are subject to scheduler back-off.
This means that if the scheduler tries these pods and they cannot be scheduled,
they will be retried with lower frequency,
allowing other pods with lower priority to be scheduled before them.

Non-preempting pods may still be preempted by other,
high-priority pods.

`PreemptionPolicy` defaults to `PreemptLowerPriority`,
which will allow pods of that PriorityClass to preempt lower-priority pods
(as is existing default behavior).
If `PreemptionPolicy` is set to `Never`,
pods in that PriorityClass will be non-preempting.

An example use case is for data science workloads.
A user may submit a job that they want to be prioritized above other workloads,
but do not wish to discard existing work by preempting running pods.
The high priority job with `PreemptionPolicy: Never` will be scheduled
ahead of other queued pods,
as soon as sufficient cluster resources &#34;naturally&#34; become free.
 --&gt;
&lt;h2 id=&#34;non-preempting-priority-class&#34;&gt;非抢占式 PriorityClass&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;设置有 &lt;code&gt;PreemptionPolicy: Never&lt;/code&gt; 的 Pod 在调度队列中会放在低优先级 Pod 的前面， 但不会
挤掉其它的 Pod。 一个非抢占式 Pod 会一直待在调度队列中直至有足够的空闲资源才会被调度。 非抢占式
Pod 与其它的 Pod 一样服务调度器补尝。 意思是如果调度器在尝试调度这些 Pod 时如果他们不能被调度，
则尝试调度的频率会越来越低，这会使得其它低优先级的 Pod 可能在他们之间完成调度。&lt;/p&gt;
&lt;p&gt;Non-preempting pods may still be preempted by other,
high-priority pods.
非抢占式 Pod 也可能被其它高优先级的 Pod 摔掉。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PreemptionPolicy&lt;/code&gt; 默认为 &lt;code&gt;PreemptLowerPriority&lt;/code&gt;, 也就是允许这个 &lt;code&gt;PriorityClass&lt;/code&gt; 的 Pod
可以抢占优先级比它低的 Pod (作为默认存在的行为)。 如果 &lt;code&gt;PreemptionPolicy&lt;/code&gt; 设置为 &lt;code&gt;Never&lt;/code&gt;,
使用这个 &lt;code&gt;PriorityClass&lt;/code&gt; 就是非抢占式的。&lt;/p&gt;
&lt;p&gt;一个使用场景示例就是数据科学的工作负载。 一个用户可能提供一个任务(job), 他想要这个任务的优先级
比其它所有工作负载的都高， 但不希望因抢占其它正在运行的 Pod 而导致废弃已经在进行的工作。 拥有
高优先级但使用 &lt;code&gt;PreemptionPolicy: Never&lt;/code&gt; 的任务会被放在调度队列中其它 Pod 之前， 当有足够
集群资源&amp;quot;自然&amp;quot;空闲时就会被调度。&lt;/p&gt;
&lt;h3 id=&#34;example-non-preempting-priority-class&#34;&gt;非抢占式 PriorityClass 示例&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;scheduling.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PriorityClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;high-priority-nonpreempting&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1000000&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;preemptionPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;globalDefault&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;description&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;This priority class will not cause other pods to be preempted.&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Pod priority

After you have one or more PriorityClasses, you can create Pods that specify one
of those PriorityClass names in their specifications. The priority admission
controller uses the `priorityClassName` field and populates the integer value of
the priority. If the priority class is not found, the Pod is rejected.

The following YAML is an example of a Pod configuration that uses the
PriorityClass created in the preceding example. The priority admission
controller checks the specification and resolves the priority of the Pod to
1000000.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  priorityClassName: high-priority
```
 --&gt;
&lt;h2 id=&#34;pod-priority&#34;&gt;Pod 优先级&lt;/h2&gt;
&lt;p&gt;在创建了一个或多个 PriorityClasses 后，在创建 Pod 时就可以在定义中指定一个 PriorityClass
名称。 优先级准入控制器使用 &lt;code&gt;priorityClassName&lt;/code&gt; 字段转㧪为优先级的整数值。 如果没有找到这个
PriorityClasses 则拒绝创建这个 Pod。&lt;/p&gt;
&lt;p&gt;下面的 YAML 中是一个 Pod 配置的示例，其中使用的前面示例中创建的 PriorityClass。优先级准入
控制器检查定义并解析设置 Pod 的优先级值为 1000000.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;imagePullPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IfNotPresent&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;priorityClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;high-priority&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;effect-of-pod-priority-on-scheduling-order&#34;&gt;Pod 优先级对调度顺序的影响&lt;/h3&gt;
&lt;p&gt;当启用优先级以后， 调度器会以优先级来组织等待调度的 Pod，高优先级的 Pod 在队列中要比低优先级的
排位靠前。 结果就是如果调度要求满足时高优先级的 Pod 可能会比低优先级的 Pod 先调度。 但如果
这个高优先级的 Pod 不能调度，调度器会继续尝试调度其它优先级比它低的 Pod。&lt;/p&gt;
&lt;!--
## Preemption

When Pods are created, they go to a queue and wait to be scheduled. The
scheduler picks a Pod from the queue and tries to schedule it on a Node. If no
Node is found that satisfies all the specified requirements of the Pod,
preemption logic is triggered for the pending Pod. Let&#39;s call the pending Pod P.
Preemption logic tries to find a Node where removal of one or more Pods with
lower priority than P would enable P to be scheduled on that Node. If such a
Node is found, one or more lower priority Pods get evicted from the Node. After
the Pods are gone, P can be scheduled on the Node.
 --&gt;
&lt;h2 id=&#34;preemption&#34;&gt;抢占&lt;/h2&gt;
&lt;p&gt;当 Pod 被创建后，就会被塞进一个队列并等待调度。 调度器会从队列中挑一个出来并尝试将其调度到一个
节点上。 如果没有找到一个能满足这个 Pod 指定的所有要求的节点，为这个等待 Pod 的抢占逻辑就会
触发。 假设这个等待的 Pod 叫 P。 抢占逻辑尝试找一个可以通过移除一个或几个优先级比 P 低的 Pod
就能让 P 调度到该节点上。 如果有这样一个节点， 一个或几个低优先级的 Pod 就会被从该节点驱逐。
在这些 Pod 消失后， P 就可以被调度到这个节点上了。&lt;/p&gt;
&lt;!--
### User exposed information

When Pod P preempts one or more Pods on Node N, `nominatedNodeName` field of Pod
P&#39;s status is set to the name of Node N. This field helps scheduler track
resources reserved for Pod P and also gives users information about preemptions
in their clusters.

Please note that Pod P is not necessarily scheduled to the &#34;nominated Node&#34;.
After victim Pods are preempted, they get their graceful termination period. If
another node becomes available while scheduler is waiting for the victim Pods to
terminate, scheduler will use the other node to schedule Pod P. As a result
`nominatedNodeName` and `nodeName` of Pod spec are not always the same. Also, if
scheduler preempts Pods on Node N, but then a higher priority Pod than Pod P
arrives, scheduler may give Node N to the new higher priority Pod. In such a
case, scheduler clears `nominatedNodeName` of Pod P. By doing this, scheduler
makes Pod P eligible to preempt Pods on another Node.
 --&gt;
&lt;h3 id=&#34;user-exposed-information&#34;&gt;暴露给用户的信息&lt;/h3&gt;
&lt;p&gt;当 Pod P 抢占节点 N 上一个或多个 Pod 的地盘， Pod P 的状态上的 &lt;code&gt;nominatedNodeName&lt;/code&gt; 字段
就会设置了 Node N 的名称。 这个字段帮助调度器跟踪为 Pod P 保留的资源同时也为用户提供集群中
的抢占信息。&lt;/p&gt;
&lt;p&gt;要注意 Pod P 不是必须要被调度到&amp;quot;提名节点&amp;quot;的。 在受害 Pod 被抢后，他们会得到一个优雅停止的时限。
如果在等待这些 Pod 停止的过程中有另一个节点变得可用，调度器就会使用另一个节点来调度 Pod . 这
就会导致 Pod 定义中的 &lt;code&gt;nominatedNodeName&lt;/code&gt; 和 &lt;code&gt;nodeName&lt;/code&gt; 并不会始终相同。 还有，如果调度
器将节点 N 上的 Pod 驱逐后，这时候出来一个优先级比 Pod P 高的 Pod 冒出来, 调度器就可能会把
这个高优先级的 Pod 调度到节点 N 上。 在这种情况下，调度器会清除 Pod P 的 &lt;code&gt;nominatedNodeName&lt;/code&gt;。
通过这种方式，调度让 Pod P 再次拥有抢占其它节点上 Pod 的权利。&lt;/p&gt;
&lt;!--
### Limitations of preemption

#### Graceful termination of preemption victims

When Pods are preempted, the victims get their
[graceful termination period](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination).
They have that much time to finish their work and exit. If they don&#39;t, they are
killed. This graceful termination period creates a time gap between the point
that the scheduler preempts Pods and the time when the pending Pod (P) can be
scheduled on the Node (N). In the meantime, the scheduler keeps scheduling other
pending Pods. As victims exit or get terminated, the scheduler tries to schedule
Pods in the pending queue. Therefore, there is usually a time gap between the
point that scheduler preempts victims and the time that Pod P is scheduled. In
order to minimize this gap, one can set graceful termination period of lower
priority Pods to zero or a small number.
 --&gt;
&lt;h3 id=&#34;limitations-of-preemption&#34;&gt;抢占式的限制&lt;/h3&gt;
&lt;h4 id=&#34;graceful-termination-of-preemption-victims&#34;&gt;优雅的停止抢占的受害 Pod&lt;/h4&gt;
&lt;p&gt;当 Pod 被抢地盘之后，受害都会有
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination&#34;&gt;优雅退场时间&lt;/a&gt;.
他们会有一定的时候来完成他们的工作并退出。 如果到了时间还没退出，就会被杀掉。 这个优雅退场时间
就造成了在调度器抢占 Pod 地盘与等待 Pod (P) 可以被调度到节点(N)上之间有一个时间窗口。 在这个
时间窗口中，调度器会继续调度等待队列中其它等待的 Pod。 当受害 Pod 退出或被终止时，调度器在
尝试调度等待队列中的 Pod。因而， 通常在调度器抢占受害 Pod 地盘与 Pod P 被调度之间也有一个时间
窗口。 为了减少这个窗口的长度， 可以将低优先级的 Pod 的优雅退场的时间设置为 0 或一个较小的数字。&lt;/p&gt;
&lt;!--
#### PodDisruptionBudget is supported, but not guaranteed

A [PodDisruptionBudget](/docs/concepts/workloads/pods/disruptions/) (PDB)
allows application owners to limit the number of Pods of a replicated application
that are down simultaneously from voluntary disruptions. Kubernetes supports
PDB when preempting Pods, but respecting PDB is best effort. The scheduler tries
to find victims whose PDB are not violated by preemption, but if no such victims
are found, preemption will still happen, and lower priority Pods will be removed
despite their PDBs being violated.
 --&gt;
&lt;h4 id=&#34;poddisruptionbudget-is-supported-but-not-guaranteed&#34;&gt;支持 PodDisruptionBudget，但不承诺会遵守&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/disruptions/&#34;&gt;PodDisruptionBudget&lt;/a&gt; (PDB)
允许应用所属者限制在计划内故障时多副本应用同时挂掉的 Pod 的数量。 在 Pod 抢占时 k8s 也是支持
PDB 的，但只是尽最大努力遵守。  调度器尝试寻找那些在抢占时不会违反PDB 的 Pod， 但是如果找不到
这样的 Pod。 抢占还是会发生，即便违反了 PDB 低优先级的 Pod 还是会被驱逐。&lt;/p&gt;
&lt;!--
#### Inter-Pod affinity on lower-priority Pods

A Node is considered for preemption only when the answer to this question is
yes: &#34;If all the Pods with lower priority than the pending Pod are removed from
the Node, can the pending Pod be scheduled on the Node?&#34;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Preemption does not necessarily remove all lower-priority
Pods. If the pending Pod can be scheduled by removing fewer than all
lower-priority Pods, then only a portion of the lower-priority Pods are removed.
Even so, the answer to the preceding question must be yes. If the answer is no,
the Node is not considered for preemption.&lt;/div&gt;
&lt;/blockquote&gt;


If a pending Pod has inter-pod affinity to one or more of the lower-priority
Pods on the Node, the inter-Pod affinity rule cannot be satisfied in the absence
of those lower-priority Pods. In this case, the scheduler does not preempt any
Pods on the Node. Instead, it looks for another Node. The scheduler might find a
suitable Node or it might not. There is no guarantee that the pending Pod can be
scheduled.

Our recommended solution for this problem is to create inter-Pod affinity only
towards equal or higher priority Pods.
 --&gt;
&lt;h4 id=&#34;inter-Pod-affinity-on-lower-priority-Pods&#34;&gt;低优先级 Pod 的 Pod 内亲和&lt;/h4&gt;
&lt;p&gt;一个只能只有在这样一个问题的答案是肯定时才会开始抢占: 如果把节点上所有优先级比等待 Pod 低的
Pod 都从节点移除，等待 Pod 是不是就能调度到这个节点上?&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 抢占并不是必须要把所有低优先级的 Pod 都干掉。 如果只在移除部分低优先级的 Pod 就可以让等待的 Pod
调度，则只有一部分低优先级的 Pod 会被干掉。 即便如此，对于上面提到的问题答案也必须是肯定的。
如果答案是否定的，则节点不会考虑抢占。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果等待的 Pod 在这个节点上有一个或多个低优先级的 Pod 存在 Pod 内亲和，少了这些低优先级的 Pod
就不能满足 Pod 内亲和规则。 在这种情况下， 调度器不会抢占节点上的任何 Pod。 而是会去打其它的节点。
调度器可能找到也可能找不到这样的节点。 这就不能承诺对等待 Pod 完成调度。&lt;/p&gt;
&lt;p&gt;对于这种情况，我们推荐的解决方案会，在创建 Pod 内亲和时只指定相同或更高优先级的 Pod。&lt;/p&gt;
&lt;!--
#### Cross node preemption

Suppose a Node N is being considered for preemption so that a pending Pod P can
be scheduled on N. P might become feasible on N only if a Pod on another Node is
preempted. Here&#39;s an example:

*   Pod P is being considered for Node N.
*   Pod Q is running on another Node in the same Zone as Node N.
*   Pod P has Zone-wide anti-affinity with Pod Q (`topologyKey:
    topology.kubernetes.io/zone`).
*   There are no other cases of anti-affinity between Pod P and other Pods in
    the Zone.
*   In order to schedule Pod P on Node N, Pod Q can be preempted, but scheduler
    does not perform cross-node preemption. So, Pod P will be deemed
    unschedulable on Node N.

If Pod Q were removed from its Node, the Pod anti-affinity violation would be
gone, and Pod P could possibly be scheduled on Node N.

We may consider adding cross Node preemption in future versions if there is
enough demand and if we find an algorithm with reasonable performance.
 --&gt;
&lt;h4 id=&#34;cross-node-preemption&#34;&gt;跨节点抢占&lt;/h4&gt;
&lt;p&gt;假设一个节点 N 被考虑作为等待的 Pod N 在其上进行抢占并调度。 P 只有在另一个节点上的一个 Pod 也
被抢占才可能被调度到节点 N 上。 下面是一个例子:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;考虑将 Pod P 放到节点 N 上。&lt;/li&gt;
&lt;li&gt;Pod Q 运行在与节点 N 同一个区域的另一个节点上。&lt;/li&gt;
&lt;li&gt;Pod P 与 Pod Q 之间有一个区域级的反亲和规则(&lt;code&gt;topologyKey: topology.kubernetes.io/zone&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;在这个区域中 Pod P 与 Pod Q 之间没有其它的反亲和情况。&lt;/li&gt;
&lt;li&gt;要把 Pod P 调度到节点 N 上， Pod Q 就要被抢占， 但调度器不会执行跨节点的抢占。 所以 Pod P
就认为是不可以调度到节点 N 上。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果 Pod Q 被从这个节点上移除， 违反反亲和性这个情况就不存在了， Pod P 就是可以被调度到节点 N 上的。&lt;/p&gt;
&lt;p&gt;如果有足够量的要求，我们考虑在未来的版本中加入跨节点的抢占，还得我们找到一个性能可以的算法。&lt;/p&gt;
&lt;!--
## Troubleshooting

Pod priority and pre-emption can have unwanted side effects. Here are some
examples of potential problems and ways to deal with them.
 --&gt;
&lt;h2 id=&#34;troubleshooting&#34;&gt;故障排查&lt;/h2&gt;
&lt;p&gt;Pod 优先级和抢占行为可能会引起一些不希望的副作用。 下面是一些潜在问题的示例以及对应的解决办法。&lt;/p&gt;
&lt;!--
### Pods are preempted unnecessarily

Preemption removes existing Pods from a cluster under resource pressure to make
room for higher priority pending Pods. If you give high priorities to
certain Pods by mistake, these unintentionally high priority Pods may cause
preemption in your cluster. Pod priority is specified by setting the
`priorityClassName` field in the Pod&#39;s specification. The integer value for
priority is then resolved and populated to the `priority` field of `podSpec`.

To address the problem, you can change the `priorityClassName` for those Pods
to use lower priority classes, or leave that field empty. An empty
`priorityClassName` is resolved to zero by default.

When a Pod is preempted, there will be events recorded for the preempted Pod.
Preemption should happen only when a cluster does not have enough resources for
a Pod. In such cases, preemption happens only when the priority of the pending
Pod (preemptor) is higher than the victim Pods. Preemption must not happen when
there is no pending Pod, or when the pending Pods have equal or lower priority
than the victims. If preemption happens in such scenarios, please file an issue.
 --&gt;
&lt;h3 id=&#34;pods-are-preempted-unnecessarily&#34;&gt;不必要的抢占&lt;/h3&gt;
&lt;p&gt;抢占就是集群在资源压力下移除现有的 Pod 为更高优先级的 Pod 腾空间。 如果误操作给一个 Pod 不该
有的高优先级，这些无意中搞出来的高优先级 Pod 就可能在集群中触发抢占。 Pod 的优先级是通过在 Pod
的定义中的 &lt;code&gt;priorityClassName&lt;/code&gt; 字段来实现的。 整数值的优先级则是解析 PriorityClass 得到
并注入到 &lt;code&gt;podSpec&lt;/code&gt; 的 &lt;code&gt;priority&lt;/code&gt; 字段中。&lt;/p&gt;
&lt;p&gt;为解决这个问题，可以通过修改这些 Pod 的 &lt;code&gt;priorityClassName&lt;/code&gt; 使用低优先级的类别， 或不设置
那个字段。 空的 &lt;code&gt;priorityClassName&lt;/code&gt; 默认解析出来的优先级值是 0.&lt;/p&gt;
&lt;p&gt;当一个 Pod 被抢占后， 被抢占的 Pod 就会有相应的事件记录。 抢占只应该发生在集群中没有足够的资源
来调度等待的 Pod。 在这种情况下， 抢占只发生在等待 Pod (抢占者)的优先级比被抢占者的要高。抢占
不能发生在没有等待 Pod， 或与等待的 Pod 拥有与被抢占 Pod 相等或更低的优先级的的情况。如果这种
情况下都发生了抢占，请到官方仓库提交问题单。&lt;/p&gt;
&lt;!--
### Pods are preempted, but the preemptor is not scheduled

When pods are preempted, they receive their requested graceful termination
period, which is by default 30 seconds. If the victim Pods do not terminate within
this period, they are forcibly terminated. Once all the victims go away, the
preemptor Pod can be scheduled.

While the preemptor Pod is waiting for the victims to go away, a higher priority
Pod may be created that fits on the same Node. In this case, the scheduler will
schedule the higher priority Pod instead of the preemptor.

This is expected behavior: the Pod with the higher priority should take the place
of a Pod with a lower priority.
 --&gt;
&lt;h3 id=&#34;pods-are-preempted-but-the-preemptor-is-not-scheduled&#34;&gt;受害者已经被抢，抢占者又没去占地盘&lt;/h3&gt;
&lt;p&gt;当 Pod 被抢时，他们会得到其所请求的优雅退场时间，这个时间默认为 30 秒。 如果被抢的 Pod 在这个
时间内没有终止，则它们会被强制终止。 当所有的被抢 Pod 都退出后，抢占者就可以被调度了。&lt;/p&gt;
&lt;p&gt;当抢占者在等待被抢占者退场时， 可能会有比抢占都优先级更高的 Pod 被创建并适合被调度到同一个节点。
在这种情况下，调度会将后来的这个更高优先级的 Pod 调度过去，而不是原本的抢占者。&lt;/p&gt;
&lt;p&gt;这是预期的行为: 高优先级的 Pod 应该占掉低优先级的 Pod 的地盘。&lt;/p&gt;
&lt;h3 id=&#34;higher-priority-pods-are-preempted-before-lower-priority-pods&#34;&gt;高优先级 Pod 比低优先级先被抢占&lt;/h3&gt;
&lt;p&gt;调度器在尝试为等待 Pod 找可以运行的节点时。 如果没有找到，调度器就会尝试在任意一个节点上移除
低优先级的 Pod 为这个等待的 Pod 腾空间。 如果一个有低优先级 Pod 的节点不适合这个等待的 Pod，
调度可能会选择另一个(相对于刚才那个节点上的 Pod)优先级更高的 Pod 进行抢占。但被抢占者依然是
比抢占 Pod 的优先级要低。&lt;/p&gt;
&lt;p&gt;当有多个节点满足被抢占的条件时， 调度器会选择被抢占 Pod 集合优先级最低的那个节点。 但是如果这些
Pod 有 PodDisruptionBudget 并且，如果它们被抢占会违反其 PDB， 则调度器可能会选择一个优先级
更高一些的 Pod 的节点。&lt;/p&gt;
&lt;p&gt;当存在多个节点可以抢占而又没有以上提到的情况时， 调度器会选择 Pod 优先级最低的那个节点。&lt;/p&gt;
&lt;h2 id=&#34;interactions-of-pod-priority-and-qos&#34;&gt;Pod 优先级与服务质量之间的相互关系&lt;/h2&gt;
&lt;p&gt;Pod 优先级和
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-qos-class&#39; target=&#39;_blank&#39;&gt;QoS 类别&lt;span class=&#39;tooltip-text&#39;&gt;QoS 类别 (服务质量类别)提供了对集群中的 Pod 分为几种不同类型的方式，并用于调度和驱逐决策。&lt;/span&gt;
&lt;/a&gt;
是两个交互和关系都比较少的两个特性， 在设置 Pod 优先级级时是否基于其 QoS 类别是没有默认限制的。
调度器在进行抢占逻辑时选择抢占目标时不会考虑 QoS。 抢占逻辑只会考虑 Pod 优先级并且尝试选择优先级
最低的目标集。 高优先级的 Pod 只有在如果移除最低优先级并不能满足调度器调度抢占 Pod 的时候，或
如果最低优先级的 Pod 受到 &lt;code&gt;PodDisruptionBudget&lt;/code&gt; 保护。&lt;/p&gt;
&lt;p&gt;同时考虑 QoS 和 Pod 优先级的唯一组件是
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/administer-cluster/out-of-resource/&#34;&gt;kubelet 资源耗尽(out-of-resource) 驱逐&lt;/a&gt;.
kubelet 排列驱逐列表时，第一是他们使用的紧缺资源是否超出了请求， 然后是优先级， 然后是消耗的
紧缺计算资源相对了 Pod 调度请求的量。更多信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/administer-cluster/out-of-resource/#evicting-end-user-pods&#34;&gt;驱逐终端用户 Pod&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;kubelet 资源耗尽(out-of-resource) 驱逐不会驱逐那些使用没有超出其请求值的 Pod。 如果一个
低优先级的 Pod 没有超出它的请求， 它就不会被驱逐。 另一个优先级相对更高的 Pod 如果超出了其请求则
可能被驱逐。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
* Read about using ResourceQuotas in connection with PriorityClasses: [limit Priority Class consumption by default](/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default)
 --&gt;
&lt;ul&gt;
&lt;li&gt;使用与 PriorityClasses 有联系的 PriorityClasses
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default&#34;&gt;对默认使用的 Priority Class进行限制&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 卷动态供应</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/dynamic-provisioning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/dynamic-provisioning/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- saad-ali
- jsafrane
- thockin
- msau42
title: Dynamic Volume Provisioning
content_type: concept
weight: 70
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
Dynamic volume provisioning allows storage volumes to be created on-demand.
Without dynamic provisioning, cluster administrators have to manually make
calls to their cloud or storage provider to create new storage volumes, and
then create [`PersistentVolume` objects](/docs/concepts/storage/persistent-volumes/)
to represent them in Kubernetes. The dynamic provisioning feature eliminates
the need for cluster administrators to pre-provision storage. Instead, it
automatically provisions storage when it is requested by users.
 --&gt;
&lt;p&gt;卷动态供应允许存储卷可以根据需要创建。 如果没有动态供应，集群管理员必须要手动调用云存储或其它存储
提供者来创建新的存储卷，然后再创建
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/&#34;&gt;&lt;code&gt;PersistentVolume&lt;/code&gt; 对象&lt;/a&gt;来
在 k8s 中代表他们。动态供应特性就是将管理员从这些预创建的任务中解放出来。 系统会根据用户的请求
自动供应存储&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Background

The implementation of dynamic volume provisioning is based on the API object `StorageClass`
from the API group `storage.k8s.io`. A cluster administrator can define as many
`StorageClass` objects as needed, each specifying a *volume plugin* (aka
*provisioner*) that provisions a volume and the set of parameters to pass to
that provisioner when provisioning.
A cluster administrator can define and expose multiple flavors of storage (from
the same or different storage systems) within a cluster, each with a custom set
of parameters. This design also ensures that end users don&#39;t have to worry
about the complexity and nuances of how storage is provisioned, but still
have the ability to select from multiple storage options.

More information on storage classes can be found
[here](/docs/concepts/storage/storage-classes/).
 --&gt;
&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;p&gt;卷动态供应是基于来自 &lt;code&gt;storage.k8s.io&lt;/code&gt; API 组的 &lt;code&gt;StorageClass&lt;/code&gt; API 对象。 集群管理员可以
根据需要创建任意数量的 &lt;code&gt;StorageClass&lt;/code&gt; 对象， 每个对象中都会指定一个 &lt;em&gt;卷插件&lt;/em&gt;(也就是 &lt;em&gt;供应者&lt;/em&gt;)
来提供卷并且包含供应卷时需要传递的参数集。集群管理员可以在集群中定义并暴露多个偏好的存储(来自相同
或不同的存储系统)，每种都有一套自定义的参数。 这种设计也保证终端用户不需要关心存储供应的复杂性和差异性
的同时还能提供多种存储选项的选择。&lt;/p&gt;
&lt;p&gt;更多关于存储类别的信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes/&#34;&gt;这里&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Enabling Dynamic Provisioning

To enable dynamic provisioning, a cluster administrator needs to pre-create
one or more StorageClass objects for users.
StorageClass objects define which provisioner should be used and what parameters
should be passed to that provisioner when dynamic provisioning is invoked.
The name of a StorageClass object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

The following manifest creates a storage class &#34;slow&#34; which provisions standard
disk-like persistent disks.

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
```

The following manifest creates a storage class &#34;fast&#34; which provisions
SSD-like persistent disks.

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
```
 --&gt;
&lt;h2 id=&#34;enabling-dynamic-provisioning&#34;&gt;启用动态供应&lt;/h2&gt;
&lt;p&gt;为启用动态供应，集群管理员需要预先为用户创建好一个或多个 &lt;code&gt;StorageClass&lt;/code&gt; 对象。&lt;code&gt;StorageClass&lt;/code&gt;
对象定义需要使用哪个供应者和在动态供应调用时需要传递给供应者的参数。 &lt;code&gt;StorageClass&lt;/code&gt; 对象的名称
必须是一个有效的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;以下的配置中定义了一个叫 &amp;ldquo;slow&amp;rdquo; &lt;code&gt;StorageClass&lt;/code&gt;， 它提供了一个标准的类磁盘的持久下磁盘。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/gce-pd&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pd-standard&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;以下的配置中定义了一个叫 &amp;ldquo;fast&amp;rdquo; &lt;code&gt;StorageClass&lt;/code&gt;， 它提供了一个类SSD盘的持久下磁盘。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fast&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/gce-pd&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pd-ssd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Using Dynamic Provisioning

Users request dynamically provisioned storage by including a storage class in
their `PersistentVolumeClaim`. Before Kubernetes v1.6, this was done via the
`volume.beta.kubernetes.io/storage-class` annotation. However, this annotation
is deprecated since v1.6. Users now can and should instead use the
`storageClassName` field of the `PersistentVolumeClaim` object. The value of
this field must match the name of a `StorageClass` configured by the
administrator (see [below](#enabling-dynamic-provisioning)).

To select the &#34;fast&#34; storage class, for example, a user would create the
following PersistentVolumeClaim:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim1
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast
  resources:
    requests:
      storage: 30Gi
```

This claim results in an SSD-like Persistent Disk being automatically
provisioned. When the claim is deleted, the volume is destroyed.
--&gt;
&lt;h2 id=&#34;using-dynamic-provisioning&#34;&gt;使用动态供应&lt;/h2&gt;
&lt;p&gt;用户通过在 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 引用一个 &lt;code&gt;StorageClass&lt;/code&gt; 申请动态供应存储。 在 k8s
v1.6 之前， 这是通过添加注解 &lt;code&gt;volume.beta.kubernetes.io/storage-class&lt;/code&gt; 实现的。但是
这个注解在 k8s v1.6 之后已经废弃。在这之后的版本用户可以在 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 对象
中使用 &lt;code&gt;storageClassName&lt;/code&gt; 代替这个注解。这个字段值必须与管理员配置的 &lt;code&gt;StorageClass&lt;/code&gt; 中的一个
(见 &lt;a href=&#34;#enabling-dynamic-provisioning&#34;&gt;上面&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;例如，想要使用 &amp;ldquo;fast&amp;rdquo; &lt;code&gt;StorageClass&lt;/code&gt;， 用户可以创建以下 PersistentVolumeClaim:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;claim1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fast&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;30Gi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个配置的 PVC 会自动供应一个类 SSD 的持久化磁盘。 当 PVC 被删除后，对应卷也会被删除。&lt;/p&gt;
&lt;!--
## Defaulting Behavior

Dynamic provisioning can be enabled on a cluster such that all claims are
dynamically provisioned if no storage class is specified. A cluster administrator
can enable this behavior by:

- Marking one `StorageClass` object as *default*;
- Making sure that the [`DefaultStorageClass` admission controller](/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)
  is enabled on the API server.

An administrator can mark a specific `StorageClass` as default by adding the
`storageclass.kubernetes.io/is-default-class` annotation to it.
When a default `StorageClass` exists in a cluster and a user creates a
`PersistentVolumeClaim` with `storageClassName` unspecified, the
`DefaultStorageClass` admission controller automatically adds the
`storageClassName` field pointing to the default storage class.

Note that there can be at most one *default* storage class on a cluster, or
a `PersistentVolumeClaim` without `storageClassName` explicitly specified cannot
be created.
 --&gt;
&lt;h2 id=&#34;默认行为&#34;&gt;默认行为&lt;/h2&gt;
&lt;p&gt;可以在集群中启用动态供应，这样所有的PVC 都可以在没有指定 StorageClass 的情况下也是动态供应的。
集群管理员可以通过以下方式启动:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将一个 &lt;code&gt;StorageClass&lt;/code&gt; 对象设置为 &lt;em&gt;默认&lt;/em&gt;;
is enabled on the API server.&lt;/li&gt;
&lt;li&gt;确保 API server 上的
&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass&#34;&gt;&lt;code&gt;DefaultStorageClass&lt;/code&gt; 准入控制&lt;/a&gt;
启用了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;管理员可以通过在一个 &lt;code&gt;StorageClass&lt;/code&gt; 添加 &lt;code&gt;storageclass.kubernetes.io/is-default-class&lt;/code&gt;
注解的方式将其设置为默认。 当集群中有默认的 &lt;code&gt;StorageClass&lt;/code&gt; 存在时，用户创建 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;
没有指定 &lt;code&gt;storageClassName&lt;/code&gt; ，&lt;code&gt;DefaultStorageClass&lt;/code&gt; 准入控制自动添加 &lt;code&gt;storageClassName&lt;/code&gt;
字段，并将其指向默认的 &lt;code&gt;StorageClass&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;要注意一个集群中最多只能有一个 &lt;em&gt;默认&lt;/em&gt; &lt;code&gt;StorageClass&lt;/code&gt;， 否则没有显示指定 &lt;code&gt;storageClassName&lt;/code&gt;
的 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 是不能创建的。&lt;/p&gt;
&lt;!--
## Topology Awareness

In [Multi-Zone](/docs/setup/multiple-zones) clusters, Pods can be spread across
Zones in a Region. Single-Zone storage backends should be provisioned in the Zones where
Pods are scheduled. This can be accomplished by setting the [Volume Binding
Mode](/docs/concepts/storage/storage-classes/#volume-binding-mode).
 --&gt;
&lt;h2 id=&#34;拓扑自洽&#34;&gt;拓扑自洽&lt;/h2&gt;
&lt;p&gt;在 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/setup/multiple-zones&#34;&gt;多区域&lt;/a&gt; 集群中，Pod 可以在同一个地区的不同区域中分布。
单区域存储后端应该供应到 Pod 被调度到的区域中。这可以通过设置
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes/#volume-binding-mode&#34;&gt;卷绑定模式&lt;/a&gt;
实现。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 网络策略</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/network-policies/</link>
      <pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/network-policies/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- thockin
- caseydavenport
- danwinship
title: Network Policies
content_type: concept
weight: 50
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), then you might consider using Kubernetes NetworkPolicies for particular applications in your cluster.  NetworkPolicies are an application-centric construct which allow you to specify how a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; is allowed to communicate with various network &#34;entities&#34; (we use the word &#34;entity&#34; here to avoid overloading the more common terms such as &#34;endpoints&#34; and &#34;services&#34;, which have specific Kubernetes connotations) over the network.

The entities that a Pod can communicate with are identified through a combination of the following 3 identifiers:

1. Other pods that are allowed (exception: a pod cannot block access to itself)
2. Namespaces that are allowed
3. IP blocks (exception: traffic to and from the node where a Pod is running is always allowed, regardless of the IP address of the Pod or the node)

When defining a pod- or namespace- based NetworkPolicy, you use a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/&#39; target=&#39;_blank&#39;&gt;selector&lt;span class=&#39;tooltip-text&#39;&gt;允许用户基于标签过滤资源列表&lt;/span&gt;
&lt;/a&gt; to specify what traffic is allowed to and from the Pod(s) that match the selector.

Meanwhile, when IP based NetworkPolicies are created, we define policies based on IP blocks (CIDR ranges).
 --&gt;
&lt;p&gt;如果想要在 IP 地址或端口层(OSI 3 或 4 层)控制网络流量，可以考虑对集群中的特定应用使用 k8s 网络策略。
网络策略是一个应用为中心的构造，它允许用户指定怎么控制一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 以允许它与其它多种网络实体(这里使用 实体(entity)避免与 &amp;ldquo;Endpoint&amp;rdquo; &amp;ldquo;Service&amp;rdquo;
这此在 k8s 中有明确含义的词混淆)通信。&lt;/p&gt;
&lt;p&gt;Pod 可以与之通信的实体可以由以下三个标识组合来识别:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;其它被允许的 Pod (例外: Pod 不可以禁止与它本身通信)&lt;/li&gt;
&lt;li&gt;被允许的命名空间&lt;/li&gt;
&lt;li&gt;IP 段(例外: Pod 所在的节点进出流量都是允许的，Pod 和 节点的 IP 如果在禁用段则会被忽略)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在定义一个基于 Pod 或 命名空间的网络策略时，可以使用&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/&#39; target=&#39;_blank&#39;&gt;选择器&lt;span class=&#39;tooltip-text&#39;&gt;允许用户基于标签过滤资源列表&lt;/span&gt;
&lt;/a&gt;
来指定只与选择器相匹配的 Pod 在允许与之进行流量往来。&lt;/p&gt;
&lt;p&gt;同时，当创建基于 IP 的网络策略时，定义的策略基于 IP 段(CIDR 范围)。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Prerequisites

Network policies are implemented by the [network plugin](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/). To use network policies, you must be using a networking solution which supports NetworkPolicy. Creating a NetworkPolicy resource without a controller that implements it will have no effect.
--&gt;
&lt;h2 id=&#34;前置条件&#34;&gt;前置条件&lt;/h2&gt;
&lt;p&gt;网络策略是由
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/&#34;&gt;网络插件&lt;/a&gt;
实现的。 要使用网络策略，就必要使用一个支持网络策略的网络解决方案。 创建一个没有实现的控制器的
&lt;code&gt;NetworkPolicy&lt;/code&gt; 资源是没有效果的&lt;/p&gt;
&lt;!--
## Isolated and Non-isolated Pods

By default, pods are non-isolated; they accept traffic from any source.

Pods become isolated by having a NetworkPolicy that selects them. Once there is any NetworkPolicy in a namespace selecting a particular pod, that pod will reject any connections that are not allowed by any NetworkPolicy. (Other pods in the namespace that are not selected by any NetworkPolicy will continue to accept all traffic.)

Network policies do not conflict; they are additive. If any policy or policies select a pod, the pod is restricted to what is allowed by the union of those policies&#39; ingress/egress rules. Thus, order of evaluation does not affect the policy result.
 --&gt;
&lt;h2 id=&#34;隔离和非隔离的-pod&#34;&gt;隔离和非隔离的 Pod&lt;/h2&gt;
&lt;p&gt;默认情况下，所有的 Pod 都是非隔离的; 它们可以接收来自任意源的流量。&lt;/p&gt;
&lt;p&gt;当 Pod 被一个 NetworkPolicy 选中时就会变成隔离的。 当在一个命名空间中有任意 NetworkPolicy
选中了某个 Pod， 这个 Pod 就会拒绝那些不被这些 NetworkPolicy 允许的连接就会被拒绝。(同一个
命名空间中其它没有被任何 NetworkPolicy 选择的 Pod 依然继续接收所有流量。)&lt;/p&gt;
&lt;p&gt;网络策略不会冲突； 它们可叠加。 如果任意策略选中了一个 Pod， 该 Pod 就会被限制在这些策略的
&lt;code&gt;ingress/egress&lt;/code&gt; 规则交集允许的范围。 因此，执行的顺序并不会影响策略的结果。&lt;/p&gt;
&lt;!--
## The NetworkPolicy resource {#networkpolicy-resource}

See the [NetworkPolicy](/docs/reference/generated/kubernetes-api/v1.19/#networkpolicy-v1-networking-k8s-io) reference for a full definition of the resource.

An example NetworkPolicy might look like this:

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; POSTing this to the API server for your cluster will have no effect unless your chosen networking solution supports network policy.&lt;/div&gt;
&lt;/blockquote&gt;


__Mandatory Fields__: As with all other Kubernetes config, a NetworkPolicy
needs `apiVersion`, `kind`, and `metadata` fields.  For general information
about working with config files, see
[Configure Containers Using a ConfigMap](/docs/tasks/configure-pod-container/configure-pod-configmap/),
and [Object Management](/docs/concepts/overview/working-with-objects/object-management).

__spec__: NetworkPolicy [spec](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status) has all the information needed to define a particular network policy in the given namespace.

__podSelector__: Each NetworkPolicy includes a `podSelector` which selects the grouping of pods to which the policy applies. The example policy selects pods with the label &#34;role=db&#34;. An empty `podSelector` selects all pods in the namespace.

__policyTypes__: Each NetworkPolicy includes a `policyTypes` list which may include either `Ingress`, `Egress`, or both. The `policyTypes` field indicates whether or not the given policy applies to ingress traffic to selected pod, egress traffic from selected pods, or both. If no `policyTypes` are specified on a NetworkPolicy then by default `Ingress` will always be set and `Egress` will be set if the NetworkPolicy has any egress rules.

__ingress__: Each NetworkPolicy may include a list of allowed `ingress` rules.  Each rule allows traffic which matches both the `from` and `ports` sections. The example policy contains a single rule, which matches traffic on a single port, from one of three sources, the first specified via an `ipBlock`, the second via a `namespaceSelector` and the third via a `podSelector`.

__egress__: Each NetworkPolicy may include a list of allowed `egress` rules.  Each rule allows traffic which matches both the `to` and `ports` sections. The example policy contains a single rule, which matches traffic on a single port to any destination in `10.0.0.0/24`.

So, the example NetworkPolicy:

1. isolates &#34;role=db&#34; pods in the &#34;default&#34; namespace for both ingress and egress traffic (if they weren&#39;t already isolated)
2. (Ingress rules) allows connections to all pods in the &#34;default&#34; namespace with the label &#34;role=db&#34; on TCP port 6379 from:

   * any pod in the &#34;default&#34; namespace with the label &#34;role=frontend&#34;
   * any pod in a namespace with the label &#34;project=myproject&#34;
   * IP addresses in the ranges 172.17.0.0–172.17.0.255 and 172.17.2.0–172.17.255.255 (ie, all of 172.17.0.0/16 except 172.17.1.0/24)
3. (Egress rules) allows connections from any pod in the &#34;default&#34; namespace with the label &#34;role=db&#34; to CIDR 10.0.0.0/24 on TCP port 5978

See the [Declare Network Policy](/docs/tasks/administer-cluster/declare-network-policy/) walkthrough for further examples.
--&gt;
&lt;h2 id=&#34;networkpolicy-resource&#34;&gt;网络策略资源&lt;/h2&gt;
&lt;p&gt;可以在 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubernetes-api/v1.19/#networkpolicy-v1-networking-k8s-io&#34;&gt;NetworkPolicy&lt;/a&gt; 查阅详细定义文档.&lt;/p&gt;
&lt;p&gt;一个 NetworkPolicy 示例可能长成这个样子:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-network-policy&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;role&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;db&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Egress&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ingress&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;ipBlock&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;cidr&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;172.17.0.0&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;/16&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;except&lt;/span&gt;:
        - &lt;span style=&#34;color:#ae81ff&#34;&gt;172.17.1.0&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;/24&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;namespaceSelector&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;project&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myproject&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;role&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;6379&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;egress&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;to&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;ipBlock&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;cidr&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10.0.0.0&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;/24&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5978&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果集群的网络解决方案(网络插件)不支持网络策略，POST 这个配置到集群 api-server 将是没有效果的
也就是说要使用网络策略，就必须要安装支持网络策略的网络插件。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;必要字段&lt;/strong&gt;: 与其它所有的 k8s 配置一样， NetworkPolicy 必须有 &lt;code&gt;apiVersion&lt;/code&gt;, &lt;code&gt;kind&lt;/code&gt;, &lt;code&gt;metadata&lt;/code&gt;
字段。 配置文件常用信息请见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-pod-configmap/&#34;&gt;使用 ConfigMap 配置容器&lt;/a&gt;,
和 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/object-management&#34;&gt;对象管理&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;spec&lt;/strong&gt;: NetworkPolicy &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;spec&lt;/a&gt; 包含了在指定命名空间创建特定网络策略的所有信息&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;podSelector&lt;/strong&gt;: 每个 NetworkPolicy 都包含了一个 &lt;code&gt;podSelector&lt;/code&gt; 字段，用户选择应用该策略的 Pod 组。
上面例子中的策略选择包含 &amp;ldquo;&lt;code&gt;role=db&lt;/code&gt;&amp;rdquo; 标签的 Pod。 如果 &lt;code&gt;podSelector&lt;/code&gt; 是空则选择该命名空间中所有的 Pod。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;policyTypes&lt;/strong&gt;: 每个 NetworkPolicy 包含一个 &lt;code&gt;policyTypes&lt;/code&gt; 字段，该字段值为一个列表，
这个列表的值可以是 &lt;code&gt;Ingress&lt;/code&gt;, &lt;code&gt;Egress&lt;/code&gt; 中的任意一个或同时包含两个。 该字段值是否包含
&lt;code&gt;Ingress&lt;/code&gt; 表示是否将该策略执行到输入流量到达选定的 Pod；
&lt;code&gt;Egress&lt;/code&gt; 表示是否将该策略执行到选择 Pod 输出的流量。
如果没有指定 &lt;code&gt;policyTypes&lt;/code&gt; 字段，则默认情况下 &lt;code&gt;Ingress&lt;/code&gt; 问题要设置，而 &lt;code&gt;Egress&lt;/code&gt; 则只在
NetworkPolicy 中包含 &lt;code&gt;egress&lt;/code&gt; 规则是才设置。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ingress&lt;/strong&gt;: 每个 NetworkPolicy 可能包含一个被允许的 &lt;code&gt;ingress&lt;/code&gt; 规则列表。
每个规则所允许的流量会同时匹配 &lt;code&gt;from&lt;/code&gt; 和 &lt;code&gt;ports&lt;/code&gt; 两个部分。上面例子中的策略就包含一个规则
这个规则匹配来自一个端口加上三个源(第一个是通过 &lt;code&gt;ipBlock&lt;/code&gt;指定一个 IP 段； 第三个是通过
&lt;code&gt;namespaceSelector&lt;/code&gt;；第三个是通过 &lt;code&gt;podSelector&lt;/code&gt;)中的一个的组合的流量&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;egress&lt;/strong&gt;: 每个 NetworkPolicy 可能包含一个被允许的 &lt;code&gt;egress&lt;/code&gt; 规则列表。 每个规则允许的流量
由 &lt;code&gt;to&lt;/code&gt; 和 &lt;code&gt;ports&lt;/code&gt; 组合匹配。 上面的例子中的策略包含一个规则，这个规则匹配一个端口加上
&lt;code&gt;10.0.0.0/24&lt;/code&gt; 中的任意一个目标的组合的流量。&lt;/p&gt;
&lt;p&gt;因此，上面例子中 NetworkPolicy 如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;在默认(default)命名空间中隔离 &amp;ldquo;role=db&amp;rdquo; 所选择 Pod 的 &lt;code&gt;ingress&lt;/code&gt; 和 &lt;code&gt;egress&lt;/code&gt; 流量(如果它们还没有被隔离)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Ingress 规则)允许以下范围的所有连接到默认(default)命名空间中包含 &amp;ldquo;role=db&amp;rdquo; 标签的 Pod 的 TCP 端口 6379:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;默认(default)命名空间中包含 &amp;ldquo;role=frontend&amp;rdquo; 标签的任意 Pod&lt;/li&gt;
&lt;li&gt;所在命名空间包含 &amp;ldquo;project=myproject&amp;rdquo; 标签的任意 Pod&lt;/li&gt;
&lt;li&gt;&lt;code&gt;172.17.0.0–172.17.0.255&lt;/code&gt; 和 &lt;code&gt;172.17.2.0–172.17.255.255&lt;/code&gt; IP 段中的地址(
&lt;code&gt;172.17.0.0/16&lt;/code&gt; 中除了 &lt;code&gt;172.17.1.0/24&lt;/code&gt; 之外的地址
)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;(Egress 规则) 允许 默认(default)命名空间中包含 &amp;ldquo;role=db&amp;rdquo; 的任意 Pod 到
CIDR 10.0.0.0/24 TCP 端口 5978 的所有连接&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;更多示例见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/declare-network-policy/&#34;&gt;声明网络策略&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Behavior of `to` and `from` selectors

There are four kinds of selectors that can be specified in an `ingress` `from` section or `egress` `to` section:

__podSelector__: This selects particular Pods in the same namespace as the NetworkPolicy which should be allowed as ingress sources or egress destinations.

__namespaceSelector__: This selects particular namespaces for which all Pods should be allowed as ingress sources or egress destinations.

__namespaceSelector__ *and* __podSelector__: A single `to`/`from` entry that specifies both `namespaceSelector` and `podSelector` selects particular Pods within particular namespaces. Be careful to use correct YAML syntax; this policy:

```yaml
  ...
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
      podSelector:
        matchLabels:
          role: client
  ...
```

contains a single `from` element allowing connections from Pods with the label `role=client` in namespaces with the label `user=alice`. But *this* policy:

```yaml
  ...
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
    - podSelector:
        matchLabels:
          role: client
  ...
```

contains two elements in the `from` array, and allows connections from Pods in the local Namespace with the label `role=client`, *or* from any Pod in any namespace with the label `user=alice`.

When in doubt, use `kubectl describe` to see how Kubernetes has interpreted the policy.

__ipBlock__: This selects particular IP CIDR ranges to allow as ingress sources or egress destinations. These should be cluster-external IPs, since Pod IPs are ephemeral and unpredictable.

Cluster ingress and egress mechanisms often require rewriting the source or destination IP
of packets. In cases where this happens, it is not defined whether this happens before or
after NetworkPolicy processing, and the behavior may be different for different
combinations of network plugin, cloud provider, `Service` implementation, etc.

In the case of ingress, this means that in some cases you may be able to filter incoming
packets based on the actual original source IP, while in other cases, the &#34;source IP&#34; that
the NetworkPolicy acts on may be the IP of a `LoadBalancer` or of the Pod&#39;s node, etc.

For egress, this means that connections from pods to `Service` IPs that get rewritten to
cluster-external IPs may or may not be subject to `ipBlock`-based policies.
 --&gt;
&lt;h2 id=&#34;to-和-from-选择器的行为方式&#34;&gt;&lt;code&gt;to&lt;/code&gt; 和 &lt;code&gt;from&lt;/code&gt; 选择器的行为方式&lt;/h2&gt;
&lt;p&gt;在 &lt;code&gt;ingress&lt;/code&gt; 的 &lt;code&gt;from&lt;/code&gt; 区 或 &lt;code&gt;egress&lt;/code&gt; 的 &lt;code&gt;to&lt;/code&gt; 区可以指定以下四种选择器：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;podSelector&lt;/strong&gt;:  选择与 NetworkPolicy 在同一个命名空间的指定 Pod 允许为 &lt;code&gt;ingress&lt;/code&gt; 的源() 或 &lt;code&gt;egress&lt;/code&gt; 的目的
&lt;strong&gt;namespaceSelector&lt;/strong&gt;: 选择指定名称空间，其中所有的 Pod 允许为 &lt;code&gt;ingress&lt;/code&gt; 的源或 &lt;code&gt;egress&lt;/code&gt; 的目的
&lt;strong&gt;namespaceSelector&lt;/strong&gt; &lt;em&gt;加&lt;/em&gt; &lt;strong&gt;podSelector&lt;/strong&gt;: 单个 &lt;code&gt;to&lt;/code&gt;/&lt;code&gt;from&lt;/code&gt; 条目可以同时指定 &lt;code&gt;namespaceSelector&lt;/code&gt; 和 &lt;code&gt;podSelector&lt;/code&gt;
来选择指定命名空间的指定 Pod。 需要注意正确地使用 YAML 语法。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ingress&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;namespaceSelector&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;user&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;alice&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;role&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;client&lt;/span&gt;
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上面的这个策略包含一个 &lt;code&gt;from&lt;/code&gt; 元素，它允许连接的 Pod 在包含 &lt;code&gt;user=alice&lt;/code&gt; 标签的命名空间中
并且要包含 &lt;code&gt;role=client&lt;/code&gt; 标签。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ingress&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;namespaceSelector&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;user&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;alice&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;role&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;client&lt;/span&gt;
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个策略包含两上 &lt;code&gt;from&lt;/code&gt; 元素的数据， 它允许连接的 Pod 有: 同命名空间中有 &lt;code&gt;role=client&lt;/code&gt; 标签的 Pod
&lt;em&gt;或&lt;/em&gt; 任意命名空间中包含 &lt;code&gt;user=alice&lt;/code&gt; 标签的任意 Pod&lt;/p&gt;
&lt;p&gt;当搞不清楚的时候，就使用 &lt;code&gt;kubectl describe&lt;/code&gt; 来看 k8s 是怎么来解释这个策略的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ipBlock&lt;/strong&gt;: 指定 IP CIDR 范围允许为 &lt;code&gt;ingress&lt;/code&gt; 的源或 &lt;code&gt;egress&lt;/code&gt; 的目的。这个 IP 范围
应该是集群外部 IP， 因为 Pod 的 IP 是临时的并且是不可预测的。&lt;/p&gt;
&lt;p&gt;集群的 &lt;code&gt;ingress&lt;/code&gt; 和 &lt;code&gt;egress&lt;/code&gt; 经常需要重写包的源或目的 IP。 如果发生了这种情况， 它发生在
NetworkPolicy 之前还是之后不不确定的，由网络插件，云提供商，&lt;code&gt;Service&lt;/code&gt; 实现的不同而有不同的行为方式&lt;/p&gt;
&lt;p&gt;对于 &lt;code&gt;ingress&lt;/code&gt;, 就意味着在某些情况下可能可以通过实际的原始源 IP 地址来过滤包，在另一情况下,
NetworkPolicy 处理的的 &amp;ldquo;源 IP&amp;rdquo; 可能是 &lt;code&gt;LoadBalancer&lt;/code&gt; 的 IP 或 Pod 所在节点的 IP 等。&lt;/p&gt;
&lt;p&gt;对于 &lt;code&gt;egress&lt;/code&gt;， 这就意味着 Pod 连接的目标 &lt;code&gt;Service&lt;/code&gt; IP 在重写到集群外部IP 就可能受 &lt;code&gt;ipBlock&lt;/code&gt;
策略控制，也可能不受控制。&lt;/p&gt;
&lt;!--
## Default policies

By default, if no policies exist in a namespace, then all ingress and egress traffic is allowed to and from pods in that namespace. The following examples let you change the default behavior
in that namespace.
 --&gt;
&lt;h2 id=&#34;默认策略&#34;&gt;默认策略&lt;/h2&gt;
&lt;p&gt;默认情况下，如果一个命名空间中没有策略存在，那这个命名空间中的 Pod 所有的 &lt;code&gt;ingress&lt;/code&gt; 和 &lt;code&gt;egress&lt;/code&gt;
流量都是允许的。 下面的例子让用户可以修改那个命名空间中的默认行为。&lt;/p&gt;
&lt;!--
### Default deny all ingress traffic

You can create a &#34;default&#34; isolation policy for a namespace by creating a NetworkPolicy that selects all pods but does not allow any ingress traffic to those pods.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-default-deny-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-default-deny-ingress.yaml&#34; download=&#34;service/networking/network-policy-default-deny-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-default-deny-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-default-deny-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-default-deny-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-deny-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



This ensures that even pods that aren&#39;t selected by any other NetworkPolicy will still be isolated. This policy does not change the default egress isolation behavior.
 --&gt;
&lt;h3 id=&#34;默认拒绝所有-ingress-流量&#34;&gt;默认拒绝所有 &lt;code&gt;ingress&lt;/code&gt; 流量&lt;/h3&gt;
&lt;p&gt;可以能过创建一个 NetworkPolicy 为命名空间创建一个&amp;quot;默认&amp;quot;的隔离策略。这个 NetworkPolicy
会选择所有的 Pod 并且不允许任何 &lt;code&gt;ingress&lt;/code&gt; 流量到这些 Pod。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-default-deny-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-default-deny-ingress.yaml&#34; download=&#34;service/networking/network-policy-default-deny-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-default-deny-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-default-deny-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-default-deny-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-deny-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;这样可以保证即便有些 Pod 没有被其它任意 NetworkPolicy 选择还是会被隔离。
这个策略不会影响默认的 &lt;code&gt;egress&lt;/code&gt; 隔离行为。&lt;/p&gt;
&lt;!--
### Default allow all ingress traffic

If you want to allow all traffic to all pods in a namespace (even if policies are added that cause some pods to be treated as &#34;isolated&#34;), you can create a policy that explicitly allows all traffic in that namespace.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-allow-all-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-allow-all-ingress.yaml&#34; download=&#34;service/networking/network-policy-allow-all-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-allow-all-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-allow-all-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-allow-all-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;allow-all-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;ingress&lt;/span&gt;:
  - {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


 --&gt;
&lt;h3 id=&#34;默认允许所有-ingress-流量&#34;&gt;默认允许所有 &lt;code&gt;ingress&lt;/code&gt; 流量&lt;/h3&gt;
&lt;p&gt;如果想要在一个命名空间中允许到达所有 Pod 的所有流量(即便已经添加了一些策略导致有些 Pod 被认为是隔离的)
可以通过创建一个策略显式地允许这个命名空间中的所有离题。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-allow-all-ingressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-allow-all-ingress.yaml&#34; download=&#34;service/networking/network-policy-allow-all-ingress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-allow-all-ingress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-allow-all-ingressyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-allow-all-ingress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;allow-all-ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;ingress&lt;/span&gt;:
  - {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;!--
### Default deny all egress traffic

You can create a &#34;default&#34; egress isolation policy for a namespace by creating a NetworkPolicy that selects all pods but does not allow any egress traffic from those pods.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-default-deny-egressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-default-deny-egress.yaml&#34; download=&#34;service/networking/network-policy-default-deny-egress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-default-deny-egress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-default-deny-egressyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-default-deny-egress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-deny-egress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Egress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



This ensures that even pods that aren&#39;t selected by any other NetworkPolicy will not be allowed egress traffic. This policy does not
change the default ingress isolation behavior.
 --&gt;
&lt;h3 id=&#34;默认拒绝所有-egress-流量&#34;&gt;默认拒绝所有 &lt;code&gt;egress&lt;/code&gt; 流量&lt;/h3&gt;
&lt;p&gt;可以通过创建一个选择所有 Pod 并不接受任何来自这些 Pod 的 egress 流量的 NetworkPolicy 来作为
这个命名空间的默认 &lt;code&gt;egress&lt;/code&gt; 隔离策略。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-default-deny-egressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-default-deny-egress.yaml&#34; download=&#34;service/networking/network-policy-default-deny-egress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-default-deny-egress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-default-deny-egressyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-default-deny-egress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-deny-egress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Egress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;这样可以保证即便有 Pod 没有被其它任意 NetworkPolicy 选中，也不会被允许其 egress 流量。
这个策略不影响默认 &lt;code&gt;ingress&lt;/code&gt; 隔离行为。&lt;/p&gt;
&lt;!--
### Default allow all egress traffic

If you want to allow all traffic from all pods in a namespace (even if policies are added that cause some pods to be treated as &#34;isolated&#34;), you can create a policy that explicitly allows all egress traffic in that namespace.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-allow-all-egressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-allow-all-egress.yaml&#34; download=&#34;service/networking/network-policy-allow-all-egress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-allow-all-egress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-allow-all-egressyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-allow-all-egress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;allow-all-egress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;egress&lt;/span&gt;:
  - {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Egress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


 --&gt;
&lt;h3 id=&#34;默认允许所有-egress-流量&#34;&gt;默认允许所有 &lt;code&gt;egress&lt;/code&gt; 流量&lt;/h3&gt;
&lt;p&gt;如果想要允许一个命名空间中来自所有 Pod 的所有流量(即便已经添加了一些策略导致有些 Pod 被认为是隔离的)，
也可以通过创建一个策略显式地允许这个命名空间中所有的 &lt;code&gt;egress&lt;/code&gt; 流量。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-allow-all-egressyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-allow-all-egress.yaml&#34; download=&#34;service/networking/network-policy-allow-all-egress.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-allow-all-egress.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-allow-all-egressyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-allow-all-egress.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;allow-all-egress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;egress&lt;/span&gt;:
  - {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Egress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;!--
### Default deny all ingress and all egress traffic

You can create a &#34;default&#34; policy for a namespace which prevents all ingress AND egress traffic by creating the following NetworkPolicy in that namespace.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-default-deny-allyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-default-deny-all.yaml&#34; download=&#34;service/networking/network-policy-default-deny-all.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-default-deny-all.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-default-deny-allyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-default-deny-all.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-deny-all&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Egress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



This ensures that even pods that aren&#39;t selected by any other NetworkPolicy will not be allowed ingress or egress traffic.
 --&gt;
&lt;h3 id=&#34;拒绝所有-ingress-和所有-egress-流量&#34;&gt;拒绝所有 &lt;code&gt;ingress&lt;/code&gt; 和所有 &lt;code&gt;egress&lt;/code&gt; 流量&lt;/h3&gt;
&lt;p&gt;You can create a &amp;ldquo;default&amp;rdquo; policy for a namespace which prevents all ingress AND egress traffic by creating the following NetworkPolicy in that namespace.
可以在一个命名空间中创建一个如下默认的 NetworkPolicy，同时禁止所有的 &lt;code&gt;ingress&lt;/code&gt; 和 &lt;code&gt;egress&lt;/code&gt; 流量。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingnetwork-policy-default-deny-allyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/network-policy-default-deny-all.yaml&#34; download=&#34;service/networking/network-policy-default-deny-all.yaml&#34;&gt;
                    &lt;code&gt;service/networking/network-policy-default-deny-all.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingnetwork-policy-default-deny-allyaml&#39;)&#34; title=&#34;Copy service/networking/network-policy-default-deny-all.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NetworkPolicy&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-deny-all&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;podSelector&lt;/span&gt;: {}
  &lt;span style=&#34;color:#f92672&#34;&gt;policyTypes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;Egress&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;这样确保即便有 Pod 没有被其它任意 NetworkPolicy 选中，也不会被允许其 &lt;code&gt;ingress&lt;/code&gt; 或 &lt;code&gt;egress&lt;/code&gt; 流量&lt;/p&gt;
&lt;!--
## SCTP support






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;



As a beta feature, this is enabled by default. To disable SCTP at a cluster level, you (or your cluster administrator) will need to disable the `SCTPSupport` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) for the API server with `--feature-gates=SCTPSupport=false,…`.
When the feature gate is enabled, you can set the `protocol` field of a NetworkPolicy to `SCTP`.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You must be using a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni&#39; target=&#39;_blank&#39;&gt;CNI&lt;span class=&#39;tooltip-text&#39;&gt;Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.&lt;/span&gt;
&lt;/a&gt; plugin that supports SCTP protocol NetworkPolicies.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;sctp-支持&#34;&gt;SCTP 支持&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;作为一个 &lt;code&gt;beta&lt;/code&gt; 版本的特性，默认是开启的。要在集群级别禁用 SCTP， 需要在 api-server 使用
&lt;code&gt;--feature-gates=SCTPSupport=false,…&lt;/code&gt; 禁用
&lt;code&gt;SCTPSupport&lt;/code&gt; &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能特性阀&lt;/a&gt;
当这个特性被启用时，可能在 NetworkPolicy 的 &lt;code&gt;protocol&lt;/code&gt; 字段使用 &lt;code&gt;SCTP&lt;/code&gt; 协议。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 必须要使用一个支持 NetworkPolicy SCTP 协议的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni&#39; target=&#39;_blank&#39;&gt;CNI&lt;span class=&#39;tooltip-text&#39;&gt;Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.&lt;/span&gt;
&lt;/a&gt; 插件&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;!--
# What you CAN&#39;T do with network policy&#39;s (at least, not yet)

As of Kubernetes 1.20, the following functionality does not exist in the NetworkPolicy API, but you might be able to implement workarounds using Operating System components (such as SELinux, OpenVSwitch, IPTables, and so on) or Layer 7 technologies (Ingress controllers, Service Mesh implementations) or admission controllers.  In case you are new to network security in Kubernetes, its worth noting that the following User Stories cannot (yet) be implemented using the NetworkPolicy API.  Some (but not all) of these user stories are actively being discussed for future releases of the NetworkPolicy API.

- Forcing internal cluster traffic to go through a common gateway (this might be best served with a service mesh or other proxy).
- Anything TLS related (use a service mesh or ingress controller for this).
- Node specific policies (you can use CIDR notation for these, but you cannot target nodes by their Kubernetes identities specifically).
- Targeting of namespaces or services by name (you can, however, target pods or namespaces by their&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels&#39; target=&#39;_blank&#39;&gt;labels&lt;span class=&#39;tooltip-text&#39;&gt;在对象上标上对用户有意义和有关联的属性&lt;/span&gt;
&lt;/a&gt;, which is often a viable workaround).
- Creation or management of &#34;Policy requests&#34; that are fulfilled by a third party.
- Default policies which are applied to all namespaces or pods (there are some third party Kubernetes distributions and projects which can do this).
- Advanced policy querying and reachability tooling.
- The ability to target ranges of Ports in a single policy declaration.
- The ability to log network security events (for example connections that are blocked or accepted).
- The ability to explicitly deny policies (currently the model for NetworkPolicies are deny by default, with only the ability to add allow rules).
- The ability to prevent loopback or incoming host traffic (Pods cannot currently block localhost access, nor do they have the ability to block access from their resident node).
 --&gt;
&lt;h2 id=&#34;不能通过网络策略做到的情况-至少目前还不能&#34;&gt;不能通过网络策略做到的情况 (至少目前还不能)&lt;/h2&gt;
&lt;p&gt;到 k8s v1.20 为止，以下功能在 NetworkPolicy API 中是不存在的， 但可能可能通过操作系统组件
(如 SELinux， OpenVSwitch， IPTables 等) 或者通过 7 层技术(Ingress 控制器，&lt;code&gt;Service Mesh&lt;/code&gt; 实现)
或者 准入控制器(admission controller) 来曲线救国。 假如用户对 k8s 网络安全了解比较少，
需要注意下面的用户故事(User Stories)(还)不能通过使用 NetworkPolicy API 实现。
有些(但不是所有)正在被讨论加入到未来版本中的 NetworkPolicy API 中。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;强制集群内流量通过一个通用网关(这个最好使用服务网格或其它代理来提供)&lt;/li&gt;
&lt;li&gt;所有与 TLS 相关的事(使用服务网格或 Ingress 控制器来实现)&lt;/li&gt;
&lt;li&gt;节点特有的策略(可以使用 CIDR 标注，但是不能能过 k8s 标识来特指一些节点)(说的啥？)&lt;/li&gt;
&lt;li&gt;通过名称将命名空间或 Service 作为目标(通常的做法是通过 Pod 或命名空间的
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels&#39; target=&#39;_blank&#39;&gt;标签&lt;span class=&#39;tooltip-text&#39;&gt;在对象上标上对用户有意义和有关联的属性&lt;/span&gt;
&lt;/a&gt;来筛选目标)&lt;/li&gt;
&lt;li&gt;通过第三方达成对&amp;quot;Policy requests&amp;quot;的创建的或管理&lt;/li&gt;
&lt;li&gt;应用到所有命名空间或 Pod 的默认策略(有些第三方 k8s 发行版本和项目提供该功能)&lt;/li&gt;
&lt;li&gt;高级策略查询和可到达性检查工具&lt;/li&gt;
&lt;li&gt;在一个策略定义中设置端口范围的能力&lt;/li&gt;
&lt;li&gt;输出网络安全事件的能力(如，连接是被禁止或接受)&lt;/li&gt;
&lt;li&gt;声明显示拒绝的策略的能力(目前的网络策略模型默认为拒绝，只能提供添加允许规则的能力)&lt;/li&gt;
&lt;li&gt;能够阻止回环网络或进入主机流量(目前 Pod 不能禁用通过 localhost 的访问，也不能禁止所在主机的访问)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/administer-cluster/declare-network-policy/&#34;&gt;声明网络策略&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;参阅 &lt;a href=&#34;https://github.com/ahmetb/kubernetes-network-policy-recipes&#34;&gt;recipes&lt;/a&gt;
启用网络策略的常见场景&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 存储容量</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-capacity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-capacity/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jsafrane
- saad-ali
- msau42
- xing-yang
- pohly
title: Storage Capacity
content_type: concept
weight: 45
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
Storage capacity is limited and may vary depending on the node on
which a pod runs: network-attached storage might not be accessible by
all nodes, or storage is local to a node to begin with.






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [alpha]&lt;/code&gt;
&lt;/div&gt;



This page describes how Kubernetes keeps track of storage capacity and
how the scheduler uses that information to schedule Pods onto nodes
that have access to enough storage capacity for the remaining missing
volumes. Without storage capacity tracking, the scheduler may choose a
node that doesn&#39;t have enough capacity to provision a volume and
multiple scheduling retries will be needed.

Tracking storage capacity is supported for &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;Container Storage Interface&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt; (CSI) drivers and
[needs to be enabled](#enabling-storage-capacity-tracking) when installing a CSI driver.
 --&gt;
&lt;p&gt;存储容量是受限的并且还基于 Pod 运行的节点: 网络存储可以不能在所有的节点上访问，或者存储只属于
Pod 启动的节点上。&lt;/p&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;本文主要介绍 k8s 是怎么保持对存储容量的跟踪并且调度器使用这些信息来把 Pod 调度到能够访问到
剩余没使用的卷有足够存储容量。 如果没有对存储容量的跟踪， 造成调度器因调度到一个没有足够存储容量
的节点而造成多次尝试重新调度。&lt;/p&gt;
&lt;p&gt;跟踪存储容量是支持
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;容器存储接口 (CSI)&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt; (CSI) 驱动并且在安装 CSI 驱动时
&lt;a href=&#34;#enabling-storage-capacity-tracking&#34;&gt;需要开启&lt;/a&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## API

There are two API extensions for this feature:
- [CSIStorageCapacity](/docs/reference/generated/kubernetes-api/v1.19/#csistoragecapacity-v1alpha1-storage-k8s-io) objects:
  these get produced by a CSI driver in the namespace
  where the driver is installed. Each object contains capacity
  information for one storage class and defines which nodes have
  access to that storage.
- [The `CSIDriverSpec.StorageCapacity` field](/docs/reference/generated/kubernetes-api/v1.19/#csidriverspec-v1-storage-k8s-io):
  when set to `true`, the Kubernetes scheduler will consider storage
  capacity for volumes that use the CSI driver.
 --&gt;
&lt;h2 id=&#34;api&#34;&gt;API&lt;/h2&gt;
&lt;p&gt;对于这个特性有两个 API 扩展:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#csistoragecapacity-v1alpha1-storage-k8s-io&#34;&gt;CSIStorageCapacity&lt;/a&gt; 对象:
这些是当驱动被安装时由 CSI 驱动在命名空间中生成的。 每个对象包含一个 StorageClass 的容量信息
并定义哪个节点可以访问这个存储。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#csidriverspec-v1-storage-k8s-io&#34;&gt;&lt;code&gt;CSIDriverSpec.StorageCapacity&lt;/code&gt; 字段&lt;/a&gt;:
当设置为 &lt;code&gt;true&lt;/code&gt;， k8s 调度器会考量使用 CSI 驱动的卷的存储容量&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Scheduling

Storage capacity information is used by the Kubernetes scheduler if:
- the `CSIStorageCapacity` feature gate is true,
- a Pod uses a volume that has not been created yet,
- that volume uses a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes&#39; target=&#39;_blank&#39;&gt;StorageClass&lt;span class=&#39;tooltip-text&#39;&gt;StorageClass 为管理员提供了一种描述不同存储类别的方式&lt;/span&gt;
&lt;/a&gt; which references a CSI driver and
  uses `WaitForFirstConsumer` [volume binding
  mode](/docs/concepts/storage/storage-classes/#volume-binding-mode),
  and
- the `CSIDriver` object for the driver has `StorageCapacity` set to
  true.

In that case, the scheduler only considers nodes for the Pod which
have enough storage available to them. This check is very
simplistic and only compares the size of the volume against the
capacity listed in `CSIStorageCapacity` objects with a topology that
includes the node.

For volumes with `Immediate` volume binding mode, the storage driver
decides where to create the volume, independently of Pods that will
use the volume. The scheduler then schedules Pods onto nodes where the
volume is available after the volume has been created.

For [CSI ephemeral volumes](/docs/concepts/storage/volumes/#csi),
scheduling always happens without considering storage capacity. This
is based on the assumption that this volume type is only used by
special CSI drivers which are local to a node and do not need
significant resources there.
 --&gt;
&lt;h2 id=&#34;scheduling&#34;&gt;调度&lt;/h2&gt;
&lt;p&gt;k8s 调度器会使用存储容量信息如果:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CSIStorageCapacity&lt;/code&gt; 功能阀启用&lt;/li&gt;
&lt;li&gt;一个 Pod 使用了一个还没有被创建的卷&lt;/li&gt;
&lt;li&gt;这卷使用一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes&#39; target=&#39;_blank&#39;&gt;StorageClass&lt;span class=&#39;tooltip-text&#39;&gt;StorageClass 为管理员提供了一种描述不同存储类别的方式&lt;/span&gt;
&lt;/a&gt;
它使用 CSI 驱动并设置 &lt;code&gt;WaitForFirstConsumer&lt;/code&gt;
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes/#volume-binding-mode&#34;&gt;卷绑定模式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;驱动的 &lt;code&gt;CSIDriver&lt;/code&gt; 对象上的 &lt;code&gt;StorageCapacity&lt;/code&gt; 被设置为 true&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在那种情况下，调度器只考虑那些对于该 Pod 有足够存储的节点。这种检测是相当简单的也就是只对卷的大小
和 包含节点拓扑的 &lt;code&gt;CSIStorageCapacity&lt;/code&gt; 对象中列举的容量进行对比。&lt;/p&gt;
&lt;p&gt;对于那些使用 &lt;code&gt;Immediate&lt;/code&gt; 卷绑定模式的卷，由存储驱动决定在哪创建卷， Pod 会独立地使用这些卷。
然后调度器再把 Pod 调度到那些已经创建好卷的节点上。&lt;/p&gt;
&lt;p&gt;对于
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#34;&gt;CSI 临时卷&lt;/a&gt;,
在调度时始终不考虑存储容量的。 这么做是因为有这么一个假设: 这个卷类型只被一个在节点范围内，不
需要用到很多资源的特殊 CSI 驱动。&lt;/p&gt;
&lt;!--
## Rescheduling

When a node has been selected for a Pod with `WaitForFirstConsumer`
volumes, that decision is still tentative. The next step is that the
CSI storage driver gets asked to create the volume with a hint that the
volume is supposed to be available on the selected node.

Because Kubernetes might have chosen a node based on out-dated
capacity information, it is possible that the volume cannot really be
created. The node selection is then reset and the Kubernetes scheduler
tries again to find a node for the Pod.
 --&gt;
&lt;h2 id=&#34;rescheduling&#34;&gt;再调度&lt;/h2&gt;
&lt;p&gt;当一个节点被一个使用 &lt;code&gt;WaitForFirstConsumer&lt;/code&gt; 卷的 Pod 选中时，这个决策也是选择性的。 下一步
就是 CSI 存储驱动被告知选择的节点应该是有足够的空间来创建这相卷&lt;/p&gt;
&lt;p&gt;因为 k8s 可能基于过期的容量信息选择一个节点，这就可能这个卷不能被真正创建。然后这次节点选择就
被重围，k8s 调度器再次尝试为 Pod 找一个节点。&lt;/p&gt;
&lt;!--
## Limitations

Storage capacity tracking increases the chance that scheduling works
on the first try, but cannot guarantee this because the scheduler has
to decide based on potentially out-dated information. Usually, the
same retry mechanism as for scheduling without any storage capacity
information handles scheduling failures.

One situation where scheduling can fail permanently is when a Pod uses
multiple volumes: one volume might have been created already in a
topology segment which then does not have enough capacity left for
another volume. Manual intervention is necessary to recover from this,
for example by increasing capacity or deleting the volume that was
already created. [Further
work](https://github.com/kubernetes/enhancements/pull/1703) is needed
to handle this automatically.
 --&gt;
&lt;h2 id=&#34;limitations&#34;&gt;限制&lt;/h2&gt;
&lt;p&gt;存储容量的跟踪会增加调度在第一次调度时成功的机会，但不能保证，因为调度器可能是基于潜在的过期信息。
通常，这与没有容量信息调度时失败后的重试机制是一样的。&lt;/p&gt;
&lt;p&gt;有一种情况是当一个 Pod 使用两个卷时调度可能会永久失： 一个卷可能已经在拓扑信息中创建，但这里
没有足够的空间来创建另一个卷。 这时间就需要人工介入来修复这种问题， 例如增加容量或删除已经创建的卷。
&lt;a href=&#34;https://github.com/kubernetes/enhancements/pull/1703&#34;&gt;剩下的工件&lt;/a&gt; 就会自动地被处理。&lt;/p&gt;
&lt;!--
## Enabling storage capacity tracking

Storage capacity tracking is an *alpha feature* and only enabled when
the `CSIStorageCapacity` [feature
gate](/docs/reference/command-line-tools-reference/feature-gates/) and
the `storage.k8s.io/v1alpha1` &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/kubernetes-api/#api-groups&#39; target=&#39;_blank&#39;&gt;API group&lt;span class=&#39;tooltip-text&#39;&gt;A set of related paths in the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; are enabled. For details on
that, see the `--feature-gates` and `--runtime-config` [kube-apiserver
parameters](/docs/reference/command-line-tools-reference/kube-apiserver/).

A quick check
whether a Kubernetes cluster supports the feature is to list
CSIStorageCapacity objects with:
```shell
kubectl get csistoragecapacities --all-namespaces
```

If your cluster supports CSIStorageCapacity, the response is either a list of CSIStorageCapacity objects or:
```
No resources found
```

If not supported, this error is printed instead:
```
error: the server doesn&#39;t have a resource type &#34;csistoragecapacities&#34;
```

In addition to enabling the feature in the cluster, a CSI
driver also has to
support it. Please refer to the driver&#39;s documentation for
details.
 --&gt;
&lt;h2 id=&#34;enabling-storage-capacity-tracking&#34;&gt;启用容量跟踪&lt;/h2&gt;
&lt;p&gt;存储容量跟踪是一个 &lt;em&gt;alpha 特性&lt;/em&gt;， 只有在
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/kubernetes-api/#api-groups&#39; target=&#39;_blank&#39;&gt;API group&lt;span class=&#39;tooltip-text&#39;&gt;A set of related paths in the Kubernetes API.&lt;/span&gt;
&lt;/a&gt;
中的 &lt;code&gt;storage.k8s.io/v1alpha1&lt;/code&gt; 被启用，同时还启用了 &lt;code&gt;CSIStorageCapacity&lt;/code&gt;
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;
更多相关信息，见
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/&#34;&gt;kube-apiserver 参数&lt;/a&gt;
中的  &lt;code&gt;--feature-gates&lt;/code&gt; 和 &lt;code&gt;--runtime-config&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;快速检查 k8s 集群是否支持该特性的办法就是列举 &lt;code&gt;CSIStorageCapacity&lt;/code&gt; 对象:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get csistoragecapacities --all-namespaces
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果集群支持 CSIStorageCapacity， 返回内容可能是一个 CSIStorageCapacity 列表或者:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;No resources found
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果不支持则会出现以下错误:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;error: the server doesn&#39;t have a resource type &amp;quot;csistoragecapacities&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;另外，要在集群中启用该功能还需要所使用的 CSI 驱动也是支持。 具体请参阅驱动的文档。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
- For more information on the design, see the
[Storage Capacity Constraints for Pod Scheduling KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1472-storage-capacity-tracking/README.md).
- For more information on further development of this feature, see the [enhancement tracking issue #1472](https://github.com/kubernetes/enhancements/issues/1472).
- Learn about [Kubernetes Scheduler](/docs/concepts/scheduling-eviction/kube-scheduler/)
 --&gt;
&lt;ul&gt;
&lt;li&gt;更多关于设计的信息见
&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1472-storage-capacity-tracking/README.md&#34;&gt;Storage Capacity Constraints for Pod Scheduling KEP&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;更多关于该特性的开发展望见
&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1472&#34;&gt;enhancement tracking issue #1472&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;概念学习 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/kube-scheduler/&#34;&gt;k8s 调度器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 通过 HostAliases 向 Pod 的 /etc/hosts 文件中添加条目</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/</link>
      <pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- rickypai
- thockin
title: Adding entries to Pod /etc/hosts with HostAliases
content_type: concept
weight: 60
min-kubernetes-server-version: 1.7
--- --&gt;
&lt;!-- overview --&gt;
&lt;!--
Adding entries to a Pod&#39;s `/etc/hosts` file provides Pod-level override of hostname resolution when DNS and other options are not applicable. You can add these custom entries with the HostAliases field in PodSpec.

Modification not using HostAliases is not suggested because the file is managed by the kubelet and can be overwritten on during Pod creation/restart.
--&gt;
&lt;p&gt;在 DNS 和其它方式都不可用时，想要向通过向 Pod 的 &lt;code&gt;/etc/hosts&lt;/code&gt; 文件添加条目的方式来重写 Pod
级别的域名解析，可能通过 &lt;code&gt;PodSpec&lt;/code&gt; 的 &lt;code&gt;HostAliases&lt;/code&gt; 字段向该 Pod 的 &lt;code&gt;/etc/hosts&lt;/code&gt; 添加自定义条目。&lt;/p&gt;
&lt;p&gt;不建议直接修改文件而不使用 &lt;code&gt;HostAliases&lt;/code&gt;， 因为这个文件是受 kubelet 管理并且在 Pod 的重新创建
或重启(能重启？)时会重写该文件，也就是直接修改文件在重建后会丢失。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Default hosts file content

Start an Nginx Pod which is assigned a Pod IP:

```shell
kubectl run nginx --image nginx
```

```
pod/nginx created
```

Examine a Pod IP:

```shell
kubectl get pods --output=wide
```

```
NAME     READY     STATUS    RESTARTS   AGE    IP           NODE
nginx    1/1       Running   0          13s    10.200.0.4   worker0
```

The hosts file content would look like this:

```shell
kubectl exec nginx -- cat /etc/hosts
```

```
# Kubernetes-managed hosts file.
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
fe00::0	ip6-mcastprefix
fe00::1	ip6-allnodes
fe00::2	ip6-allrouters
10.200.0.4	nginx
```

By default, the `hosts` file only includes IPv4 and IPv6 boilerplates like
`localhost` and its own hostname.
 --&gt;
&lt;h2 id=&#34;hosts-默认的内容&#34;&gt;hosts 默认的内容&lt;/h2&gt;
&lt;p&gt;启动一个 Nginx 的 Pod，它会被分配一个 Pod IP:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl run nginx --image nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;pod/nginx created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;检查 Pod IP:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods --output&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;wide
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME     READY     STATUS    RESTARTS   AGE    IP           NODE
nginx    1/1       Running   0          13s    10.200.0.4   worker0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过以下命令查看 hosts 文件内容&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl exec nginx -- cat /etc/hosts
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Kubernetes-managed hosts file.
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
fe00::0	ip6-mcastprefix
fe00::1	ip6-allnodes
fe00::2	ip6-allrouters
10.200.0.4	nginx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;默认情况下， &lt;code&gt;hosts&lt;/code&gt; 文件只会包含 IPv4/IPv6 如 &lt;code&gt;localhost&lt;/code&gt; 这样的模板条目和主机名条目&lt;/p&gt;
&lt;!--
## Adding additional entries with hostAliases

In addition to the default boilerplate, you can add additional entries to the
`hosts` file.
For example: to resolve `foo.local`, `bar.local` to `127.0.0.1` and `foo.remote`,
`bar.remote` to `10.1.2.3`, you can configure HostAliases for a Pod under
`.spec.hostAliases`:



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkinghostaliases-podyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/hostaliases-pod.yaml&#34; download=&#34;service/networking/hostaliases-pod.yaml&#34;&gt;
                    &lt;code&gt;service/networking/hostaliases-pod.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkinghostaliases-podyaml&#39;)&#34; title=&#34;Copy service/networking/hostaliases-pod.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hostaliases-pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;hostAliases&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;ip&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;127.0.0.1&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostnames&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;foo.local&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bar.local&amp;#34;&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;ip&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10.1.2.3&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostnames&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;foo.remote&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bar.remote&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cat-hosts&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;cat&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/etc/hosts&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



You can start a Pod with that configuration by running:

```shell
kubectl apply -f https://k8s.io/examples/service/networking/hostaliases-pod.yaml
```

```
pod/hostaliases-pod created
```

Examine a Pod&#39;s details to see its IPv4 address and its status:

```shell
kubectl get pod --output=wide
```

```
NAME                           READY     STATUS      RESTARTS   AGE       IP              NODE
hostaliases-pod                0/1       Completed   0          6s        10.200.0.5      worker0
```

The `hosts` file content looks like this:

```shell
kubectl logs hostaliases-pod
```

```
# Kubernetes-managed hosts file.
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
fe00::0	ip6-mcastprefix
fe00::1	ip6-allnodes
fe00::2	ip6-allrouters
10.200.0.5	hostaliases-pod

# Entries added by HostAliases.
127.0.0.1	foo.local	bar.local
10.1.2.3	foo.remote	bar.remote
```

with the additional entries specified at the bottom. --&gt;
&lt;h2 id=&#34;通过-hostaliases-添加额外的条目&#34;&gt;通过 hostAliases 添加额外的条目&lt;/h2&gt;
&lt;p&gt;除了模板条目，可以手动向 &lt;code&gt;hosts&lt;/code&gt; 文件添加条目
例如下面的示例中通过 Pod 的 &lt;code&gt;.spec.hostAliases&lt;/code&gt; 字段实现配置：
将 &lt;code&gt;foo.local&lt;/code&gt;, &lt;code&gt;bar.local&lt;/code&gt; 解析到 &lt;code&gt;127.0.0.1&lt;/code&gt;，
将 &lt;code&gt;foo.remote&lt;/code&gt;, &lt;code&gt;bar.remote&lt;/code&gt; 解析到 &lt;code&gt;10.1.2.3&lt;/code&gt;&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkinghostaliases-podyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/hostaliases-pod.yaml&#34; download=&#34;service/networking/hostaliases-pod.yaml&#34;&gt;
                    &lt;code&gt;service/networking/hostaliases-pod.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkinghostaliases-podyaml&#39;)&#34; title=&#34;Copy service/networking/hostaliases-pod.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hostaliases-pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;hostAliases&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;ip&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;127.0.0.1&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostnames&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;foo.local&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bar.local&amp;#34;&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;ip&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10.1.2.3&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostnames&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;foo.remote&amp;#34;&lt;/span&gt;
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bar.remote&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cat-hosts&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;cat&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;:
    - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/etc/hosts&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;用户可以通过以下命令，使用该配置启动一个 Pod:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f https://k8s.io/examples/service/networking/hostaliases-pod.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;pod/hostaliases-pod created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过以下命令查看 Pod 的信息，查看其 IPv4 地址及其状态:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pod --output&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;wide
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME                           READY     STATUS      RESTARTS   AGE       IP              NODE
hostaliases-pod                0/1       Completed   0          6s        10.200.0.5      worker0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过以下命令查看 &lt;code&gt;hosts&lt;/code&gt; 文件的内存:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl logs hostaliases-pod
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Kubernetes-managed hosts file.
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
fe00::0	ip6-mcastprefix
fe00::1	ip6-allnodes
fe00::2	ip6-allrouters
10.200.0.5	hostaliases-pod

# Entries added by HostAliases.
127.0.0.1	foo.local	bar.local
10.1.2.3	foo.remote	bar.remote
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;额外添加的条目的文件的底部。&lt;/p&gt;
&lt;!--
## Why does the kubelet manage the hosts file? {#why-does-kubelet-manage-the-hosts-file}

The kubelet [manages](https://github.com/kubernetes/kubernetes/issues/14633) the
`hosts` file for each container of the Pod to prevent Docker from
[modifying](https://github.com/moby/moby/issues/17190) the file after the
containers have already been started.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; &lt;p&gt;Avoid making manual changes to the hosts file inside a container.&lt;/p&gt;
&lt;p&gt;If you make manual changes to the hosts file,
those changes are lost when the container exits.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;why-does-kubelet-manage-the-hosts-file&#34;&gt;为啥要 kubelet 管理 hosts 文件?&lt;/h2&gt;
&lt;p&gt;使用 kubelet &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/14633&#34;&gt;管理&lt;/a&gt; the
Pod 中每个容器的 &lt;code&gt;hosts&lt;/code&gt; 文件，是防止 Docker 在容器启动后再
&lt;a href=&#34;https://github.com/moby/moby/issues/17190&#34;&gt;修改&lt;/a&gt; 该文件。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; &lt;p&gt;避免手动修改容器中的 &lt;code&gt;hosts&lt;/code&gt; 文件&lt;/p&gt;
&lt;p&gt;如果手动修改了 &lt;code&gt;hosts&lt;/code&gt; 文件， 这些改动会在容器退出时丢失。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;


      </description>
    </item>
    
    <item>
      <title>Docs: 临时卷</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/ephemeral-volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/ephemeral-volumes/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jsafrane
- saad-ali
- msau42
- xing-yang
- pohly
title: Ephemeral Volumes
content_type: concept
weight: 50
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This document describes _ephemeral volumes_ in Kubernetes. Familiarity
with [volumes](/docs/concepts/storage/volumes/) is suggested, in
particular PersistentVolumeClaim and PersistentVolume.
 --&gt;
&lt;p&gt;本文主要介绍 k8s 中的 &lt;em&gt;临时卷(ephemeral volume)&lt;/em&gt;。 建议先熟悉
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#34;&gt;卷(Volume)&lt;/a&gt;
特别是 PersistentVolumeClaim 和 持久化卷(PV)&lt;/p&gt;
&lt;!-- body --&gt;
&lt;p&gt;Some application need additional storage but don&amp;rsquo;t care whether that
data is stored persistently across restarts. For example, caching
services are often limited by memory size and can move infrequently
used data into storage that is slower than memory with little impact
on overall performance.&lt;/p&gt;
&lt;p&gt;有些应用需要额外的存储，但不关心其中的数据在重启之后是不是还在。 例如，缓存服务经常受限于内存大小
可以将不常用的数据移到其它比内存慢的存储中，这样对全局情况影响比较小。
Other applications expect some read-only input data to be present in
files, like configuration data or secret keys.&lt;/p&gt;
&lt;p&gt;另一些应用可能需要一个存放在文件中的只读输入数据， 例如配置数据或 Secret 的键。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ephemeral volumes&lt;/em&gt; are designed for these use cases. Because volumes
follow the Pod&amp;rsquo;s lifetime and get created and deleted along with the
Pod, Pods can be stopped and restarted without being limited to where
some persistent volume is available.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;临时卷&lt;/em&gt; 就是为这些应用场景设计。因为卷跟随 Pod 的生命周期，随同 Pod 一起创建和删除， Pod 可以
在不再受限于那些只有可用卷的地方停止或重启。&lt;/p&gt;
&lt;p&gt;Ephemeral volumes are specified &lt;em&gt;inline&lt;/em&gt; in the Pod spec, which
simplifies application deployment and management.&lt;/p&gt;
&lt;p&gt;临时卷是在 Pod 定义中单行配置，这样可以简化应用的部署和管理。&lt;/p&gt;
&lt;!--
### Types of ephemeral volumes

Kubernetes supports several different kinds of ephemeral volumes for
different purposes:
- [emptyDir](/docs/concepts/storage/volumes/#emptydir): empty at Pod startup,
  with storage coming locally from the kubelet base directory (usually
  the root disk) or RAM
- [configMap](/docs/concepts/storage/volumes/#configmap),
  [downwardAPI](/docs/concepts/storage/volumes/#downwardapi),
  [secret](/docs/concepts/storage/volumes/#secret): inject different
  kinds of Kubernetes data into a Pod
- [CSI ephemeral volumes](#csi-ephemeral-volumes):
  similar to the previous volume kinds, but provided by special
  [CSI drivers](https://github.com/container-storage-interface/spec/blob/master/spec.md)
  which specifically [support this feature](https://kubernetes-csi.github.io/docs/drivers.html)
- [generic ephemeral volumes](#generic-ephemeral-volumes), which
  can be provided by all storage drivers that also support persistent volumes

`emptyDir`, `configMap`, `downwardAPI`, `secret` are provided as
[local ephemeral
storage](/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage).
They are managed by kubelet on each node.

CSI ephemeral volumes *must* be provided by third-party CSI storage
drivers.

Generic ephemeral volumes *can* be provided by third-party CSI storage
drivers, but also by any other storage driver that supports dynamic
provisioning. Some CSI drivers are written specifically for CSI
ephemeral volumes and do not support dynamic provisioning: those then
cannot be used for generic ephemeral volumes.

The advantage of using third-party drivers is that they can offer
functionality that Kubernetes itself does not support, for example
storage with different performance characteristics than the disk that
is managed by kubelet, or injecting different data.
 --&gt;
&lt;h3 id=&#34;临时卷的类型&#34;&gt;临时卷的类型&lt;/h3&gt;
&lt;p&gt;k8s 支持几种不同类型的临时卷用于不同目的:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#emptydir&#34;&gt;emptyDir&lt;/a&gt;: 在 Pod 启动时是空的，来自
kubelet 基础目录的本地存储(通常是系统盘)或内存&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#configmap&#34;&gt;configMap&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#downwardapi&#34;&gt;downwardAPI&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#secret&#34;&gt;secret&lt;/a&gt;: 注入不同类型的 k8s 数据到 Pod 中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;#csi-ephemeral-volumes&#34;&gt;CSI 临时卷&lt;/a&gt;: 与上一个类型相似， 但由特殊的
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34;&gt;CSI 驱动&lt;/a&gt;
提供，并且要
&lt;a href=&#34;https://kubernetes-csi.github.io/docs/drivers.html&#34;&gt;支持该特性&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;#generic-ephemeral-volumes&#34;&gt;通用临时卷&lt;/a&gt;， 可以由所有可以支持持久化卷(PV)的存储驱动提供&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;emptyDir&lt;/code&gt;, &lt;code&gt;configMap&lt;/code&gt;, &lt;code&gt;downwardAPI&lt;/code&gt;, &lt;code&gt;secret&lt;/code&gt;  是以
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage&#34;&gt;本地临时卷&lt;/a&gt;
提供。
它们由每个节点上的 kubelet 管理。&lt;/p&gt;
&lt;p&gt;CSI 临时卷&lt;em&gt;必须&lt;/em&gt;由第三方 CSI 存储驱动提供的。&lt;/p&gt;
&lt;p&gt;通用临时卷&lt;em&gt;可以&lt;/em&gt;由第三方 CSI 存储驱动提供的，但也可以由其它支持动态供应的存储驱动提供。
有些 CSI 驱动是 CSI 临时卷专用的，并不支持动态供应: 这些就不能用于通用临时卷。&lt;/p&gt;
&lt;p&gt;用第三方驱动的好处是它们能提供一个 k8s 本身不支持的功能， 例如除由 kubelet 管理外的磁盘外的
其它拥有不同性能特性的存储或插入不同的数据。&lt;/p&gt;
&lt;!--
### CSI ephemeral volumes






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [beta]&lt;/code&gt;
&lt;/div&gt;



This feature requires the `CSIInlineVolume` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) to be enabled. It
is enabled by default starting with Kubernetes 1.16.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; CSI ephemeral volumes are only supported by a subset of CSI drivers.
The Kubernetes CSI &lt;a href=&#34;https://kubernetes-csi.github.io/docs/drivers.html&#34;&gt;Drivers list&lt;/a&gt;
shows which drivers support ephemeral volumes.&lt;/div&gt;
&lt;/blockquote&gt;


Conceptually, CSI ephemeral volumes are similar to `configMap`,
`downwardAPI` and `secret` volume types: the storage is managed locally on each
 node and is created together with other local resources after a Pod has been
scheduled onto a node. Kubernetes has no concept of rescheduling Pods
anymore at this stage. Volume creation has to be unlikely to fail,
otherwise Pod startup gets stuck. In particular, [storage capacity
aware Pod scheduling](/docs/concepts/storage/storage-capacity/) is *not*
supported for these volumes. They are currently also not covered by
the storage resource usage limits of a Pod, because that is something
that kubelet can only enforce for storage that it manages itself.


Here&#39;s an example manifest for a Pod that uses CSI ephemeral storage:

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: my-csi-app
spec:
  containers:
    - name: my-frontend
      image: busybox
      volumeMounts:
      - mountPath: &#34;/data&#34;
        name: my-csi-inline-vol
      command: [ &#34;sleep&#34;, &#34;1000000&#34; ]
  volumes:
    - name: my-csi-inline-vol
      csi:
        driver: inline.storage.kubernetes.io
        volumeAttributes:
          foo: bar
```

The `volumeAttributes` determine what volume is prepared by the
driver. These attributes are specific to each driver and not
standardized. See the documentation of each CSI driver for further
instructions.

As a cluster administrator, you can use a [PodSecurityPolicy](/docs/concepts/policy/pod-security-policy/) to control which CSI drivers can be used in a Pod, specified with the
[`allowedCSIDrivers` field](/docs/reference/generated/kubernetes-api/v1.19/#podsecuritypolicyspec-v1beta1-policy).
 --&gt;
&lt;h3 id=&#34;csi-ephemeral-volumes&#34;&gt;CSI 临时卷&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;这个特性需要启用 &lt;code&gt;CSIInlineVolume&lt;/code&gt;
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;。
从 k8s v1.16 开始该功能默认是开启的。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; CSI 临时卷只被一部分 CSI 驱动支持。 这个 k8s CSI
&lt;a href=&#34;https://kubernetes-csi.github.io/docs/drivers.html&#34;&gt;驱动列表&lt;/a&gt;
中显示了哪些驱动支持临时卷。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;在概念上， CSI 临时卷与 &lt;code&gt;configMap&lt;/code&gt;, &lt;code&gt;downwardAPI&lt;/code&gt; 和 &lt;code&gt;secret&lt;/code&gt; 这些类型的卷相似:
存储由每个节点本地管理并且在 Pod 被调度到节点上时随同其它本地资源一起创建。在这种情况下 k8s
就没有对 Pod 重新调度的概念。卷创建看下来是不可能失败的，否则 Pod 启动就会卡住。 特别是
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-capacity/&#34;&gt;Pod 调度存储容量感知&lt;/a&gt; 对这些卷是不受支持。
这种不支持目前还包含 Pod 所使用的存储资源的使用限制，因为 kubelet 只能执行那些能自己对自己进行管理存储的管理。
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;以下这个示例配置是一个使用 CSI 临时卷的 Pod:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-csi-app&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-frontend&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/data&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-csi-inline-vol&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sleep&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1000000&amp;#34;&lt;/span&gt; ]
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-csi-inline-vol&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;csi&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;driver&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;inline.storage.kubernetes.io&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeAttributes&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;volumeAttributes&lt;/code&gt; 决定驱动需要准备什么卷。 这些属性因驱动的不同而不同，没有标准。 具体配置
请参阅对应驱动的文档&lt;/p&gt;
&lt;p&gt;作为一个集群管理员，可能使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/policy/pod-security-policy/&#34;&gt;PodSecurityPolicy&lt;/a&gt;
Pod 中可以使用哪些 CSI 驱动， 通过
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#podsecuritypolicyspec-v1beta1-policy&#34;&gt;&lt;code&gt;allowedCSIDrivers&lt;/code&gt; 字段&lt;/a&gt;
来指定&lt;/p&gt;
&lt;!--
### Generic ephemeral volumes






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [alpha]&lt;/code&gt;
&lt;/div&gt;



This feature requires the `GenericEphemeralVolume` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) to be
enabled. Because this is an alpha feature, it is disabled by default.

Generic ephemeral volumes are similar to `emptyDir` volumes, just more
flexible:
- Storage can be local or network-attached.
- Volumes can have a fixed size that Pods are not able to exceed.
- Volumes may have some initial data, depending on the driver and
  parameters.
- Typical operations on volumes are supported assuming that the driver
  supports them, including
  ([snapshotting](/docs/concepts/storage/volume-snapshots/),
  [cloning](/docs/concepts/storage/volume-pvc-datasource/),
  [resizing](/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims),
  and [storage capacity tracking](/docs/concepts/storage/storage-capacity/).

Example:

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: my-app
spec:
  containers:
    - name: my-frontend
      image: busybox
      volumeMounts:
      - mountPath: &#34;/scratch&#34;
        name: scratch-volume
      command: [ &#34;sleep&#34;, &#34;1000000&#34; ]
  volumes:
    - name: scratch-volume
      ephemeral:
        volumeClaimTemplate:
          metadata:
            labels:
              type: my-frontend-volume
          spec:
            accessModes: [ &#34;ReadWriteOnce&#34; ]
            storageClassName: &#34;scratch-storage-class&#34;
            resources:
              requests:
                storage: 1Gi
```
 --&gt;
&lt;h3 id=&#34;通用临时卷&#34;&gt;通用临时卷&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;这个特性需要启用 &lt;code&gt;GenericEphemeralVolume&lt;/code&gt;
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;
来开启。 因为这是一个 alpha 特性，默认是关闭的。&lt;/p&gt;
&lt;p&gt;通用临时卷与 &lt;code&gt;emptyDir&lt;/code&gt; 卷相似，只是更加灵活:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;存储可以在本地或网络挂载&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;卷可以有固定容量， Pod 不能使用超限&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;卷可以基于驱动和参数拥有初始数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果驱动支持它们也支持对于卷的典型操作，包含
(&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volume-snapshots/&#34;&gt;快照&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volume-pvc-datasource/&#34;&gt;克隆&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims&#34;&gt;扩容&lt;/a&gt;,
和 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-capacity/&#34;&gt;存储容量跟踪&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-app&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-frontend&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/scratch&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;scratch-volume&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sleep&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1000000&amp;#34;&lt;/span&gt; ]
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;scratch-volume&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;ephemeral&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeClaimTemplate&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-frontend-volume&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ReadWriteOnce&amp;#34;&lt;/span&gt; ]
            &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;scratch-storage-class&amp;#34;&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1Gi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Lifecycle and PersistentVolumeClaim

The key design idea is that the
[parameters for a volume claim](/docs/reference/generated/kubernetes-api/v1.19/#ephemeralvolumesource-v1alpha1-core)
are allowed inside a volume source of the Pod. Labels, annotations and
the whole set of fields for a PersistentVolumeClaim are supported. When such a Pod gets
created, the ephemeral volume controller then creates an actual PersistentVolumeClaim
object in the same namespace as the Pod and ensures that the PersistentVolumeClaim
gets deleted when the Pod gets deleted.

That triggers volume binding and/or provisioning, either immediately if
the &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes&#39; target=&#39;_blank&#39;&gt;StorageClass&lt;span class=&#39;tooltip-text&#39;&gt;StorageClass 为管理员提供了一种描述不同存储类别的方式&lt;/span&gt;
&lt;/a&gt; uses immediate volume binding or when the Pod is
tentatively scheduled onto a node (`WaitForFirstConsumer` volume
binding mode). The latter is recommended for generic ephemeral volumes
because then the scheduler is free to choose a suitable node for
the Pod. With immediate binding, the scheduler is forced to select a node that has
access to the volume once it is available.

In terms of [resource ownership](/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents),
a Pod that has generic ephemeral storage is the owner of the PersistentVolumeClaim(s)
that provide that ephemeral storage. When the Pod is deleted,
the Kubernetes garbage collector deletes the PVC, which then usually
triggers deletion of the volume because the default reclaim policy of
storage classes is to delete volumes. You can create quasi-ephemeral local storage
using a StorageClass with a reclaim policy of `retain`: the storage outlives the Pod,
and in this case you need to ensure that volume clean up happens separately.

While these PVCs exist, they can be used like any other PVC. In
particular, they can be referenced as data source in volume cloning or
snapshotting. The PVC object also holds the current status of the
volume.
 --&gt;
&lt;h3 id=&#34;生命周期-和-persistentvolumeclaim&#34;&gt;生命周期 和 PersistentVolumeClaim&lt;/h3&gt;
&lt;p&gt;该设计主旨就是
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#ephemeralvolumesource-v1alpha1-core&#34;&gt;卷申领的参数&lt;/a&gt;
可以在 Pod 的卷源中。 标签，注解，PersistentVolumeClaim 的一堆字段都是支持的。 当这样一个
Pod 被创建时， 临时卷控制器实际是创建一个与 Pod 在同一个命名空间的 PersistentVolumeClaim，
并且确保这个 PersistentVolumeClaim 在 Pod 删除时也会被删除。&lt;/p&gt;
&lt;p&gt;在触发卷绑定和/或供应时，可能是即时的，如果
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes&#39; target=&#39;_blank&#39;&gt;StorageClass&lt;span class=&#39;tooltip-text&#39;&gt;StorageClass 为管理员提供了一种描述不同存储类别的方式&lt;/span&gt;
&lt;/a&gt;
使用的绑定模式是即时绑定， 也可能是在 Pod 调度到节点时(&lt;code&gt;WaitForFirstConsumer&lt;/code&gt; 卷绑定模式)。
对于通用临时卷推荐使用后者，因为这样调度器可以自由的选择合适的节点。 如果使用即时绑定，调度器就会
限制在卷变为可用时，可以访问到卷的那些节点中选择一个。&lt;/p&gt;
&lt;p&gt;在术语
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents&#34;&gt;资源所有权&lt;/a&gt;,
中， 拥有这个通用临时存储的 Pod 是这个(些)提供这些临时存储的 PersistentVolumeClaim 的拥有者。
当 Pod 被删除后， k8s 垃圾回收器就会删除 PVC，通常这就会触发卷的删除，因为 StorageClass
默认的回收策略就是删除卷。 也可以使用回收策略为保留(&lt;code&gt;retain&lt;/code&gt;)的 StorageClass 创建一个准临时本地存储：
存储会比使用它的 Pod 存在时间更久， 在这种情况下需要保证独立执行卷清理工作。&lt;/p&gt;
&lt;p&gt;当这种 PVC 存在时，它们可以当做任意其它的 PVC 来用。 特别是它们可以被用作克隆或快照所引用的数据
源。 这些 PVC 对象也包含了卷的当前状态。&lt;/p&gt;
&lt;!--
### PersistentVolumeClaim naming

Naming of the automatically created PVCs is deterministic: the name is
a combination of Pod name and volume name, with a hyphen (`-`) in the
middle. In the example above, the PVC name will be
`my-app-scratch-volume`.  This deterministic naming makes it easier to
interact with the PVC because one does not have to search for it once
the Pod name and volume name are known.

The deterministic naming also introduces a potential conflict between different
Pods (a Pod &#34;pod-a&#34; with volume &#34;scratch&#34; and another Pod with name
&#34;pod&#34; and volume &#34;a-scratch&#34; both end up with the same PVC name
&#34;pod-a-scratch&#34;) and between Pods and manually created PVCs.

Such conflicts are detected: a PVC is only used for an ephemeral
volume if it was created for the Pod. This check is based on the
ownership relationship. An existing PVC is not overwritten or
modified. But this does not resolve the conflict because without the
right PVC, the Pod cannot start.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; Take care when naming Pods and volumes inside the
same namespace, so that these conflicts can&amp;rsquo;t occur.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;persistentvolumeclaim-命名&#34;&gt;PersistentVolumeClaim 命名&lt;/h3&gt;
&lt;p&gt;自动创建的 PVC 的名称的组成规则为: Pod 名称和 卷名称的组合，中间通过连字符(&lt;code&gt;-&lt;/code&gt;)连接。
在上面的示例中， PVC 的名称就是 &lt;code&gt;my-app-scratch-volume&lt;/code&gt;. 这种决定名称的方式可以简单与
PVC 的交互， 因为当知道 Pod 名称和卷名称时就相当于直接知道了 PVC 的名称，没有必要再查找。&lt;/p&gt;
&lt;p&gt;这种命名方式也会在不同 Pod 间引入潜在的命名冲突(一个名称为 &amp;ldquo;pod-a&amp;rdquo; 的 Pod 有一个名称为 &amp;ldquo;scratch&amp;rdquo;
与一个名称为 &amp;ldquo;pod&amp;rdquo; 的Pod 有一个名称为 &amp;ldquo;a-scratch&amp;rdquo; 卷，最终都会使用同一个 PVC 名称， 就是
&amp;ldquo;pod-a-scratch&amp;rdquo;) 并且与手动创建 PVC 的 Pod 也有类似风险。&lt;/p&gt;
&lt;p&gt;这类冲突的发现机制: 如果一个 PVC 是为一个 Pod 创建的它就只能用作一个临时卷。 这种检查是基于
所有权关系的。 一个存储的 PVC 是不能被覆盖或修改的。 但是这样并不能解决冲突，因为没正确的 PVC，
Pod 是不能启动的。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在同一个命名空间计划好 Pod 和 卷的命名就能避免这种冲突的发生。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Security

Enabling the GenericEphemeralVolume feature allows users to create
PVCs indirectly if they can create Pods, even if they do not have
permission to create PVCs directly. Cluster administrators must be
aware of this. If this does not fit their security model, they have
two choices:
- Explicitly disable the feature through the feature gate, to avoid
  being surprised when some future Kubernetes version enables it
  by default.
- Use a [Pod Security
  Policy](/docs/concepts/policy/pod-security-policy/) where the
  `volumes` list does not contain the `ephemeral` volume type.

The normal namespace quota for PVCs in a namespace still applies, so
even if users are allowed to use this new mechanism, they cannot use
it to circumvent other policies.
 --&gt;
&lt;h3 id=&#34;安全&#34;&gt;安全&lt;/h3&gt;
&lt;p&gt;启用 GenericEphemeralVolume 特性允许用户在创建 Pod 时间接地创建 PVC， 即便这些用户有直接
创建 PVC 的权限. 集群管理员一定要知道这点。 如果这不符合需要的安全模型，有两种选择:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过功能阀显示地禁用这个特性，防止如果在将来的 k8s 版本中默认开启了该特性而被吓到。&lt;/li&gt;
&lt;li&gt;使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/policy/pod-security-policy/&#34;&gt;Pod 安全策略&lt;/a&gt; 保证
&lt;code&gt;volumes&lt;/code&gt; 列表中没有 &lt;code&gt;ephemeral&lt;/code&gt; 这个卷类型。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常规的命名空间的限额对于这个命名空间中的 PVC 依然是起作用的，所以即便允许用户使用这个新的机制，
也不可能使用这个特性规避其它的策略。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
### Ephemeral volumes managed by kubelet

See [local ephemeral storage](/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage).
 --&gt;
&lt;h3 id=&#34;由-kubelet-管理的临时卷&#34;&gt;由 kubelet 管理的临时卷&lt;/h3&gt;
&lt;p&gt;见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage&#34;&gt;本地临时存储&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### CSI ephemeral volumes

- For more information on the design, see the [Ephemeral Inline CSI
  volumes KEP](https://github.com/kubernetes/enhancements/blob/ad6021b3d61a49040a3f835e12c8bb5424db2bbb/keps/sig-storage/20190122-csi-inline-volumes.md).
- For more information on further development of this feature, see the [enhancement tracking issue #596](https://github.com/kubernetes/enhancements/issues/596).
 --&gt;
&lt;h3 id=&#34;csi-临时卷&#34;&gt;CSI 临时卷&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;要了解更多设计上的信息，见
&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/ad6021b3d61a49040a3f835e12c8bb5424db2bbb/keps/sig-storage/20190122-csi-inline-volumes.md&#34;&gt;Ephemeral Inline CSI volumes KEP&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;要了解该功能的未来开发情况见
&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/596&#34;&gt;enhancement tracking issue #596&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Generic ephemeral volumes

- For more information on the design, see the
[Generic ephemeral inline volumes KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1698-generic-ephemeral-volumes/README.md).
- For more information on further development of this feature, see the [enhancement tracking issue #1698](https://github.com/kubernetes/enhancements/issues/1698).
 --&gt;
&lt;h3 id=&#34;通用临时卷-1&#34;&gt;通用临时卷&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;要了解更多设计上的信息，见
&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1698-generic-ephemeral-volumes/README.md&#34;&gt;Generic ephemeral inline volumes KEP&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;要了解该功能的未来开发情况见
&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1698&#34;&gt;enhancement tracking issue #1698&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: IPv4/IPv6 双栈</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/dual-stack/</link>
      <pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/dual-stack/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- lachie83
- khenidak
- aramase
title: IPv4/IPv6 dual-stack
feature:
  title: IPv4/IPv6 dual-stack
  description: &gt;
    Allocation of IPv4 and IPv6 addresses to Pods and Services

content_type: concept
weight: 70
---
--&gt;
&lt;!-- overview --&gt;
&lt;!--





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [alpha]&lt;/code&gt;
&lt;/div&gt;



 IPv4/IPv6 dual-stack enables the allocation of both IPv4 and IPv6 addresses to &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; and &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Services&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;.

If you enable IPv4/IPv6 dual-stack networking for your Kubernetes cluster, the cluster will support the simultaneous assignment of both IPv4 and IPv6 addresses.
 --&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;IPv4/IPv6 双栈让 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 和
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 能够同时分配到 IPv4 和 IPv6 地址。&lt;/p&gt;
&lt;p&gt;如果在 k8s 集群中启用了 IPv4/IPv6 双栈网络，则集群支持同时分配 IPv4 和 IPv6 地址。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Supported Features

Enabling IPv4/IPv6 dual-stack on your Kubernetes cluster provides the following features:

   * Dual-stack Pod networking (a single IPv4 and IPv6 address assignment per Pod)
   * IPv4 and IPv6 enabled Services (each Service must be for a single address family)
   * Pod off-cluster egress routing (eg. the Internet) via both IPv4 and IPv6 interfaces
 --&gt;
&lt;h2 id=&#34;支持的特性&#34;&gt;支持的特性&lt;/h2&gt;
&lt;p&gt;在集群中开启 IPv4/IPv6 双栈可以提供以下特性:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod 双栈网络(每个 Pod 分配一个 IPv4 和 IPv6 地址)&lt;/li&gt;
&lt;li&gt;Service 启用 IPv4 和 IPv6 (每个 Service 只能是 IPv4 或 IPv6)&lt;/li&gt;
&lt;li&gt;Pod 的出站路由(如，到互联网)会同时通过 IPv4 和 IPv6 接口&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Prerequisites

The following prerequisites are needed in order to utilize IPv4/IPv6 dual-stack Kubernetes clusters:

   * Kubernetes 1.16 or later
   * Provider support for dual-stack networking (Cloud provider or otherwise must be able to provide Kubernetes nodes with routable IPv4/IPv6 network interfaces)
   * A network plugin that supports dual-stack (such as Kubenet or Calico)
 --&gt;
&lt;h2 id=&#34;前置条件&#34;&gt;前置条件&lt;/h2&gt;
&lt;p&gt;为集群启用 IPv4/IPv6 双栈需要做以下准备:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;k8s &lt;code&gt;v1.16+&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;提供商支持双栈网络(云提供商或其它的提供者必须要为 k8s 节点提供IPv4/IPv6网络接口)&lt;/li&gt;
&lt;li&gt;一个支持双栈的网络插件(如 Kubenet 或 Calico)&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Enable IPv4/IPv6 dual-stack

To enable IPv4/IPv6 dual-stack, enable the `IPv6DualStack` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) for the relevant components of your cluster, and set dual-stack cluster network assignments:

   * kube-apiserver:
      * `--feature-gates=&#34;IPv6DualStack=true&#34;`
      * `--service-cluster-ip-range=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;`
   * kube-controller-manager:
      * `--feature-gates=&#34;IPv6DualStack=true&#34;`
      * `--cluster-cidr=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;`
      * `--service-cluster-ip-range=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;`
      * `--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6` defaults to /24 for IPv4 and /64 for IPv6
   * kubelet:
      * `--feature-gates=&#34;IPv6DualStack=true&#34;`
   * kube-proxy:
      * `--cluster-cidr=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt;`
      * `--feature-gates=&#34;IPv6DualStack=true&#34;`

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;An example of an IPv4 CIDR: &lt;code&gt;10.244.0.0/16&lt;/code&gt; (though you would supply your own address range)&lt;/p&gt;
&lt;p&gt;An example of an IPv6 CIDR: &lt;code&gt;fdXY:IJKL:MNOP:15::/64&lt;/code&gt; (this shows the format but is not a valid address - see &lt;a href=&#34;https://tools.ietf.org/html/rfc4193&#34;&gt;RFC 4193&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;启用-ipv4ipv6-双栈&#34;&gt;启用 IPv4/IPv6 双栈&lt;/h2&gt;
&lt;p&gt;要启用 IPv4/IPv6 双栈, 需要打开集群中的对应组件 &lt;code&gt;IPv6DualStack&lt;/code&gt; &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;
并设置集群双栈网络分配:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kube-apiserver:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--feature-gates=&amp;quot;IPv6DualStack=true&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--service-cluster-ip-range=&amp;lt;IPv4 CIDR&amp;gt;,&amp;lt;IPv6 CIDR&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;kube-controller-manager:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--feature-gates=&amp;quot;IPv6DualStack=true&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--cluster-cidr=&amp;lt;IPv4 CIDR&amp;gt;,&amp;lt;IPv6 CIDR&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--service-cluster-ip-range=&amp;lt;IPv4 CIDR&amp;gt;,&amp;lt;IPv6 CIDR&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6&lt;/code&gt; IPv4 默认为 /24；IPv6 默认为 /64&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;kubelet:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--feature-gates=&amp;quot;IPv6DualStack=true&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;kube-proxy:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--cluster-cidr=&amp;lt;IPv4 CIDR&amp;gt;,&amp;lt;IPv6 CIDR&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--feature-gates=&amp;quot;IPv6DualStack=true&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;一个 IPv4 CIDR 示例: &lt;code&gt;10.244.0.0/16&lt;/code&gt; (应该根据需要设置IP范围)&lt;/p&gt;
&lt;p&gt;一个 IPv6 CIDR 示例: &lt;code&gt;fdXY:IJKL:MNOP:15::/64&lt;/code&gt; (这里只显示格式，但不是一个有效的值 - 具体见 &lt;a href=&#34;https://tools.ietf.org/html/rfc4193&#34;&gt;RFC 4193&lt;/a&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Services

If your cluster has IPv4/IPv6 dual-stack networking enabled, you can create &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Services&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; with either an IPv4 or an IPv6 address. You can choose the address family for the Service&#39;s cluster IP by setting a field, `.spec.ipFamily`, on that Service.
You can only set this field when creating a new Service. Setting the `.spec.ipFamily` field is optional and should only be used if you plan to enable IPv4 and IPv6 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Services&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; and &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/ingress/&#39; target=&#39;_blank&#39;&gt;Ingresses&lt;span class=&#39;tooltip-text&#39;&gt;一个用于管理外部访问集群内 Service 的 API 对象，通常是 HTTP。&lt;/span&gt;
&lt;/a&gt; on your cluster. The configuration of this field not a requirement for [egress](#egress-traffic) traffic.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The default address family for your cluster is the address family of the first service cluster IP range configured via the &lt;code&gt;--service-cluster-ip-range&lt;/code&gt; flag to the kube-controller-manager.&lt;/div&gt;
&lt;/blockquote&gt;


You can set `.spec.ipFamily` to either:

   * `IPv4`: The API server will assign an IP from a `service-cluster-ip-range` that is `ipv4`
   * `IPv6`: The API server will assign an IP from a `service-cluster-ip-range` that is `ipv6`

The following Service specification does not include the `ipFamily` field. Kubernetes will assign an IP address (also known as a &#34;cluster IP&#34;) from the first configured `service-cluster-ip-range` to this Service.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingdual-stack-default-svcyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/dual-stack-default-svc.yaml&#34; download=&#34;service/networking/dual-stack-default-svc.yaml&#34;&gt;
                    &lt;code&gt;service/networking/dual-stack-default-svc.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingdual-stack-default-svcyaml&#39;)&#34; title=&#34;Copy service/networking/dual-stack-default-svc.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



The following Service specification includes the `ipFamily` field. Kubernetes will assign an IPv6 address (also known as a &#34;cluster IP&#34;) from the configured `service-cluster-ip-range` to this Service.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingdual-stack-ipv6-svcyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/dual-stack-ipv6-svc.yaml&#34; download=&#34;service/networking/dual-stack-ipv6-svc.yaml&#34;&gt;
                    &lt;code&gt;service/networking/dual-stack-ipv6-svc.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingdual-stack-ipv6-svcyaml&#39;)&#34; title=&#34;Copy service/networking/dual-stack-ipv6-svc.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ipFamily&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IPv6&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



For comparison, the following Service specification will be assigned an IPv4 address (also known as a &#34;cluster IP&#34;) from the configured `service-cluster-ip-range` to this Service.



 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingdual-stack-ipv4-svcyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/dual-stack-ipv4-svc.yaml&#34; download=&#34;service/networking/dual-stack-ipv4-svc.yaml&#34;&gt;
                    &lt;code&gt;service/networking/dual-stack-ipv4-svc.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingdual-stack-ipv4-svcyaml&#39;)&#34; title=&#34;Copy service/networking/dual-stack-ipv4-svc.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ipFamily&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IPv4&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


 --&gt;
&lt;h2 id=&#34;service&#34;&gt;Service&lt;/h2&gt;
&lt;p&gt;如果集群启用的了 IPv4/IPv6 双栈网络，就可以创建一个带 IPv4 或 IPv6 地址的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;。
可以通过 Service 的 &lt;code&gt;.spec.ipFamily&lt;/code&gt; 来设置该 Service 是使用 IPv4 还是 IPv6 地址。
只有在创建一个新的 Service 才可以设置该字段。 &lt;code&gt;.spec.ipFamily&lt;/code&gt; 是一个可选字段，只有在需要
在 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 和 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/ingress/&#39; target=&#39;_blank&#39;&gt;Ingress&lt;span class=&#39;tooltip-text&#39;&gt;一个用于管理外部访问集群内 Service 的 API 对象，通常是 HTTP。&lt;/span&gt;
&lt;/a&gt;
上启用 IPv4 和 IPv6 时才需要配置该字段。 (这里还有一句不太明白在说啥)&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 集群默认使用的 IP 族是 kube-controller-manager 上设置 &lt;code&gt;--service-cluster-ip-range&lt;/code&gt;的
值中，前面那一个 CIDR 对应的 IP 族。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;.spec.ipFamily&lt;/code&gt; 字段的值可以是:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;IPv4&lt;/code&gt;: API server 会从 &lt;code&gt;service-cluster-ip-range&lt;/code&gt; 中分配一个 &lt;code&gt;ipv4&lt;/code&gt; 地址&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IPv6&lt;/code&gt;: API server 会从 &lt;code&gt;service-cluster-ip-range&lt;/code&gt; 中分配一个 &lt;code&gt;ipv6&lt;/code&gt; 地址&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面的这个 Service 配置文件中没有包含 &lt;code&gt;ipFamily&lt;/code&gt; 字段。 k8s 会为它分配一个 IP 地址
(也就是集群IP)，这个地址是 &lt;code&gt;service-cluster-ip-range&lt;/code&gt; 配置值中第一个个值所定义的 IP 范围内的一个 IP 地址&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingdual-stack-default-svcyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/dual-stack-default-svc.yaml&#34; download=&#34;service/networking/dual-stack-default-svc.yaml&#34;&gt;
                    &lt;code&gt;service/networking/dual-stack-default-svc.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingdual-stack-default-svcyaml&#39;)&#34; title=&#34;Copy service/networking/dual-stack-default-svc.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;下面的这个 Service 配置文件中包含 &lt;code&gt;ipFamily&lt;/code&gt; 字段。 k8s 会为它分配一个 IPv6 地址(也是集群IP)
这个地址在 &lt;code&gt;service-cluster-ip-range&lt;/code&gt; 配置的范围内&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingdual-stack-ipv6-svcyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/dual-stack-ipv6-svc.yaml&#34; download=&#34;service/networking/dual-stack-ipv6-svc.yaml&#34;&gt;
                    &lt;code&gt;service/networking/dual-stack-ipv6-svc.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingdual-stack-ipv6-svcyaml&#39;)&#34; title=&#34;Copy service/networking/dual-stack-ipv6-svc.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ipFamily&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IPv6&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;为了对应，下面的这个 Service 的配置会被分配一个 IPv4 地址(作为集群IP)，这个地址在 &lt;code&gt;service-cluster-ip-range&lt;/code&gt; 配置的范围内&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;servicenetworkingdual-stack-ipv4-svcyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/service/networking/dual-stack-ipv4-svc.yaml&#34; download=&#34;service/networking/dual-stack-ipv4-svc.yaml&#34;&gt;
                    &lt;code&gt;service/networking/dual-stack-ipv4-svc.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;servicenetworkingdual-stack-ipv4-svcyaml&#39;)&#34; title=&#34;Copy service/networking/dual-stack-ipv4-svc.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ipFamily&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IPv4&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MyApp&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;!--
### Type LoadBalancer

On cloud providers which support IPv6 enabled external load balancers, setting the `type` field to `LoadBalancer` in additional to setting `ipFamily` field to `IPv6` provisions a cloud load balancer for your Service.
 --&gt;
&lt;h3 id=&#34;type-loadbalancer&#34;&gt;Type LoadBalancer&lt;/h3&gt;
&lt;p&gt;在支持 IPv6 的云提供商上启用外部负载均衡器时，在 Service 上设置 &lt;code&gt;type&lt;/code&gt; 字段值为 &lt;code&gt;LoadBalancer&lt;/code&gt; 时，同
时还需要设置 &lt;code&gt;ipFamily&lt;/code&gt; 字段的值为 &lt;code&gt;IPv6&lt;/code&gt;。&lt;/p&gt;
&lt;!--
## Egress Traffic

The use of publicly routable and non-publicly routable IPv6 address blocks is acceptable provided the underlying &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni&#39; target=&#39;_blank&#39;&gt;CNI&lt;span class=&#39;tooltip-text&#39;&gt;Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.&lt;/span&gt;
&lt;/a&gt; provider is able to implement the transport. If you have a Pod that uses non-publicly routable IPv6 and want that Pod to reach off-cluster destinations (eg. the public Internet), you must set up IP masquerading for the egress traffic and any replies. The [ip-masq-agent](https://github.com/kubernetes-incubator/ip-masq-agent) is dual-stack aware, so you can use ip-masq-agent for IP masquerading on dual-stack clusters.
 --&gt;
&lt;h2 id=&#34;egress-traffic&#34;&gt;Egress Traffic&lt;/h2&gt;
&lt;p&gt;底层实现的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni&#39; target=&#39;_blank&#39;&gt;CNI&lt;span class=&#39;tooltip-text&#39;&gt;Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.&lt;/span&gt;
&lt;/a&gt; 提供者可以提供对
公网可路由和公网不可路由的 IPv6 地址段。如果 Pod 使用的是公网不可路由的 IPv6 地址，但要求
这个 Pod 可以访问集群外的目标(如，互联网)， 需要为外出流量及其应答做 IP 地址转换。
&lt;a href=&#34;https://github.com/kubernetes-incubator/ip-masq-agent&#34;&gt;ip-masq-agent&lt;/a&gt; 支持双栈，
所以可以使用它在双栈集群中做地址转换。&lt;/p&gt;
&lt;!--
## Known Issues

   * Kubenet forces IPv4,IPv6 positional reporting of IPs (--cluster-cidr)
 --&gt;
&lt;h2 id=&#34;已知问题&#34;&gt;已知问题&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Kubenet forces IPv4,IPv6 positional reporting of IPs (&amp;ndash;cluster-cidr)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;kubenet 强制 IPv4,IPv6 位置汇报  (&amp;ndash;cluster-cidr)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/network/validate-dual-stack&#34;&gt;验证 IPv4/IPv6 双栈网络&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 节点</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/architecture/nodes/</link>
      <pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/architecture/nodes/</guid>
      <description>
        
        
        &lt;p&gt;k8s 将用户的工作负载窗口塞进 Pod 里然后运行在节点上，根据集群节点可以是虚拟机或物理机，每个节点必须要有运行 Pod 所需要的服务，并由 k8s 控制中心管理。
一般情况下一个集群会有多个节点; 在资源受限或学习的环境，可能只有一个节点。
每个节点包含的组件有 kubelet, 容器运行环境, kube-proxy&lt;/p&gt;
&lt;h2 id=&#34;节点管理&#34;&gt;节点管理&lt;/h2&gt;
&lt;p&gt;向 api-server 添加节点的方式有以下两种:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;节点上的 kubelet 服务自动注册到控制中心&lt;/li&gt;
&lt;li&gt;管理员用户手动添加节点对象&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;当管理员用户创建节点对象或 kubelet 将节点自动注册，控制中心会检查新创建的新节点是否有效。例如，可以通过以下 JSON 创建一个新节点&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;{
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;kind&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Node&amp;#34;&lt;/span&gt;,
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;apiVersion&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;v1&amp;#34;&lt;/span&gt;,
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;metadata&amp;#34;&lt;/span&gt;: {
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10.240.79.157&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;labels&amp;#34;&lt;/span&gt;: {
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;my-first-k8s-node&amp;#34;&lt;/span&gt;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;k8s 节点是内部创建的. k8s 检查通过 kubelet 注册到 api-server 的节点. 如果节点是健康的 (当所有必要的服务都正常运行), 这个节点就可以用来运行 Pod, 否则 这个节点状态在变为健康之前，这个节点会被所有集群活动忽略.&lt;/p&gt;
&lt;p&gt;注意: k8s 会一直保留无效的节点并持续检查这个节点是否变更为健康. 用户或控制器必须要显示的删除这个节点对象，这种检测才会停止。&lt;/p&gt;
&lt;p&gt;节点的名称必须要是一个有效的&lt;a href=&#34;../../00-overview/03-working-with-objects/01-names/#DNS%20%E5%AD%90%E5%9F%9F%E5%90%8D&#34;&gt;DNS 子域名&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;节点的自注册&#34;&gt;节点的自注册&lt;/h3&gt;
&lt;p&gt;当 kubelet 参数 &lt;code&gt;--register-node&lt;/code&gt; 设置为 &lt;code&gt;true&lt;/code&gt; 时(默认值)， kubelet 会自动把所在节点注册到 api-server. 这是首选的配置方式，被多数集群搭建工具使用。&lt;/p&gt;
&lt;p&gt;对于自注册的节点， kubelet 需要以下配置参数:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;--kubeconfig&lt;/code&gt;  节点在 api-server 认证凭据所在目录&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--cloud-provider&lt;/code&gt; 怎么从云提供商获取节点元数据&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--register-node&lt;/code&gt; 自动注册到 api-server&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--register-with-taints&lt;/code&gt; 为注册节点添加 taints (格式为 &lt;key&gt;=&lt;value&gt;:&lt;effect&gt;，多个由逗号分隔)，如果 register-node 值设置为 false 则，该配置无操作&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--node-ip&lt;/code&gt; 节点的 IP 地址&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-node-labels&lt;/code&gt; 当节点注册是，添加到节点上的标签&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--node-status-update-frequency&lt;/code&gt; kubectl 向控制中心报告状态的频率&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当 &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/node/&#34;&gt;Node authorization mode&lt;/a&gt; 和 &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#noderestriction&#34;&gt;NodeRestriction admission&lt;/a&gt; 插件打开进, kubelet 只被授权创建/修改自己的节点资源&lt;/p&gt;
&lt;h3 id=&#34;节点手动管理&#34;&gt;节点手动管理&lt;/h3&gt;
&lt;p&gt;用户可以通过 kubectl 创建和修改节点对象
当用户需要手动创建一个节点对象时，需要 kubelet 设置参数  &lt;code&gt;--register-node=false&lt;/code&gt;
也可修改节点配置忽略 &lt;code&gt;--register-node&lt;/code&gt; 配置。 例如可以在存在的节点上设置标签或将节点标记为不可调度
可以通过节点的标签和Pod的节点标签选择器来控制调度。 例如， 限制某个 Pod 只能在某些节点上运行
标记一个节点为不可调度后，就会阻止调度器再向这个节点调度新的 Pod ， 但不会影响到节点上已经在运行的 Pod。 以下命令将标记指定节点为不可调度:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl cordon $NODENAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意: 属于 DaemonSet 的 Pod 会运行在不可调度的节点上， 因为 &lt;code&gt;DaemonSets&lt;/code&gt; 通常提供节点本地服务，所以即使节点被清空应用工作负载也应该运行在节点&lt;/p&gt;
&lt;h2 id=&#34;节点状态&#34;&gt;节点状态&lt;/h2&gt;
&lt;p&gt;节点状态包含以下信息:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;地址&lt;/li&gt;
&lt;li&gt;条件&lt;/li&gt;
&lt;li&gt;容量和可分配状态&lt;/li&gt;
&lt;li&gt;其它信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以通过以命令查看节点状态与其它更多信息:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl describe node &amp;lt;insert-node-name-here&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;每个部分详细信息&lt;/p&gt;
&lt;h3 id=&#34;地址&#34;&gt;地址&lt;/h3&gt;
&lt;p&gt;这些字段会根据节点是云提供或裸金属和等的不同而不同&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主机名: 由节点的内核提供，可以通过 kubelet 的 &amp;ndash;hostname-override 覆盖&lt;/li&gt;
&lt;li&gt;外部IP: 在外部网络(集群之外)路由可达的IP地址&lt;/li&gt;
&lt;li&gt;内部IP: 只在集群内部路由可达的IP地址&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;条件&#34;&gt;条件&lt;/h3&gt;
&lt;p&gt;conditions 字段描述的是 所有状态为 Running 的节点， 示例如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ready&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;True 则表示节点健康，可以接收调度 Pod&lt;/li&gt;
&lt;li&gt;False 则表示节点状不健康，不可接收 Pod&lt;/li&gt;
&lt;li&gt;Unknown node 控制器在最近一个 &lt;code&gt;node-monitor-grace-period&lt;/code&gt; 内(默认40s)没有收到节点的心跳&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DiskPressure&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;True 磁盘存储剩余空间紧张&lt;/li&gt;
&lt;li&gt;False 磁盘存储剩余空间充足&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MemoryPressure&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;True 节点内存紧张&lt;/li&gt;
&lt;li&gt;False 节点内存充足&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PIDPressure&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;True 节点上的进程太多了&lt;/li&gt;
&lt;li&gt;False 节点上进程数量适度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;NetworkUnavailable&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;True 节点网络配置错误&lt;/li&gt;
&lt;li&gt;False 节点网络配置正常&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注意: 通过命令行工具打印清楚节点的详细信息中， 条件信息中包含 &lt;code&gt;SchedulingDisabled&lt;/code&gt;， 但 SchedulingDisabled 不属于 k8s API, 而是节点被标记为不可调度&lt;/p&gt;
&lt;p&gt;节点的条件可以表现为JSON对象，如下是一个健康的节点的示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;conditions&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;:&lt;/span&gt; [
  {
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Ready&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;status&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;True&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;reason&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;KubeletReady&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;message&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubelet is posting ready status&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;lastHeartbeatTime&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2019-06-05T18:38:35Z&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;lastTransitionTime&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2019-06-05T11:41:27Z&amp;#34;&lt;/span&gt;
  }
]

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果 &lt;code&gt;Ready&lt;/code&gt; 条件的状态为 &lt;code&gt;Unknown&lt;/code&gt; 或 &lt;code&gt;False&lt;/code&gt; 持续时间超过 &lt;code&gt;pod-eviction-timeout&lt;/code&gt; (通过  kube-controller-manager 参数配置)， 这个节点上所以的 Pod 都会被节点控制器调度为删除。 默认的踢除的超时时间为5分钟。 在某些情况下，当一个节点不可达时，api-server 就不能与节点上的 kubelet 进行通信。 在 kubelet 与 api-server 重新建立连接之前对 Pod 的删除指令传达不到 kubelet. 在这段时间内这些被调度为删除的节点可以继续在分区节点上运行。&lt;/p&gt;
&lt;p&gt;节点控制器在确认 Pod 在集群中已经停止运行前是不会强制删除的。 所以可以会出现 Pod 运行在状态为  &lt;code&gt;Terminating&lt;/code&gt; 或 &lt;code&gt;Unknown&lt;/code&gt; 的不可达节点上。 有时候 k8s 不能在节点被永久移出集群后不能从基础设施自动的移除，需要管理员手动地删除对应的节点对象。从 k8s 中删除节点对象时，会同时从 api-server 中对应删除节点上运行的所有 Pod 对象，并释放其名称&lt;/p&gt;
&lt;p&gt;节点的生命周期管理器会自动创建代理节点状态的 &lt;code&gt;Taint&lt;/code&gt;. 调度器在分配 Pod 到节点时会顾及节点上的 Taint. Pod 也可以配置容忍节点的某些 &lt;code&gt;Taint&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;了解更多 Taint 相关信息见&lt;a href=&#34;../../09-scheduling-eviction/01-taint-and-toleration.md&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;容量和可分配状态&#34;&gt;容量和可分配状态&lt;/h3&gt;
&lt;p&gt;表示节点上的可用资源: CPU, 内存， 节点可接受 Pod 数量的最大值
容量(capacity)块下的字段表示节点资源总数
可分配状态(allocatable)块下的字段表示可用于普通 Pod 的资源数&lt;/p&gt;
&lt;p&gt;了解更多关于 容量和可分配状态 的信息见&lt;a href=&#34;../../../3-tasks/01-administer-cluster/28-reserve-compute-resources.md&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;其它信息&#34;&gt;其它信息&lt;/h3&gt;
&lt;p&gt;表示邛的通用信息，如 内核版本， k8s 版本(kubelet 和 kube-proxy 的版本)， Docker 版本， OS 名称。 这些信息都是节点上的 kubelet 生成的&lt;/p&gt;
&lt;h3 id=&#34;节点控制器&#34;&gt;节点控制器&lt;/h3&gt;
&lt;p&gt;节点控制器是 k8s 控制中心的组成部分，用于管理节点的各方面功能
节点控制器在节点的生命周期类扮演多个角色。 第一个就是为节点分配 CIDR 段(当 CIDR 分配开启时)
第二个是保持节点控制器内部的节点列表与云提供商提供的可用机器列表一致， 在运行在云环境时，当一个节点状态变为不健康时， 节点控制器会向云提供商查询该节点的虚拟机是否可用。 如果不可用就会从列表中删除该节点
第三个是监控节点状态， 节点控制器负责在节点变得不可达时(如， 因为某些原因收不到心跳，比如节点宕机)，更新节点就绪状态为 &lt;code&gt;ConditionUnknown&lt;/code&gt;， 如果节点持续不可达则踢出节点上所有的Pod(使用优雅终结方式)。设置就绪状态的不可达时间为 40s, 踢除 Pod 的时间为 5 分钟。节点控制器检查节点状态的时间由 &lt;code&gt;--node-monitor-period&lt;/code&gt; 配置&lt;/p&gt;
&lt;h4 id=&#34;心跳&#34;&gt;心跳&lt;/h4&gt;
&lt;p&gt;心跳由k8s 节点发送，用于帮助判定节点是否可用
心跳的形式有两种， 一个更新 &lt;code&gt;NodeStatus&lt;/code&gt; 另一个为租约对象。每个节点在 kube-node-lease 命名空间中有一个关联的租约对象。 租约是一个轻量级资源， 用户在集群范围内改善心跳的性能
由 kubelet 负责创建更新 &lt;code&gt;NodeStatus&lt;/code&gt; 和 租约对象。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kubelet  在状态发生改变或在配置的时间间隔内没有更新时更新 &lt;code&gt;NodeStatus&lt;/code&gt;， &lt;code&gt;NodeStatus&lt;/code&gt; 更新的默认的更新间隔为 5 分钟(比不可达节点默认超时的 40 秒长很多)&lt;/li&gt;
&lt;li&gt;kubelet 创建随后每隔10秒(默认时间间隔)更新租约对象。 租约对象的更新独立与 &lt;code&gt;NodeStatus&lt;/code&gt; 更新。 如果租约更新失败， kubelet 使用从 200ms 到 7s 间的指数组补尝&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;可靠性&#34;&gt;可靠性&lt;/h4&gt;
&lt;p&gt;大多数情况下， 节点控制器限制踢除速率由 &lt;code&gt;--node-eviction-rate&lt;/code&gt;(每秒) 配置， 默认 &lt;code&gt;0.1&lt;/code&gt;，即每10秒最多能踢除一个 Pod。
节点的踢除行为会在节点所有的可用区变为不健康时发生改变。 节点控制器会检查区域内同一时间不健康(&lt;code&gt;NodeReady&lt;/code&gt; 条件为 &lt;code&gt;ConditionUnknown&lt;/code&gt; 或 &lt;code&gt;ConditionFalse&lt;/code&gt;)节点的百分比。 如果不健康的节点比达达到 &lt;code&gt;--unhealthy-zone-threshold&lt;/code&gt; (默认 0.55)时踢除速率会降低: 如果集群较小(不多于 &lt;code&gt;--large-cluster-size-threshold&lt;/code&gt; 节点, 默认 50)，踢除行为停止。否则踢除速率降低至 &lt;code&gt;--secondary-node-eviction-rate&lt;/code&gt; (默认 0.01)每秒。 这个策略会在每个分区实现因为这个可用区可能与集群分隔， 但其它部分仍正常连接。 如果集群不是分散在多个云提供商的可用区，则只有一个可用区(即整个集群)。&lt;/p&gt;
&lt;p&gt;将节点分散在不同可用区的主要原因就当一个可用区整体不可用时，可以将工作负载转移到另一个可用区。 如果一个可用区的节点都不可用时节点控制器使用正常踢除速率(&lt;code&gt;--node-eviction-rate&lt;/code&gt;). 极限情况是当所有的可用区全部变得不可用时(整个集群每有一个健康节点)， 在这种情况下节点控制器认为主节点有连接问题并停止踢除行为直到连接恢复。&lt;/p&gt;
&lt;p&gt;节点控制器负责踢除包含 NoExecute Taint 节点运行的 Pod, 除了包含容忍该 Taint 的 Pod。 节点控制器会为有问题(如不可达或未就绪的节点)添加相应的 Taint, 也就是说调度器不会向不健康的节点放置 Pod.&lt;/p&gt;
&lt;p&gt;警告: &lt;code&gt;kubectl cordon&lt;/code&gt; 命令标记一个节点为不可调度，而其作为是 服务控制器会将节点从所有负载列表中移除， 并再向该节点调度流量。&lt;/p&gt;
&lt;h4 id=&#34;节点容量&#34;&gt;节点容量&lt;/h4&gt;
&lt;p&gt;节点对象记录了节点的资源容量(如: 可用内存，CPU核心数)。 自动注册的节点在注册时会报告其容量，如果节点是手动添加，则需要在添加时提供对应容量信息。
k8s 调度器保证节点有足够的资源运行分配到其上的 Pod， 调度器检测节点不所有容器请求的资源不大于节点的容量。 请求资源总和包括由kubelet 管理的所有容器， 但不包括直接由容器运行环境启动的容器。也不包括其它所有不被kubelet 控制的进程。
注意: 如果需要为非 Pod 进程保留资源，见&lt;a href=&#34;../../../3-tasks/01-administer-cluster/28-reserve-compute-resources/&#34;&gt;为系统进程保留资源&lt;/a&gt;,系统保留部分。&lt;/p&gt;
&lt;h2 id=&#34;节点拓扑&#34;&gt;节点拓扑&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;如果通过&lt;a href=&#34;../../../reference/command-line-tools-reference/feature-gates/&#34;&gt;功能特性开关&lt;/a&gt;，开启了 &lt;code&gt;TopologyManager&lt;/code&gt;， kubelet 在作资源分配决策时会参考拓扑信息。更多信息， 见&lt;a href=&#34;../../../3-tasks/01-administer-cluster/14-topology-manager/&#34;&gt;节点拓扑控制管理策略&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 节点级别的卷限制</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-limits/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-limits/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jsafrane
- saad-ali
- thockin
- msau42
title: Node-specific Volume Limits
content_type: concept
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This page describes the maximum number of volumes that can be attached
to a Node for various cloud providers.

Cloud providers like Google, Amazon, and Microsoft typically have a limit on
how many volumes can be attached to a Node. It is important for Kubernetes to
respect those limits. Otherwise, Pods scheduled on a Node could get stuck
waiting for volumes to attach.
--&gt;
&lt;p&gt;本文主要介绍在各家云提供商上每个节点能够挂载的卷的最大数量。
像 Google, Amazon, 和 Microsoft 这样典型的云提供上对于每个节点上能挂载的卷数量是有限制的。
让 k8s 遵守这个限制是比较重要的。 否则，Pod 在调度到一个节点后就会卡在等待卷挂载上。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Kubernetes default limits

The Kubernetes scheduler has default limits on the number of volumes
that can be attached to a Node:

&lt;table&gt;
  &lt;tr&gt;&lt;th&gt;Cloud service&lt;/th&gt;&lt;th&gt;Maximum volumes per Node&lt;/th&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://aws.amazon.com/ebs/&#34;&gt;Amazon Elastic Block Store (EBS)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;39&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://cloud.google.com/persistent-disk/&#34;&gt;Google Persistent Disk&lt;/a&gt;&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/services/storage/main-disks/&#34;&gt;Microsoft Azure Disk Storage&lt;/a&gt;&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
 --&gt;
&lt;h2 id=&#34;k8s-默认的限制&#34;&gt;k8s 默认的限制&lt;/h2&gt;
&lt;p&gt;k8s 调度器对每个节点上能够挂载的卷的数量是有限制的:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;云服务&lt;/th&gt;
&lt;th&gt;每个节点挂载卷的最大值&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://aws.amazon.com/ebs/&#34;&gt;Amazon Elastic Block Store (EBS)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;39&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://cloud.google.com/persistent-disk/&#34;&gt;Google Persistent Disk&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/services/storage/main-disks/&#34;&gt;Microsoft Azure Disk Storage&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
## Custom limits

You can change these limits by setting the value of the
`KUBE_MAX_PD_VOLS` environment variable, and then starting the scheduler.
CSI drivers might have a different procedure, see their documentation
on how to customize their limits.

Use caution if you set a limit that is higher than the default limit. Consult
the cloud provider&#39;s documentation to make sure that Nodes can actually support
the limit you set.

The limit applies to the entire cluster, so it affects all Nodes.
--&gt;
&lt;!--
## Custom limits

You can change these limits by setting the value of the
`KUBE_MAX_PD_VOLS` environment variable, and then starting the scheduler.
CSI drivers might have a different procedure, see their documentation
on how to customize their limits.

Use caution if you set a limit that is higher than the default limit. Consult
the cloud provider&#39;s documentation to make sure that Nodes can actually support
the limit you set.

The limit applies to the entire cluster, so it affects all Nodes.
 --&gt;
&lt;h2 id=&#34;自定义限制&#34;&gt;自定义限制&lt;/h2&gt;
&lt;p&gt;可以通过设置 &lt;code&gt;KUBE_MAX_PD_VOLS&lt;/code&gt; 环境变量的值来修改这个限制， 修改后重新启动调度器。
CSI 驱动可能需要不同的步骤，具体怎么修改这个自定义配置见他们的文档。&lt;/p&gt;
&lt;p&gt;要小心设置了一个比默认值大的限制。 参照云提供商的文档来决定节点实际支持的限制再设置这个值。&lt;/p&gt;
&lt;p&gt;这个限制会应用到整个集群，所以它会影响所有的节点。&lt;/p&gt;
&lt;!--
## Dynamic volume limits






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [stable]&lt;/code&gt;
&lt;/div&gt;



Dynamic volume limits are supported for following volume types.

- Amazon EBS
- Google Persistent Disk
- Azure Disk
- CSI

For volumes managed by in-tree volume plugins, Kubernetes automatically determines the Node
type and enforces the appropriate maximum number of volumes for the node. For example:

* On
&lt;a href=&#34;https://cloud.google.com/compute/&#34;&gt;Google Compute Engine&lt;/a&gt;,
up to 127 volumes can be attached to a node, [depending on the node
type](https://cloud.google.com/compute/docs/disks/#pdnumberlimits).

* For Amazon EBS disks on M5,C5,R5,T3 and Z1D instance types, Kubernetes allows only 25
volumes to be attached to a Node. For other instance types on
&lt;a href=&#34;https://aws.amazon.com/ec2/&#34;&gt;Amazon Elastic Compute Cloud (EC2)&lt;/a&gt;,
Kubernetes allows 39 volumes to be attached to a Node.

* On Azure, up to 64 disks can be attached to a node, depending on the node type. For more details, refer to [Sizes for virtual machines in Azure](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes).

* If a CSI storage driver advertises a maximum number of volumes for a Node (using `NodeGetInfo`), the &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-scheduler/&#39; target=&#39;_blank&#39;&gt;kube-scheduler&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.&lt;/span&gt;
&lt;/a&gt; honors that limit.
Refer to the [CSI specifications](https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetinfo) for details.

* For volumes managed by in-tree plugins that have been migrated to a CSI driver, the maximum number of volumes will be the one reported by the CSI driver.
--&gt;
&lt;h2 id=&#34;动态卷限制&#34;&gt;动态卷限制&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;以下类型的卷支持动态卷限制：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amazon EBS&lt;/li&gt;
&lt;li&gt;Google Persistent Disk&lt;/li&gt;
&lt;li&gt;Azure Disk&lt;/li&gt;
&lt;li&gt;CSI&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于由内部卷插件管理的卷， k8s 自动决定节点的类型并为节点设置合适的最大卷数量限制。 例如:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在
&lt;a href=&#34;https://cloud.google.com/compute/&#34;&gt;Google Compute Engine&lt;/a&gt;
上
&lt;a href=&#34;https://cloud.google.com/compute/docs/disks/#pdnumberlimits&#34;&gt;基于节点的类型&lt;/a&gt;.
每个节点最多可以挂载 127 个卷。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于 Amazon EBS 磁盘， 在 M5,C5,R5,T3 和 Z1D 类型的实例上，k8s 允许挂载到每个节点上的
卷的最大值是 25. 对于
&lt;a href=&#34;https://aws.amazon.com/ec2/&#34;&gt;Amazon Elastic Compute Cloud (EC2)&lt;/a&gt;
上其它类型的实例类型， k8s 允许挂载到每个节点上能挂载卷的最大值是 39&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 Azure, 基于节点类型的不同，每个节点上能挂载磁盘数量是 64， 更多信息见
&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes&#34;&gt;Azure 中虚拟机的大小&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果一个 CSI 存储驱动指定了每个节点的最大卷数量(通过 &lt;code&gt;NodeGetInfo&lt;/code&gt;)，
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-scheduler/&#39; target=&#39;_blank&#39;&gt;kube-scheduler&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.&lt;/span&gt;
&lt;/a&gt;， 也会
接受这个限制。
详见
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetinfo&#34;&gt;CSI specifications&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于由内部插件管理的卷，但已经迁移到了 CSI 驱动的，节点能够挂载的卷的最大数量会由 CSI 驱动
来报告这个值。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 控制中心与节点的通信</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/architecture/control-plane-node-communication/</link>
      <pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/architecture/control-plane-node-communication/</guid>
      <description>
        
        
        &lt;p&gt;本文主要介结控制中心(实际就是 api-server)与 k8s 集群通信的几种途径, 目的是使用户可以选择健壮的网络配置使集群可以运行在不受信的网络环境中(或在云提供商的公网IP上)&lt;/p&gt;
&lt;h2 id=&#34;从节点到控制中心的通信&#34;&gt;从节点到控制中心的通信&lt;/h2&gt;
&lt;p&gt;k8s 使用轴辐式(hub-and-spoke) API 模式，所有节点(包括节点上运行的Pod)使用到的API都指向 api-server(其它所有的组件在设计上就不提供远程服务)。api-server 配置成安全的HTTPS端口(通常是 443)来对外提供服务，并且开启一种或多种&lt;a href=&#34;../../../reference/03-access-authn-authz/01-authentication/&#34;&gt;认证&lt;/a&gt;方式。还需要开启一种或多种&lt;a href=&#34;../../../reference/03-access-authn-authz/07-authorization/&#34;&gt;授权&lt;/a&gt;方式， 特别是 匿名请求 或 service account tokens 可用(具体见认证相关部分)。
每个节点上都需要有集群的公开根证书，这样节点才能通过有效的客户凭据安全的连接到 api-server. 例如， 在默认的 GKE 部署中， 客户端为 kubelet 提供的凭据格式为客户端证书， 自动化提供客户端证书的方式见&lt;a href=&#34;../../../reference/command-line-tools-reference/08-kubelet-tls-bootstrapping/&#34;&gt;这里&lt;/a&gt;
如果集群中的Pod想要连接到 api-server 可以借助&lt;code&gt;service account&lt;/code&gt;实现安全连接， k8s 会在 Pod 启动时自动注入公开根谈不上和令牌.
在每个命名空间下有一个叫 &lt;code&gt;kubernetes&lt;/code&gt; 的 Service， 指向一个虚拟IP地址，并重写向(通过 kube-proxy)向 api-server 的HTTPS端口上。
控制中心组件也是通过安全端口与集群的 api-server 通信。
在默认的操作模式下，节点和节点上的Pod 与控制中心的连接默认就是安全的可以信赖于不受信的网络环境中&lt;/p&gt;
&lt;h2 id=&#34;从控制中心向节点的通信&#34;&gt;从控制中心向节点的通信&lt;/h2&gt;
&lt;p&gt;由控制中心(api-server)向节点的通信路径注要有两条， 第一条从 api-server 直接连接到集群中每个节点上的 kubelet 进程。 第二条是通过 api-server 的代理功能来实现 api-server 到任意节点， Pod， Service的连接&lt;/p&gt;
&lt;h3 id=&#34;从-api-server-连接到-kubelet&#34;&gt;从 api-server 连接到 kubelet&lt;/h3&gt;
&lt;p&gt;从 api-server 到 kubelet 通信主要有以下用途:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;摘取 Pod 日志&lt;/li&gt;
&lt;li&gt;通过 kubelet 终端连接到运行的Pod&lt;/li&gt;
&lt;li&gt;为 kubelet 提供端口转发功能&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;连接指向的是 kubelet 的 HTTPS 端口， 默认 api-server 不验证 kubelet 提供的证书，这样会存在中间人攻击的风险，在不受网络环境中是不安全的。&lt;/p&gt;
&lt;p&gt;要验证这个连接，需要在 api-server 配置启动参数 &lt;code&gt;--kubelet-certificate-authority&lt;/code&gt; 指向 根证书用于验证 kubelet 的服务证书。
如果做不到， 则在非受信网络或公网中使用 SSH 遂道连接 api-server 和 kubelet.
最后，需要开启 &lt;a href=&#34;../../../reference/command-line-tools-reference/07-kubelet-authentication-authorization/&#34;&gt;kubelet 认证和授权&lt;/a&gt;，以保护 kubelet API 安全。&lt;/p&gt;
&lt;h3 id=&#34;从-api-server-到-kubelet-节点-pod--service-的连接&#34;&gt;从 api-server 到 kubelet, 节点, Pod,  Service 的连接&lt;/h3&gt;
&lt;p&gt;默认情况下 从 api-server 到 kubelet, 节点, Pod,  Service 的连接是通过 HTTP 明文，因此没有认证也没加密。在连接到 节点， Pod, Service 名称对应的 API URL时可以添加 https 前缀来使用 HTTPS 来建立安全连接，但不会验证HTTPS证书也不验证客户端提供的凭据。 因此也不能保证任何完整性。 所以目前这能连接都不能用于非受信网络或公网。&lt;/p&gt;
&lt;h3 id=&#34;ssh-遂道&#34;&gt;SSH 遂道&lt;/h3&gt;
&lt;p&gt;k8s 支持通过 SSH 遂道的方式来实现从 控制中心到节点的连接路径的安全。 在这个配置中， 由 api-server 来初始化 SSH 遂道连接到集群中每个节点(连接到ssh服务监听端口22)，并通过这个连接来传输 kubelet, 节点, Pod,  Service 所有流量， 这样可以使流量不会显露给节点运行的外部网络&lt;/p&gt;
&lt;p&gt;SSH 遂道连接方式当前已经被废弃，用户在开启该功能需要清楚为什么为开启。  Konnectivity 服务是替代方案&lt;/p&gt;
&lt;h3 id=&#34;konnectivity-服务&#34;&gt;Konnectivity 服务&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;作为 SSH 遂道 的替代方案， Konnectivity 服务 通过提供 TCP 层的代理来实现 控制中心与集群之间的通信。 Konnectivity 服务主要由两部分组成， Konnectivity 服务端和Konnectivity 代理程序，分别运行在 控制中心网络和节点网络上。 Konnectivity 代理程序初始化并维护到服务端的连接。 在开启 Konnectivity 服务后，控制中心到节的所有连接都通过这些连接。
在集群中开启 Konnectivity 服务见 &lt;a href=&#34;../../../3-tasks/09-extend-kubernetes/01-setup-konnectivity/&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 控制器</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/architecture/controller/</link>
      <pubDate>Thu, 09 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/architecture/controller/</guid>
      <description>
        
        
        &lt;p&gt;在机器人技术和自动化领域，一个控制回路就是一个控制系统状态的无限循环。
以下为控制回路的一个示例: 房间内的温度控制器
当用户设定一个温度值时，就是告知温度控制器其期望状态。 当时房间内实际的问题则是当前状态。 温度控制器就会启动相应的动作(开启或关闭相应的设备)来让当前状态逐渐趋近于期望状态。&lt;/p&gt;
&lt;p&gt;在 k8s 中，控制器就是监控集群状态的控制循环，在需要的时候执行变更或请求其它服务执行变更。每一个控制器都在尝试让当前状态向期望状态演进。&lt;/p&gt;
&lt;h2 id=&#34;control-pattern&#34;&gt;控制模式&lt;/h2&gt;
&lt;p&gt;一个控制器至少会跟踪一个k8s 资源类型。 这些&lt;a href=&#34;../../00-overview/03-working-with-objects/00-kubernetes-objects/&#34;&gt;对象&lt;/a&gt;都有一个 &lt;code&gt;spec&lt;/code&gt; 字段来定义它的期望状态。该资源所对应的控制器就负责让当前状态逐渐趋近于期望状态。控制器可以直接采取动作实现状态的变更，但在 k8s 中，通常是一个控制器会向 &lt;code&gt;api-server&lt;/code&gt; 发送消息来达到这个目的。接下来可以会有实例。&lt;/p&gt;
&lt;h3 id=&#34;通过-api-server-实现控制&#34;&gt;通过 api-server 实现控制&lt;/h3&gt;
&lt;p&gt;Job 控制器就是 k8s 内置控制器的一员. 内置控制器就是通过与集群 api-server 来实现状态管理.
Job 这种 k8s 资源类型的工作模型是，运行一个或多个 Pod 来完成某个任务，完成后就停止。
(一旦完成&lt;a href=&#34;../../09-scheduling-eviction/&#34;&gt;调度&lt;/a&gt;， Pod对蟓就会成为 &lt;code&gt;kubelet&lt;/code&gt; 期望状态的组成部署)&lt;/p&gt;
&lt;p&gt;当 Job 控制器接收到一个新的任务，就需要按照任务的需求在集群中的某些节点上运行指定数量的 Pod 来完成相应的任务。 但 Job 控制器本身并不会运行任意 Pod 或容器。 而是向 api-server 发送 创建或删除 Pod 的请求。控制中心中的其它组件就会根据请求信息(有新的 Pod 需要调度并运行)，然后最终完成任务。&lt;/p&gt;
&lt;p&gt;在用户创建一个新的 Job 后，此时的期望状态就是完成这个 Job。 Job 控制器让当前状态逐渐趋近于期望状态： 创建 Job 需要完成任务的 Pod，此时就离完成 Job 进了不步。&lt;/p&gt;
&lt;p&gt;控制器也会更新那些对控制器进行配置的对象。 例如： 当一个 Job 对应的任务完成后， Job 控制器更新对应 Job 的对象状态为 &lt;code&gt;Finished&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;(就像开时提到的温度控制器有点类似，温度的控制器关闭指示灯表示当前房间内的温度达到用户设置所期望的温度)&lt;/p&gt;
&lt;h3 id=&#34;直接控制&#34;&gt;直接控制&lt;/h3&gt;
&lt;p&gt;与 Job 控制方式不同， 有些控制需要对集群外的部分组成资源进行变更。
例如，用户使用一个控制回路来保证集群中有足够的节点， 此时这个控制器就需要一些集群外的资源来实现对节点的配置管理。
控制器也是通过 api-server 来获取外部资源的期望状态，然后再直接与外部系统通信让其状态实现向期望状态的迁移。
(实际就有一个控制器可以实现集群节点的水平扩展， 见 &lt;a href=&#34;../../../3-tasks/01-administer-cluster/10-cluster-management/#cluster-autoscaling&#34;&gt;集群扩容&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;期望状态与当前状态&#34;&gt;期望状态与当前状态&lt;/h2&gt;
&lt;p&gt;k8s 是一个云原生的系统，能够处理持续不断的变更请求。
集群可以在任何时候发变更， 控制回路会自动的修复出现的问题。 也就是从始至终集群可能都不会达到一个稳定的状态。
当控制器都在正常运行且能够过成有效的变更，全局状态是否稳定就无关紧要了
(这段意思表达得不是很顺畅)&lt;/p&gt;
&lt;h2 id=&#34;设计&#34;&gt;设计&lt;/h2&gt;
&lt;p&gt;按照设计宗旨， k8s 使用很多控制器，每个控制器管理特定方面的集群状态。大多数情况下， 一个特定的控制回路(控制器)使用一种类型的资源作为期望状态，并通过管理多种类型的资源来实现期望状态。 例如：一个 Job 控制器跟踪 Job 对象(发现新发布的任务)和 Pod 对象(用来运行任务和监测任务是否完成，什么时候完成)。 在这种情况下，其它组件创建 Job， Job 控制器再创建 Pod。
使用多个简单的的控制器而不是一个内部关联的单体控制回路集合理有优势。 因为控制器可能挂掉，k8s 在设计上也是允许这种情况发生的。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 一般可以有多个控制器创建或更新同一个类型的对象。 在这种情况下， k8s 控制器保证只关注与控制资源关联的资源。 例如： 同时使用  Deployment 和 Job， 两者都会创建 Pod， 但 Job 的控制器不能删除 Deployment 创建的 Pod， 因为控制器可以傅秀对象包含的信息(标签)来区分各自不同的 Pod。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;运行控制器的方式&#34;&gt;运行控制器的方式&lt;/h2&gt;
&lt;p&gt;k8s 自带了一系列内置的控制器，运行在 &lt;code&gt;kube-controller-manager&lt;/code&gt; 中，这些内置的控制提供了重要的核心功能。
例如 Deployment 控制器和 Job 控制器就是由 k8s 本身(内置控制器)提供的控制器。 k8s 允许运行弹性的控制中心，所以当任意内置控制器挂掉后，其它节点的控制器能接替其上任。&lt;/p&gt;
&lt;p&gt;也可以找到用于扩展k8s 功能，运行在控制中心外的控制器。 如果用户愿意也可以自己编写全新的控制器。这些控制器可以以Pod的方式运行， 也可以运行在 k8s 集群之外。 具体由控制器的功能决定。&lt;/p&gt;
&lt;h2 id=&#34;引申阅读&#34;&gt;引申阅读&lt;/h2&gt;
&lt;p&gt;这里介绍怎么编写自己的控制器, 见&lt;a href=&#34;../../11-extend-kubernetes/00-extend-cluster/#extension-patterns&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Cloud Controller Manager</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/architecture/cloud-controller/</link>
      <pubDate>Wed, 15 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/architecture/cloud-controller/</guid>
      <description>
        
        
        




&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;云基础设施技术让用户可以在公有云，私有云，混合云上运行 k8s. k8s 倡导自动化， API 驱动，组件之间松耦合的基础设施&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cloud-controller-manager&lt;/code&gt; 是集成了云提供商控制逻辑的 k8s 控制中心组件。 云提供商控制管理器让集群与云提供商提供的 API 相关系并让与云平台交互的组件和与集群交互的组件分离。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;cloud-controller-manager&lt;/code&gt; 组件通过让 k8s 与底层云基础设施的交互逻辑解耦，使得云提供上功能发布的节奏与 k8s 项目功能发的节奏分离
&lt;code&gt;cloud-controller-manager&lt;/code&gt; 以插件结构的方便让不同的云提供与可以让其平台可以与 k8s 集成。&lt;/p&gt;
&lt;h2 id=&#34;设计&#34;&gt;设计&lt;/h2&gt;
&lt;p&gt;以下为 &lt;code&gt;cloud-controller-manager&lt;/code&gt; 在 k8s 架构中的位置：
&lt;img src=&#34;https://d33wubrfki0l68.cloudfront.net/7016517375d10c702489167e704dcb99e570df85/7bb53/images/docs/components-of-kubernetes.png&#34; alt=&#34;kubernetes architecture&#34;&gt;&lt;/p&gt;
&lt;p&gt;The cloud controller manager runs in the control plane as a replicated set of processes (usually, these are containers in Pods). Each cloud-controller-manager implements multiple controllers in a single process.
云提供商控制管理器在控制中心中以副本集进程的形式运行(通常为 Pod 中的容器)。 每个 &lt;code&gt;cloud-controller-manager&lt;/code&gt; 在一个进程中实现了多个控制器。&lt;/p&gt;
&lt;p&gt;{{ &lt;node&gt; }}
云提供商控制管理器通常以插件的方式运行而不是以控制中心组件的方式运行
{{ &lt;/node&gt; }}&lt;/p&gt;
&lt;h2 id=&#34;云提供商控制管理器的功用&#34;&gt;云提供商控制管理器的功用&lt;/h2&gt;
&lt;p&gt;云提供商控制管理器包含如下控制器:&lt;/p&gt;
&lt;h3 id=&#34;节点控制器&#34;&gt;节点控制器&lt;/h3&gt;
&lt;p&gt;节点控制器当在云基础设施上创建服务器时负责创建圣训的节点对象。 节点控制器获取用户在云提供商所租赁的主机的信息。 节点控制器主要有以下功能：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为控制器通过云提供商API发现的服务器初始化节点信息&lt;/li&gt;
&lt;li&gt;在节点对象上打上云提供商相关的注解和标签，例如 节点部署的区域，和可用资源(CPU, memory, 等)&lt;/li&gt;
&lt;li&gt;获取节点的主机名和网络地址&lt;/li&gt;
&lt;li&gt;验证节点的健康状况。 当一个节点不响应时，控制器会检查云提供商的 API， 确认服务器状态是否被修改为 停用/删除/终止。 如果发现节点已经被从云端删除则从 k8s 集群中删除对应的节点对象&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;有些云提供商在实现时会将其分为两个独立的控制器，分别为节点控制器和节点生命周期控制器&lt;/p&gt;
&lt;h3 id=&#34;路由控制器&#34;&gt;路由控制器&lt;/h3&gt;
&lt;p&gt;路由控制器负责在云提供商配置适当的路由以实现不同节点之间的通信。
根据云提供商的不同， 路由控制器还可能要为 Pod 网络申请一个IP段&lt;/p&gt;
&lt;h3 id=&#34;service-控制器&#34;&gt;Service 控制器&lt;/h3&gt;
&lt;p&gt;在云环境中 Service 会与一些云基础设施组件集群，比如负载均衡，IP地址， 网络包过滤，目标健康检测。 Service 控制器在创建 Service 时调用云提供商的API 设置 Service 需要的负载均衡和其它云基础设施组件&lt;/p&gt;
&lt;h2 id=&#34;授权&#34;&gt;授权&lt;/h2&gt;
&lt;p&gt;本节将分别说明云提供商管理器执行对应操作所需要访问的各种 API 对象的&lt;/p&gt;
&lt;h3 id=&#34;节点控制器-1&#34;&gt;节点控制器&lt;/h3&gt;
&lt;p&gt;节点控制器只需要访问节点对象。需要提供节点对的的读写权限&lt;/p&gt;
&lt;p&gt;&lt;code&gt;v1/Node&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Get&lt;/li&gt;
&lt;li&gt;List&lt;/li&gt;
&lt;li&gt;Create&lt;/li&gt;
&lt;li&gt;Update&lt;/li&gt;
&lt;li&gt;Patch&lt;/li&gt;
&lt;li&gt;Watch&lt;/li&gt;
&lt;li&gt;Delete&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;路由控制器-1&#34;&gt;路由控制器&lt;/h3&gt;
&lt;p&gt;路由控制器需要监听节点对象的创建来配置对应的路由。 所以需要节点对象的 &lt;code&gt;Get&lt;/code&gt; 权限&lt;/p&gt;
&lt;p&gt;&lt;code&gt;v1/Node&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Get&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;service-控制器-1&#34;&gt;Service 控制器&lt;/h3&gt;
&lt;p&gt;Service 控制器监听 Service 对象的创建，更新，删除事件来配置对应的 Endpoint&lt;/p&gt;
&lt;p&gt;访问 Service 需要 List， Watch 权限
更新 Service 需要 Patch， Update 权限
设置 Service 对的 Endpoint 需要 Create, List, Get, Watch, Update
最终需要如下：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;v1/Service&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;List&lt;/li&gt;
&lt;li&gt;Get&lt;/li&gt;
&lt;li&gt;Watch&lt;/li&gt;
&lt;li&gt;Patch&lt;/li&gt;
&lt;li&gt;Update&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;其它权限需求&#34;&gt;其它权限需求&lt;/h3&gt;
&lt;p&gt;云提供商控制管理器核心的实现中需要创建  Event 对象的权限。 为了设置安全操作，还需要创建 ServiceAccount 的权限&lt;/p&gt;
&lt;p&gt;&lt;code&gt;v1/Event&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create&lt;/li&gt;
&lt;li&gt;Patch&lt;/li&gt;
&lt;li&gt;Update&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;v1/ServiceAccount&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最终云提供商控制管理器基于 RBAC ClusterRole 配置如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;rbac.authorization.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ClusterRole&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cloud-controller-manager&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
- &lt;span style=&#34;color:#f92672&#34;&gt;apiGroups&lt;/span&gt;:
  - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;events&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;create&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;patch&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;update&lt;/span&gt;
- &lt;span style=&#34;color:#f92672&#34;&gt;apiGroups&lt;/span&gt;:
  - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;nodes&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;:
  - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;*&amp;#39;&lt;/span&gt;
- &lt;span style=&#34;color:#f92672&#34;&gt;apiGroups&lt;/span&gt;:
  - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;nodes/status&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;patch&lt;/span&gt;
- &lt;span style=&#34;color:#f92672&#34;&gt;apiGroups&lt;/span&gt;:
  - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;services&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;list&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;patch&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;update&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;watch&lt;/span&gt;
- &lt;span style=&#34;color:#f92672&#34;&gt;apiGroups&lt;/span&gt;:
  - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;serviceaccounts&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;create&lt;/span&gt;
- &lt;span style=&#34;color:#f92672&#34;&gt;apiGroups&lt;/span&gt;:
  - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;persistentvolumes&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;get&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;list&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;update&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;watch&lt;/span&gt;
- &lt;span style=&#34;color:#f92672&#34;&gt;apiGroups&lt;/span&gt;:
  - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;endpoints&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;verbs&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;create&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;get&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;list&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;watch&lt;/span&gt;
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;update&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;接下来应该看啥&#34;&gt;接下来应该看啥&lt;/h2&gt;
&lt;p&gt;这篇讲怎么运行管理 云提供商控制管理器 &lt;a href=&#34;../../../3-tasks/01-administer-cluster/09-running-cloud-controller/#cloud-controller-manager&#34;&gt;云提供商控制管理器管理&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;想要自己开发一个 云提供商控制管理器 &lt;a href=&#34;../../../3-tasks/01-administer-cluster/18-developing-cloud-controller-manager/&#34;&gt;云提供商控制管理器开发&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: k8s 是什么</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/00-what-is-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/00-what-is-k8s/</guid>
      <description>
        
        
        &lt;p&gt;k8s 是一个可扩展，可移植的开源平台，用于管理容器化的工作负载和服务，帮助实现声明式配置与自动化，有一个区大且快速发展的生态系统
源自Google 15年的生产经验&lt;/p&gt;
&lt;h2 id=&#34;部署方式演进&#34;&gt;部署方式演进&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://d33wubrfki0l68.cloudfront.net/26a177ede4d7b032362289c6fccd448fc4a91174/eb693/images/docs/container_evolution.svg&#34; alt=&#34;deployment&#34;&gt; &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/&#34;&gt;来源 kubernetes.io&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;传统部署时代&#34;&gt;传统部署时代&lt;/h3&gt;
&lt;p&gt;应用直接部署在物理服务器上，无法设置资源限制，因而可能引起资源分配问题
如果每个应用独立部署在物理机上，资源利用不充分，维护大量物理机费用高昂&lt;/p&gt;
&lt;h3 id=&#34;虚拟化部署&#34;&gt;虚拟化部署&lt;/h3&gt;
&lt;p&gt;通过虚拟机方式实现资源隔离和应用的扩展
虚拟机会在其自己的操作系统运行一个机器所有的组件&lt;/p&gt;
&lt;h3 id=&#34;容器化部署&#34;&gt;容器化部署&lt;/h3&gt;
&lt;p&gt;容器与虚拟机类似，但有更低的隔离级别，应用之间共享操作系统，因此容器被认为更轻量，与虚拟机类似，容器有独立的文件系统，CPU,内存，进程空间等。些两者都能实现与底层基础设施解耦，因此可以在不同的操作系统与云环境之间移植
容器变得流行，因为它提供了以下额外的优势&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;相较与虚拟机镜像创建更高效，更快捷的应用创建，更宽松的部署&lt;/li&gt;
&lt;li&gt;持续开发，集成，部署能通过镜像构建变得快速可靠，部署回滚也因为镜像的不可变性而变得快速和容易&lt;/li&gt;
&lt;li&gt;分离 开发和运维的关注点 在构建和发布时创建镜像而不是在部署。实现了应用与基础设施的解耦&lt;/li&gt;
&lt;li&gt;不止可以观测到系统级的信息和指数，还可以观测到应用的健康状态及其它信息&lt;/li&gt;
&lt;li&gt;实现 开发，测试，生产各环境的一致性。 可以在开发的笔记本和云运营上的环境上运行同样的镜像&lt;/li&gt;
&lt;li&gt;实现云环境和各种操作系统之间的可移植， 可运行在 Ubuntu, RHEL, CoreOS, on-premises,主流云环境，其它环境&lt;/li&gt;
&lt;li&gt;应用指数管理： 将抽像级别在虚拟操作系统运行应用提升到在一个运行在逻辑资源上的操作系统上运行应用&lt;/li&gt;
&lt;li&gt;松耦合，分布式，灵活，开放的微服务，应用被拆分为较小，独立，可以动态部署和管理。不是运行在一个专用大机器来运行一个单体应用&lt;/li&gt;
&lt;li&gt;资源隔离： 可预期的应用性能&lt;/li&gt;
&lt;li&gt;资源利用率: 高效充分&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;为啥需要-k8s它又能做啥&#34;&gt;为啥需要 k8s，它又能做啥&lt;/h2&gt;
&lt;p&gt;以上可知，应用容器化是个好东西，但在生产环境中,需要管理容器，并保证应用一直可用。 比如当一个容器挂掉以后，需要要马上再启动一个。如果这些都由一个系统来搞定，是不是管理容器就更容易了。而 k8s 就是为些而生的。 k8s 提供了运行弹性分布式系统的框架。提供应用的扩展和容灾等如下功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;服务发现和负载均衡
在 k8s 中可以通过 DNS 名称或 IP 地址来访问容器， 如果流量较高可能负载到多个容器上&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;存储编排
k8s 允许你自动挂载你选择的存储系统，比如 本地存储或公有云存储&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;自动化更新或回滚
由你来定义部署容器的状态，k8s 通过控制器来实现这些状态&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;自动化调度
将能够运行对应容器的节点加入 k8s 集群， 为容器定义需要的 CPU和内存资源数。k8s会把容器调度到合适的节点上，并充分利用节点资源&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;自愈
k8s 能重启替换挂掉的容器，根据配置检测不响应的容器，不让未就绪的容器进度流量负载列表&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;敏感数据和配置管理
通过配置管理敏感数据，而不需要将敏感数据打入到容器镜像中&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;k8s-不xxx&#34;&gt;k8s 不XXX&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;k8s 不限制应用的类型，k8s 致力于支持各种类型的工作负载，包括 无状态应用，有状态应用，数据处理工作，如果一个应用可以在容器中运行，就可以在k8s上运行&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;k8s 不发布源码，不构建应用。 可由组织文化和技术偏好决定CI/CD的工作流&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;k8s 不提供应用层的服务，比如 中间件(eg: 消息总线)，数据处理框架(eg: Spark), 数据库(eg: mysql), 缓存服务，集群存储系统(eg: Ceph) 等作为其内置服务， 这些组件可以直接运行在 k8s 上或者通过 k8s 可移植机制(Open Service Broker)让k8s 集群中的应用访问&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;k8s 不指定 日志，监控，报警解决方案，只提供集成方式，和收集导出相关metrics的机制&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;k8s 不提供也不要求一种配置 语言/系统(e.g. jsonnet)，只提供任意形式的声明式规范的声明式的API?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;k8s 不提供也不接收？ 任何 综合机器配置，维护，管理，自愈系统
k8s 也不仅仅是一个编排系统，实事上它消除了对编排的需求。 编排的技术定义是执行一个定义的工作流：先做A,然后B,再然后C. 相反的，k8s由一组独立的，可组合的管理理进程组成，它们的任务就是保证当前的状态与期望的状态是一致的。不关心怎么从A到C. 不需要中心化的控制。 这样可以使系统更易用，更强大，健壮，有更好的弹性和扩展性&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kubernetesk8s的字面意思&#34;&gt;Kubernetes(K8s),的字面意思&lt;/h2&gt;
&lt;p&gt;源自希腊词，意思是舵手或引向员，是最高的管理和控制， k8s 是缩写&lt;/p&gt;
&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://research.google.com/pubs/pub43438.html&#34;&gt;https://research.google.com/pubs/pub43438.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/&#34;&gt;https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: k8s 组件</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/01-components/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/01-components/</guid>
      <description>
        
        
        &lt;p&gt;当完成 k8s 部署后，用户就拥有一个 k8s 集群， 一个 k8s 一般包含多台工作机，称为节点，用于运行容器化应用。 每个集群至少包含一个节点。
在节点上运行应用工作负载的组件称为 Pod. k8s 控制面板 管理工作节点和集群中的 Pod. 在生产环境中，k8s 控制面板一般会运行在多个机器上，同样一个集群也会包含多个节点，用以提高集群的容错和高可用&lt;/p&gt;
&lt;p&gt;本文主要介绍一个完整可工作的集群包含哪些组件
下图标示出了一个集群的所有组件及相互关系
&lt;img src=&#34;https://d33wubrfki0l68.cloudfront.net/7016517375d10c702489167e704dcb99e570df85/7bb53/images/docs/components-of-kubernetes.png&#34; alt=&#34;k8s components diagram&#34;&gt; &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/components/&#34;&gt;来源 kubernetes.io&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;control-plane-components&#34;&gt;控制中心 组件&lt;/h2&gt;
&lt;p&gt;控制中心负责整个集群的全局决策(例如，调度)，也包含侦听和响应集群事件(例如，当应用的副本数增加时启动一个新的 Pod)
控制中心组件可以运行在集群的任意机器上。 但为了简单，创建脚本一般都会将组件放在同一个节点上，并不在这个节点上运行用户的容器。 更多示例参见
&lt;a href=&#34;TODO&#34;&gt;高可用集群&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;kube-apiserver&#34;&gt;kube-apiserver&lt;/h3&gt;
&lt;p&gt;提供 k8s API，是k8s控制中心的前端(此前端非)
k8s API 的大多数实现都在 kube-apiserver
提供了水平扩展的能力，扩容方式为部署多个实例，并提供负载均衡 &lt;a href=&#34;TODO&#34;&gt;搭建高可能集群&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;etcd&#34;&gt;etcd&lt;/h3&gt;
&lt;p&gt;强一致性和高可用键值存储，用于 k8s 存储所有集群数据
一定要对 etcd 数据做备份计划
需要深入了解 etcd 请移步 &lt;a href=&#34;https://etcd.io/docs/&#34;&gt;官网&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;kube-scheduler&#34;&gt;kube-scheduler&lt;/h3&gt;
&lt;p&gt;侦听新创建还没有指定 Node 的 Pod，为其选择一个 Node
调度因素：包含独立，集合资源需求，硬件，软件，策略，亲和性，反亲和性规范，数据位置，inter-workload interference and deadlines (TODO 没接触过，需要研究一下)&lt;/p&gt;
&lt;h3 id=&#34;kube-controller-manager&#34;&gt;kube-controller-manager&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kube-controller-manager&lt;/code&gt; 包含控制器进程，逻辑上每个 控制器是独立的进程，但为了降低复杂度，将它们打入一个可执行文件并运行在一个进程里&lt;/p&gt;
&lt;p&gt;有如下控制器：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Node 控制器: 负责监视和响应 Node 是不是挂了&lt;/li&gt;
&lt;li&gt;Replication 控制器: 负责系统中的 Pod 维护在预期的副本数&lt;/li&gt;
&lt;li&gt;Endpoints 控制器: 负责管理 Endpoints对象(Services &amp;amp; Pods)&lt;/li&gt;
&lt;li&gt;Service Account &amp;amp; Token 控制器: 为新增的命名空间创建默认的账号和令牌&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cloud-controller-manager&#34;&gt;cloud-controller-manager&lt;/h3&gt;
&lt;p&gt;k8s 控制中心集群了云环境的控制逻辑。 云控制管理器是集群与云提供商API交互的控制器。并与只与集群交互的控制器分离。
&lt;code&gt;cloud-controller-manager&lt;/code&gt; 只运行与指定云提供商对应的控制器。 如果k8s集群没有运行在云上，则集群不包含 &lt;code&gt;cloud-controller-manager&lt;/code&gt;
与 &lt;code&gt;kube-controller-manager&lt;/code&gt; 一样，&lt;code&gt;cloud-controller-manager&lt;/code&gt; 也是由多个独立的控制器放在一个二进制文件中，并运行在一个进程里。 可能运行多个运程实现水平扩展达到扩容和容灾的目的&lt;/p&gt;
&lt;p&gt;以下控制器依赖于云提供商：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Node 控制器 当机器不响应时向云提供商询问是否被删除&lt;/li&gt;
&lt;li&gt;Route 控制器 在云提供商的基础设施上创建路由&lt;/li&gt;
&lt;li&gt;Service 控制器: 创建，更新，删除云提供商的负载均衡&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;node-组件&#34;&gt;Node 组件&lt;/h2&gt;
&lt;p&gt;Node 组件运行在每个节点上，维护 Pod 运行，提供 k8s 运行环境&lt;/p&gt;
&lt;h3 id=&#34;kubelet&#34;&gt;kubelet&lt;/h3&gt;
&lt;p&gt;保证容器在 Pod 运行
读取&lt;code&gt;PodSpecs&lt;/code&gt; 保证容器的健康运行
kubelet 不管理非 k8s 创建的容器(例如由 manifests 创建的 Pod)&lt;/p&gt;
&lt;h3 id=&#34;kube-proxy&#34;&gt;kube-proxy&lt;/h3&gt;
&lt;p&gt;kube-proxy 运行在集群中每个节点上的网络代理， 实现 Service 层抽象的组成部分
kube-proxy 维护节点上的网络规则。 这些规则允计集群内外能够与 Pod 进行网络通信
kube-proxy 优先使用系统层的包过虑，如果没有则自己转发&lt;/p&gt;
&lt;h3 id=&#34;container-runtime&#34;&gt;Container Runtime&lt;/h3&gt;
&lt;p&gt;容器的运行环境，
k8s 支持容器运行环境有: Docker, containerd, CRI-O 和 任意实现了 k8s CRI(&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md&#34;&gt;Container Runtime Interface&lt;/a&gt;)的运行环境&lt;/p&gt;
&lt;h2 id=&#34;插件&#34;&gt;插件&lt;/h2&gt;
&lt;p&gt;插件基于 k8s 资源(DaemonSet, Deployment 等)来实现集群新功能。由于提供的是集群级的功能，所有插件所属的命名空间为 &lt;code&gt;kube-system&lt;/code&gt;
下面列举了一些插件，更多请见 &lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/addons/&#34;&gt;这里&lt;/a&gt;
TODO&lt;/p&gt;
&lt;h3 id=&#34;dns&#34;&gt;DNS&lt;/h3&gt;
&lt;p&gt;其它插件不是十分必要，但这个必须要有，许多示例依赖这个插件
Cluster DNS，是一个DNS服务，是对其它DNS服务的补充，为k8s 集群 Service 提供 DNS 记录
由 k8s 启动的容器会自动包含 此 DNS 的配置&lt;/p&gt;
&lt;h3 id=&#34;web-ui-dashboard&#34;&gt;Web UI (Dashboard)&lt;/h3&gt;
&lt;p&gt;一个通用的 k8s 集群 web UI. 可用于集群和集群内应用的管理和调试
&lt;a href=&#34;https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/&#34;&gt;Dashboard&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;容器资源监控&#34;&gt;容器资源监控&lt;/h3&gt;
&lt;p&gt;记录容器的时序监控数据到到一个中心数据库，并提供查看数据的UI
&lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/&#34;&gt;Container Resource Monitoring&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;集群级的日志&#34;&gt;集群级的日志&lt;/h3&gt;
&lt;p&gt;提供集群级的容器日志采集并存储到中心日志存储的机制，并提供查询和阅览接口
&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/logging/&#34;&gt;Cluster-level Logging&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;参考资料&#34;&gt;参考资料&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/components/&#34;&gt;https://kubernetes.io/docs/concepts/overview/components/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;引申阅读&#34;&gt;引申阅读&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/nodes/&#34;&gt;节点&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/controller/&#34;&gt;控制器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/&#34;&gt;调度器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://etcd.io/docs/&#34;&gt;etcd 官网&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/admin/high-availability/&#34;&gt;k8s 集群高可用&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: k8s API 说明</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/02-k8s-api/</link>
      <pubDate>Wed, 24 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/02-k8s-api/</guid>
      <description>
        
        
        &lt;p&gt;k8s API 提供查询操作 k8s 对应状态的功能。 k8s 控制中心的核心是 &lt;code&gt;api-server&lt;/code&gt; 及其提供的 HTTP API。包括用户，集群的其它组件，外部组件都是与 &lt;code&gt;api-server&lt;/code&gt; 通信&lt;/p&gt;
&lt;p&gt;全部的API约定在&lt;a href=&#34;https://git.k8s.io/community/contributors/devel/api-conventions.md&#34;&gt;这里&lt;/a&gt;
API endpoints,资源类型，示例在&lt;a href=&#34;https://kubernetes.io/docs/reference&#34;&gt;这里&lt;/a&gt;
远程访问 API 的说明在&lt;a href=&#34;https://kubernetes.io/docs/admin/accessing-the-api&#34;&gt;这里&lt;/a&gt;
k8s API 是系统声明式配置的基础， 命令行工具 &lt;code&gt;[kubectl](https://kubernetes.io/docs/user-guide/kubectl/)&lt;/code&gt; 可以用来进行对 API 对象的增删改查
k8s 以API 资源的形式也存储了序列化状态(目前使用 &lt;a href=&#34;https://coreos.com/docs/distributed-configuration/getting-started-with-etcd/&#34;&gt;etcd&lt;/a&gt;)
k8s 自身也解耦成多个模块，通过 API 进行交互&lt;/p&gt;
&lt;h2 id=&#34;api-的变化&#34;&gt;API 的变化&lt;/h2&gt;
&lt;p&gt;任何成功的系统都应该快速响应新的应用场景或现有需求的变更。 因此 k8s 在设计 API 时就提供了持续改进与成长的方式。k8s 项目主旨是不打破对已有客户端的普空性。并将此兼容性保持一段时间，让其它项目有时间来进行适配.
通常新的API 资源和新的API资源项，可以经常频繁增加， 移除资源或资源项则需要遵循 &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/deprecation-policy/&#34;&gt;API废弃策略&lt;/a&gt; TODO&lt;/p&gt;
&lt;p&gt;关于怎么做兼容和怎么修改API,依照这个(&lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api_changes.md#readme&#34;&gt;https://git.k8s.io/community/contributors/devel/sig-architecture/api_changes.md#readme&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;openapi-规范&#34;&gt;OpenAPI 规范&lt;/h2&gt;
&lt;p&gt;完整 API 规范明细见 &lt;a href=&#34;https://www.openapis.org/&#34;&gt;这里&lt;/a&gt;
k8s api-server 通过 &lt;code&gt;/openapi/v2&lt;/code&gt; 提供 OpenAPI 规范， 用户可以通过以下请求头发送请求获取响应格式&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;请求头&lt;/th&gt;
&lt;th&gt;可选值&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Accept-Encoding&lt;/td&gt;
&lt;td&gt;gzip&lt;/td&gt;
&lt;td&gt;可选&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Accept&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:application/com.github.proto-openapi.spec.v2@v1.0&#34;&gt;application/com.github.proto-openapi.spec.v2@v1.0&lt;/a&gt;+protobuf&lt;/td&gt;
&lt;td&gt;mainly for intra-cluster use&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;^&lt;/td&gt;
&lt;td&gt;application/json&lt;/td&gt;
&lt;td&gt;默认&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;^&lt;/td&gt;
&lt;td&gt;响应 &lt;code&gt;application/json&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;也可以是任务符合OpenAPI 规范的请求头
k8s 还有一个基于基于 &lt;code&gt;Protobuf&lt;/code&gt; 序列化的API，这套API 主要用于集群内通信。具体见对应模块 Go 源码文档，文档设计准则在&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/protobuf.md&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;api-版本&#34;&gt;API 版本&lt;/h2&gt;
&lt;p&gt;为方便移除资源项或修改资源结构，k8s 同时存在多个版本的 API, 每个版本的 API 路径都不一样，比如 &lt;code&gt;/api/v1&lt;/code&gt; &lt;code&gt;/apis/extensions/v1beta1&lt;/code&gt;
这样版本就通过区分就在API这一层完成，而不用到资源或资源荐，可以做到清晰明了,并提供一致的系统资源的行为。
也可以实现对过期和实验性 API 的访问控制
JSON和Protobuf序列化的定义结构变更策略与API定义资源结构的规则一致，具体如下:&lt;/p&gt;
&lt;p&gt;API 的版本和软件的版本只是间接相关的(和 Docker 类似)，详见&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/release/versioning.md&#34;&gt;k8s 版本发布&lt;/a&gt;提议&lt;/p&gt;
&lt;p&gt;不同的 API 版本提供不同级别的稳定性和支持，关于每个级别详细描述的细则见&lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api_changes.md#alpha-beta-and-stable-versions&#34;&gt;这个文档&lt;/a&gt;
可以总结为如下几个:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Alpha 级别&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;api URL名字中带有 alpha (e.g. v1alpha1)&lt;/li&gt;
&lt;li&gt;可能有不少bug,启用这个物引可能会引入bug. 默认关闭&lt;/li&gt;
&lt;li&gt;所支持的功能可能在没有任何通知的情况下被移除&lt;/li&gt;
&lt;li&gt;在未来的版本中可能会在没有通知的情况谱得与之前不兼容&lt;/li&gt;
&lt;li&gt;只推荐用于短期实验集群，因为有较高bug多，缺乏长期支持的风险&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Beta 级别&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;api URL名字中带有 beta (e.g. v2beta3)&lt;/li&gt;
&lt;li&gt;代码是充分测试的，启用这个功能基本上是安全的，默认打开&lt;/li&gt;
&lt;li&gt;整体功能不会被删除，但细节可能会修改&lt;/li&gt;
&lt;li&gt;未来版本中可能会因资源的结构与资源项的含义出现不兼容情况，如果出现这种情况,官方会提供升级与该版本的迁移指导，可能需要删除，编辑或重建 API对象，在修改过程可能需要用户仔细思考需要的改动，此过程信赖些功能的应用可能会不可用&lt;/li&gt;
&lt;li&gt;只推荐 非关键商业 用户使用，因为在未来版本中可能存在潜在的不兼容变更，如果用户有几个可独立更新集群，也可以不受此限制&lt;/li&gt;
&lt;li&gt;强烈建议用户试用beta特性并给予反馈，因为一旦从beta 版本确认为稳定版，则特性将不可更改&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stable&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;api 版本名称模式为 &lt;code&gt;vX&lt;/code&gt;， 其中&lt;code&gt;X&lt;/code&gt;为整数&lt;/li&gt;
&lt;li&gt;稳定版功能会存在于稳定版的软件的很多个版本中&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;api-分组&#34;&gt;API 分组&lt;/h2&gt;
&lt;p&gt;为了方便API的扩展， k8s 实现了 API 分组的方式。 API 组在 REST 接口的Path 上的 apiVersion 字段
&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/api-machinery/api-group.md&#34;&gt;设计提案之API分组&lt;/a&gt;
集群中一般有如下一些组:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;核心组,也被称为经典组，REST 路径中为 &lt;code&gt;/api/v1&lt;/code&gt; 对象定义 &lt;code&gt;apiVersion: v1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;其它组 REST 路径为 &lt;code&gt;/apis/$GROUP_NAME/$VERSION&lt;/code&gt; 对象定义为 &lt;code&gt;apiVersion: $GROUP_NAME/$VERSION (e.g. apiVersion: batch/v1)&lt;/code&gt; &lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/&#34;&gt;API文档&lt;/a&gt;是有完整的分级列表&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过自定义资源可以通过以下两种方式扩展API:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/&#34;&gt;CustomResourceDefinition&lt;/a&gt; 通过声明方式定义 api-server 怎么提供用户选择资源的API (TODO 这里描述不太清楚)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用户可能通过&lt;a href=&#34;https://kubernetes.io/docs/tasks/extend-kubernetes/setup-extension-api-server/&#34;&gt;实现自己的扩展api-server&lt;/a&gt;, 并通过&lt;a href=&#34;https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/&#34;&gt;聚合&lt;/a&gt;使客户端可以访问&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;api-分组开关&#34;&gt;API 分组开关&lt;/h2&gt;
&lt;p&gt;一些资源和API默认是开启的，可能通过 api-server 命令行启动参数中使用 &amp;ndash;runtime-config 进行开启或关闭
关闭 &amp;ndash;runtime-config=batch/v1=false
开启 &amp;ndash;runtime-config=batch/v2alpha1
如果有多个分组进制设置，可能key=value并用逗号分隔
注意： 开启或关闭分组或资源一需要重启 api-server 和 kube-controller-manager&lt;/p&gt;
&lt;h2 id=&#34;开启-extensionsv1beta1-组的指定资源&#34;&gt;开启 &lt;code&gt;extensions/v1beta1&lt;/code&gt; 组的指定资源&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;extensions/v1beta1&lt;/code&gt; 组默认启用资源的有 &lt;code&gt;DaemonSets&lt;/code&gt;, &lt;code&gt;Deployments&lt;/code&gt;, &lt;code&gt;StatefulSet&lt;/code&gt;, &lt;code&gt;NetworkPolicies&lt;/code&gt;, &lt;code&gt;PodSecurityPolicies&lt;/code&gt;， &lt;code&gt;ReplicaSets&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;例如要启用资源 &lt;code&gt;deployments&lt;/code&gt; 和 &lt;code&gt;daemonsets&lt;/code&gt;
也是在 api-server 启动参数中使用 &amp;ndash;runtime-config 进行开启或关闭
&lt;code&gt;--runtime-config=extensions/v1beta1/deployments=false,extensions/v1beta1/ingress=false&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;持久化&#34;&gt;持久化&lt;/h2&gt;
&lt;p&gt;k8s 将序列化的 API 资源对象保存在 etcd 中&lt;/p&gt;
&lt;h2 id=&#34;引申阅读&#34;&gt;引申阅读&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/&#34;&gt;API访问控制&lt;/a&gt;
&lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#api-conventions&#34;&gt;API约定&lt;/a&gt;
&lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/&#34;&gt;API文档&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 镜像</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/containers/00-images/</link>
      <pubDate>Sun, 19 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/containers/00-images/</guid>
      <description>
        
        
        &lt;!-- overview --&gt;
&lt;p&gt;一个容器镜像就是构建应用的二进制数据和应用所以需要的依赖。 容器镜像是一个可独立运行的可执行软件包，并定义了清楚的运行环境。
通常是用户创建一个应用的容器镜像，推送到镜像仓库，在 Pod 中引用这个镜像。&lt;/p&gt;
&lt;p&gt;本文介绍容器镜像的主要概念&lt;/p&gt;
&lt;!-- body --&gt;
&lt;h2 id=&#34;镜像名称&#34;&gt;镜像名称&lt;/h2&gt;
&lt;p&gt;镜像的名称通常是长成这些个样子的 &lt;code&gt;pause&lt;/code&gt;, &lt;code&gt;example/mycontainer&lt;/code&gt;, &lt;code&gt;kube-apiserver&lt;/code&gt;。 镜像名也可以带上镜像仓库的主机名(自建或第三方镜像仓库默认都要带上)，如：&lt;code&gt;fictional.registry.example/imagename&lt;/code&gt;。也可能有端口号，如: &lt;code&gt;fictional.registry.example:10443/imagename&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;如果镜像名称上没有镜像仓库的主机名， 则 k8s 认为使用的是 Docker 公共镜像仓库。&lt;/p&gt;
&lt;p&gt;在镜像名称的后面通常还会有一个标签(&lt;code&gt;tag&lt;/code&gt;)(就和 &lt;code&gt;docker&lt;/code&gt; 与 &lt;code&gt;podman&lt;/code&gt; 命令用的的一样)。 标签用于区分同一系列镜像的不同版本。&lt;/p&gt;
&lt;p&gt;标签名的命名规范为： 大小写字母，数字，下划线(&lt;code&gt;_&lt;/code&gt;)，点(&lt;code&gt;.&lt;/code&gt;)中划线(&lt;code&gt;-&lt;/code&gt;)，但分隔符(&lt;code&gt;_&lt;/code&gt;,&lt;code&gt;.&lt;/code&gt;,&lt;code&gt;-&lt;/code&gt;)有使用限制
如果镜像名称后没有标签，则默认使用 &lt;code&gt;latest&lt;/code&gt; 作为标签。&lt;/p&gt;
&lt;p&gt;{{ &lt;warning&gt; }}
在生产环境部署中尽量避免使用 latest 标签。 这样很难跟踪当前运行的是哪个版本，要做版本回退就更难了。
所以建议使用有含意的标签，如: &lt;code&gt;v1.42.0&lt;/code&gt;
{{ &lt;/warning&gt; }}&lt;/p&gt;
&lt;h2 id=&#34;镜像更新策略&#34;&gt;镜像更新策略&lt;/h2&gt;
&lt;p&gt;默认的更新策略为 &lt;code&gt;IfNotPresent&lt;/code&gt;, 就是让 kubelet 在找不到镜像时才从仓库拉取。 如果想要每次都强制拉取，则可以通过以下任意一种方式实现：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将容器 &lt;code&gt;imagePullPolicy&lt;/code&gt; 的值设置为 &lt;code&gt;Always&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;不设置 &lt;code&gt;imagePullPolicy&lt;/code&gt; 并让镜像使用 &lt;code&gt;latest&lt;/code&gt; 标签&lt;/li&gt;
&lt;li&gt;不设置 &lt;code&gt;imagePullPolicy&lt;/code&gt; 并镜像也不设置标签&lt;/li&gt;
&lt;li&gt;打开  &lt;a href=&#34;../../../reference/03-access-authn-authz/04-admission-controllers/#alwayspullimages&#34;&gt;AlwaysPullImages&lt;/a&gt; admission controller&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果配置了 &lt;code&gt;imagePullPolicy&lt;/code&gt; 但没有设置值，则默认为 &lt;code&gt;Always&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;带清单manifest的多架构镜像&#34;&gt;带清单(Manifest)的多架构镜像&lt;/h2&gt;
&lt;p&gt;镜像仓库在提供二进制镜像的同时也可以提供 &lt;a href=&#34;https://github.com/opencontainers/image-spec/blob/master/manifest.md&#34;&gt;容器镜像清单&lt;/a&gt;, 这个清单中列举了不同架构的镜像引用层。这样虽然镜像只有一个名字(如: &lt;code&gt;pause&lt;/code&gt;, &lt;code&gt;example/mycontainer&lt;/code&gt;, &lt;code&gt;kube-apiserve&lt;/code&gt;), 但不同的操作系统架构可以拉取到对应架构的镜像。&lt;/p&gt;
&lt;p&gt;在 k8s 中容器镜像名称是带有后缀 &lt;code&gt;-$(ARCH)&lt;/code&gt;的。 为了向后兼容， 请为旧的镜像添加后缀。 比如生成包含所有架构清单的镜像叫 &lt;code&gt;pause&lt;/code&gt;，&lt;code&gt;pause-amd64&lt;/code&gt; 就是为向后兼容旧的配置或可能存在于YAML配置文件中带后续硬编码镜像名&lt;/p&gt;
&lt;h2 id=&#34;使用私有仓库&#34;&gt;使用私有仓库&lt;/h2&gt;
&lt;p&gt;从私有仓库拉取镜像时一般都需要提供凭据。
以下为提供凭据的几种方式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在节点上配置仓库认证&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有的 Pod 都对仓库有全部访问权限&lt;/li&gt;
&lt;li&gt;需要集群管理员对节点进行配置&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;预先下载镜像&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有的 Pod 可以使用节点是缓存的镜像&lt;/li&gt;
&lt;li&gt;需要在节点上以 root 用户配置&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;基于提供商或本地插件&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果使用自定义节点配置， 用户(或云提供商)可以自己在节点上实现向镜像仓库的认证&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下为对这些认证方式更详细的说明&lt;/p&gt;
&lt;h3 id=&#34;在节点上配置仓库认证&#34;&gt;在节点上配置仓库认证&lt;/h3&gt;
&lt;p&gt;如果节点上运行的是 Docker 用户可以通过配置 Docker 运行时向私有仓库进行认证
这种方式适用于用于能近控制节点配置&lt;/p&gt;
&lt;p&gt;{{ &lt;note&gt; }}
k8s 只支持 Docker 配置的 &lt;code&gt;auths&lt;/code&gt; 和 &lt;code&gt;HttpHeaders&lt;/code&gt; 部分， Docker 凭据帮助工具(&lt;code&gt;credHelpers&lt;/code&gt; 或 &lt;code&gt;credsStore&lt;/code&gt;)是不支持的
{{ &lt;/note&gt; }}&lt;/p&gt;
&lt;p&gt;Docker 会将私有仓库的凭据存储在 &lt;code&gt;$HOME/.dockercfg&lt;/code&gt; 或 &lt;code&gt;$HOME/.docker/config.json&lt;/code&gt; 文件中，kubelet 在拉取镜像时也可以从以下列表中搜寻凭据配置文件:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;{--root-dir:-/var/lib/kubelet}/config.json&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;{cwd of kubelet}/config.json&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;${HOME}/.docker/config.json&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/.docker/config.json&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;{--root-dir:-/var/lib/kubelet}/.dockercfg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;{cwd of kubelet}/.dockercfg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;${HOME}/.dockercfg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/.dockercfg&lt;/code&gt;
{{ &lt;note&gt; }}
用户可以在 kubelet 进行的环境变量上显示地设置 HOME=/root
{{ &lt;/note&gt; }}&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下为为节点设置私有仓库凭据的推荐方式。 本示例运行在控制机/笔记本上：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为每一个想要使用的凭据执行命令 &lt;code&gt;docker login [server]&lt;/code&gt;，该命令会更本机的 &lt;code&gt;$HOME/.docker/config.json&lt;/code&gt; 文件&lt;/li&gt;
&lt;li&gt;在文本编辑器中查看 &lt;code&gt;$HOME/.docker/config.json&lt;/code&gt; 文件，保证其中只包含需要使用到的凭据。&lt;/li&gt;
&lt;li&gt;获取节点列表; 例如:
&lt;ul&gt;
&lt;li&gt;获取节点名称: &lt;code&gt;nodes=$( kubectl get nodes -o jsonpath=&#39;{range.items[*].metadata}{.name} {end}&#39; )&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;获取节点IP: &lt;code&gt;nodes=$( kubectl get nodes -o jsonpath=&#39;{range .items[*].status.addresses[?(@.type==&amp;quot;ExternalIP&amp;quot;)]}{.address} {end}&#39; )&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;将本地 &lt;code&gt;.docker/config.json&lt;/code&gt; 文件拷贝到以上列举的节点的凭据搜索目录中的一个
&lt;ul&gt;
&lt;li&gt;例如: &lt;code&gt;for n in $nodes; do scp ~/.docker/config.json root@&amp;quot;$n&amp;quot;:/var/lib/kubelet/config.json; done&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;{{ &lt;note&gt; }}
在生成环境集群中，使用配置管理工具来让配置拷贝到所有需要的节点上
{{ &lt;/note&gt; }}&lt;/p&gt;
&lt;p&gt;要验证配置是否正确，则使用私有仓库中的镜像来创建一个Pod，如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl apply -f - &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt;EOF
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;apiVersion: v1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kind: Pod
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;metadata:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  name: private-image-test-1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;spec:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  containers:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    - name: uses-private-image
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      image: $PRIVATE_IMAGE_NAME
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      imagePullPolicy: Always
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      command: [ &amp;#34;echo&amp;#34;, &amp;#34;SUCCESS&amp;#34; ]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果所有配置正确，则在一会之后可以通过命令&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl logs private-image-test-1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SUCCESS
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果怀疑命令失败，则执行命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl describe pods/private-image-test-1 | grep &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Failed&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果有失败则输出类似如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; Fri, 26 Jun 2015 15:36:13 -0700    Fri, 26 Jun 2015 15:39:13 -0700    19    {kubelet node-i2hq}    spec.containers{uses-private-image}    failed        Failed to pull image &amp;quot;user/privaterepo:v1&amp;quot;: Error: image user/privaterepo:v1 not found
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;必须要保证所有节点都是使用的同一个 &lt;code&gt;.docker/config.json&lt;/code&gt; 文件， 否则 Pod 可能在某些节点成功，某些节点则失败。 例如，在使用节点自动扩容时， 每个实例模板都需要包含 &lt;code&gt;.docker/config.json&lt;/code&gt; 或挂载包含该文件的盘
当私有仓库的凭据被添加到&lt;code&gt;.docker/config.json&lt;/code&gt;后 所有的 Pod 都可以对其中配置的任意私有仓库中拉取镜像。&lt;/p&gt;
&lt;h3 id=&#34;预先拉取镜像&#34;&gt;预先拉取镜像&lt;/h3&gt;
&lt;p&gt;{{ &lt;note&gt; }}
这种方式适用于用于用户有权限对节点配置的情况，而且不适用于节点由云提供商管理且能够自动扩容的情况
{{ &lt;/note&gt; }}&lt;/p&gt;
&lt;p&gt;默认情况下， kubelet 会尝试从指定镜像仓库拉取每一个镜像， 但是在容器 imagePullPolicy 属性的值被设置为 &lt;code&gt;IfNotPresent&lt;/code&gt; 或 &lt;code&gt;Never&lt;/code&gt;时，则会使用本地镜像。 (preferentially or exclusively, respectively).&lt;/p&gt;
&lt;p&gt;如果想要用预先摘取的方式取代镜像仓库认证， 需要保证集群中所有节点上预先摘取到的所有镜像都必须一致。
这种方式可用于预先取镜像来达到提速的目的或代替私有镜像仓库认证。
所有的 Pod 都有权访问任意预先拉取的镜像&lt;/p&gt;
&lt;h3 id=&#34;在-pod-上设置-imagepullsecrets&#34;&gt;在 Pod 上设置 &lt;code&gt;ImagePullSecrets&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;{{ &lt;note&gt; }}
这是使用私有仓库镜像的推荐方式
{{ &lt;/note&gt; }}&lt;/p&gt;
&lt;p&gt;k8s 支持在 Pod 上配置私有镜像仓库凭据&lt;/p&gt;
&lt;h4 id=&#34;创建带-docker-配置的-secret&#34;&gt;创建带 Docker 配置的 Secret&lt;/h4&gt;
&lt;p&gt;替换命令中的大写值为对应的配置，并运行此命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl create secret docker-registry &amp;lt;name&amp;gt; --docker-server&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;DOCKER_REGISTRY_SERVER --docker-username&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;DOCKER_USER --docker-password&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;DOCKER_PASSWORD --docker-email&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;DOCKER_EMAIL
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果已经有 Docker 凭据，可以不用以上命令，直接使用凭据文件创建对应的 Secret.
具体配置见&lt;a href=&#34;../../../3-tasks/02-configure-pod-container/11-pull-image-private-registry/#registry-secret-existing-credentials&#34;&gt;这里&lt;/a&gt;
这种配置方式尤其适用有多个私有镜像仓库的情况，因为 &lt;code&gt;kubectl create secret docker-registry&lt;/code&gt; 创建 Secret 的方式只适用于单个私有镜像仓库的情况。
{{ &lt;note&gt; }}
Pod 只能引用当前命名空间内的 &lt;code&gt;Secret&lt;/code&gt; , 因此需要在每个命名空间都需要创建 &lt;code&gt;Secret&lt;/code&gt;
{{ &lt;/note&gt; }}&lt;/p&gt;
&lt;h3 id=&#34;在-pod-中使用-imagepullsecrets&#34;&gt;在 Pod 中使用 &lt;code&gt;imagePullSecrets&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;当 &lt;code&gt;Secret&lt;/code&gt; 创建好后，可以配置 &lt;code&gt;imagePullSecrets&lt;/code&gt; 使用该 &lt;code&gt;Secret&lt;/code&gt;, 例如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt;EOF &amp;gt; pod.yaml
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;apiVersion: v1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;kind: Pod
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;metadata:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  name: foo
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  namespace: awesomeapps
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;spec:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  containers:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    - name: foo
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;      image: janedoe/awesomeapp:v1
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  imagePullSecrets:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    - name: myregistrykey
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;

cat &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; ./kustomization.yaml
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;resources:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;- pod.yaml
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;EOF&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;需要在每个用到私有镜像仓库的 Pod 都需要该配置。
也可以在 &lt;code&gt;ServiceAccount&lt;/code&gt; 设置 &lt;code&gt;imagePullSecrets&lt;/code&gt; 可以使用对应的 Pod 自动添加该属性
在 &lt;code&gt;ServiceAccount&lt;/code&gt; 设置 &lt;code&gt;imagePullSecrets&lt;/code&gt;具体见&lt;a href=&#34;../../../3-tasks/02-configure-pod-container/10-configure-service-account/#add-imagepullsecrets-to-a-service-account&#34;&gt;这里&lt;/a&gt;
这个配置可与节点上的 &lt;code&gt;.docker/config.json&lt;/code&gt; 配合使用，两个配置的凭据全合并到一起。&lt;/p&gt;
&lt;h2 id=&#34;应用场景&#34;&gt;应用场景&lt;/h2&gt;
&lt;p&gt;配置私有仓库镜像的方式有多种，以下为常用应用场景和推荐方案。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;集群只使用开放(如：开源)镜像， 不需要私有仓库。
- 使用 Docker Hub 上的公有镜像
&lt;ul&gt;
&lt;li&gt;不需要配置&lt;/li&gt;
&lt;li&gt;一些云提供商会自动缓存或镜像开放镜像， 可以提高可用性并减少拉取镜像的时间&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;集群使用到的镜像对外私有，对内公开
- 使用自建私有镜像仓库
&lt;ul&gt;
&lt;li&gt;可以托管在 Docker Hub 或其它地方&lt;/li&gt;
&lt;li&gt;使用上面介结的在每个节点配置 &lt;code&gt;.docker/config.json&lt;/code&gt; 的方式
- 在内网运行一个开放的内部私有镜像仓库&lt;/li&gt;
&lt;li&gt;不需要在 k8s 上做配置
- 使用一个有访问控制的镜像仓库&lt;/li&gt;
&lt;li&gt;在节点自动扩容的场景下会比手动更佳
- 在节点配置不方便的集群中使用 &lt;code&gt;imagePullSecrets&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;镜像仓库需要更严格的访问控制
- 需要打开  &lt;a href=&#34;../../../reference/03-access-authn-authz/04-admission-controllers/#alwayspullimages&#34;&gt;AlwaysPullImages&lt;/a&gt; admission controller， 否则所有 Pod 默认对所有镜像有访问权限
- 将敏感数据放的 Secret 中， 还是是打在镜像中&lt;/li&gt;
&lt;li&gt;多租户集群，每个租户需要独立的私有镜像仓库
- 需要打开  &lt;a href=&#34;../../../reference/03-access-authn-authz/04-admission-controllers/#alwayspullimages&#34;&gt;AlwaysPullImages&lt;/a&gt; admission controller， 否则所有租户的所有 Pod 默认对所有镜像有访问权限
- 私有镜像仓库需要有认证系统
- 为每个租户生成私有镜像仓库凭据，并在每个租户的命名空间中创建对应 Secret.
- 各命名空间的租户将名称的 Secret 配置到 &lt;code&gt;imagePullSecrets&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果用到的多个私有镜像仓库， 可以对每个仓库创建一个 Secret, kubelet 会将所有 imagePullSecrets 合并到一个虚拟的 &lt;code&gt;.docker/config.json&lt;/code&gt; 中。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 容器环境</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/containers/01-container-environment/</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/containers/01-container-environment/</guid>
      <description>
        
        
        &lt;!-- overview --&gt;
&lt;p&gt;本文主要介绍以环境提供容器的资源信息&lt;/p&gt;
&lt;!-- body --&gt;
&lt;h2 id=&#34;容器内的环境变量&#34;&gt;容器内的环境变量&lt;/h2&gt;
&lt;p&gt;k8s 环境为容器提供了一些重要的资源信息：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个包含&lt;a href=&#34;../00-images/&#34;&gt;镜像&lt;/a&gt;和一个或多个&lt;a href=&#34;../../05-storage/00-volumes/&#34;&gt;卷&lt;/a&gt;组合成的文件系统&lt;/li&gt;
&lt;li&gt;关于容器自身的相关信息&lt;/li&gt;
&lt;li&gt;关于集群中其它对象的相关信息&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;容器信息&#34;&gt;容器信息&lt;/h3&gt;
&lt;p&gt;容器的主机名，也就是容器运行的 Pod 的名称。 可以通过 &lt;code&gt;hostname&lt;/code&gt; 命令或 &lt;code&gt;libc&lt;/code&gt; 库中的 &lt;code&gt;gethostname&lt;/code&gt; 函数获得
Pod 的名称和所在的命名空间可能 &lt;a href=&#34;../../../3-tasks/04-inject-data-application/04-downward-api-volume-expose-pod-information/#the-downward-api&#34;&gt;downward API&lt;/a&gt;以环境变量方式访问
在 Pod 定义中用户自定义的环境变量和Docker 镜像中的环境变量都会成为容器中的环境变量。&lt;/p&gt;
&lt;h3 id=&#34;集群信息&#34;&gt;集群信息&lt;/h3&gt;
&lt;p&gt;在容器创建之前的所有 Service 列表会以环境变量方式加入容器。 这些环境变量与 Docker link 的语法一致。
例如 在一个叫 bar 的容器中加一个叫 foo 的 Service, 会加入以下环境变量:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-env&#34; data-lang=&#34;env&#34;&gt;FOO_SERVICE_HOST=&amp;lt;the host the service is running on&amp;gt;
FOO_SERVICE_PORT=&amp;lt;the port the service is running on&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果集群开启了 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/&#34;&gt;DNS 插件&lt;/a&gt;，容器也可能通过 DNS 方式获取 Service 的 IP 地址。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../03-container-lifecycle-hooks/&#34;&gt;容器生命周期钩子&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../../../3-tasks/02-configure-pod-container/16-attach-handler-lifecycle-event/&#34;&gt;生命周期事件处理器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Runtime Class</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/containers/02-runtime-class/</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/containers/02-runtime-class/</guid>
      <description>
        
        
        &lt;!-- overview --&gt;
&lt;p&gt;




&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [beta]&lt;/code&gt;
&lt;/div&gt;


本文介绍 &lt;code&gt;RuntimeClass&lt;/code&gt; 资源和运行时选择机制。
&lt;code&gt;RuntimeClass&lt;/code&gt; 是一个选择容器运行时配置的特性。而容器运行时配置则用于运行 Pod 中的容器的。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;用户可以在不同的 Pod 上配置不同的 &lt;code&gt;RuntimeClass&lt;/code&gt; 以达成性能与安全之间的平衡。 例如，有一部分工作负载需要一个高级别的信息安全保降。这时就需要选择装 Pod 调度到运行在物理虚拟化的容器运行时。 这样才有相对其它运行时更高的隔离级别同时也会有额外的资源开销。
&lt;code&gt;RuntimeClass&lt;/code&gt; 也可以用于在同一个运行时上，对不同的 Pod 使用不同的配置。&lt;/p&gt;
&lt;h2 id=&#34;设置&#34;&gt;设置&lt;/h2&gt;
&lt;p&gt;确保 &lt;code&gt;RuntimeClass&lt;/code&gt; 功能特性是开启的(默认开启)。关于如何开启或关闭功能特性见&lt;a href=&#34;../../../reference/command-line-tools-reference/feature-gates/&#34;&gt;这里&lt;/a&gt;。 apiserver 和 kubelet 的&lt;code&gt;RuntimeClass&lt;/code&gt; 功能特性必须要开启。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;配置节点上的 CRI 实现(各种运行时配置方式不同)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建相应的 &lt;code&gt;RuntimeClass&lt;/code&gt; 资源&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;配置节点上的 CRI 实现&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;基于 RuntimeClass 的配置因 容器运行时接口(CRI)具体实现的不同而不同。 具体配置文档见&lt;a href=&#34;#cri-configuration&#34;&gt;下面章节&lt;/a&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 默认情况下 &lt;code&gt;RuntimeClass&lt;/code&gt; 假定整个集群中所有节点的配置是相同的(也就是说所有节点针对容器运行时的配置是一样的)，为了支持这样的配置请见&lt;a href=&#34;#scheduling&#34;&gt;下面章节-调度&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

配置上有一个想对应的 handler 的名称， 被 &lt;code&gt;RuntimeClass&lt;/code&gt; 所引用。 这个字段的命名必须符合 DNS-1123 标签的标准(字母，数字，中划线(&lt;code&gt;-&lt;/code&gt;)组成)&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;创建相应的 &lt;code&gt;RuntimeClass&lt;/code&gt; 资源&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上一步提到的每个配置都有对应的 &lt;code&gt;handler&lt;/code&gt; 名称，用于区分不同的配置。 每一个 &lt;code&gt;handler&lt;/code&gt; 都会创建一个对就的 &lt;code&gt;RuntimeClass&lt;/code&gt; 对象。
&lt;code&gt;RuntimeClass&lt;/code&gt; 资源目前只有两个有意义的字段： 名称 (&lt;code&gt;metadata.name&lt;/code&gt;)和处理器 (&lt;code&gt;handler&lt;/code&gt;), 对象定义类似如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;node.k8s.io/v1beta1  # RuntimeClass 定义在 node.k8s.io API 组中&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;RuntimeClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myclass  # The name the RuntimeClass will be referenced by&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;# RuntimeClass is a non-namespaced resource&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;handler&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myconfiguration  # The name of the corresponding CRI configuration&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;RuntimeClass&lt;/code&gt; 对象命名必须是一个有效的 &lt;a href=&#34;../../00-overview/03-working-with-objects/01-names/#dns-subdomain-names&#34;&gt;DNS 子域名格式&lt;/a&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 建议只有集群管理员对 &lt;code&gt;RuntimeClass&lt;/code&gt; 对象有写权限(create/update/patch/delete). 一般情况这是默认配置。 更多信息见 &lt;a href=&#34;../../../reference/03-access-authn-authz/07-authorization/&#34;&gt;授权概览&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;h2 id=&#34;使用说明&#34;&gt;使用说明&lt;/h2&gt;
&lt;p&gt;当在集群中配置好了 &lt;code&gt;RuntimeClasses&lt;/code&gt;， 使用就是一件简单的事情，只需要在 Pod 定义中添加 &lt;code&gt;runtimeClassName&lt;/code&gt;就行， 例：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;runtimeClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myclass&lt;/span&gt;
  &lt;span style=&#34;color:#75715e&#34;&gt;# ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个配置会让 kublet 使用对应名称的 &lt;code&gt;RuntimeClass&lt;/code&gt; 来运行这个 Pod， 如果没有叫这个名称的 &lt;code&gt;RuntimeClass&lt;/code&gt;， 或者 CRI 不能运行这个名称对应的处理器，则这个进入 &lt;code&gt;Failed&lt;/code&gt; &lt;a href=&#34;../../03-workloads/00-pods/00-pod-lifecycle/#pod-phase&#34;&gt;状态&lt;/a&gt; 。 错误信息在对应的&lt;a href=&#34;../../../3-tasks/08-debug-application-cluster/00-debug-application-introspection/&#34;&gt;事件&lt;/a&gt;对象中
如果没有 runtimeClassName 字段则使用默认的运行时处理器，与 &lt;code&gt;RuntimeClass&lt;/code&gt; 功能特性关闭等同&lt;/p&gt;
&lt;h3 id=&#34;cri-配置&#34;&gt;CRI 配置&lt;/h3&gt;
&lt;p&gt;更多关于 CRI 运行时配置见&lt;a href=&#34;../../../1-setup/02-production-environment/00-container-runtimes/&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;dockershim&#34;&gt;dockershim&lt;/h4&gt;
&lt;p&gt;k8s 内置的 dockershim CRI 不支持运行时处理器&lt;/p&gt;
&lt;h4 id=&#34;containerd&#34;&gt;containerd&lt;/h4&gt;
&lt;p&gt;运行时处理器通过 containerd 的 &lt;code&gt;/etc/containerd/config.toml&lt;/code&gt; 配置文件时行配置， 有效的处理器配置在运行时配置区:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;[&lt;span style=&#34;color:#a6e22e&#34;&gt;plugins&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;cri&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;containerd&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;runtimes&lt;/span&gt;.&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;$&lt;/span&gt;{&lt;span style=&#34;color:#a6e22e&#34;&gt;HANDLER_NAME&lt;/span&gt;}]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;更多 containerd 的配置见文档:  &lt;a href=&#34;https://github.com/containerd/cri/blob/master/docs/config.md&#34;&gt;https://github.com/containerd/cri/blob/master/docs/config.md&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;cri-o&#34;&gt;CRI-O&lt;/h4&gt;
&lt;p&gt;运行时处理器通过 CRI-O 的 &lt;code&gt;/etc/crio/crio.conf&lt;/code&gt; 配置文件时行配置，有效的处理器配置在 &lt;a href=&#34;https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md#crioruntime-table&#34;&gt;crio.runtime table&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[crio.runtime.runtimes.${HANDLER_NAME}]
  runtime_path = &amp;quot;${PATH_TO_BINARY}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;更多 CRI-O 的配置见文档: &lt;a href=&#34;https://raw.githubusercontent.com/cri-o/cri-o/9f11d1d/docs/crio.conf.5.md&#34;&gt;https://raw.githubusercontent.com/cri-o/cri-o/9f11d1d/docs/crio.conf.5.md&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;调度&#34;&gt;调度&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;k8s v1.16， RuntimeClass 通过 scheduling 字段实现了对异构系统集群的支持. 通过对这些字段的配置实现让带有对应 RuntimeClass Pod 被调度到受到支持的节点上。而要实现支持这个调度，需要开启 &lt;a href=&#34;../../../reference/03-access-authn-authz/04-admission-controllers/#runtimeclass&#34;&gt;RuntimeClass 准入控制器&lt;/a&gt;(k8s v1.16 默认开启)&lt;/p&gt;
&lt;p&gt;为了保证 Pod 会落到 RuntimeClass 指定的节点上，需要要在节点上设置通用标签，让 &lt;code&gt;runtimeclass.scheduling.nodeSelector&lt;/code&gt; 可以选择到对应的节点。RuntimeClass 的节点选择器与 Pod 的节点选择器的交集会作为最终的节点选择条件。 如果两都有冲突，则 Pod 会被拒绝。
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

如果支持的节点上有防止其它 RuntimeClass 的 Pod 在其上运行的一毒点(Taint), 则需要在 RuntimeClass 上添加 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/&#39; target=&#39;_blank&#39;&gt;耐受(tolerations)&lt;span class=&#39;tooltip-text&#39;&gt;A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint.&lt;/span&gt;
&lt;/a&gt; 与 nodeSelector 相似， RuntimeClass 的耐受(tolerations) 会与 Pod 的 耐受(tolerations)的并集组成最终的节点&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/&#39; target=&#39;_blank&#39;&gt;耐受(tolerations)&lt;span class=&#39;tooltip-text&#39;&gt;A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint.&lt;/span&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;要了解更多关于节点选择与&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/&#39; target=&#39;_blank&#39;&gt;耐受(tolerations)&lt;span class=&#39;tooltip-text&#39;&gt;A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint.&lt;/span&gt;
&lt;/a&gt;信息，见 &lt;a href=&#34;../../09-scheduling-eviction/02-assign-pod-node/&#34;&gt;分配 Pod 到节点&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;pod-overhead&#34;&gt;Pod Overhead&lt;/h2&gt;
&lt;p&gt;




&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [beta]&lt;/code&gt;
&lt;/div&gt;


用户可以在 Pod 上配置 &lt;code&gt;overhead&lt;/code&gt; 进行资源限制。 声明 &lt;code&gt;overhead&lt;/code&gt; 可以让集群(包括调度器)在调度 Pod 时计算合适的资源。
要使用 Pod &lt;code&gt;overhead&lt;/code&gt; 需要打开功&lt;a href=&#34;../../../reference/command-line-tools-reference/feature-gates/&#34;&gt;能特性开关&lt;/a&gt;(默认开启) PodOverhead
Pod &lt;code&gt;overhead&lt;/code&gt; 是通过 RuntimeClass 的 overhead 字段进行定义的， 用户可以利用 RuntimeClass 来设置正在运行的 Pod的 &lt;code&gt;overhead&lt;/code&gt;， 并保证k8s 能够计算到这些 &lt;code&gt;overhead&lt;/code&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/runtime-class.md&#34;&gt;RuntimeClass Design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/runtime-class-scheduling.md&#34;&gt;RuntimeClass Scheduling Design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Read about the &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/pod-overhead/&#34;&gt;Pod Overhead&lt;/a&gt; concept&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/20190226-pod-overhead.md&#34;&gt;PodOverhead Feature Design&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 容器生命周期钩子</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/containers/container-lifecycle-hooks/</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/containers/container-lifecycle-hooks/</guid>
      <description>
        
        
        &lt;!-- overview --&gt;
&lt;!--
This page describes how kubelet managed Containers can use the Container lifecycle hook framework
to run code triggered by events during their management lifecycle.
--&gt;
&lt;p&gt;本文主要介绍如何使用容器生命周期钩子框架来实现由 kubelet 管理的容器通过在管理过程中的生命周期事件来触发代理运行。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;
&lt;!--
Analogous to many programming language frameworks that have component lifecycle hooks, such as Angular,
Kubernetes provides Containers with lifecycle hooks.
The hooks enable Containers to be aware of events in their management lifecycle
and run code implemented in a handler when the corresponding lifecycle hook is executed.
--&gt;
&lt;p&gt;与许多编程语言的框架的组件有生命周期钩子(如: &lt;code&gt;Angular&lt;/code&gt;)一样， k8s 也为容器提供了生命周期钩子。
生命周期钩子可以是容器收到自身生命周期管理时发生的事件，当这些事件发生时就会触发对应的钩子调用执行处理器中的代码实现&lt;/p&gt;
&lt;h2 id=&#34;容器钩子&#34;&gt;容器钩子&lt;/h2&gt;
&lt;!--
There are two hooks that are exposed to Containers:

`PostStart`

This hook executes immediately after a container is created.
However, there is no guarantee that the hook will execute before the container ENTRYPOINT.
No parameters are passed to the handler.

`PreStop`

This hook is called immediately before a container is terminated due to an API request or management event such as liveness probe failure, preemption, resource contention and others. A call to the preStop hook fails if the container is already in terminated or completed state.
It is blocking, meaning it is synchronous,
so it must complete before the call to delete the container can be sent.
No parameters are passed to the handler.

A more detailed description of the termination behavior can be found in
[Termination of Pods](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination).

--&gt;
&lt;p&gt;容器钩子有以下两个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;PostStart&lt;/code&gt;
这个钩子在容器创建后马上执行。
但是不能保证鑫子会在容器执行 &lt;code&gt;ENTRYPOINT&lt;/code&gt; 之前执行。&lt;/p&gt;
&lt;p&gt;这个钩子不会向处理器传递任何参数&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;PreStop&lt;/code&gt;
在容器被终结，比如因为一个 API 请求或诸如存活探针失败，优先级较低，资源竞争等原因，会立即执行这个钩子。 如果容器已经为终止或完成状态则调用 &lt;code&gt;preStop&lt;/code&gt; 钩子会失败。
该钩子会造成阻塞，也就是说它是同步的，所以必须在钩子调用处理器执行完成后，才能发送删除容器指令。&lt;/p&gt;
&lt;p&gt;这个钩子不会向处理器传递任何参数&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;钩子处理器的实现方式&#34;&gt;钩子处理器的实现方式&lt;/h3&gt;
&lt;!--
Containers can access a hook by implementing and registering a handler for that hook.
There are two types of hook handlers that can be implemented for Containers:

* Exec - Executes a specific command, such as `pre-stop.sh`, inside the cgroups and namespaces of the Container.
Resources consumed by the command are counted against the Container.
* HTTP - Executes an HTTP request against a specific endpoint on the Container.
--&gt;
&lt;p&gt;容器可以用过为钩子实现并注册处理器来达成钩子的使用，以下为容器可实现的两种类型的钩子处理器&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exec - 在容器的 &lt;code&gt;cgroups&lt;/code&gt; 和 &lt;code&gt;namespaces&lt;/code&gt; 权限内执行指定的命令，例如 &lt;code&gt;pre-stop.sh&lt;/code&gt;， 所需要的资源包含在容器申请的资源中&lt;/li&gt;
&lt;li&gt;HTTP - 向容器上的指定接口发送一个请求&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;钩子处理器的执行&#34;&gt;钩子处理器的执行&lt;/h3&gt;
&lt;!--
When a Container lifecycle management hook is called,
the Kubernetes management system executes the handler in the Container registered for that hook. 

Hook handler calls are synchronous within the context of the Pod containing the Container.
This means that for a `PostStart` hook,
the Container ENTRYPOINT and hook fire asynchronously.
However, if the hook takes too long to run or hangs,
the Container cannot reach a `running` state.

The behavior is similar for a `PreStop` hook.
If the hook hangs during execution,
the Pod phase stays in a `Terminating` state and is killed after `terminationGracePeriodSeconds` of pod ends.
If a `PostStart` or `PreStop` hook fails,
it kills the Container.

Users should make their hook handlers as lightweight as possible.
There are cases, however, when long running commands make sense,
such as when saving state prior to stopping a Container.
--&gt;
&lt;p&gt;当一个容器的生命周期管理钩子被调用时， k8s 管理系统就会执行注册到那个钩子上的处理器。&lt;/p&gt;
&lt;p&gt;钩子处理器调用在容器所在的 Pod 的上下文中是同步的。 也就是说对于一个 &lt;code&gt;PostStart&lt;/code&gt; 钩子， 它与容器的 &lt;code&gt;ENTRYPOINT&lt;/code&gt; 是异步执行的。
但是，如果钩子执行时间过长或挂起，容器便不会进入到 &lt;code&gt;running&lt;/code&gt; 的状态。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PreStop&lt;/code&gt; 钩子的行为模式也是相似的。 如果钩子在执行过程中挂起，则 Pod 在终结前的 &lt;code&gt;terminationGracePeriodSeconds&lt;/code&gt; 时间内都是 &lt;code&gt;Terminating&lt;/code&gt; 状态。
如果 &lt;code&gt;PostStart&lt;/code&gt; 或 &lt;code&gt;PreStop&lt;/code&gt; 执行失败，会导致对应容器被杀死&lt;/p&gt;
&lt;p&gt;用户应该尽可能地让钩子处理器越轻量越好。 但是也有些场景，运行长时间的钩子命令是有意义的，比如当保存状态比停止容器更重要时&lt;/p&gt;
&lt;h3 id=&#34;钩子的送达可靠性&#34;&gt;钩子的送达可靠性&lt;/h3&gt;
&lt;!--
Hook delivery is intended to be *at least once*,
which means that a hook may be called multiple times for any given event,
such as for `PostStart` or `PreStop`.
It is up to the hook implementation to handle this correctly.

Generally, only single deliveries are made.
If, for example, an HTTP hook receiver is down and is unable to take traffic,
there is no attempt to resend.
In some rare cases, however, double delivery may occur.
For instance, if a kubelet restarts in the middle of sending a hook,
the hook might be resent after the kubelet comes back up.
--&gt;
&lt;p&gt;对于 &lt;code&gt;PostStart&lt;/code&gt; 或 &lt;code&gt;PreStop&lt;/code&gt; 钩子投递被定为 &lt;em&gt;至少一次&lt;/em&gt;， 也就是说对于任意一个事件可能多次调用钩子。 这就需要实现的钩子处理器需要正确的应对这种情况。&lt;/p&gt;
&lt;p&gt;通常情况下，只能投递一次。如果钩子所调用的 HTTP 目标接口不可用，这种情况下不会尝试重发。 在有些不常见的场景中，也可能发生两次投递的情况。到目前为止，如果 kubelet 在钩子发送过程中重启，这个钩子可能在 kubelet 启动后再次发送&lt;/p&gt;
&lt;h3 id=&#34;如何调试钩子处理器&#34;&gt;如何调试钩子处理器&lt;/h3&gt;
&lt;!--
The logs for a Hook handler are not exposed in Pod events.
If a handler fails for some reason, it broadcasts an event.
For `PostStart`, this is the `FailedPostStartHook` event,
and for `PreStop`, this is the `FailedPreStopHook` event.
You can see these events by running `kubectl describe pod &lt;pod_name&gt;`.
Here is some example output of events from running this command:
--&gt;
&lt;p&gt;钩子处理器的日志不会输出到 Pod 的事件中。 如果一个处理器因为某些原因失败，会发送一个广播事件。 &lt;code&gt;PostStart&lt;/code&gt; 是 &lt;code&gt;FailedPostStartHook&lt;/code&gt;事件， &lt;code&gt;PreStop&lt;/code&gt;是 &lt;code&gt;FailedPreStopHook&lt;/code&gt;事件。 可以通过执行命令 &lt;code&gt;kubectl describe pod &amp;lt;pod_name&amp;gt;&lt;/code&gt; 查看这些事件， 命令输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Events:
  FirstSeen  LastSeen  Count  From                                                   SubObjectPath          Type      Reason               Message
  ---------  --------  -----  ----                                                   -------------          --------  ------               -------
  1m         1m        1      {default-scheduler }                                                          Normal    Scheduled            Successfully assigned test-1730497541-cq1d2 to gke-test-cluster-default-pool-a07e5d30-siqd
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Pulling              pulling image &amp;quot;test:1.0&amp;quot;
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Created              Created container with docker id 5c6a256a2567; Security:[seccomp=unconfined]
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Pulled               Successfully pulled image &amp;quot;test:1.0&amp;quot;
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Started              Started container with docker id 5c6a256a2567
  38s        38s       1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Killing              Killing container with docker id 5c6a256a2567: PostStart handler: Error executing in Docker Container: 1
  37s        37s       1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Killing              Killing container with docker id 8df9fdfd7054: PostStart handler: Error executing in Docker Container: 1
  38s        37s       2      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}                         Warning   FailedSync           Error syncing pod, skipping: failed to &amp;quot;StartContainer&amp;quot; for &amp;quot;main&amp;quot; with RunContainerError: &amp;quot;PostStart handler: Error executing in Docker Container: 1&amp;quot;
  1m         22s       2      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Warning   FailedPostStartHook
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实践指导
&lt;a href=&#34;../../../3-tasks/02-configure-pod-container/16-attach-handler-lifecycle-event/&#34;&gt;挂载处理器到容器生命周期事件&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: k8s 对象介绍</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/00-kubernetes-objects/</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/00-kubernetes-objects/</guid>
      <description>
        
        
        &lt;p&gt;本文介绍 k8s 对象 是怎么在 k8s API 中表示的，怎么以 &lt;code&gt;.yaml&lt;/code&gt; 格式输出 k8s 对象&lt;/p&gt;
&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;
&lt;p&gt;k8s 对象是 k8s 系统中持久化的实体， k8s 使用这些实体来表示集群的状态，它们具体可以表示如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;哪些容器的应用在(如个节点上)运行&lt;/li&gt;
&lt;li&gt;可用于运行应用的资源&lt;/li&gt;
&lt;li&gt;应用的行为策略，比如重启策略，升级，容错性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;k8s 对象是对用户一个意图的记录，当用户创建一个对象后，系统需要保证对象持续存在。用户通过创建一个来告知 k8s 需要一个什么样的 工作负载(workload), 也就是集群的期望状态(desired state)&lt;/p&gt;
&lt;p&gt;要实现对 k8s 对象的管理，比如增删改查都需要调用 k8s API, 例如，当用户可以通过 kubectl 命令来实现对API的调用。也可以通过自己写程序实现对 k8s API 的调用，调用库详见(&lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/client-libraries/&#34;&gt;https://kubernetes.io/docs/reference/using-api/client-libraries/&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;对象的-specstatus-属性&#34;&gt;对象的 &lt;code&gt;Spec&lt;/code&gt;，&lt;code&gt;Status&lt;/code&gt; 属性&lt;/h2&gt;
&lt;p&gt;基本上所有的 k8s 都包含两个嵌套对象作为属性用于管理对象的配置，其中一个对象为 &lt;code&gt;spec&lt;/code&gt;， 另一个为 &lt;code&gt;status&lt;/code&gt;， 用户在创建对象进设置 &lt;code&gt;spec&lt;/code&gt; 来定义所需资源的特性，也就是集群的期望状态。&lt;/p&gt;
&lt;p&gt;而 &lt;code&gt;status&lt;/code&gt; 字段，则是对象的当前的实际状态，由 k8s 系统及其组件进行提供和修改。 k8s 控制中心的任务就是始终让所有对象的实际状态与期望状态一致。&lt;/p&gt;
&lt;p&gt;例如: 在 k8s 中， 一个  Deployment 对象表示运行有用户集群中的一个应用，当用户创建一个 Deployment 并在 spec 对象中设置应用副本数为 3时， k8s 会读取对象属性，启动用户所期望的三个实例并更新相应的状态以达成与 spec 配置的一致。 如果其它任意一个实例失效(某一状态发生变化)， k8s 将会对 spec 与 status 之间的差异采取行动，在当前描述的情况下就会再启动一个实例代替失效的实例。更多关于对象 &lt;code&gt;spec&lt;/code&gt;, &lt;code&gt;status&lt;/code&gt;, &lt;code&gt;metadata&lt;/code&gt; 相关信息看&lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;怎么描述一个-k8s-对象&#34;&gt;怎么描述一个 k8s 对象&lt;/h2&gt;
&lt;p&gt;用户在创建 k8s 对象时，必须要提供描述期望状态的 &lt;code&gt;spec&lt;/code&gt;, 同时还需要该对象的基础信息，比如名称。 当用户使用 API 创建对象时(无论是直接调用还是通过 &lt;code&gt;kubectl&lt;/code&gt; ), 对象信息都为以JSON格式作为请求的消息体发送给 API.&lt;/p&gt;
&lt;p&gt;一般情况下使用 &lt;code&gt;yaml&lt;/code&gt; 文件为 &lt;code&gt;kubectl&lt;/code&gt;提供信息, 此时kubectl 会将 &lt;code&gt;.yaml&lt;/code&gt; 格式转化为JSON格式然后对 k8s API 发起请求&lt;/p&gt;
&lt;p&gt;以下是示例为创建一个 Deployment 必要字段和对象 &lt;code&gt;spec&lt;/code&gt;的 &lt;code&gt;yaml&lt;/code&gt; 文件：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1 # 集群版本 &amp;lt; 1.9.0 使用 apps/v1beta2&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx-deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# 需要按以下模板运行 3 个 Pod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx:1.14.2&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;要使用以上的 &lt;code&gt;yaml&lt;/code&gt; 文件创建一个 Deployment，一种方式是通过 &lt;code&gt;kubectl apply&lt;/code&gt; 命令，并将这个 &lt;code&gt;yaml&lt;/code&gt; 文件作为参数&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl apply -f nginx-deployment.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;命令输出如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment created --record
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;必要字段&#34;&gt;必要字段&lt;/h2&gt;
&lt;p&gt;在使用 &lt;code&gt;.yaml&lt;/code&gt; 创建对象时，以下是必要字段:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;apiVersion&lt;/code&gt; 使用哪个版本的 k8s API 创建这个对象&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kind&lt;/code&gt; 创建对象的类型&lt;/li&gt;
&lt;li&gt;&lt;code&gt;metadata&lt;/code&gt; 唯一标识对象的信息，
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;name&lt;/code&gt; 字符串&lt;/li&gt;
&lt;li&gt;&lt;code&gt;UID&lt;/code&gt; (系统会生成?)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;namespace&lt;/code&gt;, 可选，默认 default&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spec&lt;/code&gt; 对象实际定义(期望)， 每类对象不一样&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中 &lt;code&gt;spec&lt;/code&gt; 字段值是一个嵌套对象，其字段因不同的对象类型而有不同。&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/&#34;&gt;这个文档&lt;/a&gt;包含k8s 所有对象的创建, 比如 &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#podspec-v1-core&#34;&gt;这里是Pod的 spec 详情&lt;/a&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#deploymentspec-v1-apps&#34;&gt;这里是Deployment的 spec 详情&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;源文件&#34;&gt;源文件&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/&#34;&gt;https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;引申阅读&#34;&gt;引申阅读&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/api-overview/&#34;&gt;k8s API 概念说明&lt;/a&gt;
&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/&#34;&gt;k8s 最重要基础概念 Pod&lt;/a&gt;
&lt;a href=&#34;https://kubernetes.io/docs/concepts/architecture/controller/&#34;&gt;k8s 的控制器&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 对象命令与ID</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/names/</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/names/</guid>
      <description>
        
        
        &lt;p&gt;在 k8s 的对象中，同一类型的对象名称唯一， 比如在一个 &lt;a href=&#34;./02-namespace&#34;&gt;命名空间&lt;/a&gt;中只能有一个叫 &lt;code&gt;myapp-1234&lt;/code&gt; 在 Pod， 但不同类型的资源可以有相同的名称，比如还可以定义一个叫 &lt;code&gt;myapp-1234&lt;/code&gt; 在 Deployment。
每个对象也有一个 &lt;code&gt;UID&lt;/code&gt; 这个 &lt;code&gt;UID&lt;/code&gt; 整个集群全局唯一
用户需要定义非唯一的，用户自定义的属性，则可通过 &lt;a href=&#34;./03-label-selectors&#34;&gt;标签&lt;/a&gt; 和 &lt;a href=&#34;04-annotation&#34;&gt;注解&lt;/a&gt; 实现&lt;/p&gt;
&lt;h1 id=&#34;对象名称&#34;&gt;对象名称&lt;/h1&gt;
&lt;p&gt;对象的名称是一个字符串，体现在对象的 URL中， 比如 &lt;code&gt;/api/v1/pods/some-name&lt;/code&gt;
在同一时间，一个类型的对象名称必须唯一，但如果用户删除的这个对象，则可以用这个名称再创建一个新的对象&lt;/p&gt;
&lt;p&gt;以下是命令规范三类限制&lt;/p&gt;
&lt;h3 id=&#34;dns-subdomain-names&#34;&gt;DNS 子域名&lt;/h3&gt;
&lt;p&gt;多数类型的资源命令必须可以作为 DNS 子域名，定义在这里(&lt;a href=&#34;https://tools.ietf.org/html/rfc1123),&#34;&gt;https://tools.ietf.org/html/rfc1123),&lt;/a&gt; 总结如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不能多于 253个字符&lt;/li&gt;
&lt;li&gt;只能包含小写字母，数字， &lt;code&gt;-&lt;/code&gt;(中划线)，&lt;code&gt;.&lt;/code&gt;(点)&lt;/li&gt;
&lt;li&gt;只能以字母数字开头&lt;/li&gt;
&lt;li&gt;只能以字母数字结尾&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更多资料见 &lt;a href=&#34;https://en.wikipedia.org/wiki/Subdomain&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;dns-标签名&#34;&gt;DNS 标签名&lt;/h3&gt;
&lt;p&gt;有些资源类型命令遵循 DNS 标签名称， 规则如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最多 63 个字符&lt;/li&gt;
&lt;li&gt;只能包含小写字母，数字， &lt;code&gt;-&lt;/code&gt;(中划线)&lt;/li&gt;
&lt;li&gt;只能以字母数字开头&lt;/li&gt;
&lt;li&gt;只能以字母数字结尾&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;路径分段名&#34;&gt;路径分段名&lt;/h3&gt;
&lt;p&gt;有些资源类型的命令必须要能够编码到 路径的一段上，所以名称不能包含 &lt;code&gt;.&lt;/code&gt; 或 &lt;code&gt;..&lt;/code&gt;, 也不能包含 &lt;code&gt;/&lt;/code&gt; 或 &lt;code&gt;%&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;以下为一个命令为 &lt;code&gt;nginx-demo&lt;/code&gt; 的 Pod 示例&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx-demo&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx:1.14.2&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意: 还有一些资源类型有更严格的命名规则&lt;/p&gt;
&lt;h2 id=&#34;uids&#34;&gt;UIDs&lt;/h2&gt;
&lt;p&gt;由系统创建的对象的唯一标识，类型为字符串
k8s 集群整个生命周期内，创建的每一个对象的UID都是唯一的，用于区分可能存在或曾今过的相似对象
k8s UID 是 UUID， 使用标准为  &lt;code&gt;ISO/IEC 9834-8&lt;/code&gt;  和 &lt;code&gt;ITU-T X.667&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;引申阅读&#34;&gt;引申阅读&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;./03-label-selectors&#34;&gt;k8s 标签&lt;/a&gt;
&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/architecture/identifiers.md&#34;&gt;k8s 标识符与命令设计文档&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 命名空间(Namespaces)</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/02-namespace/</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/02-namespace/</guid>
      <description>
        
        
        &lt;p&gt;k8s 可以在一个物理集群上创建多个虚拟集群, 每个命名空间就是一个虚拟集群&lt;/p&gt;
&lt;h2 id=&#34;啥时候用命名空间&#34;&gt;啥时候用命名空间&lt;/h2&gt;
&lt;p&gt;命名空间是为用户分散于有多个组或项目下的场景设计的。 如果只有二三十个用户就不用想了，当需要用到命名空间的特性时才考虑用命名空间
命名空间是对象名称的一个作用域，对象命名只需要在一个命名空间唯一即可，命名空间不可以嵌套且一个资源对象只能属于一个命名空间
命名空间是集群中多个(组)用户分配资源的一个方式(&lt;a href=&#34;../../../08-policy/01-resource-quotas&#34;&gt;通过资源配额&lt;/a&gt;)
未来版本，一个命名空间下的对象可能有相同的默认访问控制策略
不必要用命名空间来区分差异较小的资源，比如同一个软件的不同版本，可以同一个命名空间下使用标签(labels)区分这些对象&lt;/p&gt;
&lt;h2 id=&#34;管理命名空间&#34;&gt;管理命名空间&lt;/h2&gt;
&lt;p&gt;命名空间的创建和删除请见&lt;a href=&#34;../../../../3-tasks/01-administer-cluster/34-namespaces&#34;&gt;管理指南命名空间部分&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;注意: 在自定义命名空间是，避免使用 &lt;code&gt;kube-&lt;/code&gt; 作为前缀， 这个前缀是 k8s 命名空间保留字&lt;/p&gt;
&lt;h3 id=&#34;查看&#34;&gt;查看&lt;/h3&gt;
&lt;p&gt;可以通过以下命令查看当前集群所有命名空间:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get namespaces
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME              STATUS   AGE
default           Active   1d
kube-node-lease   Active   1d
kube-public       Active   1d
kube-system       Active   1d
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;k8s 启动时会初始化创建以下4个命名空间:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;default：&lt;/code&gt; 当对象不指定命名空间时所属的命名空间&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kube-system&lt;/code&gt;： 由系统创建对象所在的命名空间&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kube-public&lt;/code&gt;：可以被所有人访问(包括未授权用户)，一般只能被系统使用，在这个命名空间的对象，整个集群都可访问，公开只是为了方便，不是强制要求&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kube-node-lease&lt;/code&gt;: 用于放置用于存放与每个节点关联的租约对象，在集群扩容时改善节点心跳性能&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;在请求中添加命名空间&#34;&gt;在请求中添加命名空间&lt;/h3&gt;
&lt;p&gt;为当前请求添加命名空间 使用 &lt;code&gt;--namespace&lt;/code&gt; 参数
示例如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl run nginx --image&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx --namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&amp;lt;insert-namespace-name-here&amp;gt;
kubectl get pods --namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&amp;lt;insert-namespace-name-here&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;设置默认命名空间&#34;&gt;设置默认命名空间&lt;/h3&gt;
&lt;p&gt;持久化配置 kubectl 默认操作的命名空间&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl config set-context --current --namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&amp;lt;insert-namespace-name-here&amp;gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Validate it&lt;/span&gt;
kubectl config view --minify | grep namespace:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;命名空间与dns的关系&#34;&gt;命名空间与DNS的关系&lt;/h3&gt;
&lt;p&gt;当用户创建 &lt;a href=&#34;../../../services-networking/service/&#34;&gt;Service&lt;/a&gt; 时会对应生成一条 &lt;a href=&#34;../../../services-networking/03-dns-pod-service/&#34;&gt;DNS 记录&lt;/a&gt;, 而这条记录中的格式为 &lt;code&gt;&amp;lt;service-name&amp;gt;.&amp;lt;namespace-name&amp;gt;.svc.cluster.local&lt;/code&gt; 也就是说 通过 &lt;code&gt;&amp;lt;service-name&amp;gt;&lt;/code&gt; 只能解析到本命名空间的服务，这在不同命名空间使用同一套配置时相关有用，比如开发，演示，生产等不同环境。如果跨命名空间访问 Service 需要使用全限定名(FQDN)，一般来说只需要 &lt;code&gt;&amp;lt;service-name&amp;gt;.&amp;lt;namespace-name&amp;gt;&lt;/code&gt; 也是可以的&lt;/p&gt;
&lt;h2 id=&#34;那些不属于任何命名空间的对象&#34;&gt;那些不属于任何命名空间的对象&lt;/h2&gt;
&lt;p&gt;大多数 k8s 资源( pod, service, replication controller 等)都会属于某一个命名空间。 而 命名空间 资源则不属于任何命名空间。 还有一个底层资源 比如 &lt;a href=&#34;../../../01-architecture/00-nodes/&#34;&gt;节点&lt;/a&gt;， persistentVolumes 也不属于任何命名空间&lt;/p&gt;
&lt;p&gt;以下命令可以查看资源是否属于命名空间:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 在一个命名空间中&lt;/span&gt;
kubectl api-resources --namespaced&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;true

&lt;span style=&#34;color:#75715e&#34;&gt;# 不属于任何命名空间&lt;/span&gt;
kubectl api-resources --namespaced&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Node&lt;/li&gt;
&lt;li&gt;persistentVolumes&lt;/li&gt;
&lt;li&gt;Events: 根据事件关联的对象，可能有属于命名空间也可能不属于命名空间&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 标签和标签选择器</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/labels/</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/labels/</guid>
      <description>
        
        
        &lt;p&gt;标签是附加在对象上的键值对，应该与所在的对象有关且具有意义
标签可以用于组织和筛选一组对象
标签可以在对象创建时就定义，也可以在对象创建后任何时间添加或修改
一个对象可以有多个标签，但同一个对象所有标签的名称必须唯一
标签可以让UI或命令行工具快速地查询和监听对象，所以标签不适用于非标识性的内容，非标识性内容应该使用注解(&lt;a href=&#34;../04-annotation/&#34;&gt;annotations&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;动机&#34;&gt;动机&lt;/h2&gt;
&lt;p&gt;标签让用户可以以松耦合的形式把组织架构映射到系统对象上，而不需要客户端存在这些映射
服务部署和批处理流水线通常胡是多维的实体(比如: 多个分区或部署， 多条发布线， 多个层级， 每个次级又有多个微服务)。 要管理这些对象经常需要多维度分割操作， 这就需要打破严格的层级结构的表现形式， 特别是由基础设施而不是由用户决定的死板的层级结构&lt;/p&gt;
&lt;p&gt;常用标签示例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;release&amp;quot; : &amp;quot;stable&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;release&amp;quot; : &amp;quot;canary&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;environment&amp;quot; : &amp;quot;dev&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;environment&amp;quot; : &amp;quot;qa&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;environment&amp;quot; : &amp;quot;production&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;tier&amp;quot; : &amp;quot;frontend&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;tier&amp;quot; : &amp;quot;backend&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;tier&amp;quot; : &amp;quot;cache&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;partition&amp;quot; : &amp;quot;customerA&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;partition&amp;quot; : &amp;quot;customerB&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;track&amp;quot; : &amp;quot;daily&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;track&amp;quot; : &amp;quot;weekly&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;syntax-and-character-set&#34;&gt;语法和字符集&lt;/h2&gt;
&lt;p&gt;标签由 键值对组成。 合法的键可以由两上部分组成， 一个可选的前级加上本身的名称中间用斜线(&lt;code&gt;/&lt;/code&gt;)分隔。
名称部分 不得多于63个字符，必须以字母或数字 ([a-z0-9A-Z])开头和结束，中间部分可以包含中划线(&lt;code&gt;-&lt;/code&gt;)，下划线 &lt;code&gt;(_)&lt;/code&gt;，点 (&lt;code&gt;.&lt;/code&gt;),字母,数字。
如果要使用前缀， 前端必须是一个合法的 DNS 字域名，由多个 DNS 标签组成，中间由点(&lt;code&gt;.&lt;/code&gt;)分隔， 总长度不超过 253 个字符&lt;/p&gt;
&lt;p&gt;如果一个标签键没有前缀则假定它是属于用于私有的，
由系统自动化组件(e.g. &lt;code&gt;kube-scheduler&lt;/code&gt;, &lt;code&gt;kube-controller-manager&lt;/code&gt;, &lt;code&gt;kube-apiserver&lt;/code&gt;, &lt;code&gt;kubectl&lt;/code&gt;, 或其它第三方自动化工具), 在给用户对象加标签时必须加前缀， &lt;code&gt;kubernetes.io/&lt;/code&gt;和 &lt;code&gt;k8s.io/&lt;/code&gt; 为 k8s 核心组件保留前缀&lt;/p&gt;
&lt;p&gt;值 可以为空，不多于63个字符，必须以字母或数字 ([&lt;code&gt;a-z0-9A-Z&lt;/code&gt;])开头和结束，中间可以包含中划线(&lt;code&gt;-&lt;/code&gt;)，下划线 &lt;code&gt;(_)&lt;/code&gt;，点 (.),字母,数字&lt;/p&gt;
&lt;p&gt;以下示例中的 Pod 包含 &lt;code&gt;environment: production&lt;/code&gt; 和 &lt;code&gt;app: nginx&lt;/code&gt; 两个标签:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;label-demo&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;environment&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;production&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx:1.14.2&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;标签选择器&#34;&gt;标签选择器&lt;/h2&gt;
&lt;p&gt;与对象名称和对象UID不同，对象标签在对象之间不需要唯一，并且一般来说，会有多个对象有相同的标签
用户或客户端程序可以选择器可以通过标签选择器选取一个对象集. 标签选择器是 k8s 核心分组r 基础
目前标签选择器支持两种选择方式： 等值选择，集合选择。
在待值选择时， 一个标签选择器可以由多个选择条件组成，每个条件用逗号分隔，表示匹配同时满足这些条件的对象，所以这里逗号相当与逻辑与(&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;)关系
空选择器或不使用选择在不同的情况下，有不同的表现。用户标签的 API 需要要正确和有意义的说明文档
注意: 对于有些 API 类型，比如 &lt;code&gt;ReplicaSets&lt;/code&gt; 同一个命名空间两个不同实例的标签选择器不能有交叉， 否些控制器就会将些认为是冲突，导致不能别副本数是否正确(TODO 这里需要有一个示例，描述不太好理解)&lt;/p&gt;
&lt;p&gt;警告: 在编写选择器条件时需要注意，对于 等值选择，集合选择 都没有逻辑与(&lt;code&gt;||&lt;/code&gt;)操作符&lt;/p&gt;
&lt;h3 id=&#34;等值选择&#34;&gt;等值选择&lt;/h3&gt;
&lt;p&gt;等值选择可以分别对于标签的 键和值的相等与不相等，只要对象的标签含有选择器所有的条件相匹配的标签就会选中，不管该对象是否还有其它标签
操作符&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;=,==, 表示等于; 对象标签的键和值都要一致&lt;/li&gt;
&lt;li&gt;!= 表示不等于; 对象标签有该键，但不是该值
例：&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;environment = production # 筛选包含标签键为 environment， 且对应值为 production 的对象
tier != frontend # 筛选包含标签键为 tier，且对应值不为 frontend 的对象
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;筛选&lt;code&gt;production&lt;/code&gt;环境中层级除 &lt;code&gt;frontend&lt;/code&gt; 外的对象可以写成 &lt;code&gt;environment=production,tier!=frontend&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;等值选择标签选择器的一个应用场景为为 Pod 指定 节点选择的条件。以下 Pod 节点选择的条件为 &lt;code&gt;accelerator=nvidia-tesla-p100&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cuda-test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cuda-test&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;k8s.gcr.io/cuda-vector-add:v0.1&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;limits&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;nvidia.com/gpu&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;accelerator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nvidia-tesla-p100&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;集合选择器&#34;&gt;集合选择器&lt;/h3&gt;
&lt;p&gt;标签的集合选择器可以且于多值过虑。 支持三种操作符 &lt;code&gt;in&lt;/code&gt;,&lt;code&gt;notin&lt;/code&gt; 和 &lt;code&gt;exists&lt;/code&gt;(仅用在键上)
示例:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;environment in (production, qa) # 筛选 有键为 environment 且对应值为 production 或 qa 的对象
tier notin (frontend, backend) # 筛选 有键为 tier 且值 不是frontend 也不是 backend 的对象和所有不包含键为 tier 的对象
partition # 筛选 存在键为 tier 不管值是啥的对象
!partition # 筛选 不存在键为 tier 不管值是啥的对象
partition,environment notin (qa) # 筛选 存在键为 partition 且 environment 的值不是 qa
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;多个条件之间的逗号赞同一逻辑与(&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;)
&lt;code&gt;environment=production&lt;/code&gt; 与 &lt;code&gt;environment in (production)&lt;/code&gt; 等同
&lt;code&gt;!=&lt;/code&gt; 与 &lt;code&gt;notin&lt;/code&gt; 单值时等同
两种标签选择方式可以组合使用 例： &lt;code&gt;partition in (customerA, customerB),environment!=qa&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;选择器在-api-上的使用&#34;&gt;选择器在 API 上的使用&lt;/h2&gt;
&lt;h3 id=&#34;list-watch-操作时过虑对象&#34;&gt;LIST/ WATCH 操作时过虑对象&lt;/h3&gt;
&lt;p&gt;在时行LIST/ WATCH 操作时可以通过标签选择筛选需要的对象。 上节提及的两种方式都可以使用， 在URL中请求参数类似如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;等值选择: &lt;code&gt;?labelSelector=environment%3Dproduction,tier%3Dfrontend&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;集合选择: &lt;code&gt;?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;也可以用于 REST 客户端 或 &lt;code&gt;kubectl&lt;/code&gt;
例如 &lt;code&gt;kubectl&lt;/code&gt; 中使用&lt;/p&gt;
&lt;p&gt;等值选择&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get pods -l environment&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;production,tier&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;frontend
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;集合选择&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get pods -l &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;environment in (production),tier in (frontend)&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;集合选择的表达更宽泛，比如这个可以达到或的效果&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get pods -l &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;environment in (production, qa)&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;逻辑否的效果&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get pods -l &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;environment,environment notin (frontend)&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;set-references-in-api-objects-啥意思&#34;&gt;Set references in API objects 啥意思&lt;/h3&gt;
&lt;p&gt;有些 k8s 对象，比如 &lt;a href=&#34;../../../services-networking/service/&#34;&gt;Service&lt;/a&gt; &lt;a href=&#34;../../../03-workloads/01-controllers/01-replicationcontroller/&#34;&gt;ReplicationController&lt;/a&gt; 也是通过标签选择器来限定被其管理的资源对象，比如 &lt;a href=&#34;../../../03-workloads/00-pods/&#34;&gt;Pod&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;service-和-replicationcontroller-对选择器的使用&#34;&gt;&lt;code&gt;Service&lt;/code&gt; 和 &lt;code&gt;ReplicationController&lt;/code&gt; 对选择器的使用&lt;/h3&gt;
&lt;p&gt;Service 通过标签选择器来指定负载均衡的 Pod
ReplicationController 也是通过标签选择器来指定被其管理的 Pod&lt;/p&gt;
&lt;p&gt;只支持等值选择，可以为 &lt;code&gt;yaml&lt;/code&gt; 或 &lt;code&gt;JSON&lt;/code&gt; 格式
示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;component&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis # 相当于 component=redis 或 component in (redis)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;支持集合选择的资源&#34;&gt;支持集合选择的资源&lt;/h3&gt;
&lt;p&gt;较新的资源，如 &lt;code&gt;Job&lt;/code&gt;, &lt;code&gt;Deployment&lt;/code&gt;, &lt;code&gt;ReplicaSet&lt;/code&gt;, &lt;code&gt;DaemonSet&lt;/code&gt; 都支持集合选择&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;component&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
    - {&lt;span style=&#34;color:#f92672&#34;&gt;key: tier, operator: In, values&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;cache]}&lt;/span&gt;
    - {&lt;span style=&#34;color:#f92672&#34;&gt;key: environment, operator: NotIn, values&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;dev]}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;matchLabels&lt;/code&gt; 是一个键值对字典&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;component&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 等同与&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
    - {&lt;span style=&#34;color:#f92672&#34;&gt;key: component, operator: In, values&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;redis]}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;matchExpressions&lt;/code&gt; 是一个条件的集合
其条件中可用的操作符(operator)包括: &lt;code&gt;In&lt;/code&gt;, &lt;code&gt;NotIn&lt;/code&gt;, &lt;code&gt;Exists&lt;/code&gt;, &lt;code&gt;DoesNotExist&lt;/code&gt;
&lt;code&gt;NotIn&lt;/code&gt; 的值必须非空
包括 &lt;code&gt;matchLabels&lt;/code&gt; 和 &lt;code&gt;matchExpressions&lt;/code&gt; 定义的条件,所有条件之间的关系为逻辑与.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;选择节点集合&#34;&gt;选择节点集合&lt;/h3&gt;
&lt;p&gt;标签的另一个应用场景为筛选 Pod 可以调度的 节点。 具体见&lt;a href=&#34;../../../09-scheduling-eviction/02-assign-pod-node/&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 注解 (Annotations)</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/04-annotation/</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/04-annotation/</guid>
      <description>
        
        
        &lt;p&gt;用户可以将任非标识符元素关联到对象上，其它库或工具可以读取这些元数据&lt;/p&gt;
&lt;h2 id=&#34;关联元数据到对象&#34;&gt;关联元数据到对象&lt;/h2&gt;
&lt;p&gt;用户可以通过标签或注意的方式将元数据关联到对象上，标签用于选择或查找符合条件的对象， 而在注解中的元数据则不是用于选择或查找对象的，其中的内容可以很小也可以很大，可以是结构化数据也可以是非结构化数据，还可以使用标签不允许的字符
注解与标签类似，也是键值对形式的字典，例如:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;metadata&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;:&lt;/span&gt; {
  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;annotations&amp;#34;&lt;/span&gt;: {
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;key1&amp;#34;&lt;/span&gt; : &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;value1&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;key2&amp;#34;&lt;/span&gt; : &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;value2&amp;#34;&lt;/span&gt;
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以用注解存储的常见数据示例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;由声明式配置层管理的字段. 把这些字段作为注解关联到对象是为能与诸如以下字段做区分: 由客户端或服务端设置的默认值; 由自动扩容系统自动生成的字段&lt;/li&gt;
&lt;li&gt;构建，发布，镜像相关信息如 时间戳，发布编号，git 分支，PR 编号，镜像 hash, 镜像库地址&lt;/li&gt;
&lt;li&gt;日志, 监控, 分析, 审计仓库的信息.&lt;/li&gt;
&lt;li&gt;关于客户端库或工具可用于调试目的的信息 例如：名称, 版本, 构建信息.&lt;/li&gt;
&lt;li&gt;来自用户或 工具/系统 信息, 例如 对象在外部系统中的URLs地址&lt;/li&gt;
&lt;li&gt;轻量级回滚帮助信息， 例如, 配置或检查点.&lt;/li&gt;
&lt;li&gt;对使用对象的用户提供修改指导或非标准特性的使用说明&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当然这些信息可以保存在外部的数据库或系统中，但这样制作部署，管理，自省的工具库就不那么容易了&lt;/p&gt;
&lt;h2 id=&#34;语法和字符集&#34;&gt;语法和字符集&lt;/h2&gt;
&lt;p&gt;标签由 键值对组成。 合法的键可以由两上部分组成， 一个可选的前级加上本身的名称中间用斜线(&lt;code&gt;/&lt;/code&gt;)分隔。
名称部分 不得多于63个字符，必须以字母或数字 ([a-z0-9A-Z])开头和结束，中间部分可以包含中划线(&lt;code&gt;-&lt;/code&gt;)，下划线 &lt;code&gt;(_)&lt;/code&gt;，点 (&lt;code&gt;.&lt;/code&gt;),字母,数字。
如果要使用前缀， 前端必须是一个合法的 DNS 字域名，由多个 DNS 标签组成，中间由点(&lt;code&gt;.&lt;/code&gt;)分隔， 总长度不超过 253 个字符&lt;/p&gt;
&lt;p&gt;如果一个标签键没有前缀则假定它是属于用于私有的，
由系统自动化组件(e.g. &lt;code&gt;kube-scheduler&lt;/code&gt;, &lt;code&gt;kube-controller-manager&lt;/code&gt;, &lt;code&gt;kube-apiserver&lt;/code&gt;, &lt;code&gt;kubectl&lt;/code&gt;, 或其它第三方自动化工具), 在给用户对象加标签时必须加前缀， &lt;code&gt;kubernetes.io/&lt;/code&gt;和 &lt;code&gt;k8s.io/&lt;/code&gt; 为 k8s 核心组件保留前缀&lt;/p&gt;
&lt;p&gt;以下示例一个包含一个注解 &lt;code&gt;imageregistry: https://hub.docker.com/&lt;/code&gt; 的 Pod&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;annotations-demo&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;imageregistry&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://hub.docker.com/&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx:1.14.2&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
      </description>
    </item>
    
    <item>
      <title>Docs: 字段选择器</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/05-field-selectors/</link>
      <pubDate>Fri, 03 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/05-field-selectors/</guid>
      <description>
        
        
        &lt;p&gt;用户可以通过字段选择器的以对象的一个或多个字段的值作为选择条件实现对 &lt;a href=&#34;../00-kubernetes-objects/&#34;&gt;k8s 对象&lt;/a&gt;的选择。示例如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;metadata.name=my-service&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;metadata.namespace!=default&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;status.phase=Pending&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下 &lt;code&gt;kubectl&lt;/code&gt; 命令通过选择器，选择 &lt;code&gt;status.phase&lt;/code&gt; 字段值是 &lt;code&gt;Running&lt;/code&gt; 的对象:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get pods --field-selector status.phase&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Running
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意: 字段选择器是基本的资源选择器。 默认没有设置选择条件。 也就是选择该类型所有的对象。这就让以下两个 &lt;code&gt;kubectl&lt;/code&gt; 命令等效 &lt;code&gt;kubectl get pods&lt;/code&gt;， &lt;code&gt;kubectl get pods --field-selector &amp;quot;&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;支持选择的字段&#34;&gt;支持选择的字段&lt;/h2&gt;
&lt;p&gt;字段选择器支持的字段因 k8s 资源不同而不同。 但所有的资源都支持 &lt;code&gt;metadata.name&lt;/code&gt; 和 &lt;code&gt;metadata.namespace&lt;/code&gt;， 使用不支持的字段会报错。
比如以下示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get ingress --field-selector foo.bar&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;baz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;错误信息如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Error from server (BadRequest): Unable to find &amp;quot;ingresses&amp;quot; that match label selector &amp;quot;&amp;quot;, field selector &amp;quot;foo.bar=baz&amp;quot;: &amp;quot;foo.bar&amp;quot; is not a known field selector: only &amp;quot;metadata.name&amp;quot;, &amp;quot;metadata.namespace&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;支持的操作符&#34;&gt;支持的操作符&lt;/h2&gt;
&lt;p&gt;字段选择器支持的操作符有 &lt;code&gt;=&lt;/code&gt;, &lt;code&gt;==,&lt;/code&gt; 和 &lt;code&gt;!=&lt;/code&gt;， 其中 (&lt;code&gt;=&lt;/code&gt; 和 &lt;code&gt;==&lt;/code&gt; 效果一样)。 例如以下示例表示，选择所有不属于 &lt;code&gt;default&lt;/code&gt; 命名空间的 &lt;code&gt;Service&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get services  --all-namespaces --field-selector metadata.namespace!&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;default
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;多个选择条件&#34;&gt;多个选择条件&lt;/h2&gt;
&lt;p&gt;与标签选择类似， 字段选择和多个条件也可以通过逗号分隔，例如以下示例表示选择所有 &lt;code&gt;status.phase&lt;/code&gt; 不是 &lt;code&gt;Running&lt;/code&gt; 且 &lt;code&gt;spec.restartPolicy&lt;/code&gt; 字段值为 &lt;code&gt;Always&lt;/code&gt; 的 Pod&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get pods --field-selector&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;status.phase!&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Running,spec.restartPolicy&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Always
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;同时筛选多个类型的资源&#34;&gt;同时筛选多个类型的资源&lt;/h2&gt;
&lt;p&gt;字段选择器可以同时对多种类型对象进行筛选， 例如以下示例表示，选择所有不属于 &lt;code&gt;default&lt;/code&gt; 命名空间的 &lt;code&gt;Statefulsets&lt;/code&gt; 和 &lt;code&gt;Services&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;default
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
      </description>
    </item>
    
    <item>
      <title>Docs: 标签设置指导</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/06-common-labels/</link>
      <pubDate>Fri, 03 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/06-common-labels/</guid>
      <description>
        
        
        &lt;p&gt;用户可以使用 kubectl 和 dashboard 外的可视化管理工具。一些通用的描述对象信息的标签配置可以让这些工具更好地工作
为了工具能更好的使用这些标签，建议标签以方便查询的方式来定义对象信息
元数据是围绕应用这个概念来组织的。k8s 并不是一个平台即服务(PaaS),也没有一个对应用有一个强制的格式规范。只是通过元数据来提供应用的描述和信息。关于应用所包含的信息的的规范是相关宽松的
注意: 只能推荐使用这些标签。以方便对应用的管理，这些都不是 k8s 核心工具所必要的
共享标签和注解需要共享一个共同的前缀 &lt;code&gt;app.kubernetes.io&lt;/code&gt;, 没有前缀的标签是用户私有的。在共享标签上使用使用共享前缀是为了保证不会影响到用户私有的标签。&lt;/p&gt;
&lt;h2 id=&#34;标签&#34;&gt;标签&lt;/h2&gt;
&lt;p&gt;为了给读者一个标签使用的整体印象，以下标签可以用在第一个资源对象上。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;标签键&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;th&gt;示例值&lt;/th&gt;
&lt;th&gt;值数据类型&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;app.kubernetes.io/name&lt;/td&gt;
&lt;td&gt;应用名称&lt;/td&gt;
&lt;td&gt;&lt;code&gt;mysql&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;app.kubernetes.io/instance&lt;/td&gt;
&lt;td&gt;用于识别应用实例的唯一名称&lt;/td&gt;
&lt;td&gt;&lt;code&gt;wordpress-abcxzy&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;app.kubernetes.io/version&lt;/td&gt;
&lt;td&gt;应用的当前版本 (如 版本号, 版本哈希, 等.)&lt;/td&gt;
&lt;td&gt;&lt;code&gt;5.7.21&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;app.kubernetes.io/component&lt;/td&gt;
&lt;td&gt;架构中的结构名&lt;/td&gt;
&lt;td&gt;&lt;code&gt;database&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;app.kubernetes.io/part-of&lt;/td&gt;
&lt;td&gt;这个对象是哪个应用的一部分&lt;/td&gt;
&lt;td&gt;&lt;code&gt;wordpress&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;app.kubernetes.io/managed-by&lt;/td&gt;
&lt;td&gt;用于管理这个对象的工具&lt;/td&gt;
&lt;td&gt;&lt;code&gt;helm&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;string&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;以下是一个 &lt;code&gt;StatefulSet&lt;/code&gt; 的实践示例&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StatefulSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress-abcxzy&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/version&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;5.7.21&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/component&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;database&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/part-of&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/managed-by&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;helm&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;应用及应用实例&#34;&gt;应用及应用实例&lt;/h2&gt;
&lt;p&gt;一个应用可能在 k8s 集群的一个命名空间中安装一次或多次。比如 安装多个 wordpress 的(不同)网站
应用名和其实例名都应该作区分， 比如 一个 wordpress 应用为 &lt;code&gt;app.kubernetes.io/name: wordpress&lt;/code&gt;, 其实例可以设置为 &lt;code&gt;app.kubernetes.io/instance: wordpress-abcxzy&lt;/code&gt; 这就应用和实例就比较好识别，当一个应用有多个实例是每个实例的名称都要唯一。&lt;/p&gt;
&lt;h2 id=&#34;示例&#34;&gt;示例&lt;/h2&gt;
&lt;p&gt;以下示例展示这些标签的不同用法&lt;/p&gt;
&lt;h3 id=&#34;一个简单的无状态-service&#34;&gt;一个简单的无状态 Service&lt;/h3&gt;
&lt;p&gt;以下应用场景为 用一个 Deployment 和 Service 对象部署一个简单的无状态服务。 以下为其标签的最简配置
Deployment 用于管理应用运行的 Pod&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myservice&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myservice-abcxzy&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Service 用于应用接入&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myservice&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myservice-abcxzy&lt;/span&gt;
...

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;带数据库的web应用&#34;&gt;带数据库的Web应用&lt;/h3&gt;
&lt;p&gt;稍复杂一点的应用场景: 一个web 应用(WordPress)用到一个数据库(MySQL), 通过 Helm 安装&lt;/p&gt;
&lt;p&gt;WordPress 的 Deployment&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress-abcxzy&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/version&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;4.9.4&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/managed-by&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;helm&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/component&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;server&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/part-of&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;WordPress 的 Service&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress-abcxzy&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/version&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;4.9.4&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/managed-by&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;helm&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/component&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;server&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/part-of&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;MySQL 的 StatefulSet&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StatefulSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql-abcxzy&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/version&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;5.7.21&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/managed-by&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;helm&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/component&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;database&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/part-of&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress&lt;/span&gt;
...

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;MySQL 的 Service&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/instance&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql-abcxzy&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/version&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;5.7.21&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/managed-by&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;helm&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/component&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;database&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;app.kubernetes.io/part-of&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;wordpress&lt;/span&gt;
...

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在 StatefulSet 和 Service 可以找到它们自己的信息所属的应用的信息。这些在更大的应用中相当有用。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 初始化容器</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/init-containers/</link>
      <pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/init-containers/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- erictune
title: Init Containers
content_type: concept
weight: 40
---
--&gt;
&lt;!-- overview --&gt;
&lt;!--
This page provides an overview of init containers: specialized containers that run
before app containers in a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;.
Init containers can contain utilities or setup scripts not present in an app image.

You can specify init containers in the Pod specification alongside the `containers`
array (which describes app containers).
  --&gt;
&lt;p&gt;本文主要介绍初始化容器: 一种在 Pod 中在应用容器启动之前运行的专用容器。
初始化容器用于提供应用镜像没有的工具或初始化脚本。
初始化容器定义与应用容器定义是相邻关系&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Understanding init containers

A &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; can have multiple containers
running apps within it, but it can also have one or more init containers, which are run
before the app containers are started.

Init containers are exactly like regular containers, except:

* Init containers always run to completion.
* Each init container must complete successfully before the next one starts.

If a Pod&#39;s init container fails, Kubernetes repeatedly restarts the Pod until the init container
succeeds. However, if the Pod has a `restartPolicy` of Never, Kubernetes does not restart the Pod.

To specify an init container for a Pod, add the `initContainers` field into
the Pod specification, as an array of objects of type
[Container](/docs/reference/generated/kubernetes-api/v1.19/#container-v1-core),
alongside the app `containers` array.
The status of the init containers is returned in `.status.initContainerStatuses`
field as an array of the container statuses (similar to the `.status.containerStatuses`
field).
 --&gt;
&lt;h2 id=&#34;理解初始化容器是做什么的&#34;&gt;理解初始化容器是做什么的&lt;/h2&gt;
&lt;p&gt;一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 中可以饮上多个应用容器，同时还可以
有一个或多个初始化容器，这些初始化容器会在应用容器之前运行并结束。&lt;/p&gt;
&lt;p&gt;初始化容器与普通容器并无太多不同，除了以下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;初始化化容器运行在有限时间结束&lt;/li&gt;
&lt;li&gt;只有在上一初始化容器运行结束后，下一个才能开始运行。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果 Pod 的初始化容器挂了， k8s 会不停重启该 Pod 直至初始化容器成功运行完成。
但如果 Pod 的 &lt;code&gt;restartPolicy&lt;/code&gt; 值为 &lt;code&gt;Never&lt;/code&gt;， 则 k8s 不会重启该 Pod。&lt;/p&gt;
&lt;p&gt;要为一个 Pod 配置初始化容器， 需要在配置中添加 &lt;code&gt;initContainers&lt;/code&gt; 字段， 字段值为一个类型为
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#container-v1-core&#34;&gt;Container&lt;/a&gt;
的对象数组， 与 &lt;code&gt;containers&lt;/code&gt; 字段相邻。
初始化容器返回的状态存放于 &lt;code&gt;.status.initContainerStatuses&lt;/code&gt; 字段，是一个与容器状态字段(&lt;code&gt;.status.containerStatuses&lt;/code&gt;)类似的数组&lt;/p&gt;
&lt;!--
### Differences from regular containers

Init containers support all the fields and features of app containers,
including resource limits, volumes, and security settings. However, the
resource requests and limits for an init container are handled differently,
as documented in [Resources](#resources).

Also, init containers do not support `lifecycle`, `livenessProbe`, `readinessProbe`, or
`startupProbe` because they must run to completion before the Pod can be ready.

If you specify multiple init containers for a Pod, Kubelet runs each init
container sequentially. Each init container must succeed before the next can run.
When all of the init containers have run to completion, Kubelet initializes
the application containers for the Pod and runs them as usual.
 --&gt;
&lt;h3 id=&#34;初始化容器和普通容器有啥区别&#34;&gt;初始化容器和普通容器有啥区别&lt;/h3&gt;
&lt;p&gt;初始化容器动脚应用容器的所有字段和特性， 包括资源限制， 数据卷， 安全设置。
但是对初始化化容器对资源的限制处理方式有所区别，具体见 &lt;a href=&#34;#resources&#34;&gt;资源&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;还有，初始化容器不支持 &lt;code&gt;lifecycle&lt;/code&gt;, &lt;code&gt;livenessProbe&lt;/code&gt;, &lt;code&gt;readinessProbe&lt;/code&gt;, &lt;code&gt;startupProbe&lt;/code&gt;，
因为只有初始化容器执行完 Pod 才可能进入就绪状态。&lt;/p&gt;
&lt;p&gt;如果一个 Pod 中配置了多个初始化容器，则 kubelet 会顺序依次运行。
只有在上一初始化容器运行结束后，下一个才能开始运行。
当所有初始化容器运行完成后， kubelet 才会初始化 Pod 中的应用容器并以常规方式运行&lt;/p&gt;
&lt;!--
## Using init containers

Because init containers have separate images from app containers, they
have some advantages for start-up related code:

* Init containers can contain utilities or custom code for setup that are not present in an app
  image. For example, there is no need to make an image `FROM` another image just to use a tool like
  `sed`, `awk`, `python`, or `dig` during setup.
* The application image builder and deployer roles can work independently without
  the need to jointly build a single app image.
* Init containers can run with a different view of the filesystem than app containers in the
  same Pod. Consequently, they can be given access to
  &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secrets&lt;span class=&#39;tooltip-text&#39;&gt;存放如密码, OAuth, ssh 密钥等敏感信息&lt;/span&gt;
&lt;/a&gt; that app containers cannot access.
* Because init containers run to completion before any app containers start, init containers offer
  a mechanism to block or delay app container startup until a set of preconditions are met. Once
  preconditions are met, all of the app containers in a Pod can start in parallel.
* Init containers can securely run utilities or custom code that would otherwise make an app
  container image less secure. By keeping unnecessary tools separate you can limit the attack
  surface of your app container image.
 --&gt;
&lt;h2 id=&#34;怎么使用初始化容器&#34;&gt;怎么使用初始化容器&lt;/h2&gt;
&lt;p&gt;因为初始化容器与应用容器是使用不同的镜像，所以在执行初始化相关代码有些优势:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;初始化容器可以包含应用容器中没有的工作或自定义代码。
例如， 不需要为了用像 &lt;code&gt;sed&lt;/code&gt;, &lt;code&gt;awk&lt;/code&gt;, &lt;code&gt;python&lt;/code&gt;, &lt;code&gt;dig&lt;/code&gt; 来做初始化而要在应用镜像中加入
&lt;code&gt;FROM&lt;/code&gt; 其它的镜像。&lt;/li&gt;
&lt;li&gt;应用镜像的构建和部署角色可以独立工作，而不需要打在一个应用镜像里&lt;/li&gt;
&lt;li&gt;同一个 Pod 的初始化化容器可以与应用容器运行在不同文件系统视角下， 因而可以让初始化容器可以读取
应用容器不能读取的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secrets&lt;span class=&#39;tooltip-text&#39;&gt;存放如密码, OAuth, ssh 密钥等敏感信息&lt;/span&gt;
&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;因为只能在初始化容器运行完成后应用容器才能启动， 初始化容器就提供了一种机制，可以让容器在
达成一定前置条件之前应用容器会被阻塞或延迟。 当前置条件达成， Pod 中所有的应用容器可以并行启动。&lt;/li&gt;
&lt;li&gt;可以在初始化容器可以安全地运行放在应用容器中可以不那么安全的工具或自定义代码。
通过将不必要的工具从应用镜像中移出(到初始化容器中)可以减少应用容器的攻击面&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Examples
Here are some ideas for how to use init containers:

* Wait for a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; to
  be created, using a shell one-line command like:
  ```shell
  for i in {1..100}; do sleep 1; if dig myservice; then exit 0; fi; done; exit 1
  ```

* Register this Pod with a remote server from the downward API with a command like:
  ```shell
  curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d &#39;instance=$(&lt;POD_NAME&gt;)&amp;ip=$(&lt;POD_IP&gt;)&#39;
  ```

* Wait for some time before starting the app container with a command like
  ```shell
  sleep 60
  ```

* Clone a Git repository into a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;Volume&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;

* Place values into a configuration file and run a template tool to dynamically
  generate a configuration file for the main app container. For example,
  place the `POD_IP` value in a configuration and generate the main app
  configuration file using Jinja.
 --&gt;
&lt;h3 id=&#34;示例&#34;&gt;示例&lt;/h3&gt;
&lt;p&gt;以下为几个怎么用初始化容器的点子:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;等待一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 创建完成，
使用以下 shell 命令:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i in &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;1..100&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;; &lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt; sleep 1; &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; dig myservice; &lt;span style=&#34;color:#66d9ef&#34;&gt;then&lt;/span&gt; exit 0; &lt;span style=&#34;color:#66d9ef&#34;&gt;fi&lt;/span&gt;; &lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;; exit &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;通过以下命令将该 Pod 注册到一个远程服务的 WEB API
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;instance=$(&amp;lt;POD_NAME&amp;gt;)&amp;amp;ip=$(&amp;lt;POD_IP&amp;gt;)&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;等待一定时间后再启动 Pod中的应用容器
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sleep &lt;span style=&#34;color:#ae81ff&#34;&gt;60&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;从 Git 仓库中克隆一个库到 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;Volume&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;将配置值放入配置文件，通过一个模板工具为应用容器动态生成配置文件， 例， 将 &lt;code&gt;POD_IP&lt;/code&gt; 放丰配置文件中
通过如 &lt;code&gt;Jinja&lt;/code&gt; 这样的工具生成主应用的配置文件&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
#### Init containers in use

This example defines a simple Pod that has two init containers.
The first waits for `myservice`, and the second waits for `mydb`. Once both
init containers complete, the Pod runs the app container from its `spec` section.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: [&#39;sh&#39;, &#39;-c&#39;, &#39;echo The app is running! &amp;&amp; sleep 3600&#39;]
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: [&#39;sh&#39;, &#39;-c&#39;, &#34;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done&#34;]
  - name: init-mydb
    image: busybox:1.28
    command: [&#39;sh&#39;, &#39;-c&#39;, &#34;until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done&#34;]
```

You can start this Pod by running:

```shell
kubectl apply -f myapp.yaml
```
```
pod/myapp-pod created
```

And check on its status with:
```shell
kubectl get -f myapp.yaml
```
```
NAME        READY     STATUS     RESTARTS   AGE
myapp-pod   0/1       Init:0/2   0          6m
```

or for more details:
```shell
kubectl describe -f myapp.yaml
```
```
Name:          myapp-pod
Namespace:     default
[...]
Labels:        app=myapp
Status:        Pending
[...]
Init Containers:
  init-myservice:
[...]
    State:         Running
[...]
  init-mydb:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Containers:
  myapp-container:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Events:
  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message
  ---------    --------    -----    ----                      -------------                           --------      ------        -------
  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201
  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image &#34;busybox&#34;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image &#34;busybox&#34;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container with docker id 5ced34a04634; Security:[seccomp=unconfined]
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container with docker id 5ced34a04634
```

To see logs for the init containers in this Pod, run:
```shell
kubectl logs myapp-pod -c init-myservice # Inspect the first init container
kubectl logs myapp-pod -c init-mydb      # Inspect the second init container
```

At this point, those init containers will be waiting to discover Services named
`mydb` and `myservice`.

Here&#39;s a configuration you can use to make those Services appear:

```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
---
apiVersion: v1
kind: Service
metadata:
  name: mydb
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9377
```

To create the `mydb` and `myservice` services:

```shell
kubectl apply -f services.yaml
```
```
service/myservice created
service/mydb created
```

You&#39;ll then see that those init containers complete, and that the `myapp-pod`
Pod moves into the Running state:

```shell
kubectl get -f myapp.yaml
```
```
NAME        READY     STATUS    RESTARTS   AGE
myapp-pod   1/1       Running   0          9m
```

This simple example should provide some inspiration for you to create your own
init containers. [What&#39;s next](#whats-next) contains a link to a more detailed example.
 --&gt;
&lt;h4 id=&#34;初始化容器实践&#34;&gt;初始化容器实践&lt;/h4&gt;
&lt;p&gt;本示例定义一个简单的Pod， 其中包含两个初始化容器。 第一个等待 &lt;code&gt;myservice&lt;/code&gt; 的创建，
第二个等待 &lt;code&gt;mydb&lt;/code&gt; 的创建。 当这两个初始化容器都运行完成，则运行 &lt;code&gt;spec&lt;/code&gt; 配置的应用容器。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myapp-pod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myapp&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myapp-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox:1.28&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sh&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;-c&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;echo The app is running! &amp;amp;&amp;amp; sleep 3600&amp;#39;&lt;/span&gt;]
  &lt;span style=&#34;color:#f92672&#34;&gt;initContainers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;init-myservice&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox:1.28&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sh&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;-c&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done&amp;#34;&lt;/span&gt;]
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;init-mydb&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox:1.28&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sh&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;-c&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done&amp;#34;&lt;/span&gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;通过以下命令启动该 Pod&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl apply -f myapp.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;pod/myapp-pod created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过以下命令查看 Pod 状态&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get -f myapp.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;结果类似如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME        READY     STATUS     RESTARTS   AGE
myapp-pod   0/1       Init:0/2   0          6m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过以下命令可以查看更详情的信息&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl describe -f myapp.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;结果类似如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:          myapp-pod
Namespace:     default
[...]
Labels:        app=myapp
Status:        Pending
[...]
Init Containers:
  init-myservice:
[...]
    State:         Running
[...]
  init-mydb:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Containers:
  myapp-container:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Events:
  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message
  ---------    --------    -----    ----                      -------------                           --------      ------        -------
  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201
  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image &amp;quot;busybox&amp;quot;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image &amp;quot;busybox&amp;quot;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container with docker id 5ced34a04634; Security:[seccomp=unconfined]
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container with docker id 5ced34a04634
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过以下命令查看初始化容器的日志&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl logs myapp-pod -c init-myservice &lt;span style=&#34;color:#75715e&#34;&gt;# 查看第一个初始化容器&lt;/span&gt;
kubectl logs myapp-pod -c init-mydb      &lt;span style=&#34;color:#75715e&#34;&gt;# 查看第二个初始化容器&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;此时，这两个初始化容器都分别在等待 &lt;code&gt;myservice&lt;/code&gt; 和 &lt;code&gt;mydb&lt;/code&gt; 两个
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 被创建
以下为创建所需要 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 的定义文件&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myservice&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mydb&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9377&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;通过以下命令创建 &lt;code&gt;myservice&lt;/code&gt; 和 &lt;code&gt;mydb&lt;/code&gt; 两个
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl apply -f services.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;service/myservice created
service/mydb created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这时候再使用以下命令就会发现初始化容器已经完成， 名叫 myapp-pod 的 Pod 进入运行状态&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get -f myapp.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME        READY     STATUS    RESTARTS   AGE
myapp-pod   1/1       Running   0          9m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个简单示例可以对用户创建自己的初始化容器有所启发。 更多关于初始化容器的示例见 &lt;a href=&#34;#whats-next&#34;&gt;相关资料&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Detailed behavior

During Pod startup, the kubelet delays running init containers until the networking
and storage are ready. Then the kubelet runs the Pod&#39;s init containers in the order
they appear in the Pod&#39;s spec.

Each init container must exit successfully before
the next container starts. If a container fails to start due to the runtime or
exits with failure, it is retried according to the Pod `restartPolicy`. However,
if the Pod `restartPolicy` is set to Always, the init containers use
`restartPolicy` OnFailure.

A Pod cannot be `Ready` until all init containers have succeeded. The ports on an
init container are not aggregated under a Service. A Pod that is initializing
is in the `Pending` state but should have a condition `Initialized` set to true.

If the Pod [restarts](#pod-restart-reasons), or is restarted, all init containers
must execute again.

Changes to the init container spec are limited to the container image field.
Altering an init container image field is equivalent to restarting the Pod.

Because init containers can be restarted, retried, or re-executed, init container
code should be idempotent. In particular, code that writes to files on `EmptyDirs`
should be prepared for the possibility that an output file already exists.

Init containers have all of the fields of an app container. However, Kubernetes
prohibits `readinessProbe` from being used because init containers cannot
define readiness distinct from completion. This is enforced during validation.

Use `activeDeadlineSeconds` on the Pod and `livenessProbe` on the container to
prevent init containers from failing forever. The active deadline includes init
containers.

The name of each app and init container in a Pod must be unique; a
validation error is thrown for any container sharing a name with another.
 --&gt;
&lt;h2 id=&#34;一些细节行为&#34;&gt;一些细节行为&lt;/h2&gt;
&lt;p&gt;在 Pod 的启动过程中， kubelet 会等待网络和存储就绪后才会运行初始化容器。
kubelet 会按照定义配置中的顺序运行初始化容器。&lt;/p&gt;
&lt;p&gt;后一个初始化容器已经在前一个运行且成功退出后才启动。 如果一个容器因为运行环境而挂掉或错误退出，
会根据 Pod 上配置 &lt;code&gt;restartPolicy&lt;/code&gt; 进行重试。 但如果 Pod &lt;code&gt;restartPolicy&lt;/code&gt; 值为 &lt;code&gt;Always&lt;/code&gt;，
初始化容器对应 &lt;code&gt;restartPolicy&lt;/code&gt; 的值为 &lt;code&gt;OnFailure&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;如果 Pod &lt;a href=&#34;#pod-restart-reasons&#34;&gt;重启&lt;/a&gt;，或已经重启，所有的初始化容器都会重新执行。&lt;/p&gt;
&lt;p&gt;初始化容器的定义配置中能修改的字段只有容器的 &lt;code&gt;image&lt;/code&gt; 字段，
如果修改初始化容器的 &lt;code&gt;image&lt;/code&gt; 字段，则表示要重启该 Pod。&lt;/p&gt;
&lt;p&gt;因为初始化容器可以重启，重试，或重新执行， 所以初始化容器的代码必须是幂等的。 特别是，如果代码
需要向 &lt;code&gt;EmptyDirs&lt;/code&gt; 写入文件，需要考虑到输出文件已经存在的情况。&lt;/p&gt;
&lt;p&gt;初始化容器包含应用容器拥有的所有字段。但 kubelet 禁止在初始化容器上使用 &lt;code&gt;readinessProbe&lt;/code&gt;，
因为定义的就绪探针与一个执行完就退出的任务是不适用的。 这会在验证是检查，如果出现则报错。&lt;/p&gt;
&lt;p&gt;在 Pod 上使用 &lt;code&gt;activeDeadlineSeconds&lt;/code&gt; 和容器上使用 &lt;code&gt;livenessProbe&lt;/code&gt; 可以防止初始化容器
一直失败的情况出现。 活跃死线包含初始化容器。&lt;/p&gt;
&lt;p&gt;在 Pod 中的应用容器和初始化容器的名称全局(Pod 作用域内)唯一。 如果有相同的名称在验证时会报错。&lt;/p&gt;
&lt;!--
### Resources

Given the ordering and execution for init containers, the following rules
for resource usage apply:

* The highest of any particular resource request or limit defined on all init
  containers is the *effective init request/limit*
* The Pod&#39;s *effective request/limit* for a resource is the higher of:
  * the sum of all app containers request/limit for a resource
  * the effective init request/limit for a resource
* Scheduling is done based on effective requests/limits, which means
  init containers can reserve resources for initialization that are not used
  during the life of the Pod.
* The QoS (quality of service) tier of the Pod&#39;s *effective QoS tier* is the
  QoS tier for init containers and app containers alike.

Quota and limits are applied based on the effective Pod request and
limit.

Pod level control groups (cgroups) are based on the effective Pod request and
limit, the same as the scheduler.
 --&gt;
&lt;h3 id=&#34;resources&#34;&gt;资源&lt;/h3&gt;
&lt;p&gt;为了让初始化容器能获取到执行所需要的资源，以下为资源申请的规则:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在所有初始化容器中的资源 下限/上限 的单项资源最高值为 &lt;em&gt;有效初始化下限/上限&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Pod 的每项资源的 &lt;em&gt;有效 下限/上限&lt;/em&gt; 是以下中较大的一个:
&lt;ul&gt;
&lt;li&gt;所有应用容器该资源的 下限/上限 的总和&lt;/li&gt;
&lt;li&gt;该资源的 有效初始化下限/上限&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;调度是基于 资源 有效 下限/上限来进行的。也就是初始化容器可以申请用于初始化的资源可能在 Pod
的余生中都用不到了。&lt;/li&gt;
&lt;li&gt;Pod 的 有效 Qos 层中的 QoS (服务质量)层就是初始化容器和应用容器通用的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;配额和限制都是基于有效 Pod 下限/上限执行。&lt;/p&gt;
&lt;p&gt;Pod 级别的控制组(cgroups)基于 有效 Pod 下限/上限， 与调度器一致&lt;/p&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Pod restart reasons {#pod-restart-reasons}

A Pod can restart, causing re-execution of init containers, for the following
reasons:

* A user updates the Pod specification, causing the init container image to change.
  Any changes to the init container image restarts the Pod. App container image
  changes only restart the app container.
* The Pod infrastructure container is restarted. This is uncommon and would
  have to be done by someone with root access to nodes.
* All containers in a Pod are terminated while `restartPolicy` is set to Always,
  forcing a restart, and the init container completion record has been lost due
  to garbage collection.
 --&gt;
&lt;h3 id=&#34;pod-restart-reasons&#34;&gt;引起 Pod 重启的原因&lt;/h3&gt;
&lt;p&gt;可能引起 一个 Pod 重启并导致初始化容器的重新执行的原因如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;某个用户更新的 Pod 定义配置， 导致初始化容器的镜像变更。 任意对初始化容器镜像的修改都会导致 Pod 重启。
应用容器镜像变更只能重启该应用容器&lt;/li&gt;
&lt;li&gt;Pod 基础设施容器被重启，这种情况不常见， 只能由拥有节点 root 权限的用户进行。&lt;/li&gt;
&lt;li&gt;Pod 中所有的容器都被终止，因为&lt;code&gt;restartPolicy&lt;/code&gt; 的值为 &lt;code&gt;Always&lt;/code&gt;，所以强制重启， 此时初始化容器
的完成记录因为垃圾清楚而丢失。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
* Read about [creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container)
* Learn how to [debug init containers](/docs/tasks/debug-application-cluster/debug-init-containers/)
 --&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container&#34;&gt;创建带有初始化容器的 Pod&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/debug-application-cluster/debug-init-containers/&#34;&gt;调试初始化容器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Pod 拓扑分布约束条件</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/pod-topology-spread-constraints/</link>
      <pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/pod-topology-spread-constraints/</guid>
      <description>
        
        
        &lt;!--
---
title: Pod Topology Spread Constraints
content_type: concept
weight: 40
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--  





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [beta]&lt;/code&gt;
&lt;/div&gt;



You can use _topology spread constraints_ to control how &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; are spread across your cluster among failure-domains such as regions, zones, nodes, and other user-defined topology domains. This can help to achieve high availability as well as efficient resource utilization.
--&gt;
&lt;p&gt;




&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [beta]&lt;/code&gt;
&lt;/div&gt;


用户可以通过 &lt;em&gt;拓扑分布约束条件&lt;/em&gt; 来控制 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
在包含集群中故障域 （如 地区，分区，节点和其它用户定义拓扑域）中是怎样分布的。
该功能可以帮助用户在实现高可用的同时充分利用资源。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Prerequisites

### Enable Feature Gate

The `EvenPodsSpread` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
must be enabled for the
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API Server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; **and**
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-scheduler/&#39; target=&#39;_blank&#39;&gt;scheduler&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.&lt;/span&gt;
&lt;/a&gt;.

### Node Labels

Topology spread constraints rely on node labels to identify the topology domain(s) that each Node is in. For example, a Node might have labels: `node=node1,zone=us-east-1a,region=us-east-1`

Suppose you have a 4-node cluster with the following labels:

```
NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &lt;none&gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &lt;none&gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &lt;none&gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &lt;none&gt;   2m43s   v1.16.0   node=node4,zone=zoneB
```

Then the cluster is logically viewed as below:

```
+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
```

Instead of manually applying labels, you can also reuse the [well-known labels](/docs/reference/kubernetes-api/labels-annotations-taints/) that are created and populated automatically on most clusters.
 --&gt;
&lt;h2 id=&#34;准备工作&#34;&gt;准备工作&lt;/h2&gt;
&lt;h3 id=&#34;打开功能开关&#34;&gt;打开功能开关&lt;/h3&gt;
&lt;p&gt;需要打开
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API Server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; &lt;strong&gt;和&lt;/strong&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-scheduler/&#39; target=&#39;_blank&#39;&gt;scheduler&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.&lt;/span&gt;
&lt;/a&gt;
中叫 &lt;code&gt;EvenPodsSpread&lt;/code&gt; 的&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;为节点添加恰当的标签&#34;&gt;为节点添加恰当的标签&lt;/h3&gt;
&lt;p&gt;拓扑分布约束条件信赖于节点标签来区分其所在的拓扑域。 例如， 某节点标签可以为:
&lt;code&gt;node=node1,zone=us-east-1a,region=us-east-1&lt;/code&gt;
假设集群中有4个节点，标签如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &amp;lt;none&amp;gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &amp;lt;none&amp;gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &amp;lt;none&amp;gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &amp;lt;none&amp;gt;   2m43s   v1.16.0   node=node4,zone=zoneB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;那么该集群的逻辑视图如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;相较于手动添加标签，可以重用在大多数集群会自动创建和添加的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/kubernetes-api/labels-annotations-taints/&#34;&gt;常用标签&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Spread Constraints for Pods

### API

The field `pod.spec.topologySpreadConstraints` is introduced in 1.16 as below:

```
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  topologySpreadConstraints:
    - maxSkew: &lt;integer&gt;
      topologyKey: &lt;string&gt;
      whenUnsatisfiable: &lt;string&gt;
      labelSelector: &lt;object&gt;
```

You can define one or multiple `topologySpreadConstraint` to instruct the kube-scheduler how to place each incoming Pod in relation to the existing Pods across your cluster. The fields are:

- **maxSkew** describes the degree to which Pods may be unevenly distributed. It&#39;s the maximum permitted difference between the number of matching Pods in any two topology domains of a given topology type. It must be greater than zero.
- **topologyKey** is the key of node labels. If two Nodes are labelled with this key and have identical values for that label, the scheduler treats both Nodes as being in the same topology. The scheduler tries to place a balanced number of Pods into each topology domain.
- **whenUnsatisfiable** indicates how to deal with a Pod if it doesn&#39;t satisfy the spread constraint:
  - `DoNotSchedule` (default) tells the scheduler not to schedule it.
  - `ScheduleAnyway` tells the scheduler to still schedule it while prioritizing nodes that minimize the skew.
- **labelSelector** is used to find matching Pods. Pods that match this label selector are counted to determine the number of Pods in their corresponding topology domain. See [Label Selectors](/docs/concepts/overview/working-with-objects/labels/#label-selectors) for more details.

You can read more about this field by running `kubectl explain Pod.spec.topologySpreadConstraints`.
 --&gt;
&lt;h2 id=&#34;pod-的扩散约束&#34;&gt;Pod 的扩散约束&lt;/h2&gt;
&lt;h3 id=&#34;api&#34;&gt;API&lt;/h3&gt;
&lt;p&gt;在 &lt;code&gt;1.16&lt;/code&gt; 版本中加入了 &lt;code&gt;pod.spec.topologySpreadConstraints&lt;/code&gt; 字段，如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  topologySpreadConstraints:
    - maxSkew: &amp;lt;integer&amp;gt;
      topologyKey: &amp;lt;string&amp;gt;
      whenUnsatisfiable: &amp;lt;string&amp;gt;
      labelSelector: &amp;lt;object&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用户可以在 Pod 上定义一个或多个 &lt;code&gt;topologySpreadConstraint&lt;/code&gt;， 用于指导 &lt;code&gt;kube-scheduler&lt;/code&gt;
在集群中有与已经存在的 Pod 相关的新的 Pod 时应该怎么放置。有如下字段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;maxSkew&lt;/strong&gt; 该字段描述 Pod 分布不均匀的程度。 在指定拓扑类型的两个拓扑域中特定 Pod 数量相差数允许的最大值，这个值必须大于 0&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;topologyKey&lt;/strong&gt; 该字段使用节点的标签键， 如果有两个节点包含一个键，且该键值也相同，
调度器会将这两个节点认为在同一个拓扑。 调度器会尝试让两个拓扑域中的 Pod 数量平衡。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;whenUnsatisfiable&lt;/strong&gt; 该字段设置怎么处理不满足分布约束的Pod
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;DoNotSchedule&lt;/code&gt; (默认) 让调度器不要调度该 Pod&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ScheduleAnyway&lt;/code&gt; 让调度器仍然调度，但调度到不均匀度(skew)最低的节点&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;labelSelector&lt;/strong&gt; 用于找到匹配的 Pod。 匹配到的 Pod 会作为对应拓扑域的的一员(参与数量统计)
更多标签和选择器见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/labels/#label-selectors&#34;&gt;标签和选择器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更新多关于该字段的信息请查看 &lt;code&gt;kubectl explain Pod.spec.topologySpreadConstraints&lt;/code&gt; 命令结果。&lt;/p&gt;
&lt;!--
### Example: One TopologySpreadConstraint

Suppose you have a 4-node cluster where 3 Pods labeled `foo:bar` are located in node1, node2 and node3 respectively (`P` represents Pod):

```
+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
|   P   |   P   |   P   |       |
+-------+-------+-------+-------+
```

If we want an incoming Pod to be evenly spread with existing Pods across zones, the spec can be given as:



 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintsone-constraintyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/one-constraint.yaml&#34; download=&#34;pods/topology-spread-constraints/one-constraint.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/one-constraint.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintsone-constraintyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/one-constraint.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



`topologyKey: zone` implies the even distribution will only be applied to the nodes which have label pair &#34;zone:&amp;lt;any value&amp;gt;&#34; present. `whenUnsatisfiable: DoNotSchedule` tells the scheduler to let it stay pending if the incoming Pod can’t satisfy the constraint.

If the scheduler placed this incoming Pod into &#34;zoneA&#34;, the Pods distribution would become [3, 1], hence the actual skew is 2 (3 - 1) - which violates `maxSkew: 1`. In this example, the incoming Pod can only be placed onto &#34;zoneB&#34;:

```
+---------------+---------------+      +---------------+---------------+
|     zoneA     |     zoneB     |      |     zoneA     |     zoneB     |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |  OR  | node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
|   P   |   P   |   P   |   P   |      |   P   |   P   |  P P  |       |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
```

You can tweak the Pod spec to meet various kinds of requirements:

- Change `maxSkew` to a bigger value like &#34;2&#34; so that the incoming Pod can be placed onto &#34;zoneA&#34; as well.
- Change `topologyKey` to &#34;node&#34; so as to distribute the Pods evenly across nodes instead of zones. In the above example, if `maxSkew` remains &#34;1&#34;, the incoming Pod can only be placed onto &#34;node4&#34;.
- Change `whenUnsatisfiable: DoNotSchedule` to `whenUnsatisfiable: ScheduleAnyway` to ensure the incoming Pod to be always schedulable (suppose other scheduling APIs are satisfied). However, it’s preferred to be placed onto the topology domain which has fewer matching Pods. (Be aware that this preferability is jointly normalized with other internal scheduling priorities like resource usage ratio, etc.)
 --&gt;
&lt;h3 id=&#34;示例-单个-topologyspreadconstraint&#34;&gt;示例: 单个 TopologySpreadConstraint&lt;/h3&gt;
&lt;p&gt;假设有一个4节点的集群中有三个标签包含标签为 &lt;code&gt;foo:bar&lt;/code&gt;的 Pod， 分别分布在 1，2，3 号节点上(一个 &lt;code&gt;P&lt;/code&gt; 代表一个 Pod)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
|   P   |   P   |   P   |       |
+-------+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果想要新加入的 Pod 与之前的三个节点均匀的分布在不同的区域内， 可以使用如下配置:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintsone-constraintyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/one-constraint.yaml&#34; download=&#34;pods/topology-spread-constraints/one-constraint.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/one-constraint.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintsone-constraintyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/one-constraint.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;配置中 &lt;code&gt;topologyKey: zone&lt;/code&gt; 表示均匀分布只针对包含标签 &lt;code&gt;zone:&amp;lt;any value&amp;gt;&lt;/code&gt;的节点
&lt;code&gt;whenUnsatisfiable: DoNotSchedul&lt;/code&gt; 表示针对不满足约束的的 Pod， 调度器应该让其挂起&lt;/p&gt;
&lt;p&gt;如果调度器将 Pod 分配的 “zoneA”中， 则 Pod 分布就变成 [3,1], 这时偏差(skew)就为 2(3 - 1)
这就与 &lt;code&gt;maxSkew: 1&lt;/code&gt; 相违背。所以在本例中，新加入的 Pod 就只能被分配到  “zoneB”:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+---------------+      +---------------+---------------+
|     zoneA     |     zoneB     |      |     zoneA     |     zoneB     |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |  OR  | node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
|   P   |   P   |   P   |   P   |      |   P   |   P   |  P P  |       |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用户也可以通过调整配置实现不同的需求&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将 &lt;code&gt;maxSkew&lt;/code&gt; 设置为大于 &lt;code&gt;2&lt;/code&gt;， 这样新加入 Pod 也可以分配到 “zoneA”中&lt;/li&gt;
&lt;li&gt;将 &lt;code&gt;topologyKey&lt;/code&gt; 设置为 &amp;ldquo;node&amp;rdquo;, 则 Pod 的均匀分布范围就从区域变为节点&lt;/li&gt;
&lt;li&gt;将 &lt;code&gt;whenUnsatisfiable&lt;/code&gt; 设置为 &lt;code&gt;ScheduleAnyway&lt;/code&gt; 来保证新加入的 Pod 都能被调度(假设满足其它的调度 API)
但是，会被优先调度到匹配 Pod 少的拓扑域中(也要注意这个优先还需要连同其它内部调度优先级如资源使用率等一起考量)。&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Example: Multiple TopologySpreadConstraints

This builds upon the previous example. Suppose you have a 4-node cluster where 3 Pods labeled `foo:bar` are located in node1, node2 and node3 respectively (`P` represents Pod):

```
+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
|   P   |   P   |   P   |       |
+-------+-------+-------+-------+
```

You can use 2 TopologySpreadConstraints to control the Pods spreading on both zone and node:



 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintstwo-constraintsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/two-constraints.yaml&#34; download=&#34;pods/topology-spread-constraints/two-constraints.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/two-constraints.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintstwo-constraintsyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/two-constraints.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;node&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



In this case, to match the first constraint, the incoming Pod can only be placed onto &#34;zoneB&#34;; while in terms of the second constraint, the incoming Pod can only be placed onto &#34;node4&#34;. Then the results of 2 constraints are ANDed, so the only viable option is to place on &#34;node4&#34;.

Multiple constraints can lead to conflicts. Suppose you have a 3-node cluster across 2 zones:

```
+---------------+-------+
|     zoneA     | zoneB |
+-------+-------+-------+
| node1 | node2 | node3 |
+-------+-------+-------+
|  P P  |   P   |  P P  |
+-------+-------+-------+
```

If you apply &#34;two-constraints.yaml&#34; to this cluster, you will notice &#34;mypod&#34; stays in `Pending` state. This is because: to satisfy the first constraint, &#34;mypod&#34; can only be put to &#34;zoneB&#34;; while in terms of the second constraint, &#34;mypod&#34; can only put to &#34;node2&#34;. Then a joint result of &#34;zoneB&#34; and &#34;node2&#34; returns nothing.

To overcome this situation, you can either increase the `maxSkew` or modify one of the constraints to use `whenUnsatisfiable: ScheduleAnyway`.
 --&gt;
&lt;h3 id=&#34;示例-多个-topologyspreadconstraint&#34;&gt;示例: 多个 TopologySpreadConstraint&lt;/h3&gt;
&lt;p&gt;与上一个示例相同， 假设有一个4节点的集群中有三个标签包含标签为 &lt;code&gt;foo:bar&lt;/code&gt;的 Pod，
分别分布在 1，2，3 号节点上 (一个 &lt;code&gt;P&lt;/code&gt; 代表一个 Pod)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
|   P   |   P   |   P   |       |
+-------+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这次用两个 TopologySpreadConstraints， 同时通过 区域 和节点为控制 Pod 的分布&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintstwo-constraintsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/two-constraints.yaml&#34; download=&#34;pods/topology-spread-constraints/two-constraints.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/two-constraints.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintstwo-constraintsyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/two-constraints.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;node&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;在这种情况下， 要符合第一个约束， 新加入的 Pod 就分被分配到 “zoneB”
再要符合第二个约束，新加入的 Pod 就分被分配到  “node4”
而这两个约束之间是逻辑与关系，也就最终可分配的就 “node4”。&lt;/p&gt;
&lt;p&gt;多个约束可能产生冲突。比如集群在两个区域中有三个节点:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+-------+
|     zoneA     | zoneB |
+-------+-------+-------+
| node1 | node2 | node3 |
+-------+-------+-------+
|  P P  |   P   |  P P  |
+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果在这个集群中执行 &lt;code&gt;two-constraints.yaml&lt;/code&gt;， 就会发现名称为 &lt;code&gt;mypod&lt;/code&gt; 的 Pod 状态一直是 &lt;code&gt;Pending&lt;/code&gt;。
这是因为，要符合第一个约束， 就只能分配到 “zoneB”， 同时要符合第二个约束， 就只能分配到 “node2”
而 “zoneB” 与 “node2” 的交集为空集。&lt;/p&gt;
&lt;p&gt;要解决这种情况， 可以通过增加 &lt;code&gt;maxSkew&lt;/code&gt; 的值，
或 修改其中一个约束的 &lt;code&gt;whenUnsatisfiable&lt;/code&gt;值为&lt;code&gt;ScheduleAnyway&lt;/code&gt;&lt;/p&gt;
&lt;!--  
### Conventions

There are some implicit conventions worth noting here:

- Only the Pods holding the same namespace as the incoming Pod can be matching candidates.

- Nodes without `topologySpreadConstraints[*].topologyKey` present will be bypassed. It implies that:

  1. the Pods located on those nodes do not impact `maxSkew` calculation - in the above example, suppose &#34;node1&#34; does not have label &#34;zone&#34;, then the 2 Pods will be disregarded, hence the incoming Pod will be scheduled into &#34;zoneA&#34;.
  2. the incoming Pod has no chances to be scheduled onto this kind of nodes - in the above example, suppose a &#34;node5&#34; carrying label `{zone-typo: zoneC}` joins the cluster, it will be bypassed due to the absence of label key &#34;zone&#34;.

- Be aware of what will happen if the incomingPod’s `topologySpreadConstraints[*].labelSelector` doesn’t match its own labels. In the above example, if we remove the incoming Pod’s labels, it can still be placed onto &#34;zoneB&#34; since the constraints are still satisfied. However, after the placement, the degree of imbalance of the cluster remains unchanged - it’s still zoneA having 2 Pods which hold label {foo:bar}, and zoneB having 1 Pod which holds label {foo:bar}. So if this is not what you expect, we recommend the workload’s `topologySpreadConstraints[*].labelSelector` to match its own labels.

- If the incoming Pod has `spec.nodeSelector` or `spec.affinity.nodeAffinity` defined, nodes not matching them will be bypassed.

    Suppose you have a 5-node cluster ranging from zoneA to zoneC:

    ```
    +---------------+---------------+-------+
    |     zoneA     |     zoneB     | zoneC |
    +-------+-------+-------+-------+-------+
    | node1 | node2 | node3 | node4 | node5 |
    +-------+-------+-------+-------+-------+
    |   P   |   P   |   P   |       |       |
    +-------+-------+-------+-------+-------+
    ```

    and you know that &#34;zoneC&#34; must be excluded. In this case, you can compose the yaml as below, so that &#34;mypod&#34; will be placed onto &#34;zoneB&#34; instead of &#34;zoneC&#34;. Similarly `spec.nodeSelector` is also respected.

    

 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintsone-constraint-with-nodeaffinityyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&#34; download=&#34;pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintsone-constraint-with-nodeaffinityyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;affinity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nodeAffinity&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelectorTerms&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NotIn&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;zoneC&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


--&gt;
&lt;h3 id=&#34;约定&#34;&gt;约定&lt;/h3&gt;
&lt;p&gt;以下为一些值得注意的隐性约定:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;只有在同一个命名空间中的 Pod 才能作为匹配候选者&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;没有 &lt;code&gt;topologySpreadConstraints[*].topologyKey&lt;/code&gt; 的节点会被当作旁路，隐含的意思为:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;这些只点上的 Pod 不会被用于计算 &lt;code&gt;maxSkew&lt;/code&gt;， 在上面的例子中，假设 &lt;code&gt;node1&lt;/code&gt; 上没有标签 &lt;code&gt;zone&lt;/code&gt;，
这时其上的两个 Pod 会被忽略， 这样新加入的 Pod 就会被分配到  “zoneA”&lt;/li&gt;
&lt;li&gt;新加入的 Pod 也不会有机会被分配到此类节点上， 在上面的例子中，
假设集群中加入了一个 “node5” 上面有个标签为 &lt;code&gt;zone-typo: zoneC&lt;/code&gt;
这个节点(区域)会因为没有标签键 &lt;code&gt;zone&lt;/code&gt; 而被忽略&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;还可能发生的一种情况是 新加入的 Pod 上的 &lt;code&gt;topologySpreadConstraints[*].labelSelector&lt;/code&gt;
与自身的标签不匹配。在上面的例子中， 如果删除新加入 Pod 上的标签，该 Pod 也会被分配到 “zoneB”。
因为约束条件是满足的。 但是在 Pod 分配之后，集群的均衡程度并没有改变， 也就是 &lt;code&gt;zoneA&lt;/code&gt; 中
有两个包含标签 &lt;code&gt;foo:bar&lt;/code&gt;的 Pod， &lt;code&gt;zoneB&lt;/code&gt; 中 有一个包含标签 &lt;code&gt;foo:bar&lt;/code&gt;的 Pod。 如果这不是预期的行为，
官方推荐工作负载的 &lt;code&gt;topologySpreadConstraints[*].labelSelector&lt;/code&gt; 需要匹配自身的标签。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果新加入的 Pod 还定义了 &lt;code&gt;spec.nodeSelector&lt;/code&gt; 或 &lt;code&gt;spec.affinity.nodeAffinity&lt;/code&gt;
不匹配的节点也会被忽略。&lt;/p&gt;
&lt;p&gt;假如一个包含5个节点的集群，有A，B，C三个分区:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+---------------+-------+
|     zoneA     |     zoneB     | zoneC |
+-------+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 | node5 |
+-------+-------+-------+-------+-------+
|   P   |   P   |   P   |       |       |
+-------+-------+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果想让 zoneC 被排除， 这时可以使用以下配置，让 &lt;code&gt;mypod&lt;/code&gt; 被分配到 &lt;code&gt;zoneB&lt;/code&gt; 而不是 &lt;code&gt;zoneC&lt;/code&gt;
同样的 spec.nodeSelector 也要考量&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintsone-constraint-with-nodeaffinityyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&#34; download=&#34;pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintsone-constraint-with-nodeaffinityyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;affinity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nodeAffinity&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelectorTerms&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NotIn&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;zoneC&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Cluster-level default constraints






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [alpha]&lt;/code&gt;
&lt;/div&gt;



It is possible to set default topology spread constraints for a cluster. Default
topology spread constraints are applied to a Pod if, and only if:

- It doesn&#39;t define any constraints in its `.spec.topologySpreadConstraints`.
- It belongs to a service, replication controller, replica set or stateful set.

Default constraints can be set as part of the `PodTopologySpread` plugin args
in a [scheduling profile](/docs/reference/scheduling/profiles).
The constraints are specified with the same [API above](#api), except that
`labelSelector` must be empty. The selectors are calculated from the services,
replication controllers, replica sets or stateful sets that the Pod belongs to.

An example configuration might look like follows:

```yaml
apiVersion: kubescheduler.config.k8s.io/v1alpha2
kind: KubeSchedulerConfiguration

profiles:
  pluginConfig:
    - name: PodTopologySpread
      args:
        defaultConstraints:
          - maxSkew: 1
            topologyKey: failure-domain.beta.kubernetes.io/zone
            whenUnsatisfiable: ScheduleAnyway
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The score produced by default scheduling constraints might conflict with the
score produced by the
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/scheduling/profiles/#scheduling-plugins&#34;&gt;&lt;code&gt;DefaultPodTopologySpread&lt;/code&gt; plugin&lt;/a&gt;.
It is recommended that you disable this plugin in the scheduling profile when
using default constraints for &lt;code&gt;PodTopologySpread&lt;/code&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;集群级的默认约束&#34;&gt;集群级的默认约束&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;可以为一个集群设置默认的拓扑分布约束条件，默认拓扑分布约束条件能且仅能适用于:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod 没有在 &lt;code&gt;.spec.topologySpreadConstraints&lt;/code&gt; 中定义任何约束条件&lt;/li&gt;
&lt;li&gt;Pod 属于
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-replication-controller&#39; target=&#39;_blank&#39;&gt;ReplicationController&lt;span class=&#39;tooltip-text&#39;&gt;一个 (废弃的) API 对象用于管理多副本应用&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#39; target=&#39;_blank&#39;&gt;ReplicaSet&lt;span class=&#39;tooltip-text&#39;&gt;ReplicaSet 保证在任意时间点上某个 Pod 有指定数量副本在运行&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/statefulset/&#39; target=&#39;_blank&#39;&gt;StatefulSet&lt;span class=&#39;tooltip-text&#39;&gt;管理一个 Pod 集合的部署与容量伸缩， 这些 Pod 所以使用的存储是持久的(Pod 被替代后，新的 Pod 继承老 Pod 的存储)， Pod 的标识也是持久化的(重建 Pod 后名字不会变)&lt;/span&gt;
&lt;/a&gt; 之一&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;默认的约束条件可以作为 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/scheduling/profiles&#34;&gt;scheduling profile&lt;/a&gt;
中 &lt;code&gt;PodTopologySpread&lt;/code&gt; 插件参数的一部分。 这些约束条件可以能过同 &lt;a href=&#34;#api&#34;&gt;API&lt;/a&gt; 一样设置，
除了 &lt;code&gt;labelSelector&lt;/code&gt; 必须为空。 选择器通过 Pod 所属的&lt;br&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-replication-controller&#39; target=&#39;_blank&#39;&gt;ReplicationController&lt;span class=&#39;tooltip-text&#39;&gt;一个 (废弃的) API 对象用于管理多副本应用&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#39; target=&#39;_blank&#39;&gt;ReplicaSet&lt;span class=&#39;tooltip-text&#39;&gt;ReplicaSet 保证在任意时间点上某个 Pod 有指定数量副本在运行&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/statefulset/&#39; target=&#39;_blank&#39;&gt;StatefulSet&lt;span class=&#39;tooltip-text&#39;&gt;管理一个 Pod 集合的部署与容量伸缩， 这些 Pod 所以使用的存储是持久的(Pod 被替代后，新的 Pod 继承老 Pod 的存储)， Pod 的标识也是持久化的(重建 Pod 后名字不会变)&lt;/span&gt;
&lt;/a&gt;
计算得出。&lt;/p&gt;
&lt;p&gt;以下为示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubescheduler.config.k8s.io/v1alpha2&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;KubeSchedulerConfiguration&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;profiles&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;pluginConfig&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PodTopologySpread&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;defaultConstraints&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;failure-domain.beta.kubernetes.io/zone&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ScheduleAnyway&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 由默认调度约束计算的结果可能与&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/scheduling/profiles/#scheduling-plugins&#34;&gt;&lt;code&gt;DefaultPodTopologySpread&lt;/code&gt; plugin&lt;/a&gt;计算结果相冲突。
建议用户在使用&lt;code&gt;PodTopologySpread&lt;/code&gt;的默认约束时，关掉调度配置中的插件。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Comparison with PodAffinity/PodAntiAffiniaty

In Kubernetes, directives related to &#34;Affinity&#34; control how Pods are
scheduled - more packed or more scattered.

- For `PodAffinity`, you can try to pack any number of Pods into qualifying
  topology domain(s)
- For `PodAntiAffinity`, only one Pod can be scheduled into a
  single topology domain.

The &#34;EvenPodsSpread&#34; feature provides flexible options to distribute Pods evenly across different
topology domains - to achieve high availability or cost-saving. This can also help on rolling update
workloads and scaling out replicas smoothly. See [Motivation](https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation) for more details.
 --&gt;
&lt;h2 id=&#34;约束条件-vs-podaffinitypodantiaffiniaty&#34;&gt;约束条件 vs &lt;code&gt;PodAffinity/PodAntiAffiniaty&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;在 k8s 中， 与 &lt;code&gt;Affinity&lt;/code&gt; 相关用于控制 Pod 怎么调度的指令，或集中或分散&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于 &lt;code&gt;PodAffinity&lt;/code&gt;， 用户可以尝试向有资格的拓扑域中塞进任意数量的 Pod&lt;/li&gt;
&lt;li&gt;对于 &lt;code&gt;PodAntiAffinity&lt;/code&gt;， 一个 Pod 只能被调度到一个拓扑域中&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;EvenPodsSpread&lt;/code&gt; 特性提供了灵活的选项来让 Pod 均匀的分布到不同的拓扑域中，来达到高可用或减少开支的目的。
这也可以让滚动发布和动态扩容变得更平滑。更多信息见
&lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation&#34;&gt;Motivation&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Known Limitations

As of 1.18, at which this feature is Beta, there are some known limitations:

- Scaling down a Deployment may result in imbalanced Pods distribution.
- Pods matched on tainted nodes are respected. See [Issue 80921](https://github.com/kubernetes/kubernetes/issues/80921)
 --&gt;
&lt;h2 id=&#34;已知的限制&#34;&gt;已知的限制&lt;/h2&gt;
&lt;p&gt;到 &lt;code&gt;1.18&lt;/code&gt;，该特性还是 &lt;code&gt;Beta&lt;/code&gt; 状态， 还有以下已知的限制:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;收缩 Deployment 的容量可能导致 Pod 分布的不均匀。&lt;/li&gt;
&lt;li&gt;匹配到有 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/&#39; target=&#39;_blank&#39;&gt;Taint&lt;span class=&#39;tooltip-text&#39;&gt;A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups.&lt;/span&gt;
&lt;/a&gt; 节点也会被计入， 见&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/80921&#34;&gt;Issue 80921&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Pod 预设信息</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/podpreset/</link>
      <pubDate>Tue, 28 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/podpreset/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jessfraz
title: Pod Presets
content_type: concept
weight: 50
---
 --&gt;
&lt;!-- overview --&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.6 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;This page provides an overview of PodPresets, which are objects for injecting
certain information into pods at creation time. The information can include
secrets, volumes, volume mounts, and environment variables.&lt;/p&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.6 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;本文简单介绍 &lt;code&gt;PodPreset&lt;/code&gt;， 一个用于在特定时间向 Pod 中注入特定信息的对象。
可注入的信息包括
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;存放如密码, OAuth, ssh 密钥等敏感信息&lt;/span&gt;
&lt;/a&gt;,
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;卷(Volume)&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;,
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;卷(Volume)&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt; 挂载,
和环境变量&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Understanding Pod presets

A PodPreset is an API resource for injecting additional runtime requirements
into a Pod at creation time.
You use [label selectors](/docs/concepts/overview/working-with-objects/labels/#label-selectors)
to specify the Pods to which a given PodPreset applies.

Using a PodPreset allows pod template authors to not have to explicitly provide
all information for every pod. This way, authors of pod templates consuming a
specific service do not need to know all the details about that service.
--&gt;
&lt;h2 id=&#34;理解-pod-预设信息&#34;&gt;理解 Pod 预设信息&lt;/h2&gt;
&lt;p&gt;PodPreset 是在Pod 创建时向其中注入的运行环境需要的额外信息的 API 对象资源。
用户可以通过 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/labels/#label-selectors&#34;&gt;label selectors&lt;/a&gt;
来指定哪些 PodPreset 要用到该 Pod 上面。&lt;/p&gt;
&lt;p&gt;PodPreset 可以让 Pod 模板的创建者不必要为每个 Pod 提供都提供所有信息。
通过这种方式，Pod 模板的创建者在消费特定服务时，不需要知道该服务的所有细节&lt;/p&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Enable PodPreset in your cluster {#enable-pod-preset}

In order to use Pod presets in your cluster you must ensure the following:

1. You have enabled the API type `settings.k8s.io/v1alpha1/podpreset`. For
   example, this can be done by including `settings.k8s.io/v1alpha1=true` in
   the `--runtime-config` option for the API server. In minikube add this flag
   `--extra-config=apiserver.runtime-config=settings.k8s.io/v1alpha1=true` while
   starting the cluster.
1. You have enabled the admission controller named `PodPreset`. One way to doing this
   is to include `PodPreset` in the `--enable-admission-plugins` option value specified
   for the API server. For example, if you use Minikube, add this flag:

   ```shell
   --extra-config=apiserver.enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset
   ```

   while starting your cluster.
 --&gt;
&lt;h2 id=&#34;打开集群中的-podpreset&#34;&gt;打开集群中的 &lt;code&gt;PodPreset&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;为了能使用 Pod 预设信息，集群需要保证达到以下条件:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;需要启用 API 类型 &lt;code&gt;settings.k8s.io/v1alpha1/podpreset&lt;/code&gt;. 具体操作是:
在 api-server 中的 &lt;code&gt;--runtime-config&lt;/code&gt; 选项中添加 &lt;code&gt;settings.k8s.io/v1alpha1=true&lt;/code&gt;;
对于 minikube， 需要在集群启动时添加
&lt;code&gt;--extra-config=apiserver.runtime-config=settings.k8s.io/v1alpha1=true&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;需要启用一个叫 &lt;code&gt;PodPreset&lt;/code&gt; 的准入控制器。
一种方式是在 api-server &lt;code&gt;--enable-admission-plugins&lt;/code&gt; 选择值中添加 &lt;code&gt;PodPreset&lt;/code&gt;
对于 minikube, 则在集群启动时添加以下参数:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt; --extra-config&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;apiserver.enable-admission-plugins&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## How it works

Kubernetes provides an admission controller (`PodPreset`) which, when enabled,
applies Pod Presets to incoming pod creation requests.
When a pod creation request occurs, the system does the following:

1. Retrieve all `PodPresets` available for use.
1. Check if the label selectors of any `PodPreset` matches the labels on the
   pod being created.
1. Attempt to merge the various resources defined by the `PodPreset` into the
   Pod being created.
1. On error, throw an event documenting the merge error on the pod, and create
   the pod _without_ any injected resources from the `PodPreset`.
1. Annotate the resulting modified Pod spec to indicate that it has been
   modified by a `PodPreset`. The annotation is of the form
   `podpreset.admission.kubernetes.io/podpreset-&lt;pod-preset name&gt;: &#34;&lt;resource version&gt;&#34;`.

Each Pod can be matched by zero or more PodPresets; and each PodPreset can be
applied to zero or more Pods. When a PodPreset is applied to one or more
Pods, Kubernetes modifies the Pod Spec. For changes to `env`, `envFrom`, and
`volumeMounts`, Kubernetes modifies the container spec for all containers in
the Pod; for changes to `volumes`, Kubernetes modifies the Pod Spec.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;A Pod Preset is capable of modifying the following fields in a Pod spec when appropriate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;.spec.containers&lt;/code&gt; field&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;.spec.initContainers&lt;/code&gt; field&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;工作原理&#34;&gt;工作原理&lt;/h2&gt;
&lt;p&gt;k8s 提供了一个准入控制器(&lt;code&gt;PodPreset&lt;/code&gt;), 当这个控制器打开时，就会向进入的 Pod 创建请求执行。
当一个 Pod 的创建请求发生时， 系统会做以下操作:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;取得所有可用的 &lt;code&gt;PodPresets&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;检查 &lt;code&gt;PodPresets&lt;/code&gt; 标签选择器是否与新创建的 Pod 上的标签匹配&lt;/li&gt;
&lt;li&gt;尝试将 &lt;code&gt;PodPreset&lt;/code&gt; 中定义的各种资源合并到新创建的 Pod&lt;/li&gt;
&lt;li&gt;如果出错， 抛出一个一个事件描述合并出错到 Pod 上， 并在 &lt;em&gt;不&lt;/em&gt; 注意任意 &lt;code&gt;PodPreset&lt;/code&gt; 资源的情况下创建 Pod&lt;/li&gt;
&lt;li&gt;将由 &lt;code&gt;PodPreset&lt;/code&gt; 修改的结果加入到注解备查。注解格式为 &lt;code&gt;podpreset.admission.kubernetes.io/podpreset-&amp;lt;pod-preset name&amp;gt;: &amp;quot;&amp;lt;resource version&amp;gt;&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
### Disable Pod Preset for a specific pod

There may be instances where you wish for a Pod to not be altered by any Pod
preset mutations. In these cases, you can add an annotation in the Pod&#39;s `.spec`
of the form: `podpreset.admission.kubernetes.io/exclude: &#34;true&#34;`.
 --&gt;
&lt;h3 id=&#34;在指定-pod-禁用-podpreset&#34;&gt;在指定 Pod 禁用 &lt;code&gt;PodPreset&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;针对某些实例，用户可能不希望其被 &lt;code&gt;PodPreset&lt;/code&gt; 修改， 这种情况下， 用户可以在 Pod 定义上添加注解，
格式为 &lt;code&gt;podpreset.admission.kubernetes.io/exclude: &amp;quot;true&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
See [Injecting data into a Pod using PodPreset](/docs/tasks/inject-data-application/podpreset/)

For more information about the background, see the [design proposal for PodPreset](https://git.k8s.io/community/contributors/design-proposals/service-catalog/pod-preset.md).
 --&gt;
&lt;p&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/inject-data-application/podpreset/&#34;&gt;使用 PodPreset 向 Pod 注入数据&lt;/a&gt;
更多背景信息， 见&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/service-catalog/pod-preset.md&#34;&gt;design proposal for PodPreset&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
