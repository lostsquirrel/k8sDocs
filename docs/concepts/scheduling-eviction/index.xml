<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes – 调度与驱逐</title>
    <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/</link>
    <description>Recent content in 调度与驱逐 on Kubernetes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    
	  <atom:link href="https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: k8s 中的调度器</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/kube-scheduler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/kube-scheduler/</guid>
      <description>
        
        
        &lt;!--
---
title: Kubernetes Scheduler
content_type: concept
weight: 10
---
 --&gt;
&lt;!-- overview --&gt;
&lt;p&gt;在 k8s 中， 调度指的是搞清楚
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
是不是与
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;节点&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt;
匹配，当匹配了
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;Kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt;
才能在这个节点运行这个 Pod。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Scheduling overview {#scheduling}

A scheduler watches for newly created Pods that have no Node assigned. For
every Pod that the scheduler discovers, the scheduler becomes responsible
for finding the best Node for that Pod to run on. The scheduler reaches
this placement decision taking into account the scheduling principles
described below.

If you want to understand why Pods are placed onto a particular Node,
or if you&#39;re planning to implement a custom scheduler yourself, this
page will help you learn about scheduling.
 --&gt;
&lt;h2 id=&#34;scheduling&#34;&gt;调度概览&lt;/h2&gt;
&lt;p&gt;一个调度器会监测新创建但还没分配节点的 Pod。对于这个调度器发现的每一个 Pod， 它就将负责为这个
Pod 找到其运行的最佳节点。 调度器在进程调度决策时会考量现面介绍的调度原则。&lt;/p&gt;
&lt;p&gt;如果你想为明白为啥这个 Pod 会放到一个特定的节点，或你计划自己实现一个自定义调度器，本文会帮助你
学习调度。&lt;/p&gt;
&lt;!--
## kube-scheduler

[kube-scheduler](/docs/reference/command-line-tools-reference/kube-scheduler/)
is the default scheduler for Kubernetes and runs as part of the
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-control-plane&#39; target=&#39;_blank&#39;&gt;control plane&lt;span class=&#39;tooltip-text&#39;&gt;暴露那些定义，部署，管理容器生命周期的 API 和接口的容器编排层。&lt;/span&gt;
&lt;/a&gt;.
kube-scheduler is designed so that, if you want and need to, you can
write your own scheduling component and use that instead.

For every newly created pod or other unscheduled pods, kube-scheduler
selects an optimal node for them to run on. However, every container in
pods has different requirements for resources and every pod also has
different requirements. Therefore, existing nodes need to be filtered
according to the specific scheduling requirements.

In a cluster, Nodes that meet the scheduling requirements for a Pod
are called _feasible_ nodes. If none of the nodes are suitable, the pod
remains unscheduled until the scheduler is able to place it.

The scheduler finds feasible Nodes for a Pod and then runs a set of
functions to score the feasible Nodes and picks a Node with the highest
score among the feasible ones to run the Pod. The scheduler then notifies
the API server about this decision in a process called _binding_.

Factors that need taken into account for scheduling decisions include
individual and collective resource requirements, hardware / software /
policy constraints, affinity and anti-affinity specifications, data
locality, inter-workload interference, and so on.
 --&gt;
&lt;h2 id=&#34;kube-scheduler&#34;&gt;kube-scheduler&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/&#34;&gt;kube-scheduler&lt;/a&gt;
是 k8s 默认的调度器，并作为
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-control-plane&#39; target=&#39;_blank&#39;&gt;Control Plane&lt;span class=&#39;tooltip-text&#39;&gt;暴露那些定义，部署，管理容器生命周期的 API 和接口的容器编排层。&lt;/span&gt;
&lt;/a&gt;
的一部分运行。
kube-scheduler 在设计时就考虑到，如果用户希望并且有必要就可以编写并使用自己的调度模块，来代替默认
的调度器。&lt;/p&gt;
&lt;p&gt;对于每新创建的 Pod 或其它未调度的 Pod，kube-scheduler 选择一个最合适它们节点让他们运行。但是，
Pod 中的每个容器对资源有不同的保底要求同时每个 Pod 也有不同的要求。 因此，会根据这些调度要求过
滤现有的节点。&lt;/p&gt;
&lt;p&gt;在一个集群中， 那些满足 Pod 调度要求的节点就叫做 &lt;em&gt;可用的&lt;/em&gt; 节点。 如果没有一个节点合适，则 Pod
在调度器为其找到可用的节点之前就是未调度状态。&lt;/p&gt;
&lt;p&gt;调度器为一个 Pod 找到一系列可用的节点，然后对这些可用节点运行一系统函数进行算分，选择这些可用节点
中得分最高的可用节点来运行这个 Pod。 这时候调度器会通过一个叫做 &lt;em&gt;绑定(binding)&lt;/em&gt; 的过程通知 API 服务关于这次的决定&lt;/p&gt;
&lt;p&gt;在做调度决策时需要考量的因素包含 单个各总体的资源需求， 硬件 / 软件 策略约束， 亲和性和反亲和性规范，
数据地区，内部工作负载干扰，等等。&lt;/p&gt;
&lt;!--
### Node selection in kube-scheduler {#kube-scheduler-implementation}

kube-scheduler selects a node for the pod in a 2-step operation:

1. Filtering
1. Scoring

The _filtering_ step finds the set of Nodes where it&#39;s feasible to
schedule the Pod. For example, the PodFitsResources filter checks whether a
candidate Node has enough available resource to meet a Pod&#39;s specific
resource requests. After this step, the node list contains any suitable
Nodes; often, there will be more than one. If the list is empty, that
Pod isn&#39;t (yet) schedulable.

In the _scoring_ step, the scheduler ranks the remaining nodes to choose
the most suitable Pod placement. The scheduler assigns a score to each Node
that survived filtering, basing this score on the active scoring rules.

Finally, kube-scheduler assigns the Pod to the Node with the highest ranking.
If there is more than one node with equal scores, kube-scheduler selects
one of these at random.

There are two supported ways to configure the filtering and scoring behavior
of the scheduler:


1. [Scheduling Policies](/docs/reference/scheduling/policies) allow you to configure _Predicates_ for filtering and _Priorities_ for scoring.
1. [Scheduling Profiles](/docs/reference/scheduling/config/#profiles) allow you to configure Plugins that implement different scheduling stages, including: `QueueSort`, `Filter`, `Score`, `Bind`, `Reserve`, `Permit`, and others. You can also configure the kube-scheduler to run different profiles.

 --&gt;
&lt;h3 id=&#34;kube-scheduler-implementation&#34;&gt;kube-scheduler 中的节点选择&lt;/h3&gt;
&lt;p&gt;kube-scheduler 为 Pod 选择一个节点有 2 步操作:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;过滤&lt;/li&gt;
&lt;li&gt;算分&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;过滤&lt;/em&gt; 这一步是找到那些可以调度这个 Pod 的节点集合。 例如， PodFitsResources 过虑器检查这个
候选节点的可用资源是否满足 Pod 配置的资源要求。 在这一步之后，节点列表中还包含有任意可用节点；
通常会有不止一个。 如果列表是空，则这个 Pod 就不可调度。&lt;/p&gt;
&lt;p&gt;在 &lt;em&gt;算分&lt;/em&gt; 这一步，调度器会将剩余的节点排名挑选最适合放置 Pod 的地方。 调度会为每个在上步中留在
列表中的节点分配一个分数， 基于这个分数来激活算分规则&lt;/p&gt;
&lt;p&gt;最终， kube-scheduler 将这个 Pod 分配给排名第一的节点。如果有多个节点并列第一(分数一样)，
kube-scheduler 会从中随机选一个。&lt;/p&gt;
&lt;p&gt;支持以下两种配置调度器的过滤和算分行为:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/scheduling/policies&#34;&gt;调度策略&lt;/a&gt;
允许用户为过滤的 &lt;em&gt;Predicates&lt;/em&gt; 和为算分的 &lt;em&gt;优先级&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/scheduling/config/#profiles&#34;&gt;调度方案&lt;/a&gt;
允许用户配置不同的插件，这些插件实现了不同的调度阶段，包括:
&lt;code&gt;QueueSort&lt;/code&gt;, &lt;code&gt;Filter&lt;/code&gt;, &lt;code&gt;Score&lt;/code&gt;, &lt;code&gt;Bind&lt;/code&gt;, &lt;code&gt;Reserve&lt;/code&gt;, &lt;code&gt;Permit&lt;/code&gt;, 等。
也可以配置 kube-scheduler 运行不同的方案&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
* Read about [scheduler performance tuning](/docs/concepts/scheduling-eviction/scheduler-perf-tuning/)
* Read about [Pod topology spread constraints](/docs/concepts/workloads/pods/pod-topology-spread-constraints/)
* Read the [reference documentation](/docs/reference/command-line-tools-reference/kube-scheduler/) for kube-scheduler
* Learn about [configuring multiple schedulers](/docs/tasks/extend-kubernetes/configure-multiple-schedulers/)
* Learn about [topology management policies](/docs/tasks/administer-cluster/topology-manager/)
* Learn about [Pod Overhead](/docs/concepts/scheduling-eviction/pod-overhead/)
* Learn about scheduling of Pods that use volumes in:
  * [Volume Topology Support](/docs/concepts/storage/storage-classes/#volume-binding-mode)
  * [Storage Capacity Tracking](/docs/concepts/storage/storage-capacity/)
  * [Node-specific Volume Limits](/docs/concepts/storage/storage-limits/)
 --&gt;
&lt;ul&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/scheduler-perf-tuning/&#34;&gt;调度器性能优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/pod-topology-spread-constraints/&#34;&gt;Pod 拓扑散布约束&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;文档 &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/&#34;&gt;kube-scheduler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/extend-kubernetes/configure-multiple-schedulers/&#34;&gt;配置多个调度器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/administer-cluster/topology-manager/&#34;&gt;拓扑管理策略&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/pod-overhead/&#34;&gt;Pod 上限&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;关于使用卷的 Pod 的调度:
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes/#volume-binding-mode&#34;&gt;卷对拓扑的支持&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-capacity/&#34;&gt;存储容量跟踪&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-limits/&#34;&gt;节点级别的卷限制&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 毒点(Taint)与耐受性(Toleration)</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- davidopp
- kevin-wangzefeng
- bsalamat
title: Taints and Tolerations
content_type: concept
weight: 40
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
[_Node affinity_](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity),
is a property of &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; that *attracts* them to
a set of &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;nodes&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; (either as a preference or a
hard requirement). _Taints_ are the opposite -- they allow a node to repel a set of pods.

_Tolerations_ are applied to pods, and allow (but do not require) the pods to schedule
onto nodes with matching taints.

Taints and tolerations work together to ensure that pods are not scheduled
onto inappropriate nodes. One or more taints are applied to a node; this
marks that the node should not accept any pods that do not  the taints.
--&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity&#34;&gt;&lt;em&gt;节点亲和性&lt;/em&gt;&lt;/a&gt;,
是
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
的一个属性，这个属性会 &lt;em&gt;吸引&lt;/em&gt; 它们到一个
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;节点&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt;
集合(无论是偏好还是硬性要求)。
&lt;em&gt;毒点(Taint)&lt;/em&gt; 则正好相反 &amp;ndash; 它们允许一个节点排斥特定集合内的 Pod。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;耐受性(Toleration)&lt;/em&gt; 是会应用到 Pod 上的， 它允许(但不是必须)调度到拥有对应 毒点(Taint)的节点上。&lt;/p&gt;
&lt;p&gt;毒点(Taint)和耐受性(Toleration) 一起工作，以保证 Pod 不会调度到不合适的节点上。一个节点可以
应用一个或多个毒点(Taint)；这个标记表示节点不应该接收任意不能耐受(tolerate) 这些毒点(Taint)
的 Pod。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;h2 id=&#34;concepts&#34;&gt;Concepts&lt;/h2&gt;
&lt;p&gt;You add a taint to a node using &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubectl/kubectl-commands#taint&#34;&gt;kubectl taint&lt;/a&gt;.
For example,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl taint nodes node1 key1&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;value1:NoSchedule
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;places a taint on node &lt;code&gt;node1&lt;/code&gt;. The taint has key &lt;code&gt;key1&lt;/code&gt;, value &lt;code&gt;value1&lt;/code&gt;, and taint effect &lt;code&gt;NoSchedule&lt;/code&gt;.
This means that no pod will be able to schedule onto &lt;code&gt;node1&lt;/code&gt; unless it has a matching toleration.&lt;/p&gt;
&lt;p&gt;To remove the taint added by the command above, you can run:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl taint nodes node1 key1&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;value1:NoSchedule-
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You specify a toleration for a pod in the PodSpec. Both of the following tolerations &amp;ldquo;match&amp;rdquo; the
taint created by the &lt;code&gt;kubectl taint&lt;/code&gt; line above, and thus a pod with either toleration would be able
to schedule onto &lt;code&gt;node1&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:
- &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;key1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Equal&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;value1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NoSchedule&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:
- &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;key1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Exists&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NoSchedule&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here&amp;rsquo;s an example of a pod that uses tolerations:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;podspod-with-tolerationyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/pod-with-toleration.yaml&#34; download=&#34;pods/pod-with-toleration.yaml&#34;&gt;
                    &lt;code&gt;pods/pod-with-toleration.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podspod-with-tolerationyaml&#39;)&#34; title=&#34;Copy pods/pod-with-toleration.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;imagePullPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IfNotPresent&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;example-key&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Exists&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NoSchedule&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;The default value for &lt;code&gt;operator&lt;/code&gt; is &lt;code&gt;Equal&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A toleration &amp;ldquo;matches&amp;rdquo; a taint if the keys are the same and the effects are the same, and:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;code&gt;operator&lt;/code&gt; is &lt;code&gt;Exists&lt;/code&gt; (in which case no &lt;code&gt;value&lt;/code&gt; should be specified), or&lt;/li&gt;
&lt;li&gt;the &lt;code&gt;operator&lt;/code&gt; is &lt;code&gt;Equal&lt;/code&gt; and the &lt;code&gt;value&lt;/code&gt;s are equal.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;There are two special cases:&lt;/p&gt;
&lt;p&gt;An empty &lt;code&gt;key&lt;/code&gt; with operator &lt;code&gt;Exists&lt;/code&gt; matches all keys, values and effects which means this
will tolerate everything.&lt;/p&gt;
&lt;p&gt;An empty &lt;code&gt;effect&lt;/code&gt; matches all effects with key &lt;code&gt;key1&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;The above example used &lt;code&gt;effect&lt;/code&gt; of &lt;code&gt;NoSchedule&lt;/code&gt;. Alternatively, you can use &lt;code&gt;effect&lt;/code&gt; of &lt;code&gt;PreferNoSchedule&lt;/code&gt;.
This is a &amp;ldquo;preference&amp;rdquo; or &amp;ldquo;soft&amp;rdquo; version of &lt;code&gt;NoSchedule&lt;/code&gt; &amp;ndash; the system will &lt;em&gt;try&lt;/em&gt; to avoid placing a
pod that does not tolerate the taint on the node, but it is not required. The third kind of &lt;code&gt;effect&lt;/code&gt; is
&lt;code&gt;NoExecute&lt;/code&gt;, described later.&lt;/p&gt;
&lt;p&gt;You can put multiple taints on the same node and multiple tolerations on the same pod.
The way Kubernetes processes multiple taints and tolerations is like a filter: start
with all of a node&amp;rsquo;s taints, then ignore the ones for which the pod has a matching toleration; the
remaining un-ignored taints have the indicated effects on the pod. In particular,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if there is at least one un-ignored taint with effect &lt;code&gt;NoSchedule&lt;/code&gt; then Kubernetes will not schedule
the pod onto that node&lt;/li&gt;
&lt;li&gt;if there is no un-ignored taint with effect &lt;code&gt;NoSchedule&lt;/code&gt; but there is at least one un-ignored taint with
effect &lt;code&gt;PreferNoSchedule&lt;/code&gt; then Kubernetes will &lt;em&gt;try&lt;/em&gt; to not schedule the pod onto the node&lt;/li&gt;
&lt;li&gt;if there is at least one un-ignored taint with effect &lt;code&gt;NoExecute&lt;/code&gt; then the pod will be evicted from
the node (if it is already running on the node), and will not be
scheduled onto the node (if it is not yet running on the node).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, imagine you taint a node like this&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl taint nodes node1 key1&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;value1:NoSchedule
kubectl taint nodes node1 key1&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;value1:NoExecute
kubectl taint nodes node1 key2&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;value2:NoSchedule
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And a pod has two tolerations:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:
- &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;key1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Equal&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;value1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NoSchedule&amp;#34;&lt;/span&gt;
- &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;key1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Equal&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;value1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NoExecute&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In this case, the pod will not be able to schedule onto the node, because there is no
toleration matching the third taint. But it will be able to continue running if it is
already running on the node when the taint is added, because the third taint is the only
one of the three that is not tolerated by the pod.&lt;/p&gt;
&lt;p&gt;Normally, if a taint with effect &lt;code&gt;NoExecute&lt;/code&gt; is added to a node, then any pods that do
not tolerate the taint will be evicted immediately, and pods that do tolerate the
taint will never be evicted. However, a toleration with &lt;code&gt;NoExecute&lt;/code&gt; effect can specify
an optional &lt;code&gt;tolerationSeconds&lt;/code&gt; field that dictates how long the pod will stay bound
to the node after the taint is added. For example,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:
- &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;key1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Equal&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;value1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NoExecute&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;tolerationSeconds&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3600&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;means that if this pod is running and a matching taint is added to the node, then
the pod will stay bound to the node for 3600 seconds, and then be evicted. If the
taint is removed before that time, the pod will not be evicted.&lt;/p&gt;
&lt;h2 id=&#34;example-use-cases&#34;&gt;Example Use Cases&lt;/h2&gt;
&lt;p&gt;Taints and tolerations are a flexible way to steer pods &lt;em&gt;away&lt;/em&gt; from nodes or evict
pods that shouldn&amp;rsquo;t be running. A few of the use cases are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dedicated Nodes&lt;/strong&gt;: If you want to dedicate a set of nodes for exclusive use by
a particular set of users, you can add a taint to those nodes (say,
&lt;code&gt;kubectl taint nodes nodename dedicated=groupName:NoSchedule&lt;/code&gt;) and then add a corresponding
toleration to their pods (this would be done most easily by writing a custom
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/access-authn-authz/admission-controllers/&#34;&gt;admission controller&lt;/a&gt;).
The pods with the tolerations will then be allowed to use the tainted (dedicated) nodes as
well as any other nodes in the cluster. If you want to dedicate the nodes to them &lt;em&gt;and&lt;/em&gt;
ensure they &lt;em&gt;only&lt;/em&gt; use the dedicated nodes, then you should additionally add a label similar
to the taint to the same set of nodes (e.g. &lt;code&gt;dedicated=groupName&lt;/code&gt;), and the admission
controller should additionally add a node affinity to require that the pods can only schedule
onto nodes labeled with &lt;code&gt;dedicated=groupName&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Nodes with Special Hardware&lt;/strong&gt;: In a cluster where a small subset of nodes have specialized
hardware (for example GPUs), it is desirable to keep pods that don&amp;rsquo;t need the specialized
hardware off of those nodes, thus leaving room for later-arriving pods that do need the
specialized hardware. This can be done by tainting the nodes that have the specialized
hardware (e.g. &lt;code&gt;kubectl taint nodes nodename special=true:NoSchedule&lt;/code&gt; or
&lt;code&gt;kubectl taint nodes nodename special=true:PreferNoSchedule&lt;/code&gt;) and adding a corresponding
toleration to pods that use the special hardware. As in the dedicated nodes use case,
it is probably easiest to apply the tolerations using a custom
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/access-authn-authz/admission-controllers/&#34;&gt;admission controller&lt;/a&gt;.
For example, it is recommended to use &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/manage-resources-containers/#extended-resources&#34;&gt;Extended
Resources&lt;/a&gt;
to represent the special hardware, taint your special hardware nodes with the
extended resource name and run the
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration&#34;&gt;ExtendedResourceToleration&lt;/a&gt;
admission controller. Now, because the nodes are tainted, no pods without the
toleration will schedule on them. But when you submit a pod that requests the
extended resource, the &lt;code&gt;ExtendedResourceToleration&lt;/code&gt; admission controller will
automatically add the correct toleration to the pod and that pod will schedule
on the special hardware nodes. This will make sure that these special hardware
nodes are dedicated for pods requesting such hardware and you don&amp;rsquo;t have to
manually add tolerations to your pods.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Taint based Evictions&lt;/strong&gt;: A per-pod-configurable eviction behavior
when there are node problems, which is described in the next section.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;taint-based-evictions&#34;&gt;Taint based Evictions&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;NoExecute&lt;/code&gt; taint effect, mentioned above, affects pods that are already
running on the node as follows&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pods that do not tolerate the taint are evicted immediately&lt;/li&gt;
&lt;li&gt;pods that tolerate the taint without specifying &lt;code&gt;tolerationSeconds&lt;/code&gt; in
their toleration specification remain bound forever&lt;/li&gt;
&lt;li&gt;pods that tolerate the taint with a specified &lt;code&gt;tolerationSeconds&lt;/code&gt; remain
bound for the specified amount of time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The node controller automatically taints a Node when certain conditions
are true. The following taints are built in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/not-ready&lt;/code&gt;: Node is not ready. This corresponds to
the NodeCondition &lt;code&gt;Ready&lt;/code&gt; being &amp;ldquo;&lt;code&gt;False&lt;/code&gt;&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/unreachable&lt;/code&gt;: Node is unreachable from the node
controller. This corresponds to the NodeCondition &lt;code&gt;Ready&lt;/code&gt; being &amp;ldquo;&lt;code&gt;Unknown&lt;/code&gt;&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/out-of-disk&lt;/code&gt;: Node becomes out of disk.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/memory-pressure&lt;/code&gt;: Node has memory pressure.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/disk-pressure&lt;/code&gt;: Node has disk pressure.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/network-unavailable&lt;/code&gt;: Node&amp;rsquo;s network is unavailable.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/unschedulable&lt;/code&gt;: Node is unschedulable.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.cloudprovider.kubernetes.io/uninitialized&lt;/code&gt;: When the kubelet is started
with &amp;ldquo;external&amp;rdquo; cloud provider, this taint is set on a node to mark it
as unusable. After a controller from the cloud-controller-manager initializes
this node, the kubelet removes this taint.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In case a node is to be evicted, the node controller or the kubelet adds relevant taints
with &lt;code&gt;NoExecute&lt;/code&gt; effect. If the fault condition returns to normal the kubelet or node
controller can remove the relevant taint(s).&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The control plane limits the rate of adding node new taints to nodes. This rate limiting
manages the number of evictions that are triggered when many nodes become unreachable at
once (for example: if there is a network disruption).&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;You can specify &lt;code&gt;tolerationSeconds&lt;/code&gt; for a Pod to define how long that Pod stays bound
to a failing or unresponsive Node.&lt;/p&gt;
&lt;p&gt;For example, you might want to keep an application with a lot of local state
bound to node for a long time in the event of network partition, hoping
that the partition will recover and thus the pod eviction can be avoided.
The toleration you set for that Pod might look like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:
- &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;node.kubernetes.io/unreachable&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Exists&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NoExecute&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;tolerationSeconds&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;6000&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;Kubernetes automatically adds a toleration for
&lt;code&gt;node.kubernetes.io/not-ready&lt;/code&gt; and &lt;code&gt;node.kubernetes.io/unreachable&lt;/code&gt;
with &lt;code&gt;tolerationSeconds=300&lt;/code&gt;,
unless you, or a controller, set those tolerations explicitly.&lt;/p&gt;
&lt;p&gt;These automatically-added tolerations mean that Pods remain bound to
Nodes for 5 minutes after one of these problems is detected.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/daemonset/&#34;&gt;DaemonSet&lt;/a&gt; pods are created with
&lt;code&gt;NoExecute&lt;/code&gt; tolerations for the following taints with no &lt;code&gt;tolerationSeconds&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/unreachable&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/not-ready&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This ensures that DaemonSet pods are never evicted due to these problems.&lt;/p&gt;
&lt;!--
## Taint Nodes by Condition

The node lifecycle controller automatically creates taints corresponding to
Node conditions with `NoSchedule` effect.
Similarly the scheduler does not check Node conditions; instead the scheduler checks taints. This assures that Node conditions don&#39;t affect what&#39;s scheduled onto the Node. The user can choose to ignore some of the Node&#39;s problems (represented as Node conditions) by adding appropriate Pod tolerations.

The DaemonSet controller automatically adds the following `NoSchedule`
tolerations to all daemons, to prevent DaemonSets from breaking.

  * `node.kubernetes.io/memory-pressure`
  * `node.kubernetes.io/disk-pressure`
  * `node.kubernetes.io/out-of-disk` (*only for critical pods*)
  * `node.kubernetes.io/unschedulable` (1.10 or later)
  * `node.kubernetes.io/network-unavailable` (*host network only*)

Adding these tolerations ensures backward compatibility. You can also add
arbitrary tolerations to DaemonSets.
 --&gt;
&lt;h2 id=&#34;taint-nodes-by-condition&#34;&gt;根据条件给节点上毒点(Taint)&lt;/h2&gt;
&lt;p&gt;节点的生命周期控制自动地根据节点状况创建 &lt;code&gt;NoSchedule&lt;/code&gt; 效果的毒点(Taint)。
类似地，调度是不会检查节点状况的；替代方式是调度器检查毒点(Taint)。 这样做能保证节点状况不会影响
已经调度到这个节点上的工作负载。 用户也可以通过向 Pod 上添加对应的耐受性(Toleration)
选择忽略一个节点上的问题(以节点状况表示的)。&lt;/p&gt;
&lt;p&gt;DaemonSet 控制器自动添加以下 &lt;code&gt;NoSchedule&lt;/code&gt; 耐受性(Toleration)到所有的守护进程， 防止 DaemonSet 挂掉。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/memory-pressure&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/disk-pressure&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/out-of-disk&lt;/code&gt; (&lt;em&gt;仅对关键 Pod&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/unschedulable&lt;/code&gt; (v1.10+)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/network-unavailable&lt;/code&gt; (&lt;em&gt;仅主机网络&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;添加这些 耐受性(Toleration) 能确保向后兼容。 用户还可以向 DaemonSet 添加任意 耐受性(Toleration)&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Read about &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/administer-cluster/out-of-resource/&#34;&gt;out of resource handling&lt;/a&gt; and how you can configure it&lt;/li&gt;
&lt;li&gt;Read about &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/pod-priority-preemption/&#34;&gt;pod priority&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
