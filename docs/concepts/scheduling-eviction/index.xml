<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes – 调度与驱逐</title>
    <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/</link>
    <description>Recent content in 调度与驱逐 on Kubernetes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    
	  <atom:link href="https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: k8s 中的调度器</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/kube-scheduler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/kube-scheduler/</guid>
      <description>
        
        
        &lt;!--
---
title: Kubernetes Scheduler
content_type: concept
weight: 10
---
 --&gt;
&lt;!-- overview --&gt;
&lt;p&gt;在 k8s 中， 调度指的是搞清楚
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
是不是与
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;节点&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt;
匹配，当匹配了
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;Kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt;
才能在这个节点运行这个 Pod。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Scheduling overview {#scheduling}

A scheduler watches for newly created Pods that have no Node assigned. For
every Pod that the scheduler discovers, the scheduler becomes responsible
for finding the best Node for that Pod to run on. The scheduler reaches
this placement decision taking into account the scheduling principles
described below.

If you want to understand why Pods are placed onto a particular Node,
or if you&#39;re planning to implement a custom scheduler yourself, this
page will help you learn about scheduling.
 --&gt;
&lt;h2 id=&#34;scheduling&#34;&gt;调度概览&lt;/h2&gt;
&lt;p&gt;一个调度器会监测新创建但还没分配节点的 Pod。对于这个调度器发现的每一个 Pod， 它就将负责为这个
Pod 找到其运行的最佳节点。 调度器在进程调度决策时会考量现面介绍的调度原则。&lt;/p&gt;
&lt;p&gt;如果你想为明白为啥这个 Pod 会放到一个特定的节点，或你计划自己实现一个自定义调度器，本文会帮助你
学习调度。&lt;/p&gt;
&lt;!--
## kube-scheduler

[kube-scheduler](/docs/reference/command-line-tools-reference/kube-scheduler/)
is the default scheduler for Kubernetes and runs as part of the
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-control-plane&#39; target=&#39;_blank&#39;&gt;control plane&lt;span class=&#39;tooltip-text&#39;&gt;暴露那些定义，部署，管理容器生命周期的 API 和接口的容器编排层。&lt;/span&gt;
&lt;/a&gt;.
kube-scheduler is designed so that, if you want and need to, you can
write your own scheduling component and use that instead.

For every newly created pod or other unscheduled pods, kube-scheduler
selects an optimal node for them to run on. However, every container in
pods has different requirements for resources and every pod also has
different requirements. Therefore, existing nodes need to be filtered
according to the specific scheduling requirements.

In a cluster, Nodes that meet the scheduling requirements for a Pod
are called _feasible_ nodes. If none of the nodes are suitable, the pod
remains unscheduled until the scheduler is able to place it.

The scheduler finds feasible Nodes for a Pod and then runs a set of
functions to score the feasible Nodes and picks a Node with the highest
score among the feasible ones to run the Pod. The scheduler then notifies
the API server about this decision in a process called _binding_.

Factors that need taken into account for scheduling decisions include
individual and collective resource requirements, hardware / software /
policy constraints, affinity and anti-affinity specifications, data
locality, inter-workload interference, and so on.
 --&gt;
&lt;h2 id=&#34;kube-scheduler&#34;&gt;kube-scheduler&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/&#34;&gt;kube-scheduler&lt;/a&gt;
是 k8s 默认的调度器，并作为
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-control-plane&#39; target=&#39;_blank&#39;&gt;Control Plane&lt;span class=&#39;tooltip-text&#39;&gt;暴露那些定义，部署，管理容器生命周期的 API 和接口的容器编排层。&lt;/span&gt;
&lt;/a&gt;
的一部分运行。
kube-scheduler 在设计时就考虑到，如果用户希望并且有必要就可以编写并使用自己的调度模块，来代替默认
的调度器。&lt;/p&gt;
&lt;p&gt;对于每新创建的 Pod 或其它未调度的 Pod，kube-scheduler 选择一个最合适它们节点让他们运行。但是，
Pod 中的每个容器对资源有不同的保底要求同时每个 Pod 也有不同的要求。 因此，会根据这些调度要求过
滤现有的节点。&lt;/p&gt;
&lt;p&gt;在一个集群中， 那些满足 Pod 调度要求的节点就叫做 &lt;em&gt;可用的&lt;/em&gt; 节点。 如果没有一个节点合适，则 Pod
在调度器为其找到可用的节点之前就是未调度状态。&lt;/p&gt;
&lt;p&gt;调度器为一个 Pod 找到一系列可用的节点，然后对这些可用节点运行一系统函数进行算分，选择这些可用节点
中得分最高的可用节点来运行这个 Pod。 这时候调度器会通过一个叫做 &lt;em&gt;绑定(binding)&lt;/em&gt; 的过程通知 API 服务关于这次的决定&lt;/p&gt;
&lt;p&gt;在做调度决策时需要考量的因素包含 单个各总体的资源需求， 硬件 / 软件 策略约束， 亲和性和反亲和性规范，
数据地区，内部工作负载干扰，等等。&lt;/p&gt;
&lt;!--
### Node selection in kube-scheduler {#kube-scheduler-implementation}

kube-scheduler selects a node for the pod in a 2-step operation:

1. Filtering
1. Scoring

The _filtering_ step finds the set of Nodes where it&#39;s feasible to
schedule the Pod. For example, the PodFitsResources filter checks whether a
candidate Node has enough available resource to meet a Pod&#39;s specific
resource requests. After this step, the node list contains any suitable
Nodes; often, there will be more than one. If the list is empty, that
Pod isn&#39;t (yet) schedulable.

In the _scoring_ step, the scheduler ranks the remaining nodes to choose
the most suitable Pod placement. The scheduler assigns a score to each Node
that survived filtering, basing this score on the active scoring rules.

Finally, kube-scheduler assigns the Pod to the Node with the highest ranking.
If there is more than one node with equal scores, kube-scheduler selects
one of these at random.

There are two supported ways to configure the filtering and scoring behavior
of the scheduler:


1. [Scheduling Policies](/docs/reference/scheduling/policies) allow you to configure _Predicates_ for filtering and _Priorities_ for scoring.
1. [Scheduling Profiles](/docs/reference/scheduling/config/#profiles) allow you to configure Plugins that implement different scheduling stages, including: `QueueSort`, `Filter`, `Score`, `Bind`, `Reserve`, `Permit`, and others. You can also configure the kube-scheduler to run different profiles.

 --&gt;
&lt;h3 id=&#34;kube-scheduler-implementation&#34;&gt;kube-scheduler 中的节点选择&lt;/h3&gt;
&lt;p&gt;kube-scheduler 为 Pod 选择一个节点有 2 步操作:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;过滤&lt;/li&gt;
&lt;li&gt;算分&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;过滤&lt;/em&gt; 这一步是找到那些可以调度这个 Pod 的节点集合。 例如， PodFitsResources 过虑器检查这个
候选节点的可用资源是否满足 Pod 配置的资源要求。 在这一步之后，节点列表中还包含有任意可用节点；
通常会有不止一个。 如果列表是空，则这个 Pod 就不可调度。&lt;/p&gt;
&lt;p&gt;在 &lt;em&gt;算分&lt;/em&gt; 这一步，调度器会将剩余的节点排名挑选最适合放置 Pod 的地方。 调度会为每个在上步中留在
列表中的节点分配一个分数， 基于这个分数来激活算分规则&lt;/p&gt;
&lt;p&gt;最终， kube-scheduler 将这个 Pod 分配给排名第一的节点。如果有多个节点并列第一(分数一样)，
kube-scheduler 会从中随机选一个。&lt;/p&gt;
&lt;p&gt;支持以下两种配置调度器的过滤和算分行为:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/scheduling/policies&#34;&gt;调度策略&lt;/a&gt;
允许用户为过滤的 &lt;em&gt;Predicates&lt;/em&gt; 和为算分的 &lt;em&gt;优先级&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/scheduling/config/#profiles&#34;&gt;调度方案&lt;/a&gt;
允许用户配置不同的插件，这些插件实现了不同的调度阶段，包括:
&lt;code&gt;QueueSort&lt;/code&gt;, &lt;code&gt;Filter&lt;/code&gt;, &lt;code&gt;Score&lt;/code&gt;, &lt;code&gt;Bind&lt;/code&gt;, &lt;code&gt;Reserve&lt;/code&gt;, &lt;code&gt;Permit&lt;/code&gt;, 等。
也可以配置 kube-scheduler 运行不同的方案&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
* Read about [scheduler performance tuning](/docs/concepts/scheduling-eviction/scheduler-perf-tuning/)
* Read about [Pod topology spread constraints](/docs/concepts/workloads/pods/pod-topology-spread-constraints/)
* Read the [reference documentation](/docs/reference/command-line-tools-reference/kube-scheduler/) for kube-scheduler
* Learn about [configuring multiple schedulers](/docs/tasks/extend-kubernetes/configure-multiple-schedulers/)
* Learn about [topology management policies](/docs/tasks/administer-cluster/topology-manager/)
* Learn about [Pod Overhead](/docs/concepts/scheduling-eviction/pod-overhead/)
* Learn about scheduling of Pods that use volumes in:
  * [Volume Topology Support](/docs/concepts/storage/storage-classes/#volume-binding-mode)
  * [Storage Capacity Tracking](/docs/concepts/storage/storage-capacity/)
  * [Node-specific Volume Limits](/docs/concepts/storage/storage-limits/)
 --&gt;
&lt;ul&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/scheduler-perf-tuning/&#34;&gt;调度器性能优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/pod-topology-spread-constraints/&#34;&gt;Pod 拓扑散布约束&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;文档 &lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/&#34;&gt;kube-scheduler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/extend-kubernetes/configure-multiple-schedulers/&#34;&gt;配置多个调度器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/administer-cluster/topology-manager/&#34;&gt;拓扑管理策略&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/pod-overhead/&#34;&gt;Pod 上限&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;关于使用卷的 Pod 的调度:
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes/#volume-binding-mode&#34;&gt;卷对拓扑的支持&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-capacity/&#34;&gt;存储容量跟踪&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-limits/&#34;&gt;节点级别的卷限制&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 毒点(Taint)与耐受性(Toleration)</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- davidopp
- kevin-wangzefeng
- bsalamat
title: Taints and Tolerations
content_type: concept
weight: 40
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
[_Node affinity_](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity),
is a property of &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; that *attracts* them to
a set of &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;nodes&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; (either as a preference or a
hard requirement). _Taints_ are the opposite -- they allow a node to repel a set of pods.

_Tolerations_ are applied to pods, and allow (but do not require) the pods to schedule
onto nodes with matching taints.

Taints and tolerations work together to ensure that pods are not scheduled
onto inappropriate nodes. One or more taints are applied to a node; this
marks that the node should not accept any pods that do not  the taints.
--&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity&#34;&gt;&lt;em&gt;节点亲和性&lt;/em&gt;&lt;/a&gt;,
是
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
的一个属性，这个属性会 &lt;em&gt;吸引&lt;/em&gt; 它们到一个
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;节点&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt;
集合(无论是偏好还是硬性要求)。
&lt;em&gt;毒点(Taint)&lt;/em&gt; 则正好相反 &amp;ndash; 它们允许一个节点排斥特定集合内的 Pod。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;耐受性(Toleration)&lt;/em&gt; 是会应用到 Pod 上的， 它允许(但不是必须)调度到拥有对应 毒点(Taint)的节点上。&lt;/p&gt;
&lt;p&gt;毒点(Taint)和耐受性(Toleration) 一起工作，以保证 Pod 不会调度到不合适的节点上。一个节点可以
应用一个或多个毒点(Taint)；这个标记表示节点不应该接收任意不能耐受(tolerate) 这些毒点(Taint)
的 Pod。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Concepts

You add a taint to a node using [kubectl taint](/docs/reference/generated/kubectl/kubectl-commands#taint).
For example,

```shell
kubectl taint nodes node1 key1=value1:NoSchedule
```

places a taint on node `node1`. The taint has key `key1`, value `value1`, and taint effect `NoSchedule`.
This means that no pod will be able to schedule onto `node1` unless it has a matching toleration.

To remove the taint added by the command above, you can run:
```shell
kubectl taint nodes node1 key1=value1:NoSchedule-
```

You specify a toleration for a pod in the PodSpec. Both of the following tolerations &#34;match&#34; the
taint created by the `kubectl taint` line above, and thus a pod with either toleration would be able
to schedule onto `node1`:

```yaml
tolerations:
- key: &#34;key1&#34;
  operator: &#34;Equal&#34;
  value: &#34;value1&#34;
  effect: &#34;NoSchedule&#34;
```

```yaml
tolerations:
- key: &#34;key1&#34;
  operator: &#34;Exists&#34;
  effect: &#34;NoSchedule&#34;
```

Here&#39;s an example of a pod that uses tolerations:



 













&lt;table class=&#34;includecode&#34; id=&#34;podspod-with-tolerationyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/pod-with-toleration.yaml&#34; download=&#34;pods/pod-with-toleration.yaml&#34;&gt;
                    &lt;code&gt;pods/pod-with-toleration.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podspod-with-tolerationyaml&#39;)&#34; title=&#34;Copy pods/pod-with-toleration.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;imagePullPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IfNotPresent&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;example-key&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Exists&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NoSchedule&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



The default value for `operator` is `Equal`.

A toleration &#34;matches&#34; a taint if the keys are the same and the effects are the same, and:

* the `operator` is `Exists` (in which case no `value` should be specified), or
* the `operator` is `Equal` and the `value`s are equal.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;There are two special cases:&lt;/p&gt;
&lt;p&gt;An empty &lt;code&gt;key&lt;/code&gt; with operator &lt;code&gt;Exists&lt;/code&gt; matches all keys, values and effects which means this
will tolerate everything.&lt;/p&gt;
&lt;p&gt;An empty &lt;code&gt;effect&lt;/code&gt; matches all effects with key &lt;code&gt;key1&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;


The above example used `effect` of `NoSchedule`. Alternatively, you can use `effect` of `PreferNoSchedule`.
This is a &#34;preference&#34; or &#34;soft&#34; version of `NoSchedule` -- the system will *try* to avoid placing a
pod that does not tolerate the taint on the node, but it is not required. The third kind of `effect` is
`NoExecute`, described later.

You can put multiple taints on the same node and multiple tolerations on the same pod.
The way Kubernetes processes multiple taints and tolerations is like a filter: start
with all of a node&#39;s taints, then ignore the ones for which the pod has a matching toleration; the
remaining un-ignored taints have the indicated effects on the pod. In particular,

* if there is at least one un-ignored taint with effect `NoSchedule` then Kubernetes will not schedule
the pod onto that node
* if there is no un-ignored taint with effect `NoSchedule` but there is at least one un-ignored taint with
effect `PreferNoSchedule` then Kubernetes will *try* to not schedule the pod onto the node
* if there is at least one un-ignored taint with effect `NoExecute` then the pod will be evicted from
the node (if it is already running on the node), and will not be
scheduled onto the node (if it is not yet running on the node).

For example, imagine you taint a node like this

```shell
kubectl taint nodes node1 key1=value1:NoSchedule
kubectl taint nodes node1 key1=value1:NoExecute
kubectl taint nodes node1 key2=value2:NoSchedule
```

And a pod has two tolerations:

```yaml
tolerations:
- key: &#34;key1&#34;
  operator: &#34;Equal&#34;
  value: &#34;value1&#34;
  effect: &#34;NoSchedule&#34;
- key: &#34;key1&#34;
  operator: &#34;Equal&#34;
  value: &#34;value1&#34;
  effect: &#34;NoExecute&#34;
```

In this case, the pod will not be able to schedule onto the node, because there is no
toleration matching the third taint. But it will be able to continue running if it is
already running on the node when the taint is added, because the third taint is the only
one of the three that is not tolerated by the pod.

Normally, if a taint with effect `NoExecute` is added to a node, then any pods that do
not tolerate the taint will be evicted immediately, and pods that do tolerate the
taint will never be evicted. However, a toleration with `NoExecute` effect can specify
an optional `tolerationSeconds` field that dictates how long the pod will stay bound
to the node after the taint is added. For example,

```yaml
tolerations:
- key: &#34;key1&#34;
  operator: &#34;Equal&#34;
  value: &#34;value1&#34;
  effect: &#34;NoExecute&#34;
  tolerationSeconds: 3600
```

means that if this pod is running and a matching taint is added to the node, then
the pod will stay bound to the node for 3600 seconds, and then be evicted. If the
taint is removed before that time, the pod will not be evicted. --&gt;
&lt;h2 id=&#34;concepts&#34;&gt;概念&lt;/h2&gt;
&lt;p&gt;用户可以使用
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#taint&#34;&gt;kubectl taint&lt;/a&gt;
来给一个节点添加一个毒点(Taint),
例如，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl taint nodes node1 key1&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;value1:NoSchedule
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在节点 &lt;code&gt;node1&lt;/code&gt; 上放置一个毒点(Taint)。 这个 毒点(Taint)的键是 &lt;code&gt;key1&lt;/code&gt;, 值是 &lt;code&gt;value1&lt;/code&gt;，
效果是 &lt;code&gt;NoSchedule&lt;/code&gt;。 它的含义是如果 Pod 是没有对应的耐受性(Toleration) 就不能调度到节点
&lt;code&gt;node1&lt;/code&gt; 上。&lt;/p&gt;
&lt;p&gt;要移除上面的命令添加的 毒点(Taint)，可以使用:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl taint nodes node1 key1&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;value1:NoSchedule-
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;用户可以在 Pod 的 &lt;code&gt;.spec&lt;/code&gt; 中指定耐受性(Toleration)。 下面的两个耐受性(Toleration) 就可以
匹配由上面&lt;code&gt;kubectl taint&lt;/code&gt; 命令定义的耐受性(Toleration)，也就是一个 Pod 如果包含以下耐受性(Toleration)
中的任意一个就可以调度到节点  &lt;code&gt;node1&lt;/code&gt; 上:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:
- &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;key1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Equal&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;value1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NoSchedule&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:
- &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;key1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Exists&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NoSchedule&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;现在是一个使用耐受性(Toleration)的示例:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;podspod-with-tolerationyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/pod-with-toleration.yaml&#34; download=&#34;pods/pod-with-toleration.yaml&#34;&gt;
                    &lt;code&gt;pods/pod-with-toleration.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podspod-with-tolerationyaml&#39;)&#34; title=&#34;Copy pods/pod-with-toleration.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;imagePullPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IfNotPresent&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;example-key&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Exists&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NoSchedule&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;&lt;code&gt;operator&lt;/code&gt; 的默认值是 &lt;code&gt;Equal&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;一个耐受性(Toleration)匹配一个毒点(Taint)条件是 键要相同并且效果要一样，同时:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;operator&lt;/code&gt; is &lt;code&gt;Exists&lt;/code&gt; (这种情况不应该指定值), 或&lt;/li&gt;
&lt;li&gt;&lt;code&gt;operator&lt;/code&gt; is &lt;code&gt;Equal&lt;/code&gt; 并且 &lt;code&gt;value&lt;/code&gt; 是相等的。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;还有两种特殊情况:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;operator&lt;/code&gt; is &lt;code&gt;Exists&lt;/code&gt;, &lt;code&gt;key&lt;/code&gt; 是空表示匹配所有的键，值和效果，也就代表耐受所有毒点(Taint)&lt;/p&gt;
&lt;p&gt;如果 &lt;code&gt;effect&lt;/code&gt; 是空则匹配键 &lt;code&gt;key1&lt;/code&gt; 的所有效果&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面示例果使用的 &lt;code&gt;effect&lt;/code&gt; 是 &lt;code&gt;NoSchedule&lt;/code&gt;. 还有另一个选择是让 &lt;code&gt;effect&lt;/code&gt; 使用 &lt;code&gt;PreferNoSchedule&lt;/code&gt;.
这是 &lt;code&gt;NoSchedule&lt;/code&gt; 的 &amp;ldquo;偏好&amp;rdquo; 或 &amp;ldquo;软&amp;rdquo; 版本 &amp;ndash; 也就是系统会 &lt;em&gt;尝试&lt;/em&gt; 避免将那些不能耐受这个
毒点(Taint) 的 Pod 放到这个节点上，但不是一定不能放。 &lt;code&gt;effect&lt;/code&gt; 第三个选择是 &lt;code&gt;NoExecute&lt;/code&gt; 后面
会介绍。&lt;/p&gt;
&lt;p&gt;用户可以在一个节点上添加多个毒点(Taint)，也可以在一个 Pod 上定义多个 耐受性(Toleration).
k8s 处理多个 毒点(Taint)和 耐受性(Toleration)的方式与过虑器类似: 从节点的所有 毒点(Taint)
开始， 然后忽略掉那些与 Pod 耐受性(Toleration)匹配的；剩下的就是对 Pod 生效的毒点(Taint)。
特别是，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果剩下未忽略的毒点(Taint)中至少有一个效果是 &lt;code&gt;NoSchedule&lt;/code&gt; 则 k8s 不会将这个 Pod 调度到这个节点上&lt;/li&gt;
&lt;li&gt;如果剩下未忽略的毒点(Taint)中没有效果是 &lt;code&gt;NoSchedule&lt;/code&gt;， 但是至少有一个效果是 &lt;code&gt;PreferNoSchedule&lt;/code&gt;
则 k8s 会 &lt;em&gt;尝试&lt;/em&gt; 不将这个 Pod 调度到这个节点上&lt;/li&gt;
&lt;li&gt;如果剩下未忽略的毒点(Taint)中至少有一个效果是 &lt;code&gt;NoExecute&lt;/code&gt;，则 Pod 会被从节点驱逐(如果 Pod
已经在节点上运行)，或不会被调度到这个节点(如果它还没有在这个节点上运行)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如，假设将一个节点添加如下毒点(Taint)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl taint nodes node1 key1&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;value1:NoSchedule
kubectl taint nodes node1 key1&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;value1:NoExecute
kubectl taint nodes node1 key2&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;value2:NoSchedule
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;和一个有以下两个耐受性(Toleration) Pod:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:
- &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;key1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Equal&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;value1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NoSchedule&amp;#34;&lt;/span&gt;
- &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;key1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Equal&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;value1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NoExecute&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在这种情况下， Pod 是会能调度到这个节点上的， 因为没有一个 耐受性(Toleration)能匹配到第三个
毒点(Taint)。 但如果这个 Pod 在这些毒点(Taint)添加前已经在这个节点上运行，则它还是可以继续在
上面运行，因为 Pod 不耐受的毒点(Taint)只有第三个。&lt;/p&gt;
&lt;p&gt;通常情况下，如果一个效果是 &lt;code&gt;NoExecute&lt;/code&gt; 的毒点(Taint)被添加到节点上，则所有不能耐受这个毒点(Taint)
的 Pod 立马就会被驱逐，同时耐受这个毒点(Taint) Pod 则永远不会被驱逐。 但是，效果是  &lt;code&gt;NoExecute&lt;/code&gt;
的 耐受性(Toleration) 还可以指定一个可选的 &lt;code&gt;tolerationSeconds&lt;/code&gt; 字段，表示在节点添加 毒点(Taint)
后 Pod 还可以在节点上保持绑定状态多久。例如，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:
- &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;key1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Equal&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;value1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NoExecute&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;tolerationSeconds&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3600&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;的含义就是如果这个 Pod 运行在一个添加了这个耐受性(Toleration)匹配 毒点(Taint)的节点， 则
Pod 会继续与节点保持绑定 3600 秒，然后被驱逐。 如果在这个时间之前，这个 毒点(Taint) 被从该
节点移除，则 Pod 不会被驱逐。&lt;/p&gt;
&lt;!--
## Example Use Cases

Taints and tolerations are a flexible way to steer pods *away* from nodes or evict
pods that shouldn&#39;t be running. A few of the use cases are

* **Dedicated Nodes**: If you want to dedicate a set of nodes for exclusive use by
a particular set of users, you can add a taint to those nodes (say,
`kubectl taint nodes nodename dedicated=groupName:NoSchedule`) and then add a corresponding
toleration to their pods (this would be done most easily by writing a custom
[admission controller](/docs/reference/access-authn-authz/admission-controllers/)).
The pods with the tolerations will then be allowed to use the tainted (dedicated) nodes as
well as any other nodes in the cluster. If you want to dedicate the nodes to them *and*
ensure they *only* use the dedicated nodes, then you should additionally add a label similar
to the taint to the same set of nodes (e.g. `dedicated=groupName`), and the admission
controller should additionally add a node affinity to require that the pods can only schedule
onto nodes labeled with `dedicated=groupName`.

* **Nodes with Special Hardware**: In a cluster where a small subset of nodes have specialized
hardware (for example GPUs), it is desirable to keep pods that don&#39;t need the specialized
hardware off of those nodes, thus leaving room for later-arriving pods that do need the
specialized hardware. This can be done by tainting the nodes that have the specialized
hardware (e.g. `kubectl taint nodes nodename special=true:NoSchedule` or
`kubectl taint nodes nodename special=true:PreferNoSchedule`) and adding a corresponding
toleration to pods that use the special hardware. As in the dedicated nodes use case,
it is probably easiest to apply the tolerations using a custom
[admission controller](/docs/reference/access-authn-authz/admission-controllers/).
For example, it is recommended to use [Extended
Resources](/docs/concepts/configuration/manage-resources-containers/#extended-resources)
to represent the special hardware, taint your special hardware nodes with the
extended resource name and run the
[ExtendedResourceToleration](/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration)
admission controller. Now, because the nodes are tainted, no pods without the
toleration will schedule on them. But when you submit a pod that requests the
extended resource, the `ExtendedResourceToleration` admission controller will
automatically add the correct toleration to the pod and that pod will schedule
on the special hardware nodes. This will make sure that these special hardware
nodes are dedicated for pods requesting such hardware and you don&#39;t have to
manually add tolerations to your pods.

* **Taint based Evictions**: A per-pod-configurable eviction behavior
when there are node problems, which is described in the next section.
 --&gt;
&lt;h2 id=&#34;example-use-cases&#34;&gt;使用场景示例&lt;/h2&gt;
&lt;p&gt;毒点(Taint) 和 耐受性(Toleration) 是控制 Pod &lt;em&gt;远离&lt;/em&gt; 某些节点或驱逐那些不应该运行的 Pod。
一些使用场景:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;专用节点&lt;/strong&gt;: 如果想要将一组专用节点让一组特定用户独占，可以在这个节点上添加一个毒点(Taint)
(就比如, &lt;code&gt;kubectl taint nodes nodename dedicated=groupName:NoSchedule&lt;/code&gt;)然后在他们
的 Pod 上添加对应的耐受性(Toleration)(实现这个最简单的办法是写一个自定义
&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/&#34;&gt;准入控制&lt;/a&gt;
)。
这些有这个耐受性(Toleration)的 Pod 就被允许使用这些毒掉的(专用)节点和集群中的其它节点。
如果想将这些节点给他们独占 &lt;em&gt;并且&lt;/em&gt; 确保他们也 &lt;em&gt;只能&lt;/em&gt; 使用这些专用节点, 还应该在这一组节点上添加
与毒点(Taint)类似的标签(例如 &lt;code&gt;dedicated=groupName&lt;/code&gt;), 然后在准入控制器上另外添加一个节点亲和性
来标这些 Pod 只能调度到那些有 &lt;code&gt;dedicated=groupName&lt;/code&gt; 标签的节点。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;拥有特殊硬件的节点&lt;/strong&gt;: 在一个集群中，有一小部分节点有专用的硬件(例如 GPU)，这时就希望不要
将那些不需要这些专用硬件的 Pod 调度到这些节点上， 这样就可以把这些节点留给后来的为那些需要这些专用
硬件的 Pod。 要做到这一点，可以在这些有专用硬件的节点上加毒点(Taint)(例如，
&lt;code&gt;kubectl taint nodes nodename special=true:NoSchedule&lt;/code&gt; 或
&lt;code&gt;kubectl taint nodes nodename special=true:PreferNoSchedule&lt;/code&gt;
)再在需要这些专用硬件的 Pod 上添加对应的耐受性(Toleration)。 与上面的专用节点使用场景一样，
应用这些 耐受性(Toleration)最简单的方式就是使用自定义
&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/&#34;&gt;准入控制器&lt;/a&gt;.
例如推荐使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/manage-resources-containers/#extended-resources&#34;&gt;扩展资源&lt;/a&gt;
来表示特殊的硬件， 使用扩展资源的名称作为节点的 毒点(Taint) 然后运行
&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration&#34;&gt;ExtendedResourceToleration&lt;/a&gt;
准入控制器。 此时， 因为节点上已经打了 毒点(Taint)没有对应耐受性(Toleration)的 Pod 不会调度
到这些节点上。 但当提供一个要求这些扩展资源的 Pod 时， &lt;code&gt;ExtendedResourceToleration&lt;/code&gt; 准入
控制器会自动为这些 Pod 添加对应的耐受性(Toleration)，这样这些 Pod 就会被调度到这些有特殊硬件
的节点上。 这会保证这些特殊硬件的节点只会被这些要求这个硬件的 Pod 单独使用，并且不需要手动为这些
Pod 添加耐受性(Toleration)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Taint based Evictions&lt;/strong&gt;: A per-pod-configurable eviction behavior
when there are node problems, which is described in the next section.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;基于毒点(Taint)的驱逐行为&lt;/strong&gt;: 一个每个 Pod 都可配置的驱逐行为就是当节点出问题时进行的，
这会在下一节点介绍。&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Taint based Evictions






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;



The `NoExecute` taint effect, mentioned above, affects pods that are already
running on the node as follows

 * pods that do not tolerate the taint are evicted immediately
 * pods that tolerate the taint without specifying `tolerationSeconds` in
   their toleration specification remain bound forever
 * pods that tolerate the taint with a specified `tolerationSeconds` remain
   bound for the specified amount of time

The node controller automatically taints a Node when certain conditions
are true. The following taints are built in:

 * `node.kubernetes.io/not-ready`: Node is not ready. This corresponds to
   the NodeCondition `Ready` being &#34;`False`&#34;.
 * `node.kubernetes.io/unreachable`: Node is unreachable from the node
   controller. This corresponds to the NodeCondition `Ready` being &#34;`Unknown`&#34;.
 * `node.kubernetes.io/out-of-disk`: Node becomes out of disk.
 * `node.kubernetes.io/memory-pressure`: Node has memory pressure.
 * `node.kubernetes.io/disk-pressure`: Node has disk pressure.
 * `node.kubernetes.io/network-unavailable`: Node&#39;s network is unavailable.
 * `node.kubernetes.io/unschedulable`: Node is unschedulable.
 * `node.cloudprovider.kubernetes.io/uninitialized`: When the kubelet is started
    with &#34;external&#34; cloud provider, this taint is set on a node to mark it
    as unusable. After a controller from the cloud-controller-manager initializes
    this node, the kubelet removes this taint.

In case a node is to be evicted, the node controller or the kubelet adds relevant taints
with `NoExecute` effect. If the fault condition returns to normal the kubelet or node
controller can remove the relevant taint(s).

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The control plane limits the rate of adding node new taints to nodes. This rate limiting
manages the number of evictions that are triggered when many nodes become unreachable at
once (for example: if there is a network disruption).&lt;/div&gt;
&lt;/blockquote&gt;


You can specify `tolerationSeconds` for a Pod to define how long that Pod stays bound
to a failing or unresponsive Node.

For example, you might want to keep an application with a lot of local state
bound to node for a long time in the event of network partition, hoping
that the partition will recover and thus the pod eviction can be avoided.
The toleration you set for that Pod might look like:

```yaml
tolerations:
- key: &#34;node.kubernetes.io/unreachable&#34;
  operator: &#34;Exists&#34;
  effect: &#34;NoExecute&#34;
  tolerationSeconds: 6000
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;Kubernetes automatically adds a toleration for
&lt;code&gt;node.kubernetes.io/not-ready&lt;/code&gt; and &lt;code&gt;node.kubernetes.io/unreachable&lt;/code&gt;
with &lt;code&gt;tolerationSeconds=300&lt;/code&gt;,
unless you, or a controller, set those tolerations explicitly.&lt;/p&gt;
&lt;p&gt;These automatically-added tolerations mean that Pods remain bound to
Nodes for 5 minutes after one of these problems is detected.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;


[DaemonSet](/docs/concepts/workloads/controllers/daemonset/) pods are created with
`NoExecute` tolerations for the following taints with no `tolerationSeconds`:

  * `node.kubernetes.io/unreachable`
  * `node.kubernetes.io/not-ready`

This ensures that DaemonSet pods are never evicted due to these problems.
 --&gt;
&lt;h2 id=&#34;taint-based-evictions&#34;&gt;基于毒点(Taint) 的驱逐&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;上面提到的影响为 &lt;code&gt;NoExecute&lt;/code&gt; 毒点(Taint)，对已经在节点上运行的 Pod 的影响如下&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不能耐受这个 毒点(Taint) 的 Pod 会立马被踢出&lt;/li&gt;
&lt;li&gt;能够耐受这个毒点(Taint)并且在耐受定义中没有指定 &lt;code&gt;tolerationSeconds&lt;/code&gt; 的 Pod 会一直继续在这个节点上运行&lt;/li&gt;
&lt;li&gt;能够耐受这个毒点(Taint)并且在耐受定义中指定了 &lt;code&gt;tolerationSeconds&lt;/code&gt; 则会继续在这个节点上运行指定的这个时间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;节点控制器会在特定条件满足是自动给节点添加 毒点(Taint). 以下为内置毒点(Taint):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/not-ready&lt;/code&gt;: 节点还没有就绪。 这对应的节点状态(NodeCondition) &lt;code&gt;Ready&lt;/code&gt; 是 &amp;ldquo;&lt;code&gt;False&lt;/code&gt;&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/unreachable&lt;/code&gt;: 节点控制器连接不到节点。 这对应的节点状态(NodeCondition) &lt;code&gt;Ready&lt;/code&gt; 是 &amp;ldquo;&lt;code&gt;Unknown&lt;/code&gt;&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/out-of-disk&lt;/code&gt;: 节点硬盘不足&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/memory-pressure&lt;/code&gt;: 节点有内存使用紧张&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/disk-pressure&lt;/code&gt;: 节点有硬盘使用紧张&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/network-unavailable&lt;/code&gt;: 节点网络不可用&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/unschedulable&lt;/code&gt;: 节点不能作为调度目标&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.cloudprovider.kubernetes.io/uninitialized&lt;/code&gt;: 当 kubelet 使用&amp;quot;外部&amp;quot;云提供商启动时，
使用这个 毒点(Taint) 将其标记为不可用。 在 &lt;code&gt;cloud-controller-manager&lt;/code&gt; 中的控制器初始化这
个节点之后， kubelet 会移除这个毒点(Taint)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有一种情况是一个节点被驱逐了，节点控制器或 kubelet 会添加相应效果为 &lt;code&gt;NoExecute&lt;/code&gt; 的毒点(Taint)。
如果这个节点从错误状态重新变回正常状态节点控制器或 kubelet 会移除相应的毒点(Taint)。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 控制中心会限制添加到节点上的 毒点(Taint) 的速率。这个速率限制是在许多节点一下都变得不可达时管理
被驱逐的数量(例如: 发生了网络抖动)&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;可以为 Pod 指定 &lt;code&gt;tolerationSeconds&lt;/code&gt; 来定义在节点失效或不响应时继续让这个 Pod 保持在这个节点
上的时间。&lt;/p&gt;
&lt;p&gt;例如，当一个有许多本地状态应用与在出现网络分区的情况下继续保持在这个节点上相当长一段时间，以期望
这个分区在些期间能够恢复，这样可以避免 Pod 被驱逐。 设置在这个 Pod 上的耐受性(Toleration)可能
会长成这个样子:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:
- &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;node.kubernetes.io/unreachable&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Exists&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;NoExecute&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;tolerationSeconds&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;6000&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;除非有用户或控制器显示地设置这些 耐受性(Toleration)，否则 k8s 会自动地在添加
&lt;code&gt;node.kubernetes.io/not-ready&lt;/code&gt; 和 &lt;code&gt;node.kubernetes.io/unreachable&lt;/code&gt; 在其中设置
&lt;code&gt;tolerationSeconds=300&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这些自动添加耐受性(Toleration)表示，在侦测到这个问题时 Pod 最多还会在这个节点上保留 5 分钟。&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/daemonset/&#34;&gt;DaemonSet&lt;/a&gt; 的 Pod 在创建时会指定
下面这些毒点(Taint)对应的效果为 &lt;code&gt;NoExecute&lt;/code&gt; 并且没有 &lt;code&gt;tolerationSeconds&lt;/code&gt; 的耐受性(Toleration):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/unreachable&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/not-ready&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这会确保 DaemonSet 的 Pod 永远不会因为这些原因而被驱逐。&lt;/p&gt;
&lt;!--
## Taint Nodes by Condition

The node lifecycle controller automatically creates taints corresponding to
Node conditions with `NoSchedule` effect.
Similarly the scheduler does not check Node conditions; instead the scheduler checks taints. This assures that Node conditions don&#39;t affect what&#39;s scheduled onto the Node. The user can choose to ignore some of the Node&#39;s problems (represented as Node conditions) by adding appropriate Pod tolerations.

The DaemonSet controller automatically adds the following `NoSchedule`
tolerations to all daemons, to prevent DaemonSets from breaking.

  * `node.kubernetes.io/memory-pressure`
  * `node.kubernetes.io/disk-pressure`
  * `node.kubernetes.io/out-of-disk` (*only for critical pods*)
  * `node.kubernetes.io/unschedulable` (1.10 or later)
  * `node.kubernetes.io/network-unavailable` (*host network only*)

Adding these tolerations ensures backward compatibility. You can also add
arbitrary tolerations to DaemonSets.
 --&gt;
&lt;h2 id=&#34;taint-nodes-by-condition&#34;&gt;根据条件给节点上毒点(Taint)&lt;/h2&gt;
&lt;p&gt;节点的生命周期控制自动地根据节点状况创建 &lt;code&gt;NoSchedule&lt;/code&gt; 效果的毒点(Taint)。
类似地，调度是不会检查节点状况的；替代方式是调度器检查毒点(Taint)。 这样做能保证节点状况不会影响
已经调度到这个节点上的工作负载。 用户也可以通过向 Pod 上添加对应的耐受性(Toleration)
选择忽略一个节点上的问题(以节点状况表示的)。&lt;/p&gt;
&lt;p&gt;DaemonSet 控制器自动添加以下 &lt;code&gt;NoSchedule&lt;/code&gt; 耐受性(Toleration)到所有的守护进程， 防止 DaemonSet 挂掉。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/memory-pressure&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/disk-pressure&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/out-of-disk&lt;/code&gt; (&lt;em&gt;仅对关键 Pod&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/unschedulable&lt;/code&gt; (v1.10+)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;node.kubernetes.io/network-unavailable&lt;/code&gt; (&lt;em&gt;仅主机网络&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;添加这些 耐受性(Toleration) 能确保向后兼容。 用户还可以向 DaemonSet 添加任意 耐受性(Toleration)&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
* Read about [out of resource handling](/docs/tasks/administer-cluster/out-of-resource/) and how you can configure it
* Read about [pod priority](/docs/concepts/configuration/pod-priority-preemption/)
 --&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/administer-cluster/out-of-resource/&#34;&gt;资源不足的处理&lt;/a&gt; 与配置&lt;/li&gt;
&lt;li&gt;概念 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/pod-priority-preemption/&#34;&gt;Pod 优先级&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 分配 Pod 到节点</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/assign-pod-node/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/assign-pod-node/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- davidopp
- kevin-wangzefeng
- bsalamat
title: Assigning Pods to Nodes
content_type: concept
weight: 50
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
You can constrain a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; to only be able to run on particular
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;Node(s)&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt;, or to prefer to run on particular nodes.
There are several ways to do this, and the recommended approaches all use
[label selectors](/docs/concepts/overview/working-with-objects/labels/) to make the selection.
Generally such constraints are unnecessary, as the scheduler will automatically do a reasonable placement
(e.g. spread your pods across nodes, not place the pod on a node with insufficient free resources, etc.)
but there are some circumstances where you may want more control on a node where a pod lands, for example to ensure
that a pod ends up on a machine with an SSD attached to it, or to co-locate pods from two different
services that communicate a lot into the same availability zone.
 --&gt;
&lt;p&gt;用户可以约束一个
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
只能运行在特定一个(些)
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;节点&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt;
上, 或偏好运行在特定一个(些)节点上。 有几种方式可以做到这一点， 推荐的方式都使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/labels/&#34;&gt;标签选择器&lt;/a&gt;
进行选择。 通常这种约束不是必要的，因为调度器会自动地进行合理调度(例如: 将 Pod 分散到节点中，
不会将 Pod 放在资源不足的节点上，等) 但也有些情况希望控制 Pod 调度的节点，例如，确保 Pod
最终调度到一个使用 SSD 的节点， 或将两有大量通信的 &lt;code&gt;Service&lt;/code&gt; 的 Pod 放置在同一个可用区&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## nodeSelector

`nodeSelector` is the simplest recommended form of node selection constraint.
`nodeSelector` is a field of PodSpec. It specifies a map of key-value pairs. For the pod to be eligible
to run on a node, the node must have each of the indicated key-value pairs as labels (it can have
additional labels as well). The most common usage is one key-value pair.

Let&#39;s walk through an example of how to use `nodeSelector`.
 --&gt;
&lt;h2 id=&#34;nodeselector&#34;&gt;&lt;code&gt;nodeSelector&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;nodeSelector&lt;/code&gt; 是推荐的最简单的节点选择的形式。
&lt;code&gt;nodeSelector&lt;/code&gt; 是 Pod &lt;code&gt;.spec&lt;/code&gt; 的一个字段。 它是一个键值对形式的字典。 用户选择 Pod 运行的节点。
这个节点必须要有 &lt;code&gt;nodeSelector&lt;/code&gt; 上指定的每一个键值对作为标签(它还可以有额外的标签)。 最常用的是一个键值对。&lt;/p&gt;
&lt;p&gt;让我们使用一个示例来展示 &lt;code&gt;nodeSelector&lt;/code&gt; 是怎么用的。&lt;/p&gt;
&lt;!--
### Step Zero: Prerequisites

This example assumes that you have a basic understanding of Kubernetes pods and that you have [set up a Kubernetes cluster](/docs/setup/).
 --&gt;
&lt;h3 id=&#34;step-zero-prerequisites&#34;&gt;第零步: 准备&lt;/h3&gt;
&lt;p&gt;这个示例假定你对 k8s Pod  有基本理解，并且你需要
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/setup/&#34;&gt;搭建一个 k8s 集群&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### Step One: Attach label to the node

Run `kubectl get nodes` to get the names of your cluster&#39;s nodes. Pick out the one that you want to add a label to, and then run `kubectl label nodes &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt;` to add a label to the node you&#39;ve chosen. For example, if my node name is &#39;kubernetes-foo-node-1.c.a-robinson.internal&#39; and my desired label is &#39;disktype=ssd&#39;, then I can run `kubectl label nodes kubernetes-foo-node-1.c.a-robinson.internal disktype=ssd`.

You can verify that it worked by re-running `kubectl get nodes --show-labels` and checking that the node now has a label. You can also use `kubectl describe node &#34;nodename&#34;` to see the full list of labels of the given node.
 --&gt;
&lt;h3 id=&#34;step-one-attach-label-to-the-node&#34;&gt;第一步: 给节点打标签&lt;/h3&gt;
&lt;p&gt;运行&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;查看集群中的节点。 选一个喜欢的， 运行&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl label nodes &amp;lt;node-name&amp;gt; &amp;lt;label-key&amp;gt;=&amp;lt;label-value&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在你选择的节点上加一个标签。 例如， 如果我的节点名称为
&amp;lsquo;kubernetes-foo-node-1.c.a-robinson.internal&amp;rsquo;
我想要加的标签是 &amp;lsquo;disktype=ssd&amp;rsquo;， 要运行的命令就是&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;kubectl label nodes kubernetes-foo-node-1.c.a-robinson.internal disktype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;ssd&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;要检查标签是不是加上了，可以运行&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get nodes --show-labels
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;看看那个节点是不是
有这个标签。 也可以使用&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl describe node &amp;quot;nodename&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看这个节点的完整标签列表。&lt;/p&gt;
&lt;h3 id=&#34;step-two-add-a-nodeselector-field-to-your-pod-configuration&#34;&gt;第二步: 在 Pod 配置中添加 &lt;code&gt;nodeSelector&lt;/code&gt; 字段&lt;/h3&gt;
&lt;p&gt;选一个你想运行的 Pod 配置文件， 然后给它加一个下面这样的 &lt;code&gt;nodeSelector&lt;/code&gt;。
例如， 如果 Pod 的配置是这样的&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;加上 &lt;code&gt;nodeSelector&lt;/code&gt; 后就是这样:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;podspod-nginxyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/pod-nginx.yaml&#34; download=&#34;pods/pod-nginx.yaml&#34;&gt;
                    &lt;code&gt;pods/pod-nginx.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podspod-nginxyaml&#39;)&#34; title=&#34;Copy pods/pod-nginx.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;imagePullPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;IfNotPresent&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;disktype&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ssd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;当运行&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f https://k8s.io/examples/pods/pod-nginx.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;后， 这个 Pod 就会被调度到那个打过标签的节点。 要验证可以运行命令&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods -o wide
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;查看输出结果中这个 Pod 的 &amp;ldquo;NODE&amp;rdquo; 列是不是指定向的是那个节点&lt;/p&gt;
&lt;!--
## Interlude: built-in node labels {#built-in-node-labels}

In addition to labels you [attach](#step-one-attach-label-to-the-node), nodes come pre-populated
with a standard set of labels. See [Well-Known Labels, Annotations and Taints](/docs/reference/kubernetes-api/labels-annotations-taints/) for a list of these.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The value of these labels is cloud provider specific and is not guaranteed to be reliable.
For example, the value of &lt;code&gt;kubernetes.io/hostname&lt;/code&gt; may be the same as the Node name in some environments
and a different value in other environments.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;built-in-node-labels&#34;&gt;插一嘴: 内置节点标签&lt;/h2&gt;
&lt;p&gt;在你
&lt;a href=&#34;#step-one-attach-label-to-the-node&#34;&gt;加&lt;/a&gt;
的标签外。 节点也可能预先就有一些标签标签。 详见
&lt;a href=&#34;https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/&#34;&gt;常见 标签, 注解和毒点(Taint)&lt;/a&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 这些标签的值是与云提供商关联的，而且并不保证可靠。 例如， 在一些环境中 &lt;code&gt;kubernetes.io/hostname&lt;/code&gt;
的值可能与节点名称相同，另一些环境则不同&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;!--
## Node isolation/restriction

Adding labels to Node objects allows targeting pods to specific nodes or groups of nodes.
This can be used to ensure specific pods only run on nodes with certain isolation, security, or regulatory properties.
When using labels for this purpose, choosing label keys that cannot be modified by the kubelet process on the node is strongly recommended.
This prevents a compromised node from using its kubelet credential to set those labels on its own Node object,
and influencing the scheduler to schedule workloads to the compromised node.

The `NodeRestriction` admission plugin prevents kubelets from setting or modifying labels with a `node-restriction.kubernetes.io/` prefix.
To make use of that label prefix for node isolation:

1. Ensure you are using the [Node authorizer](/docs/reference/access-authn-authz/node/) and have _enabled_ the [NodeRestriction admission plugin](/docs/reference/access-authn-authz/admission-controllers/#noderestriction).
2. Add labels under the `node-restriction.kubernetes.io/` prefix to your Node objects, and use those labels in your node selectors.
For example, `example.com.node-restriction.kubernetes.io/fips=true` or `example.com.node-restriction.kubernetes.io/pci-dss=true`.
 --&gt;
&lt;h2 id=&#34;node-isolation-restriction&#34;&gt;节点的限离与限制&lt;/h2&gt;
&lt;p&gt;在节点对象上加标签可以使得 Pod 运行在指定的节点或一组节点。 这可以用来确保指定 Pod 只能运行中
有特定隔离，安全或监控性质的节点上。
当将节点用于这个目的时， 在选择标签键时强烈推荐那些不能被节点上运行 kubelet 进程修改的那些键。
这能防止在节点被黑后，攻击者使用节点上 kubelet 的凭据在它的节点对象上设置这些标签，然后影响
调度器将工作负载调度到被黑的节点。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;NodeRestriction&lt;/code&gt; 准入插件会防止 kubelet 设置或修改使用 &lt;code&gt;node-restriction.kubernetes.io/&lt;/code&gt;
前缀的标签。要使用这个标签前缀来实现节点隔离:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;确保你在使用 &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/node/&#34;&gt;Node authorizer&lt;/a&gt; 和
&lt;em&gt;启用了&lt;/em&gt;
&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#noderestriction&#34;&gt;NodeRestriction 准入插件&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;在节点对象上添加 &lt;code&gt;node-restriction.kubernetes.io/&lt;/code&gt; 前缀的标签，并在节点选择器中使用这些标签。
例如， &lt;code&gt;example.com.node-restriction.kubernetes.io/fips=true&lt;/code&gt; 或 &lt;code&gt;example.com.node-restriction.kubernetes.io/pci-dss=true&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
## Affinity and anti-affinity

`nodeSelector` provides a very simple way to constrain pods to nodes with particular labels. The affinity/anti-affinity
feature, greatly expands the types of constraints you can express. The key enhancements are

1. The affinity/anti-affinity language is more expressive. The language offers more matching rules
   besides exact matches created with a logical AND operation;
2. you can indicate that the rule is &#34;soft&#34;/&#34;preference&#34; rather than a hard requirement, so if the scheduler
   can&#39;t satisfy it, the pod will still be scheduled;
3. you can constrain against labels on other pods running on the node (or other topological domain),
   rather than against labels on the node itself, which allows rules about which pods can and cannot be co-located

The affinity feature consists of two types of affinity, &#34;node affinity&#34; and &#34;inter-pod affinity/anti-affinity&#34;.
Node affinity is like the existing `nodeSelector` (but with the first two benefits listed above),
while inter-pod affinity/anti-affinity constrains against pod labels rather than node labels, as
described in the third item listed above, in addition to having the first and second properties listed above.
 --&gt;
&lt;h2 id=&#34;affinity-and-anti-affinity&#34;&gt;亲和性与反亲和性&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;nodeSelector&lt;/code&gt; 提供了一个十分简单的方式可以使用特定标签将 Pod 约束到某些节点上。 而亲和性与反亲和性
特性则是大大地扩展了对约束的表达能力。 主要增加有&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;亲和性/反亲和性 语言有更好的表达能力。 在精确匹配的基础上，通过逻辑与(&lt;code&gt;AND&lt;/code&gt;)操作提供更多的匹配规则&lt;/li&gt;
&lt;li&gt;可以将规则设置为 &amp;ldquo;软&amp;rdquo;/&amp;ldquo;偏好&amp;rdquo;，而不是强制要求，这样如果调度器找不到满足这个规则的节点也可以
让 Pod 完成调度&lt;/li&gt;
&lt;li&gt;还可以将约束应用在运行在同一个节点(或其它拓扑域)的 Pod 上，而不止是节点本身的标签。 这使得
这些规则可以约束哪些 Pod 不能待在一起&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;亲和特性由两种类型的亲和性组成， &amp;ldquo;节点亲和性&amp;rdquo; 和 &amp;ldquo;Pod 间亲和性/反亲和性&amp;rdquo;。
节点亲和性与 &lt;code&gt;nodeSelector&lt;/code&gt; 相似(但是有上面提到的增加中的前两条优势)，
而 Pod 间亲和性/反亲和性是基于 Pod 的标签而不是节点的标签进行约束的， 就如上面第三条提到的，
同时第一二条的优势也是有的&lt;/p&gt;
&lt;!--
### Node affinity

Node affinity is conceptually similar to `nodeSelector` -- it allows you to constrain which nodes your
pod is eligible to be scheduled on, based on labels on the node.

There are currently two types of node affinity, called `requiredDuringSchedulingIgnoredDuringExecution` and
`preferredDuringSchedulingIgnoredDuringExecution`. You can think of them as &#34;hard&#34; and &#34;soft&#34; respectively,
in the sense that the former specifies rules that *must* be met for a pod to be scheduled onto a node (just like
`nodeSelector` but using a more expressive syntax), while the latter specifies *preferences* that the scheduler
will try to enforce but will not guarantee. The &#34;IgnoredDuringExecution&#34; part of the names means that, similar
to how `nodeSelector` works, if labels on a node change at runtime such that the affinity rules on a pod are no longer
met, the pod will still continue to run on the node. In the future we plan to offer
`requiredDuringSchedulingRequiredDuringExecution` which will be just like `requiredDuringSchedulingIgnoredDuringExecution`
except that it will evict pods from nodes that cease to satisfy the pods&#39; node affinity requirements.

Thus an example of `requiredDuringSchedulingIgnoredDuringExecution` would be &#34;only run the pod on nodes with Intel CPUs&#34;
and an example `preferredDuringSchedulingIgnoredDuringExecution` would be &#34;try to run this set of pods in failure
zone XYZ, but if it&#39;s not possible, then allow some to run elsewhere&#34;.

Node affinity is specified as field `nodeAffinity` of field `affinity` in the PodSpec.

Here&#39;s an example of a pod that uses node affinity:



 













&lt;table class=&#34;includecode&#34; id=&#34;podspod-with-node-affinityyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/pod-with-node-affinity.yaml&#34; download=&#34;pods/pod-with-node-affinity.yaml&#34;&gt;
                    &lt;code&gt;pods/pod-with-node-affinity.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podspod-with-node-affinityyaml&#39;)&#34; title=&#34;Copy pods/pod-with-node-affinity.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;with-node-affinity&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;affinity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nodeAffinity&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelectorTerms&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/e2e-az-name&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;e2e-az1&lt;/span&gt;
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;e2e-az2&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;weight&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;preference&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;another-node-label-key&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;another-node-label-value&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;with-node-affinity&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:2.0&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



This node affinity rule says the pod can only be placed on a node with a label whose key is
`kubernetes.io/e2e-az-name` and whose value is either `e2e-az1` or `e2e-az2`. In addition,
among nodes that meet that criteria, nodes with a label whose key is `another-node-label-key` and whose
value is `another-node-label-value` should be preferred.

You can see the operator `In` being used in the example. The new node affinity syntax supports the following operators: `In`, `NotIn`, `Exists`, `DoesNotExist`, `Gt`, `Lt`.
You can use `NotIn` and `DoesNotExist` to achieve node anti-affinity behavior, or use
[node taints](/docs/concepts/scheduling-eviction/taint-and-toleration/) to repel pods from specific nodes.

If you specify both `nodeSelector` and `nodeAffinity`, *both* must be satisfied for the pod
to be scheduled onto a candidate node.

If you specify multiple `nodeSelectorTerms` associated with `nodeAffinity` types, then the pod can be scheduled onto a node **if one of the** `nodeSelectorTerms` can be satisfied.

If you specify multiple `matchExpressions` associated with `nodeSelectorTerms`, then the pod can be scheduled onto a node **only if all** `matchExpressions` is satisfied.

If you remove or change the label of the node where the pod is scheduled, the pod won&#39;t be removed. In other words, the affinity selection works only at the time of scheduling the pod.

The `weight` field in `preferredDuringSchedulingIgnoredDuringExecution` is in the range 1-100. For each node that meets all of the scheduling requirements (resource request, RequiredDuringScheduling affinity expressions, etc.), the scheduler will compute a sum by iterating through the elements of this field and adding &#34;weight&#34; to the sum if the node matches the corresponding MatchExpressions. This score is then combined with the scores of other priority functions for the node. The node(s) with the highest total score are the most preferred.
 --&gt;
&lt;!--
### Node affinity

Node affinity is conceptually similar to `nodeSelector` -- it allows you to constrain which nodes your
pod is eligible to be scheduled on, based on labels on the node.

There are currently two types of node affinity, called `requiredDuringSchedulingIgnoredDuringExecution` and
`preferredDuringSchedulingIgnoredDuringExecution`. You can think of them as &#34;hard&#34; and &#34;soft&#34; respectively,
in the sense that the former specifies rules that *must* be met for a pod to be scheduled onto a node (just like
`nodeSelector` but using a more expressive syntax), while the latter specifies *preferences* that the scheduler
will try to enforce but will not guarantee. The &#34;IgnoredDuringExecution&#34; part of the names means that, similar
to how `nodeSelector` works, if labels on a node change at runtime such that the affinity rules on a pod are no longer
met, the pod will still continue to run on the node. In the future we plan to offer
`requiredDuringSchedulingRequiredDuringExecution` which will be just like `requiredDuringSchedulingIgnoredDuringExecution`
except that it will evict pods from nodes that cease to satisfy the pods&#39; node affinity requirements.

Thus an example of `requiredDuringSchedulingIgnoredDuringExecution` would be &#34;only run the pod on nodes with Intel CPUs&#34;
and an example `preferredDuringSchedulingIgnoredDuringExecution` would be &#34;try to run this set of pods in failure
zone XYZ, but if it&#39;s not possible, then allow some to run elsewhere&#34;.

Node affinity is specified as field `nodeAffinity` of field `affinity` in the PodSpec.

Here&#39;s an example of a pod that uses node affinity:



 













&lt;table class=&#34;includecode&#34; id=&#34;podspod-with-node-affinityyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/pod-with-node-affinity.yaml&#34; download=&#34;pods/pod-with-node-affinity.yaml&#34;&gt;
                    &lt;code&gt;pods/pod-with-node-affinity.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podspod-with-node-affinityyaml&#39;)&#34; title=&#34;Copy pods/pod-with-node-affinity.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;with-node-affinity&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;affinity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nodeAffinity&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelectorTerms&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/e2e-az-name&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;e2e-az1&lt;/span&gt;
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;e2e-az2&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;weight&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;preference&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;another-node-label-key&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;another-node-label-value&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;with-node-affinity&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:2.0&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



This node affinity rule says the pod can only be placed on a node with a label whose key is
`kubernetes.io/e2e-az-name` and whose value is either `e2e-az1` or `e2e-az2`. In addition,
among nodes that meet that criteria, nodes with a label whose key is `another-node-label-key` and whose
value is `another-node-label-value` should be preferred.

You can see the operator `In` being used in the example. The new node affinity syntax supports the following operators: `In`, `NotIn`, `Exists`, `DoesNotExist`, `Gt`, `Lt`.
You can use `NotIn` and `DoesNotExist` to achieve node anti-affinity behavior, or use
[node taints](/docs/concepts/scheduling-eviction/taint-and-toleration/) to repel pods from specific nodes.

If you specify both `nodeSelector` and `nodeAffinity`, *both* must be satisfied for the pod
to be scheduled onto a candidate node.

If you specify multiple `nodeSelectorTerms` associated with `nodeAffinity` types, then the pod can be scheduled onto a node **if one of the** `nodeSelectorTerms` can be satisfied.

If you specify multiple `matchExpressions` associated with `nodeSelectorTerms`, then the pod can be scheduled onto a node **only if all** `matchExpressions` is satisfied.

If you remove or change the label of the node where the pod is scheduled, the pod won&#39;t be removed. In other words, the affinity selection works only at the time of scheduling the pod.

The `weight` field in `preferredDuringSchedulingIgnoredDuringExecution` is in the range 1-100. For each node that meets all of the scheduling requirements (resource request, RequiredDuringScheduling affinity expressions, etc.), the scheduler will compute a sum by iterating through the elements of this field and adding &#34;weight&#34; to the sum if the node matches the corresponding MatchExpressions. This score is then combined with the scores of other priority functions for the node. The node(s) with the highest total score are the most preferred.
 --&gt;
&lt;h3 id=&#34;node-affinity&#34;&gt;节点亲和性&lt;/h3&gt;
&lt;p&gt;节点亲和性概念上与 &lt;code&gt;nodeSelector&lt;/code&gt; 类似 &amp;ndash; 它允许用户基于节点上的标签来约束哪些 Pod 应该调度
到哪个节点上。&lt;/p&gt;
&lt;p&gt;目前有两种类型的节点亲和性，叫做 &lt;code&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; 和
&lt;code&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/code&gt;. 可以认为他们分别相当于 &amp;ldquo;硬&amp;rdquo; 和 &amp;ldquo;软&amp;rdquo;，
从这个意义上说，前一个类型指定规则 &lt;em&gt;必须要&lt;/em&gt; 被满足了 Pod 才会被调度到一个节点(就像 &lt;code&gt;nodeSelector&lt;/code&gt;
但使用表达能力更强的语法)， 而后者指定的是 &lt;em&gt;优先&lt;/em&gt;，也是就调度会尝试满足但不保证会满足。
名称中的  &amp;ldquo;IgnoredDuringExecution&amp;rdquo; 的含义是如果节点的标签在运行时改变，Pod 上的节点亲和性
规则不再满足，Pod 还是继续会在节点上运行，就和 &lt;code&gt;nodeSelector&lt;/code&gt; 的工作方式类似。在未来还计划
提供 &lt;code&gt;requiredDuringSchedulingRequiredDuringExecution&lt;/code&gt; 它就和
&lt;code&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; 差不多，区别就是它会在节点不满足 Pod
上的节点亲和性时会将 Pod 从节点上驱逐。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; 的一个示例用法: 只能把 Pod 运行在
使用英特尔 CPU 的节点上。
&lt;code&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; 的一个示例用法: 尝试把这组 Pod 运行在
XYZ 这个失效区，但如果办不到， 也允许在其它地方运行。&lt;/p&gt;
&lt;p&gt;节点亲和性是通过 Pod &lt;code&gt;.spec.affinity.nodeAffinity&lt;/code&gt; 字段指定的。&lt;/p&gt;
&lt;p&gt;下面是一个使用节点亲和性的 Pod 的示例:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;podspod-with-node-affinityyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/pod-with-node-affinity.yaml&#34; download=&#34;pods/pod-with-node-affinity.yaml&#34;&gt;
                    &lt;code&gt;pods/pod-with-node-affinity.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podspod-with-node-affinityyaml&#39;)&#34; title=&#34;Copy pods/pod-with-node-affinity.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;with-node-affinity&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;affinity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nodeAffinity&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelectorTerms&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/e2e-az-name&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;e2e-az1&lt;/span&gt;
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;e2e-az2&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;weight&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;preference&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;another-node-label-key&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;another-node-label-value&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;with-node-affinity&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:2.0&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;这个节点亲和性规则的要求是 Pod 只能被放在一个拥有一个标签，这个标签的键是 &lt;code&gt;kubernetes.io/e2e-az-name&lt;/code&gt;
，标签的值是 &lt;code&gt;e2e-az1&lt;/code&gt; 或 &lt;code&gt;e2e-az2&lt;/code&gt;. 另外，在满足上一条件的节点中，包含一个键为 &lt;code&gt;another-node-label-key&lt;/code&gt;
值为 &lt;code&gt;another-node-label-value&lt;/code&gt; 的标签的节点优先。&lt;/p&gt;
&lt;p&gt;可以看到这个示例中使用了 &lt;code&gt;In&lt;/code&gt; 操作符。 新的节点亲和性语法支持以下操作符: &lt;code&gt;In&lt;/code&gt;, &lt;code&gt;NotIn&lt;/code&gt;,
&lt;code&gt;Exists&lt;/code&gt;, &lt;code&gt;DoesNotExist&lt;/code&gt;, &lt;code&gt;Gt&lt;/code&gt;, &lt;code&gt;Lt&lt;/code&gt;. 可以使用 &lt;code&gt;NotIn&lt;/code&gt; 和 &lt;code&gt;DoesNotExist&lt;/code&gt; 来达成反
亲和行为， 或使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/&#34;&gt;节点毒点(Taint)&lt;/a&gt; 让 Pod
避开指定节点。&lt;/p&gt;
&lt;p&gt;如果同时指定了 &lt;code&gt;nodeSelector&lt;/code&gt; 和 &lt;code&gt;nodeAffinity&lt;/code&gt;，只有同时满足的候选节点才能让 Pod 调度上去。&lt;/p&gt;
&lt;p&gt;如果在 &lt;code&gt;nodeAffinity&lt;/code&gt; 中配置了多个 &lt;code&gt;nodeSelectorTerms&lt;/code&gt;，当 &lt;strong&gt;有一个&lt;/strong&gt; &lt;code&gt;nodeSelectorTerms&lt;/code&gt;
被满足 Pod 就可以调度到这个节点。(这个有点歧义)&lt;/p&gt;
&lt;p&gt;如果在 &lt;code&gt;nodeSelectorTerms&lt;/code&gt; 中指定了多个 &lt;code&gt;matchExpressions&lt;/code&gt;， Pod &lt;strong&gt;在且仅在所有&lt;/strong&gt;
&lt;code&gt;matchExpressions&lt;/code&gt; 都满足时才能被调度到这个节点上&lt;/p&gt;
&lt;p&gt;当移除或修改 Pod 运行的节点上的标签时， Pod 不会被移除。 换一句话说， 亲和性配置只有在 Pod
调度时生效。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; 中 &lt;code&gt;weight&lt;/code&gt; 字段的范围是 &lt;code&gt;1-100&lt;/code&gt;
对于满足所有调度要求(资源要求， &lt;code&gt;RequiredDuringScheduling&lt;/code&gt; 亲和性表达式，等)，如果节点满足
对应的匹配表达式，调度器会迭代元素中的这个字段，并将值加到 &amp;ldquo;权重&amp;rdquo; 得分中。再将这个分与节点的
其它优先级函数得分组合。 最终得分最就的节点(一个或多个)就有最高的优先级&lt;/p&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
#### Node affinity per scheduling profile






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.20 [beta]&lt;/code&gt;
&lt;/div&gt;



When configuring multiple [scheduling profiles](/docs/reference/scheduling/config/#multiple-profiles), you can associate
a profile with a Node affinity, which is useful if a profile only applies to a specific set of Nodes.
To do so, add an `addedAffinity` to the args of the [`NodeAffinity` plugin](/docs/reference/scheduling/config/#scheduling-plugins)
in the [scheduler configuration](/docs/reference/scheduling/config/). For example:

```yaml
apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration

profiles:
  - schedulerName: default-scheduler
  - schedulerName: foo-scheduler
    pluginConfig:
      - name: NodeAffinity
        args:
          addedAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: scheduler-profile
                  operator: In
                  values:
                  - foo
```

The `addedAffinity` is applied to all Pods that set `.spec.schedulerName` to `foo-scheduler`, in addition to the
NodeAffinity specified in the PodSpec.
That is, in order to match the Pod, Nodes need to satisfy `addedAffinity` and the Pod&#39;s `.spec.NodeAffinity`.

Since the `addedAffinity` is not visible to end users, its behavior might be unexpected to them. We
recommend to use node labels that have clear correlation with the profile&#39;s scheduler name.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The DaemonSet controller, which &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/daemonset/#scheduled-by-default-scheduler&#34;&gt;creates Pods for DaemonSets&lt;/a&gt;
is not aware of scheduling profiles. For this reason, it is recommended that you keep a scheduler profile, such as the
&lt;code&gt;default-scheduler&lt;/code&gt;, without any &lt;code&gt;addedAffinity&lt;/code&gt;. Then, the Daemonset&amp;rsquo;s Pod template should use this scheduler name.
Otherwise, some Pods created by the Daemonset controller might remain unschedulable.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h4 id=&#34;node-affinity-per-scheduling-profile&#34;&gt;每个调度方案的节点亲和性&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.20 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;在配置多个
&lt;a href=&#34;https://kubernetes.io/docs/reference/scheduling/config/#multiple-profiles&#34;&gt;调度方案&lt;/a&gt;
时，可以在调度方案是添加节点亲和性，如果一个方案只会应用到指定的节点集时，这么做是相当有用的。
而要在方案中加亲和性则要在
&lt;a href=&#34;https://kubernetes.io/docs/reference/scheduling/config/&#34;&gt;调度器配置&lt;/a&gt;
中的
&lt;a href=&#34;https://kubernetes.io/docs/reference/scheduling/config/#scheduling-plugins&#34;&gt;&lt;code&gt;NodeAffinity&lt;/code&gt; 插件&lt;/a&gt;
的参数中添加 &lt;code&gt;addedAffinity&lt;/code&gt;。 例如:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubescheduler.config.k8s.io/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;KubeSchedulerConfiguration&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;profiles&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;schedulerName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default-scheduler&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;schedulerName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo-scheduler&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;pluginConfig&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NodeAffinity&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;addedAffinity&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelectorTerms&lt;/span&gt;:
              - &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
                - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;scheduler-profile&lt;/span&gt;
                  &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
                  &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
                  - &lt;span style=&#34;color:#ae81ff&#34;&gt;foo&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;addedAffinity&lt;/code&gt; 会应用到所有设置 &lt;code&gt;.spec.schedulerName&lt;/code&gt; 为 &lt;code&gt;foo-scheduler&lt;/code&gt; 的 Pod,
再加上这些 Pod &lt;code&gt;.spec&lt;/code&gt; 中指定的节点亲和性配置为最终的节点亲和性。&lt;/p&gt;
&lt;p&gt;因为 &lt;code&gt;addedAffinity&lt;/code&gt; 对终端用户是不可见的，导致最终行为不符合用户预期。
建议使用的标签要与方案的调度器名称有清晰的关联。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;code&gt;DaemonSet&lt;/code&gt; 控制器，是用来
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/daemonset/#scheduled-by-default-scheduler&#34;&gt;为 DaemonSet 创建 Pod 的&lt;/a&gt;
，它是不能感知到调度方案的。 因为这个原因， 推荐保留一个没有任何 &lt;code&gt;addedAffinity&lt;/code&gt; 的调度方案，就
如上面例子中的 &lt;code&gt;default-scheduler&lt;/code&gt;。 这样 &lt;code&gt;Daemonset&lt;/code&gt; 的 Pod 模板就可以使用这个调度器名称。
否则，有些由 &lt;code&gt;DaemonSet&lt;/code&gt; 控制器创建的 Pod 就能就一直是不可调度状态。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Inter-pod affinity and anti-affinity

Inter-pod affinity and anti-affinity allow you to constrain which nodes your pod is eligible to be scheduled *based on
labels on pods that are already running on the node* rather than based on labels on nodes. The rules are of the form
&#34;this pod should (or, in the case of anti-affinity, should not) run in an X if that X is already running one or more pods that meet rule Y&#34;.
Y is expressed as a LabelSelector with an optional associated list of namespaces; unlike nodes, because pods are namespaced
(and therefore the labels on pods are implicitly namespaced),
a label selector over pod labels must specify which namespaces the selector should apply to. Conceptually X is a topology domain
like node, rack, cloud provider zone, cloud provider region, etc. You express it using a `topologyKey` which is the
key for the node label that the system uses to denote such a topology domain; for example, see the label keys listed above
in the section [Interlude: built-in node labels](#built-in-node-labels).

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Inter-pod affinity and anti-affinity require substantial amount of
processing which can slow down scheduling in large clusters significantly. We do
not recommend using them in clusters larger than several hundred nodes.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Pod anti-affinity requires nodes to be consistently labelled, in other words every node in the cluster must have an appropriate label matching &lt;code&gt;topologyKey&lt;/code&gt;. If some or all nodes are missing the specified &lt;code&gt;topologyKey&lt;/code&gt; label, it can lead to unintended behavior.&lt;/div&gt;
&lt;/blockquote&gt;


As with node affinity, there are currently two types of pod affinity and anti-affinity, called `requiredDuringSchedulingIgnoredDuringExecution` and
`preferredDuringSchedulingIgnoredDuringExecution` which denote &#34;hard&#34; vs. &#34;soft&#34; requirements.
See the description in the node affinity section earlier.
An example of `requiredDuringSchedulingIgnoredDuringExecution` affinity would be &#34;co-locate the pods of service A and service B
in the same zone, since they communicate a lot with each other&#34;
and an example `preferredDuringSchedulingIgnoredDuringExecution` anti-affinity would be &#34;spread the pods from this service across zones&#34;
(a hard requirement wouldn&#39;t make sense, since you probably have more pods than zones).

Inter-pod affinity is specified as field `podAffinity` of field `affinity` in the PodSpec.
And inter-pod anti-affinity is specified as field `podAntiAffinity` of field `affinity` in the PodSpec.
 --&gt;
&lt;h3 id=&#34;inter-pod-affinity-and-anti-affinity&#34;&gt;Pod 间的亲和性与反亲和性&lt;/h3&gt;
&lt;p&gt;Pod 间的亲和性与反亲和性允许用户通过 &lt;em&gt;已经运行在节点上的 Pod 的标签&lt;/em&gt; 而不是节点的标签来决定一个 Pod
应该调度到哪个节点上。
其中的规则的形式是这样的: 这个 Pod 应该(或，在反亲和性的情况下，不应该)运行在 X 上，如果
X 上已经运行了一个或多个匹配 规则 Y 的 Pod。 规则 Y 是以标签选择器的形式存在的，其它可以还有
一个可选的命名空间列表，因为与节点不一样， Pod 是有命名空间的(因此 Pod 上的标签隐形的就是有命名空间的)
对于 Pod 标签的标签选择器必须指定这个选择器应用的命名空间。 概念 X 是一个与节点, 机架，云提供商
的区域，云提供商的地区，等一样的拓扑域，可以使用一个 &lt;code&gt;topologyKey&lt;/code&gt; 来表示它，也就是系统用来
表示这个拓扑域的节点标签的键。 例子见上面的
&lt;a href=&#34;#built-in-node-labels&#34;&gt;插一嘴: 内置节点标签&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Pod 间的亲和性与反亲和性需要大量的处理过程，如果在大集群中会极是的减慢调度的速度。 不建议使用在
集群节点大于几百的节点中。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Pod 反亲和性需要节点都要打上标签，也就是集群中的每个节点必须要有与 &lt;code&gt;topologyKey&lt;/code&gt; 匹配的适当的
标签。 如果其中的一些或全部没有指定 &lt;code&gt;topologyKey&lt;/code&gt; 标签，如能会导致意外的情况。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;与节点亲和性一样， Pod 的亲和性与反亲和性目前也有两种类型，
&lt;code&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; 和
&lt;code&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/code&gt;
分别表示为 &amp;ldquo;硬&amp;rdquo; vs. &amp;ldquo;软&amp;rdquo; 要求。
&lt;code&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; 亲和性的一个应用示例:
因为 Service A 和 Service B 之间有大量通信，把它们的 Pod 放置在同一个可用区。
&lt;code&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; 反亲和性的一个应用示例:
把这个 Service 的 Pod 分散到不同的区域中(在这种情况下，硬性要求就可能会出问题了， 因为 Pod 的
数量可能是大于区域的)&lt;/p&gt;
&lt;p&gt;Pod 间的亲和性是通过 Pod &lt;code&gt;.spec.affinity.podAffinity&lt;/code&gt; 来指定的。 而反亲和性则是由
&lt;code&gt;.spec.affinity.podAntiAffinity&lt;/code&gt; 来指定的。&lt;/p&gt;
&lt;h4 id=&#34;an-example-of-a-pod-that-uses-pod-affinity&#34;&gt;一个使用 Pod 亲和性的示例:&lt;/h4&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;podspod-with-pod-affinityyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/pod-with-pod-affinity.yaml&#34; download=&#34;pods/pod-with-pod-affinity.yaml&#34;&gt;
                    &lt;code&gt;pods/pod-with-pod-affinity.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podspod-with-pod-affinityyaml&#39;)&#34; title=&#34;Copy pods/pod-with-pod-affinity.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;with-pod-affinity&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;affinity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;podAffinity&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;security&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;S1&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;failure-domain.beta.kubernetes.io/zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;podAntiAffinity&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;weight&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;podAffinityTerm&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
            - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;security&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
              - &lt;span style=&#34;color:#ae81ff&#34;&gt;S2&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;failure-domain.beta.kubernetes.io/zone&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;with-pod-affinity&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:2.0&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;这个 Pod 中的亲和性定义有一个 Pod 亲和性规则和一个 Pod 反亲和性规则， 在这个例子中
&lt;code&gt;podAffinity&lt;/code&gt; 是 &lt;code&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; 而
&lt;code&gt;podAntiAffinity&lt;/code&gt; 是 &lt;code&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/code&gt;.
Pod 亲和性规则的含义是: Pod 仅能被调度到一个所在区域中至少有一个已经运行了包含标签键为
&lt;code&gt;security&lt;/code&gt; 值为 &lt;code&gt;S1&lt;/code&gt; 的 Pod 的节点(也就是如果节点 N 有一个标签键为  &lt;code&gt;topology.kubernetes.io/zone&lt;/code&gt;
假定标签值为 &lt;code&gt;V&lt;/code&gt;, 然后集群中至少还有另一个节点标签键为  &lt;code&gt;topology.kubernetes.io/zone&lt;/code&gt;
标签值为 &lt;code&gt;V&lt;/code&gt;, 后面这个节点上运行了一个 Pod 包含了标签 &lt;code&gt;security: S1&lt;/code&gt;).
Pod 反亲和性规则的含义是: Pod 不应当被调度到节点所在区域中的其它节点上有运行 Pod 包含标签为
&lt;code&gt;security: S2&lt;/code&gt; 的节点上。 更多关于 Pod 亲和性与反亲和性的示例见
&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/scheduling/podaffinity.md&#34;&gt;设计文稿&lt;/a&gt;
其中
&lt;code&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/code&gt;
和
&lt;code&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/code&gt;
都有&lt;/p&gt;
&lt;p&gt;Pod 亲和性与反亲和性和合法操作符有 &lt;code&gt;In&lt;/code&gt;, &lt;code&gt;NotIn&lt;/code&gt;, &lt;code&gt;Exists&lt;/code&gt;, &lt;code&gt;DoesNotExist&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;原则上， &lt;code&gt;topologyKey&lt;/code&gt; 可以是任意合法的标签键， 但为了性能和安全的原因，对 &lt;code&gt;topologyKey&lt;/code&gt;
有些约束:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对于 Pod 亲和性，&lt;code&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; 和
&lt;code&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; 中都不允许有空的 &lt;code&gt;topologyKey&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;对于 Pod 反亲和性，&lt;code&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; 和
&lt;code&gt;preferredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; 中也都不允许有空的 &lt;code&gt;topologyKey&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;对于 &lt;code&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; Pod 反亲和性， 引入
&lt;code&gt;LimitPodHardAntiAffinityTopology&lt;/code&gt; 准入控制器来限制 &lt;code&gt;topologyKey&lt;/code&gt; 为 &lt;code&gt;kubernetes.io/hostname&lt;/code&gt;
如果想要用在自定义拓扑上，则需要修改准入控制器或直接禁用它&lt;/li&gt;
&lt;li&gt;除了上面的情况， &lt;code&gt;topologyKey&lt;/code&gt; 可以是任意合法的标签键&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在 &lt;code&gt;labelSelector&lt;/code&gt; 和 &lt;code&gt;topologyKey&lt;/code&gt; 外，可以选择指定一个可选的 &lt;code&gt;namespaces&lt;/code&gt; 列表，使得
&lt;code&gt;labelSelector&lt;/code&gt; 需要来匹配它们(与 &lt;code&gt;labelSelector&lt;/code&gt; 和 &lt;code&gt;topologyKey&lt;/code&gt; 定义同级)。
如果没有指定或值为空，默认为 亲和性与反亲和性 所在的命名空间中的 Pod。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/code&gt; 亲和性与反亲和性的所有 &lt;code&gt;matchExpressions&lt;/code&gt;
必须要求被满足时 Pod 才能被调度到这个节点上&lt;/p&gt;
&lt;!--
#### More Practical Use-cases

Interpod Affinity and AntiAffinity can be even more useful when they are used with higher
level collections such as ReplicaSets, StatefulSets, Deployments, etc.  One can easily configure that a set of workloads should
be co-located in the same defined topology, eg., the same node.
 --&gt;
&lt;h4 id=&#34;more-practical-use-cases&#34;&gt;更实用的应用场景&lt;/h4&gt;
&lt;p&gt;Pod 亲和性与反亲和性在被用在如 ReplicaSet, StatefulSet, Deployment, 等这些更高一级的资源上
时可以发挥更大的作用。 这样可以很空间地配置将一组工作负载放在同一个拓扑中，例如， 同一个节点上。&lt;/p&gt;
&lt;!--
##### Always co-located in the same node

In a three node cluster, a web application has in-memory cache such as redis. We want the web-servers to be co-located with the cache as much as possible.

Here is the yaml snippet of a simple redis deployment with three replicas and selector label `app=store`. The deployment has `PodAntiAffinity` configured to ensure the scheduler does not co-locate replicas on a single node.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-cache
spec:
  selector:
    matchLabels:
      app: store
  replicas: 3
  template:
    metadata:
      labels:
        app: store
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: &#34;kubernetes.io/hostname&#34;
      containers:
      - name: redis-server
        image: redis:3.2-alpine
```

The below yaml snippet of the webserver deployment has `podAntiAffinity` and `podAffinity` configured. This informs the scheduler that all its replicas are to be co-located with pods that have selector label `app=store`. This will also ensure that each web-server replica does not co-locate on a single node.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-server
spec:
  selector:
    matchLabels:
      app: web-store
  replicas: 3
  template:
    metadata:
      labels:
        app: web-store
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web-store
            topologyKey: &#34;kubernetes.io/hostname&#34;
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: &#34;kubernetes.io/hostname&#34;
      containers:
      - name: web-app
        image: nginx:1.16-alpine
```

If we create the above two deployments, our three node cluster should look like below.

|       node-1         |       node-2        |       node-3       |
|:--------------------:|:-------------------:|:------------------:|
| *webserver-1*        |   *webserver-2*     |    *webserver-3*   |
|  *cache-1*           |     *cache-2*       |     *cache-3*      |

As you can see, all the 3 replicas of the `web-server` are automatically co-located with the cache as expected.

```
kubectl get pods -o wide
```
The output is similar to this:
```
NAME                           READY     STATUS    RESTARTS   AGE       IP           NODE
redis-cache-1450370735-6dzlj   1/1       Running   0          8m        10.192.4.2   kube-node-3
redis-cache-1450370735-j2j96   1/1       Running   0          8m        10.192.2.2   kube-node-1
redis-cache-1450370735-z73mh   1/1       Running   0          8m        10.192.3.1   kube-node-2
web-server-1287567482-5d4dz    1/1       Running   0          7m        10.192.2.3   kube-node-1
web-server-1287567482-6f7v5    1/1       Running   0          7m        10.192.4.3   kube-node-3
web-server-1287567482-s330j    1/1       Running   0          7m        10.192.3.2   kube-node-2
```
 --&gt;
&lt;h5 id=&#34;always-co-located-in-the-same-node&#34;&gt;始终调度在同一个节点上&lt;/h5&gt;
&lt;p&gt;在一个3节点的集群中，有一个使用的内存缓存如 redis 的 web 应用。 我们希望 web 服务尽量与缓存
协同调度。&lt;/p&gt;
&lt;p&gt;下面是一个简单的三副本的 redis Deployment 的 yaml 片断， 其中选择器标签为 &lt;code&gt;app=store&lt;/code&gt;。
Deployment 中有 &lt;code&gt;PodAntiAffinity&lt;/code&gt; 配置，确保调度器不会同副本调度在同一个节点上。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis-cache&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;store&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;store&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;affinity&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;podAntiAffinity&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
              - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;app&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
                - &lt;span style=&#34;color:#ae81ff&#34;&gt;store&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/hostname&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis-server&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis:3.2-alpine&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;下面是 web 服务的 Deployment yaml 片断， 其中配置了 &lt;code&gt;podAntiAffinity&lt;/code&gt; 和 &lt;code&gt;podAffinity&lt;/code&gt;
这些配置告知调度器它的所有副本都要调度到与标签为 &lt;code&gt;app=store&lt;/code&gt; 的 Pod 在一起。这也会保证每个
web 服务的副本不会被调度到同一个节点上&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;web-server&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;web-store&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;web-store&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;affinity&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;podAntiAffinity&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
              - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;app&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
                - &lt;span style=&#34;color:#ae81ff&#34;&gt;web-store&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/hostname&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;podAffinity&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
              - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;app&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
                - &lt;span style=&#34;color:#ae81ff&#34;&gt;store&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/hostname&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;web-app&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx:1.16-alpine&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果我们创建了上面的两个 Deployment，我们集群中的三个节点看起来应该就是下面这样。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-1&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-2&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;em&gt;webserver-1&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;em&gt;webserver-2&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;em&gt;webserver-3&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;em&gt;cache-1&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;em&gt;cache-2&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;em&gt;cache-3&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;如你所见，就如预期中的那样 &lt;code&gt;web-server&lt;/code&gt; 的三个副本自动地与缓存调度在一起。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get pods -o wide
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                           READY     STATUS    RESTARTS   AGE       IP           NODE
redis-cache-1450370735-6dzlj   1/1       Running   0          8m        10.192.4.2   kube-node-3
redis-cache-1450370735-j2j96   1/1       Running   0          8m        10.192.2.2   kube-node-1
redis-cache-1450370735-z73mh   1/1       Running   0          8m        10.192.3.1   kube-node-2
web-server-1287567482-5d4dz    1/1       Running   0          7m        10.192.2.3   kube-node-1
web-server-1287567482-6f7v5    1/1       Running   0          7m        10.192.4.3   kube-node-3
web-server-1287567482-s330j    1/1       Running   0          7m        10.192.3.2   kube-node-2
&lt;/code&gt;&lt;/pre&gt;&lt;!--
##### Never co-located in the same node

The above example uses `PodAntiAffinity` rule with `topologyKey: &#34;kubernetes.io/hostname&#34;` to deploy the redis cluster so that
no two instances are located on the same host.
See [ZooKeeper tutorial](/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure)
for an example of a StatefulSet configured with anti-affinity for high availability, using the same technique.
 --&gt;
&lt;h5 id=&#34;never-co-located-in-the-same-node&#34;&gt;永远不要调度到同一个节点上&lt;/h5&gt;
&lt;p&gt;上面的例子中使用包含 &lt;code&gt;topologyKey: &amp;quot;kubernetes.io/hostname&amp;quot;&lt;/code&gt; 的 &lt;code&gt;PodAntiAffinity&lt;/code&gt; 来
部署 redis 集群，这样任意两个实例都不会被调度到同一个主机上。
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure&#34;&gt;ZooKeeper 教程&lt;/a&gt;
中的一个示例中，一个 StatefulSet 配置了反亲和性来实现高可用，其中使用了同样的技巧。&lt;/p&gt;
&lt;!--
## nodeName

`nodeName` is the simplest form of node selection constraint, but due
to its limitations it is typically not used.  `nodeName` is a field of
PodSpec.  If it is non-empty, the scheduler ignores the pod and the
kubelet running on the named node tries to run the pod.  Thus, if
`nodeName` is provided in the PodSpec, it takes precedence over the
above methods for node selection.

Some of the limitations of using `nodeName` to select nodes are:

-   If the named node does not exist, the pod will not be run, and in
    some cases may be automatically deleted.
-   If the named node does not have the resources to accommodate the
    pod, the pod will fail and its reason will indicate why,
    for example OutOfmemory or OutOfcpu.
-   Node names in cloud environments are not always predictable or
    stable.

Here is an example of a pod config file using the `nodeName` field:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
  nodeName: kube-01
```

The above pod will run on the node kube-01.
 --&gt;
&lt;h2 id=&#34;nodename&#34;&gt;nodeName&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;nodeName&lt;/code&gt; 是实现节点选择约束的最简单方式，但因为它的限制，通常是不会使用的。
&lt;code&gt;nodeName&lt;/code&gt; 是 Pod  &lt;code&gt;.spec&lt;/code&gt; 的一个字段。 如果它不是空， 调度器就会忽略这个 Pod， 由 &lt;code&gt;nodeName&lt;/code&gt;
所设置的名称的节点上的 &lt;code&gt;kubelet&lt;/code&gt; 来尝试运行这个 Pod。 这样， 如果 Pod &lt;code&gt;.spec&lt;/code&gt;  中定义了
&lt;code&gt;nodeName&lt;/code&gt;， 则它会比上面提到的所有节点选择的优先级都高。&lt;/p&gt;
&lt;p&gt;使用 &lt;code&gt;nodeName&lt;/code&gt; 进行节点选择的限制有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果没有一个名称是这个的节点， 则 Pod 不会被运行，在某些情况下可以会被自动删除。&lt;/li&gt;
&lt;li&gt;如果叫这个名称的节点上资源不足以运行这个节点，这个 Pod 就会失败，它的状态信息的原因中会显示为啥，
比如 &lt;code&gt;OutOfmemory&lt;/code&gt; 或 &lt;code&gt;OutOfcpu&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;云环境中的节点名称可能不是始终可预期的或稳定的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面就是一个使用 &lt;code&gt;nodeName&lt;/code&gt; 字段的 Pod 配置的示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;nodeName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube-01&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上面这个 Pod 会运行在节点 &lt;code&gt;kube-01&lt;/code&gt; 上&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
[Taints](/docs/concepts/scheduling-eviction/taint-and-toleration/) allow a Node to *repel* a set of Pods.

The design documents for
[node affinity](https://git.k8s.io/community/contributors/design-proposals/scheduling/nodeaffinity.md)
and for [inter-pod affinity/anti-affinity](https://git.k8s.io/community/contributors/design-proposals/scheduling/podaffinity.md) contain extra background information about these features.

Once a Pod is assigned to a Node, the kubelet runs the Pod and allocates node-local resources.
The [topology manager](/docs/tasks/administer-cluster/topology-manager/) can take part in node-level
resource allocation decisions.
 --&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/&#34;&gt;毒点(Taint)&lt;/a&gt;
允许节点 &lt;em&gt;排斥&lt;/em&gt; 一些 Pod.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/scheduling/nodeaffinity.md&#34;&gt;节点亲和性&lt;/a&gt;
的设计文稿&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/scheduling/podaffinity.md&#34;&gt;Pod 间的 亲和性/反亲和性&lt;/a&gt; 包含了这个特性额外的背景信息&lt;/p&gt;
&lt;p&gt;当一个 Pod 被分配给一个节点时， kubelet 运行这个节点并分配节点级别的资源。
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/tasks/administer-cluster/topology-manager/&#34;&gt;拓扑管理器&lt;/a&gt;  可以参与节点级别资源
分配的决定。&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
