<!doctype html>
<html lang="zh" class="no-js">
  <head>
    



<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.75.1" />

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">

<link rel="shortcut icon" type="image/png" href="/k8sDocs/images/favicon.png">
<link rel="apple-touch-icon" href="/k8sDocs/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="manifest" href="/k8sDocs/manifest.webmanifest">
<link rel="apple-touch-icon" href="/k8sDocs/images/kubernetes-192x192.png">

<title>管理容器资源 | Kubernetes</title><meta property="og:title" content="管理容器资源" />
<meta property="og:description" content="Automatically places containers based on their resource requirements and other constraints, while not sacrificing availability. Mix critical and best-effort workloads in order to drive up utilization and save even more resources. --- -- PodPod 表示集群中运行的一组容器的集合 , you can optionally specify how much of each resource a Container一" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/manage-resources-containers/" />
<meta property="article:modified_time" content="2020-12-28T17:58:55+08:00" /><meta property="og:site_name" content="Kubernetes" />
<meta itemprop="name" content="管理容器资源">
<meta itemprop="description" content="Automatically places containers based on their resource requirements and other constraints, while not sacrificing availability. Mix critical and best-effort workloads in order to drive up utilization and save even more resources. --- -- PodPod 表示集群中运行的一组容器的集合 , you can optionally specify how much of each resource a Container一">
<meta itemprop="dateModified" content="2020-12-28T17:58:55+08:00" />
<meta itemprop="wordCount" content="6133">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="管理容器资源"/>
<meta name="twitter:description" content="Automatically places containers based on their resource requirements and other constraints, while not sacrificing availability. Mix critical and best-effort workloads in order to drive up utilization and save even more resources. --- -- PodPod 表示集群中运行的一组容器的集合 , you can optionally specify how much of each resource a Container一"/>





<link rel="preload" href="/k8sDocs/scss/main.min.537757ec8e00d87ec206229d5725b6d05e078dd447818e448f0802685603e730.css" as="style">
<link href="/k8sDocs/scss/main.min.537757ec8e00d87ec206229d5725b6d05e078dd447818e448f0802685603e730.css" rel="stylesheet" integrity="">


<script src="/k8sDocs/js/jquery-3.3.1.min.js" ></script>





<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "url": "https://kubernetes.io",
    "logo": "https://kubernetes.io/images/favicon.png",
    "potentialAction": {
      "@type": "SearchAction",
      "target": "https://lostsquirrel.github.io/k8sDocs/search/?q={search_term_string}",
      "query-input": "required name=search_term_string"
    }

  }
</script>
<meta name="theme-color" content="#326ce5">




<link rel="stylesheet" href=/k8sDocs/css/base_fonts.css>

<link rel="stylesheet" href="/k8sDocs/css/jquery-ui.min.css">
<link rel="stylesheet" href="/k8sDocs/css/callouts.css">
<link rel="stylesheet" href="/k8sDocs/css/custom.css">
<link rel="stylesheet" href="/k8sDocs/css/custom-jekyll/tags.css">



<meta name="description" content="Automatically places containers based on their resource requirements and other constraints, while not sacrificing availability. Mix critical and best-effort workloads in order to drive up utilization and save even more resources. --- -- PodPod 表示集群中运行的一组容器的集合 , you can optionally specify how much of each resource a Container一">
<meta property="og:description" content="Automatically places containers based on their resource requirements and other constraints, while not sacrificing availability. Mix critical and best-effort workloads in order to drive up utilization and save even more resources. --- -- PodPod 表示集群中运行的一组容器的集合 , you can optionally specify how much of each resource a Container一">
<meta name="twitter:description" content="Automatically places containers based on their resource requirements and other constraints, while not sacrificing availability. Mix critical and best-effort workloads in order to drive up utilization and save even more resources. --- -- PodPod 表示集群中运行的一组容器的集合 , you can optionally specify how much of each resource a Container一">
<meta property="og:url" content="https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/manage-resources-containers/">
<meta property="og:title" content="管理容器资源">
<meta name="twitter:title" content="管理容器资源">
<meta name="twitter:image" content="https://kubernetes.io/images/favicon.png" />

<meta name="twitter:image:alt" content="Kubernetes">

<meta property="og:image" content="/k8sDocs/images/kubernetes-horizontal-color.png">

<meta property="og:type" content="article">



<script src="/k8sDocs/js/script.js"></script>


    <title>管理容器资源 | Kubernetes</title>
  </head>
  <body class="td-page td-documentation">
    <header>
      
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark  flex-row td-navbar" data-auto-burger="primary">
        <a class="navbar-brand" href="/k8sDocs/"></a>
	<div class="td-navbar-nav-scroll ml-md-auto" id="main_navbar">

		<ul class="navbar-nav mt-2 mt-lg-0">
			
			
			
			
			<li class="nav-item mr-2 mb-lg-0">
				
				<a class="nav-link active" href="/k8sDocs/docs/" >Documentation</span></a>
			</li>
			
			
			
			
			
			
			
			
			
			
			
			
			
			
		</ul>
	</div>
  

</nav>

      
      <div class="header-filler"></div>
    </header>
    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-md-nowrap">
          <div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
            




<div id="td-sidebar-menu" class="td-sidebar__inner">
  
  <form class="td-sidebar__search d-flex align-items-center d-lg-none">
    

    <button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type="button" data-toggle="collapse" data-target="#td-section-nav" aria-controls="td-docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    </button>
  </form>
  
  <nav class="collapse td-sidebar-nav" id="td-section-nav">
    
    






<ul class="td-sidebar-nav__section pr-md-3">
  <li class="td-sidebar-nav__section-title">
    <a  href="/k8sDocs/docs/" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">Kubernetes Documentation</a>
  </li>
  <ul>
    <li class="collapse show" id="k8sdocsdocs">
      
      
      
      
      






<ul class="td-sidebar-nav__section pr-md-3">
  <li class="td-sidebar-nav__section-title">
    <a  href="/k8sDocs/docs/concepts/" class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">概念(Concepts)</a>
  </li>
  <ul>
    <li class="collapse show" id="k8sdocsdocsconcepts">
      
      
      
      
      






<ul class="td-sidebar-nav__section pr-md-3">
  <li class="td-sidebar-nav__section-title">
    <a  href="/k8sDocs/docs/concepts/overview/" class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">概览</a>
  </li>
  <ul>
    <li class="collapse " id="k8sdocsdocsconceptsoverview">
      
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsoverview00-what-is-k8s" href="/k8sDocs/docs/concepts/overview/00-what-is-k8s/">k8s 是什么</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsoverview01-components" href="/k8sDocs/docs/concepts/overview/01-components/">k8s 组件</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsoverview02-k8s-api" href="/k8sDocs/docs/concepts/overview/02-k8s-api/">k8s API 说明</a>
      
      
      
      






<ul class="td-sidebar-nav__section pr-md-3">
  <li class="td-sidebar-nav__section-title">
    <a  href="/k8sDocs/docs/concepts/overview/working-with-objects/" class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">k8s 对象管理</a>
  </li>
  <ul>
    <li class="collapse " id="k8sdocsdocsconceptsoverviewworking-with-objects">
      
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsoverviewworking-with-objects00-kubernetes-objects" href="/k8sDocs/docs/concepts/overview/working-with-objects/00-kubernetes-objects/">k8s 对象介绍</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsoverviewworking-with-objectsnames" href="/k8sDocs/docs/concepts/overview/working-with-objects/names/">对象命令与ID</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsoverviewworking-with-objects02-namespace" href="/k8sDocs/docs/concepts/overview/working-with-objects/02-namespace/">命名空间(Namespaces)</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsoverviewworking-with-objectslabels" href="/k8sDocs/docs/concepts/overview/working-with-objects/labels/">标签和标签选择器</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsoverviewworking-with-objects04-annotation" href="/k8sDocs/docs/concepts/overview/working-with-objects/04-annotation/">注解 (Annotations)</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsoverviewworking-with-objects05-field-selectors" href="/k8sDocs/docs/concepts/overview/working-with-objects/05-field-selectors/">字段选择器</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsoverviewworking-with-objects06-common-labels" href="/k8sDocs/docs/concepts/overview/working-with-objects/06-common-labels/">标签设置指导</a>
      
      
    </li>
  </ul>
</ul>

      
      
    </li>
  </ul>
</ul>

      
      
      
      






<ul class="td-sidebar-nav__section pr-md-3">
  <li class="td-sidebar-nav__section-title">
    <a  href="/k8sDocs/docs/concepts/architecture/" class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">集群架构</a>
  </li>
  <ul>
    <li class="collapse " id="k8sdocsdocsconceptsarchitecture">
      
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsarchitecturenodes" href="/k8sDocs/docs/concepts/architecture/nodes/">节点</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsarchitecturecontrol-plane-node-communication" href="/k8sDocs/docs/concepts/architecture/control-plane-node-communication/">控制中心与节点的通信</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsarchitecturecontroller" href="/k8sDocs/docs/concepts/architecture/controller/">控制器</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsarchitecturecloud-controller" href="/k8sDocs/docs/concepts/architecture/cloud-controller/">Cloud Controller Manager</a>
      
      
    </li>
  </ul>
</ul>

      
      
      
      






<ul class="td-sidebar-nav__section pr-md-3">
  <li class="td-sidebar-nav__section-title">
    <a  href="/k8sDocs/docs/concepts/containers/" class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">容器</a>
  </li>
  <ul>
    <li class="collapse " id="k8sdocsdocsconceptscontainers">
      
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptscontainers00-images" href="/k8sDocs/docs/concepts/containers/00-images/">镜像</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptscontainers01-container-environment" href="/k8sDocs/docs/concepts/containers/01-container-environment/">容器环境</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptscontainers02-runtime-class" href="/k8sDocs/docs/concepts/containers/02-runtime-class/">Runtime Class</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptscontainerscontainer-lifecycle-hooks" href="/k8sDocs/docs/concepts/containers/container-lifecycle-hooks/">容器生命周期钩子</a>
      
      
    </li>
  </ul>
</ul>

      
      
      
      






<ul class="td-sidebar-nav__section pr-md-3">
  <li class="td-sidebar-nav__section-title">
    <a  href="/k8sDocs/docs/concepts/workloads/" class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">工作负载(Workload)</a>
  </li>
  <ul>
    <li class="collapse " id="k8sdocsdocsconceptsworkloads">
      
      
      
      
      






<ul class="td-sidebar-nav__section pr-md-3">
  <li class="td-sidebar-nav__section-title">
    <a  href="/k8sDocs/docs/concepts/workloads/pods/" class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Pod</a>
  </li>
  <ul>
    <li class="collapse " id="k8sdocsdocsconceptsworkloadspods">
      
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsworkloadspodspod-lifecycle" href="/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/">Pod 生命周期</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsworkloadspodsinit-containers" href="/k8sDocs/docs/concepts/workloads/pods/init-containers/">初始化容器</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsworkloadspodspod-topology-spread-constraints" href="/k8sDocs/docs/concepts/workloads/pods/pod-topology-spread-constraints/">Pod 拓扑分布约束条件</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsworkloadspodspodpreset" href="/k8sDocs/docs/concepts/workloads/pods/podpreset/">Pod 预设信息</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsworkloadspodsdisruptions" href="/k8sDocs/docs/concepts/workloads/pods/disruptions/">Disruptions</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsworkloadspodsephemeral-containers" href="/k8sDocs/docs/concepts/workloads/pods/ephemeral-containers/">临时容器</a>
      
      
    </li>
  </ul>
</ul>

      
      
      
      






<ul class="td-sidebar-nav__section pr-md-3">
  <li class="td-sidebar-nav__section-title">
    <a  href="/k8sDocs/docs/concepts/workloads/controllers/" class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">控制器</a>
  </li>
  <ul>
    <li class="collapse " id="k8sdocsdocsconceptsworkloadscontrollers">
      
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsworkloadscontrollersdeployment" href="/k8sDocs/docs/concepts/workloads/controllers/deployment/">Deployment</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsworkloadscontrollersreplicaset" href="/k8sDocs/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsworkloadscontrollersstatefulset" href="/k8sDocs/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsworkloadscontrollersdaemonset" href="/k8sDocs/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsworkloadscontrollersjob" href="/k8sDocs/docs/concepts/workloads/controllers/job/">Job</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsworkloadscontrollersgarbage-collection" href="/k8sDocs/docs/concepts/workloads/controllers/garbage-collection/">垃圾回收</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsworkloadscontrollersttlafterfinished" href="/k8sDocs/docs/concepts/workloads/controllers/ttlafterfinished/">用于已完成资源的 TTL 控制器</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsworkloadscontrollerscron-jobs" href="/k8sDocs/docs/concepts/workloads/controllers/cron-jobs/">CronJob</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsworkloadscontrollersreplicationcontroller" href="/k8sDocs/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationController</a>
      
      
    </li>
  </ul>
</ul>

      
      
    </li>
  </ul>
</ul>

      
      
      
      






<ul class="td-sidebar-nav__section pr-md-3">
  <li class="td-sidebar-nav__section-title">
    <a  href="/k8sDocs/docs/concepts/services-networking/" class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">Service, 负载均衡, 网络</a>
  </li>
  <ul>
    <li class="collapse " id="k8sdocsdocsconceptsservices-networking">
      
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsservices-networkingservice" href="/k8sDocs/docs/concepts/services-networking/service/">Service</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsservices-networkingservice-topology" href="/k8sDocs/docs/concepts/services-networking/service-topology/">Service Topology</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsservices-networkingdns-pod-service" href="/k8sDocs/docs/concepts/services-networking/dns-pod-service/">Service 和 Pod 的 DNS</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsservices-networkingconnect-applications-service" href="/k8sDocs/docs/concepts/services-networking/connect-applications-service/">通过 Service 连接应用</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsservices-networkingendpoint-slices" href="/k8sDocs/docs/concepts/services-networking/endpoint-slices/">EndpointSlice</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsservices-networkingingress" href="/k8sDocs/docs/concepts/services-networking/ingress/">Ingress</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsservices-networkingingress-controllers" href="/k8sDocs/docs/concepts/services-networking/ingress-controllers/">Ingress 控制器</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsservices-networkingnetwork-policies" href="/k8sDocs/docs/concepts/services-networking/network-policies/">网络策略</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsservices-networkingadd-entries-to-pod-etc-hosts-with-host-aliases" href="/k8sDocs/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/">通过 HostAliases 向 Pod 的 /etc/hosts 文件中添加条目</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsservices-networkingdual-stack" href="/k8sDocs/docs/concepts/services-networking/dual-stack/">IPv4/IPv6 双栈</a>
      
      
    </li>
  </ul>
</ul>

      
      
      
      






<ul class="td-sidebar-nav__section pr-md-3">
  <li class="td-sidebar-nav__section-title">
    <a  href="/k8sDocs/docs/concepts/storage/" class="align-left pl-0 pr-2 collapsed td-sidebar-link td-sidebar-link__section">存储(Storage)</a>
  </li>
  <ul>
    <li class="collapse " id="k8sdocsdocsconceptsstorage">
      
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsstoragevolumes" href="/k8sDocs/docs/concepts/storage/volumes/">卷(Volume)</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsstoragepersistent-volumes" href="/k8sDocs/docs/concepts/storage/persistent-volumes/">持久化卷(PV)</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsstoragevolume-snapshots" href="/k8sDocs/docs/concepts/storage/volume-snapshots/">卷快照</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsstoragevolume-pvc-datasource" href="/k8sDocs/docs/concepts/storage/volume-pvc-datasource/">CSI 卷克隆</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsstoragestorage-classes" href="/k8sDocs/docs/concepts/storage/storage-classes/">StorageClass</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsstoragevolume-snapshot-classes" href="/k8sDocs/docs/concepts/storage/volume-snapshot-classes/">VolumeSnapshotClass</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsstoragedynamic-provisioning" href="/k8sDocs/docs/concepts/storage/dynamic-provisioning/">卷动态供应</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsstoragestorage-capacity" href="/k8sDocs/docs/concepts/storage/storage-capacity/">存储容量</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsstorageephemeral-volumes" href="/k8sDocs/docs/concepts/storage/ephemeral-volumes/">临时卷</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsstoragestorage-limits" href="/k8sDocs/docs/concepts/storage/storage-limits/">节点级别的卷限制</a>
      
      
    </li>
  </ul>
</ul>

      
      
      
      






<ul class="td-sidebar-nav__section pr-md-3">
  <li class="td-sidebar-nav__section-title">
    <a  href="/k8sDocs/docs/concepts/configuration/" class="align-left pl-0 pr-2 active td-sidebar-link td-sidebar-link__section">配置</a>
  </li>
  <ul>
    <li class="collapse show" id="k8sdocsdocsconceptsconfiguration">
      
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsconfigurationoverview" href="/k8sDocs/docs/concepts/configuration/overview/">配置最佳实践</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsconfigurationconfigmap" href="/k8sDocs/docs/concepts/configuration/configmap/">ConfigMap</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page " id="m-k8sdocsdocsconceptsconfigurationsecret" href="/k8sDocs/docs/concepts/configuration/secret/">Secret</a>
      
      
      
      
      
      <a class="td-sidebar-link td-sidebar-link__page  active" id="m-k8sdocsdocsconceptsconfigurationmanage-resources-containers" href="/k8sDocs/docs/concepts/configuration/manage-resources-containers/">管理容器资源</a>
      
      
    </li>
  </ul>
</ul>

      
      
    </li>
  </ul>
</ul>

      
      
    </li>
  </ul>
</ul>

  </nav>
</div>




          </div>
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            <nav aria-label="breadcrumb" class="d-none d-md-block d-print-none">
	<ol class="breadcrumb spb-1">
		













<li class="breadcrumb-item" >
	<a href="https://lostsquirrel.github.io/k8sDocs/docs/">Kubernetes Documentation</a>
</li>




<li class="breadcrumb-item" >
	<a href="https://lostsquirrel.github.io/k8sDocs/docs/concepts/">概念(Concepts)</a>
</li>




<li class="breadcrumb-item" >
	<a href="https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/">配置</a>
</li>




<li class="breadcrumb-item active" aria-current="page">
	<a href="https://lostsquirrel.github.io/k8sDocs/docs/concepts/configuration/manage-resources-containers/">管理容器资源</a>
</li>

	</ol>
</nav	>

            
              
            
              
    <div class="td-content">
    
<p>
  <a href="https://github.com/kubernetes/website/edit/master/content/zh/docs/concepts/configuration/manage-resources-containers.md" id="editPageButton" target="_blank" data-proofer-ignore>
    Edit This Page
  </a>
</p>

  <h1>管理容器资源</h1>
  
  

<!--
---
title: Managing Resources for Containers
content_type: concept
weight: 40
feature:
  title: Automatic bin packing
  description: >
    Automatically places containers based on their resource requirements and other constraints, while not sacrificing availability. Mix critical and best-effort workloads in order to drive up utilization and save even more resources.
---
 -->
<!-- overview -->
<!--
When you specify a <a class='glossary-tooltip' href='/k8sDocs/concepts/workloads/pods/' target='_blank'>Pod<span class='tooltip-text'>Pod 表示集群中运行的一组容器的集合</span>
</a>, you can optionally specify how
much of each resource a <a class='glossary-tooltip' href='/k8sDocs/concepts/overview/what-is-kubernetes/#why-containers' target='_blank'>Container<span class='tooltip-text'>一个包含应用及其所有信赖的轻量的可移植的可运行的镜像</span>
</a> needs.
The most common resources to specify are CPU and memory (RAM); there are others.

When you specify the resource _request_ for Containers in a Pod, the scheduler uses this
information to decide which node to place the Pod on. When you specify a resource _limit_
for a Container, the kubelet enforces those limits so that the running container is not
allowed to use more of that resource than the limit you set. The kubelet also reserves
at least the _request_ amount of that system resource specifically for that container
to use.
 -->
<p>在配置一个
<a class='glossary-tooltip' href='/k8sDocs/concepts/workloads/pods/' target='_blank'>Pod<span class='tooltip-text'>Pod 表示集群中运行的一组容器的集合</span>
</a>
时，用户可以选择对每个
<a class='glossary-tooltip' href='/k8sDocs/concepts/overview/what-is-kubernetes/#why-containers' target='_blank'>Container<span class='tooltip-text'>一个包含应用及其所有信赖的轻量的可移植的可运行的镜像</span>
</a>
设置其所使用的资源。
最常设置的资源是 CPU 和 内存 (RAM); 但还是有其他的。</p>
<p>当用户为一个 Pod 中的容器设置资源 <em>请求</em> ，调度器会使用这些信息来决定将 Pod 放到哪个节点上。
当为容器设置资源 <em>限制</em> 时， kubelet 会执行这些限制以保证容器在运行时使用的资源不会超过这个设置
的资源限制。 kubelet 也会为这些容器保留资源 <em>请求</em> 所设置的的资源的数量给予容器使用。</p>
<!-- body -->
<!--
## Requests and limits

If the node where a Pod is running has enough of a resource available, it's possible (and
allowed) for a container to use more resource than its `request` for that resource specifies.
However, a container is not allowed to use more than its resource `limit`.

For example, if you set a `memory` request of 256 MiB for a container, and that container is in
a Pod scheduled to a Node with 8GiB of memory and no other Pods, then the container can try to use
more RAM.

If you set a `memory` limit of 4GiB for that Container, the kubelet (and
<a class='glossary-tooltip' href='/docs/setup/production-environment/container-runtimes' target='_blank'>container runtime<span class='tooltip-text'>容器运行环境就是负责运行容器的软件</span>
</a>) enforce the limit.
The runtime prevents the container from using more than the configured resource limit. For example:
when a process in the container tries to consume more than the allowed amount of memory,
the system kernel terminates the process that attempted the allocation, with an out of memory
(OOM) error.

Limits can be implemented either reactively (the system intervenes once it sees a violation)
or by enforcement (the system prevents the container from ever exceeding the limit). Different
runtimes can have different ways to implement the same restrictions.

<blockquote class="note">
  <div><strong>说明：</strong> If a Container specifies its own memory limit, but does not specify a memory request, Kubernetes
automatically assigns a memory request that matches the limit. Similarly, if a Container specifies its own
CPU limit, but does not specify a CPU request, Kubernetes automatically assigns a CPU request that matches
the limit.</div>
</blockquote>

 -->
<h2 id="requests-and-limits">请求与限制</h2>
<p>如果 Pod 运行的节点上的某种资源足够， 这就使得这个容器可能(也允许)使用比 <code>request</code> 所设置的资源
更多的资源，但容器不能使用比 <code>limit</code> 设置的资源的限制。</p>
<p>例如， 如果设置容器的 <code>memory</code> 资源请求为 256 MiB，然后容器所在的 Pod 被调度到一个有 8GiB
内存的节点上并且这个节点上没有其它的 Pod， 这时容器就可以常用使用更多的 RAM.</p>
<p>如果设置了容器 <code>memory</code> 限制为 4GiB， kubelet
(和 <a class='glossary-tooltip' href='/docs/setup/production-environment/container-runtimes' target='_blank'>容器运行环境<span class='tooltip-text'>容器运行环境就是负责运行容器的软件</span>
</a>)
会执行这个限制。容器运行环境会防止容器使用超过容器资源限制所配置的资源数额。 例如，当容器中的一
个进行尝试使用超过允许的内存数量，系统内核就会终止这个尝试申请的进程，错误信息的内存不足(OOM).</p>
<p>这些限制的实现可以是反应式的(当发现起限时系统干涉)或通过强制(系统防止容器使用超过限制资源)。
不同容器运行环境可能以不同的方式实现同样的限制。</p>
<blockquote class="note">
  <div><strong>说明：</strong> 如果一个容器设置内存限制(<code>limit</code>)，但没有设置内存请求(<code>request</code>)，k8s 会自动为其分配一个与限制数额相同的请求。类似地
如果容器指定 CPU 限制(<code>limit</code>)，但没设置 CPU 请求(<code>request</code>)， k8s 会自动为其分配一个与限制数额相同的请求。</div>
</blockquote>

<!--
## Resource types

*CPU* and *memory* are each a *resource type*. A resource type has a base unit.
CPU represents compute processing and is specified in units of [Kubernetes CPUs](#meaning-of-cpu).
Memory is specified in units of bytes.
If you're using Kubernetes v1.14 or newer, you can specify _huge page_ resources.
Huge pages are a Linux-specific feature where the node kernel allocates blocks of memory
that are much larger than the default page size.

For example, on a system where the default page size is 4KiB, you could specify a limit,
`hugepages-2Mi: 80Mi`. If the container tries allocating over 40 2MiB huge pages (a
total of 80 MiB), that allocation fails.

<blockquote class="note">
  <div><strong>说明：</strong> You cannot overcommit <code>hugepages-*</code> resources.
This is different from the <code>memory</code> and <code>cpu</code> resources.</div>
</blockquote>


CPU and memory are collectively referred to as *compute resources*, or just
*resources*. Compute
resources are measurable quantities that can be requested, allocated, and
consumed. They are distinct from
[API resources](/docs/concepts/overview/kubernetes-api/). API resources, such as Pods and
[Services](/docs/concepts/services-networking/service/) are objects that can be read and modified
through the Kubernetes API server.
 -->
<h2 id="resource-types">资源类型</h2>
<p><em>CPU</em> 和 <em>memory</em> 都是 <em>资源类型</em> 的一种。 每种资源类型都有一个基础单位。CPU 代表计算处理
并以
<a href="#meaning-of-cpu">Kubernetes CPUs</a>
为设置的基础单位。 内存是以字节为单位来设置的。 如果使用的是 k8s v1.14+, 可以设置 <em>huge page</em> 资源。
Huge page 是一个 Linux 的特性，当节点内存分配内存块时可以多默认的 page size 大很多</p>
<p>例如，在一个系统中默认的 page size 是 4KiB, 设置了一个限制为 <code>hugepages-2Mi: 80Mi</code>.
如果容器尝试分配了超出 40 个 2Mib 的 huge page(总共就是 80 Mib), 这个分配就会失败。</p>
<blockquote class="note">
  <div><strong>说明：</strong> 用户并不能过量使用 <code>hugepages-*</code> 资源。 这与 <code>memory</code> 与 <code>cpu</code> 资源不同。</div>
</blockquote>

<p>CPU 和 内存都可以被认为是 <em>计算资源</em>, 或者直接称为 <em>资源</em>。 计算资源作为可以请求，分配，和使用
的可量化资源。 他们与
<a href="/k8sDocs/docs/concepts/overview/kubernetes-api/">API 资源</a> 不同的。 API 资源，如
Pod 和
<a href="/k8sDocs/docs/concepts/services-networking/service/">Services</a>
是可以通过 k8s API 服务读取和修改的对象。</p>
<!--
## Resource requests and limits of Pod and Container

Each Container of a Pod can specify one or more of the following:

* `spec.containers[].resources.limits.cpu`
* `spec.containers[].resources.limits.memory`
* `spec.containers[].resources.limits.hugepages-<size>`
* `spec.containers[].resources.requests.cpu`
* `spec.containers[].resources.requests.memory`
* `spec.containers[].resources.requests.hugepages-<size>`

Although requests and limits can only be specified on individual Containers, it
is convenient to talk about Pod resource requests and limits. A
*Pod resource request/limit* for a particular resource type is the sum of the
resource requests/limits of that type for each Container in the Pod.
-->
<h2 id="resource-requests-and-limits-of-pod-and-container">Pod 和 容器对资源的请求和限制</h2>
<p>一个 Pod 中的每个容器都可以指定以下配置中的一个或多个:</p>
<ul>
<li><code>spec.containers[].resources.limits.cpu</code></li>
<li><code>spec.containers[].resources.limits.memory</code></li>
<li><code>spec.containers[].resources.limits.hugepages-&lt;size&gt;</code></li>
<li><code>spec.containers[].resources.requests.cpu</code></li>
<li><code>spec.containers[].resources.requests.memory</code></li>
<li><code>spec.containers[].resources.requests.hugepages-&lt;size&gt;</code></li>
</ul>
<p>尽管资源请求和限制只能被设置在独立的容器上，对 Pod 资源的请求和限制也是很方便的。
对于一个特定类别的 <em>Pod 资源请求/限制</em>就是 Pod 中所有容器该类型的资源 请求/限制 的总和。</p>
<!--
## Resource units in Kubernetes

### Meaning of CPU

Limits and requests for CPU resources are measured in *cpu* units.
One cpu, in Kubernetes, is equivalent to **1 vCPU/Core** for cloud providers and **1 hyperthread** on bare-metal Intel processors.

Fractional requests are allowed. A Container with
`spec.containers[].resources.requests.cpu` of `0.5` is guaranteed half as much
CPU as one that asks for 1 CPU. The expression `0.1` is equivalent to the
expression `100m`, which can be read as "one hundred millicpu". Some people say
"one hundred millicores", and this is understood to mean the same thing. A
request with a decimal point, like `0.1`, is converted to `100m` by the API, and
precision finer than `1m` is not allowed. For this reason, the form `100m` might
be preferred.

CPU is always requested as an absolute quantity, never as a relative quantity;
0.1 is the same amount of CPU on a single-core, dual-core, or 48-core machine.
 -->
<h2 id="resource-units-in-kubernetes">k8s 中的资源单元</h2>
<h3 id="meaning-of-cpu">CPU 的含义</h3>
<p>对 CPU 资源的请求和限制是以 <em>cpu</em> 的单元来计量的。在 k8s 中 1 个单位的 CPU， 与其等效的是
云提供商的 <strong>1 个核心(vCPU/Core)</strong> 和 是裸金属的 Intel 处理器的 <strong>1 个超线程(hyperthread)</strong></p>
<p>允许使用小数的请求。 一个容器中如果设置 <code>spec.containers[].resources.requests.cpu</code> 为
<code>0.5</code> 就表示它至少要保证提供给容器二分之一个 CPU 的资源。 <code>0.1</code> 等同与 <code>100m</code>， 可以被读作
&ldquo;一百微 CPU&rdquo;. 也有人说的是 &ldquo;一百微核心&rdquo;，只要知道这说的是一个意思就行。 配置中如果设置为像
<code>0.1</code> 这样的小数，会被 API 转化为 <code>100m</code>，不能设置比 <code>1m</code> 更小的粒度。所以 <code>100m</code> 这种格式
更合用。</p>
<p>CPU 始终是以绝对数量请求的，绝不是相对数量； 0.1 在单核，双核，48 核的机器表示是一样的数量。</p>
<!--
### Meaning of memory

Limits and requests for `memory` are measured in bytes. You can express memory as
a plain integer or as a fixed-point number using one of these suffixes:
E, P, T, G, M, K. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,
Mi, Ki. For example, the following represent roughly the same value:

```shell
128974848, 129e6, 129M, 123Mi
```

Here's an example.
The following Pod has two Containers. Each Container has a request of 0.25 cpu
and 64MiB (2<sup>26</sup> bytes) of memory. Each Container has a limit of 0.5
cpu and 128MiB of memory. You can say the Pod has a request of 0.5 cpu and 128
MiB of memory, and a limit of 1 cpu and 256MiB of memory.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  - name: log-aggregator
    image: images.my-company.example/log-aggregator:v6
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
```
 -->
<h3 id="meaning-of-memory">内存的含义</h3>
<p>对 <code>memory</code> 的请求和限制是字节来计量的。 可以直接以整数或小数加以下后缀中的一个: E, P, T, G, M, K.
也可以使用 2 的幂的计量单位: Ei, Pi, Ti, Gi, Mi, Ki. 例如，下面几个值的值大约是相等的:</p>
<pre><code>128974848, 129e6, 129M, 123Mi
</code></pre><p>下面是一个例子。下面这个 Pod 中有两个容器。 每个容器请求 0.25 CPU 和 64MiB (2<sup>26</sup> 字节)内存。
每个容器限制 0.5 CPU 和 128MiB 内存。 这样就可以说这个 Pod 请求了 0.5 CPU 和 128 MiB 内存，
限制为 1 CPU 和 256MiB 内存。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">frontend</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">containers</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">app</span>
    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">images.my-company.example/app:v4</span>
    <span style="color:#f92672">resources</span>:
      <span style="color:#f92672">requests</span>:
        <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;64Mi&#34;</span>
        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;250m&#34;</span>
      <span style="color:#f92672">limits</span>:
        <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;128Mi&#34;</span>
        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;500m&#34;</span>
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">log-aggregator</span>
    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">images.my-company.example/log-aggregator:v6</span>
    <span style="color:#f92672">resources</span>:
      <span style="color:#f92672">requests</span>:
        <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;64Mi&#34;</span>
        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;250m&#34;</span>
      <span style="color:#f92672">limits</span>:
        <span style="color:#f92672">memory</span>: <span style="color:#e6db74">&#34;128Mi&#34;</span>
        <span style="color:#f92672">cpu</span>: <span style="color:#e6db74">&#34;500m&#34;</span>
</code></pre></div><!--
## How Pods with resource requests are scheduled

When you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum capacity for each of the resource types: the
amount of CPU and memory it can provide for Pods. The scheduler ensures that,
for each resource type, the sum of the resource requests of the scheduled
Containers is less than the capacity of the node. Note that although actual memory
or CPU resource usage on nodes is very low, the scheduler still refuses to place
a Pod on a node if the capacity check fails. This protects against a resource
shortage on a node when resource usage later increases, for example, during a
daily peak in request rate.
 -->
<h2 id="how-pods-with-resource-requests-are-scheduled">带有资源请求的 Pod 是怎么调度的</h2>
<p>当用户创建一个 Pod 时， k8s 调度器会为 Pod 选择一个节点让它在上面运行。 每个节点都有对每个资源
类型的最大容量: 可以供给 Pod 运行的 CPU 的数量和内存数量。调度器会确保被调度的容器所请求的
各种资源的总和要小于节点对应资源的容量。 即便节点上实际内存或 CPU 资源都很低，调度器依然会在容量
检查失败后拒绝将 Pod 放在这个节点上。 这是为了防止后续资源使用增加而导致资源短缺，例如，每天的
请求峰值。</p>
<!--
## How Pods with resource limits are run

When the kubelet starts a Container of a Pod, it passes the CPU and memory limits
to the container runtime.

When using Docker:

- The `spec.containers[].resources.requests.cpu` is converted to its core value,
  which is potentially fractional, and multiplied by 1024. The greater of this number
  or 2 is used as the value of the
  [`--cpu-shares`](https://docs.docker.com/engine/reference/run/#cpu-share-constraint)
  flag in the `docker run` command.

- The `spec.containers[].resources.limits.cpu` is converted to its millicore value and
  multiplied by 100. The resulting value is the total amount of CPU time that a container can use
  every 100ms. A container cannot use more than its share of CPU time during this interval.

  <blockquote class="note">
  <div><strong>说明：</strong> The default quota period is 100ms. The minimum resolution of CPU quota is 1ms.</div>
</blockquote>


- The `spec.containers[].resources.limits.memory` is converted to an integer, and
  used as the value of the
  [`--memory`](https://docs.docker.com/engine/reference/run/#/user-memory-constraints)
  flag in the `docker run` command.

If a Container exceeds its memory limit, it might be terminated. If it is
restartable, the kubelet will restart it, as with any other type of runtime
failure.

If a Container exceeds its memory request, it is likely that its Pod will
be evicted whenever the node runs out of memory.

A Container might or might not be allowed to exceed its CPU limit for extended
periods of time. However, it will not be killed for excessive CPU usage.

To determine whether a Container cannot be scheduled or is being killed due to
resource limits, see the
[Troubleshooting](#troubleshooting) section.
 -->
<h2 id="how-pods-with-resource-limits-are-run">带有资源限制的 Pod 是怎么运行的</h2>
<p>当 kubelet 为一个 Pod 启动一个容器时，它会给容器运行时传递 CPU 和 内存的限制。</p>
<p>在使用 Docker 时:</p>
<ul>
<li>
<p><code>spec.containers[].resources.requests.cpu</code> 会被转化为核心值，一般来说它很可能是个小数，
并乘上 1024，得出的结果与 2 相比比较大的一个会作为 <code>docker run</code> 命令的
<a href="https://docs.docker.com/engine/reference/run/#cpu-share-constraint"><code>--cpu-shares</code></a>
的值。</p>
</li>
<li>
<p><code>spec.containers[].resources.limits.cpu</code> 会被转化为微核心值并乘以 100. 得出的结果就是
每 100ms 中这个容器可以使用的 CPU 时间。容器在每个时间段不能使用超出其限制的 CPU 时间</p>
<blockquote class="note">
  <div><strong>说明：</strong> 默认的配额时间段是 100ms， CPU 配置的最小粒度是 1ms</div>
</blockquote>

</li>
<li>
<p><code>spec.containers[].resources.limits.memory</code> 会被转化为一个整数，并作为 <code>docker run</code>
命令的
<a href="https://docs.docker.com/engine/reference/run/#/user-memory-constraints"><code>--memory</code></a>
标记的值。</p>
</li>
</ul>
<p>如果容器超出了内存限制，它就可能被终止。 如果容器是可以重启的，kubelet 就会把它重启了，就像任意
其它类型的运行失败一样。</p>
<p>如果一个容器超出了其请求的内存，它会在节点内存耗尽时被踢出去。</p>
<p>一个容器可能允许也可能不允许超出 CPU 使用时间限制。 但是它不会因为 CPU 使用超限而被杀掉。</p>
<p>决定容器不能被调度或因资源限制而被杀掉的因素见 <a href="#troubleshooting">故障检查</a> 章节</p>
<!--
### Monitoring compute & memory resource usage

The resource usage of a Pod is reported as part of the Pod status.

If optional [tools for monitoring](/docs/tasks/debug-application-cluster/resource-usage-monitoring/)
are available in your cluster, then Pod resource usage can be retrieved either
from the [Metrics API](/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#the-metrics-api)
directly or from your monitoring tools.
 -->
<h3 id="monitoring-compute-memory-resource-usage">监控计算和内存资源使用</h3>
<p>Pod 所使用的资源会作为 Pod 状态报告的一部分出现。</p>
<p>如果集群中有可选的
<a href="/k8sDocs/docs/tasks/debug-application-cluster/resource-usage-monitoring/">监控工具</a>，
Pod 资源使用可以通过
<a href="/k8sDocs/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#the-metrics-api">Metrics API</a>
或直接通过监控工作中的一种中获取。</p>
<!-- feature gate LocalStorageCapacityIsolation -->
<!--
## Local ephemeral storage







<div style="margin-top: 10px; margin-bottom: 10px;">
<b>功能特性状态:</b> <code>Kubernetes v1.10 [beta]</code>
</div>



Nodes have local ephemeral storage, backed by
locally-attached writeable devices or, sometimes, by RAM.
"Ephemeral" means that there is no long-term guarantee about durability.

Pods use ephemeral local storage for scratch space, caching, and for logs.
The kubelet can provide scratch space to Pods using local ephemeral storage to
mount [`emptyDir`](/docs/concepts/storage/volumes/#emptydir)
 <a class='glossary-tooltip' href='/k8sDocs/docs/concepts/storage/volumes/' target='_blank'>volumes<span class='tooltip-text'>一个可以被 Pod 中的容器访问的包含数据的目录</span>
</a> into containers.

The kubelet also uses this kind of storage to hold
[node-level container logs](/docs/concepts/cluster-administration/logging/#logging-at-the-node-level),
container images, and the writable layers of running containers.

<blockquote class="caution">
  <div><strong>注意：</strong> If a node fails, the data in its ephemeral storage can be lost.<br>
Your applications cannot expect any performance SLAs (disk IOPS for example)
from local ephemeral storage.</div>
</blockquote>


As a beta feature, Kubernetes lets you track, reserve and limit the amount
of ephemeral local storage a Pod can consume.
 -->
<h2 id="local-ephemeral-storage">本地临时存储</h2>
<!-- feature gate LocalStorageCapacityIsolation -->





<div style="margin-top: 10px; margin-bottom: 10px;">
<b>功能特性状态:</b> <code>Kubernetes v1.10 [beta]</code>
</div>


<p>节点有本地临时存储，这些存储由本地挂载可写设备或有时候是 RAM 来提供。&ldquo;临时&rdquo; 的意思就是对
数据的持久性没有长期保证。</p>
<p>Pod 可以使用临时本地存储作为暂存空间，缓存或日志。 kubelet 可以使用临时本地存储提供暂存空间
来挂载
<a href="/docs/concepts/storage/volumes/#emptydir"><code>emptyDir</code></a>
到容器中的
<a class='glossary-tooltip' href='/k8sDocs/docs/concepts/storage/volumes/' target='_blank'>卷(Volume)<span class='tooltip-text'>一个可以被 Pod 中的容器访问的包含数据的目录</span>
</a></p>
<p>kubelet 还可以使用这种类型的存储来放置
<a href="/k8sDocs/docs/concepts/cluster-administration/logging/#logging-at-the-node-level">节点级别的容器日志</a>,
容器镜像，运行容器的可写层。</p>
<blockquote class="caution">
  <div><strong>注意：</strong> 如果节点挂了，临时存储中的数据就可能丢失。并且应用并不能对本地临时存储有任何性能 SLA(例如,磁盘 IOPS)
有任何要求</div>
</blockquote>

<p>作为一个 beta 特性， k8s 可以让用户跟踪，保留，限制一个 Pod 可以使用的临时本地存储。</p>
<h3 id="configurations-for-local-ephemeral-storage">Configurations for local ephemeral storage</h3>
<p>Kubernetes supports two ways to configure local ephemeral storage on a node:
<div id="local_storage_configurations">
<ul><li><a href="#local_storage_configurations-0">Single filesystem</a></li><li><a href="#local_storage_configurations-1">Two filesystems</a></li></ul><div id="local_storage_configurations-0"><p>In this configuration, you place all different kinds of ephemeral local data
(<code>emptyDir</code> volumes, writeable layers, container images, logs) into one filesystem.
The most effective way to configure the kubelet means dedicating this filesystem
to Kubernetes (kubelet) data.</p>
<p>The kubelet also writes
<a href="/docs/concepts/cluster-administration/logging/#logging-at-the-node-level">node-level container logs</a>
and treats these similarly to ephemeral local storage.</p>
<p>The kubelet writes logs to files inside its configured log directory (<code>/var/log</code>
by default); and has a base directory for other locally stored data
(<code>/var/lib/kubelet</code> by default).</p>
<p>Typically, both <code>/var/lib/kubelet</code> and <code>/var/log</code> are on the system root filesystem,
and the kubelet is designed with that layout in mind.</p>
<p>Your node can have as many other filesystems, not used for Kubernetes,
as you like.</p>
</div><div id="local_storage_configurations-1"><p>You have a filesystem on the node that you&rsquo;re using for ephemeral data that
comes from running Pods: logs, and <code>emptyDir</code> volumes. You can use this filesystem
for other data (for example: system logs not related to Kubernetes); it can even
be the root filesystem.</p>
<p>The kubelet also writes
<a href="/docs/concepts/cluster-administration/logging/#logging-at-the-node-level">node-level container logs</a>
into the first filesystem, and treats these similarly to ephemeral local storage.</p>
<p>You also use a separate filesystem, backed by a different logical storage device.
In this configuration, the directory where you tell the kubelet to place
container image layers and writeable layers is on this second filesystem.</p>
<p>The first filesystem does not hold any image layers or writeable layers.</p>
<p>Your node can have as many other filesystems, not used for Kubernetes,
as you like.</p>
</div></div><script>$(function(){$("#local_storage_configurations").tabs();});</script></p>
<p>The kubelet can measure how much local storage it is using. It does this provided
that:</p>
<ul>
<li>the <code>LocalStorageCapacityIsolation</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
is enabled (the feature is on by default), and</li>
<li>you have set up the node using one of the supported configurations
for local ephemeral storage.</li>
</ul>
<p>If you have a different configuration, then the kubelet does not apply resource
limits for ephemeral local storage.</p>
<blockquote class="note">
  <div><strong>说明：</strong> The kubelet tracks <code>tmpfs</code> emptyDir volumes as container memory use, rather
than as local ephemeral storage.</div>
</blockquote>

<h3 id="setting-requests-and-limits-for-local-ephemeral-storage">Setting requests and limits for local ephemeral storage</h3>
<p>You can use <em>ephemeral-storage</em> for managing local ephemeral storage. Each Container of a Pod can specify one or more of the following:</p>
<ul>
<li><code>spec.containers[].resources.limits.ephemeral-storage</code></li>
<li><code>spec.containers[].resources.requests.ephemeral-storage</code></li>
</ul>
<p>Limits and requests for <code>ephemeral-storage</code> are measured in bytes. You can express storage as
a plain integer or as a fixed-point number using one of these suffixes:
E, P, T, G, M, K. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,
Mi, Ki. For example, the following represent roughly the same value:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">128974848, 129e6, 129M, 123Mi
</code></pre></div><p>In the following example, the Pod has two Containers. Each Container has a request of 2GiB of local ephemeral storage. Each Container has a limit of 4GiB of local ephemeral storage. Therefore, the Pod has a request of 4GiB of local ephemeral storage, and a limit of 8GiB of local ephemeral storage.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">frontend</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">containers</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">app</span>
    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">images.my-company.example/app:v4</span>
    <span style="color:#f92672">resources</span>:
      <span style="color:#f92672">requests</span>:
        <span style="color:#f92672">ephemeral-storage</span>: <span style="color:#e6db74">&#34;2Gi&#34;</span>
      <span style="color:#f92672">limits</span>:
        <span style="color:#f92672">ephemeral-storage</span>: <span style="color:#e6db74">&#34;4Gi&#34;</span>
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">log-aggregator</span>
    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">images.my-company.example/log-aggregator:v6</span>
    <span style="color:#f92672">resources</span>:
      <span style="color:#f92672">requests</span>:
        <span style="color:#f92672">ephemeral-storage</span>: <span style="color:#e6db74">&#34;2Gi&#34;</span>
      <span style="color:#f92672">limits</span>:
        <span style="color:#f92672">ephemeral-storage</span>: <span style="color:#e6db74">&#34;4Gi&#34;</span>
</code></pre></div><h3 id="how-pods-with-ephemeral-storage-requests-are-scheduled">How Pods with ephemeral-storage requests are scheduled</h3>
<p>When you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum amount of local ephemeral storage it can provide for Pods. For more information, see <a href="/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">Node Allocatable</a>.</p>
<p>The scheduler ensures that the sum of the resource requests of the scheduled Containers is less than the capacity of the node.</p>
<h3 id="resource-emphemeralstorage-consumption">Ephemeral storage consumption management</h3>
<p>If the kubelet is managing local ephemeral storage as a resource, then the
kubelet measures storage use in:</p>
<ul>
<li><code>emptyDir</code> volumes, except <em>tmpfs</em> <code>emptyDir</code> volumes</li>
<li>directories holding node-level logs</li>
<li>writeable container layers</li>
</ul>
<p>If a Pod is using more ephemeral storage than you allow it to, the kubelet
sets an eviction signal that triggers Pod eviction.</p>
<p>For container-level isolation, if a Container&rsquo;s writable layer and log
usage exceeds its storage limit, the kubelet marks the Pod for eviction.</p>
<p>For pod-level isolation the kubelet works out an overall Pod storage limit by
summing the limits for the containers in that Pod. In this case, if the sum of
the local ephemeral storage usage from all containers and also the Pod&rsquo;s <code>emptyDir</code>
volumes exceeds the overall Pod storage limit, then the kubelet also marks the Pod
for eviction.</p>
<blockquote class="caution">
  <div><strong>注意：</strong> <p>If the kubelet is not measuring local ephemeral storage, then a Pod
that exceeds its local storage limit will not be evicted for breaching
local storage resource limits.</p>
<p>However, if the filesystem space for writeable container layers, node-level logs,
or <code>emptyDir</code> volumes falls low, the node
<a class='glossary-tooltip' href='/docs/concepts/scheduling-eviction/taint-and-toleration/' target='_blank'>taints<span class='tooltip-text'>A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups.</span>
</a> itself as short on local storage
and this taint triggers eviction for any Pods that don&rsquo;t specifically tolerate the taint.</p>
<p>See the supported <a href="#configurations-for-local-ephemeral-storage">configurations</a>
for ephemeral local storage.</p>
</div>
</blockquote>

<p>The kubelet supports different ways to measure Pod storage use:</p>
<div id="resource-emphemeralstorage-measurement">
<ul><li><a href="#resource-emphemeralstorage-measurement-0">Periodic scanning</a></li><li><a href="#resource-emphemeralstorage-measurement-1">Filesystem project quota</a></li></ul><div id="resource-emphemeralstorage-measurement-0"><p>The kubelet performs regular, scheduled checks that scan each
<code>emptyDir</code> volume, container log directory, and writeable container layer.</p>
<p>The scan measures how much space is used.</p>
<blockquote class="note">
  <div><strong>说明：</strong> <p>In this mode, the kubelet does not track open file descriptors
for deleted files.</p>
<p>If you (or a container) create a file inside an <code>emptyDir</code> volume,
something then opens that file, and you delete the file while it is
still open, then the inode for the deleted file stays until you close
that file but the kubelet does not categorize the space as in use.</p>
</div>
</blockquote>
</div><div id="resource-emphemeralstorage-measurement-1"><div style="margin-top: 10px; margin-bottom: 10px;">
<b>功能特性状态:</b> <code>Kubernetes v1.15 [alpha]</code>
</div>
<p>Project quotas are an operating-system level feature for managing
storage use on filesystems. With Kubernetes, you can enable project
quotas for monitoring storage use. Make sure that the filesystem
backing the <code>emptyDir</code> volumes, on the node, provides project quota support.
For example, XFS and ext4fs offer project quotas.</p>
<blockquote class="note">
  <div><strong>说明：</strong> Project quotas let you monitor storage use; they do not enforce limits.</div>
</blockquote>
<p>Kubernetes uses project IDs starting from <code>1048576</code>. The IDs in use are
registered in <code>/etc/projects</code> and <code>/etc/projid</code>. If project IDs in
this range are used for other purposes on the system, those project
IDs must be registered in <code>/etc/projects</code> and <code>/etc/projid</code> so that
Kubernetes does not use them.</p>
<p>Quotas are faster and more accurate than directory scanning. When a
directory is assigned to a project, all files created under a
directory are created in that project, and the kernel merely has to
keep track of how many blocks are in use by files in that project.<br>
If a file is created and deleted, but has an open file descriptor,
it continues to consume space. Quota tracking records that space accurately
whereas directory scans overlook the storage used by deleted files.</p>
<p>If you want to use project quotas, you should:</p>
<ul>
<li>
<p>Enable the <code>LocalStorageCapacityIsolationFSQuotaMonitoring=true</code>
<a href="/docs/reference/command-line-tools-reference/feature-gates/">feature gate</a>
in the kubelet configuration.</p>
</li>
<li>
<p>Ensure that the root filesystem (or optional runtime filesystem)
has project quotas enabled. All XFS filesystems support project quotas.
For ext4 filesystems, you need to enable the project quota tracking feature
while the filesystem is not mounted.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#75715e"># For ext4, with /dev/block-device not mounted</span>
sudo tune2fs -O project -Q prjquota /dev/block-device
</code></pre></div></li>
<li>
<p>Ensure that the root filesystem (or optional runtime filesystem) is
mounted with project quotas enabled. For both XFS and ext4fs, the
mount option is named <code>prjquota</code>.</p>
</li>
</ul>
</div></div><script>$(function(){$("#resource-emphemeralstorage-measurement").tabs();});</script>
<h2 id="extended-resources">Extended resources</h2>
<p>Extended resources are fully-qualified resource names outside the
<code>kubernetes.io</code> domain. They allow cluster operators to advertise and users to
consume the non-Kubernetes-built-in resources.</p>
<p>There are two steps required to use Extended Resources. First, the cluster
operator must advertise an Extended Resource. Second, users must request the
Extended Resource in Pods.</p>
<h3 id="managing-extended-resources">Managing extended resources</h3>
<h4 id="node-level-extended-resources">Node-level extended resources</h4>
<p>Node-level extended resources are tied to nodes.</p>
<h5 id="device-plugin-managed-resources">Device plugin managed resources</h5>
<p>See <a href="/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">Device
Plugin</a>
for how to advertise device plugin managed resources on each node.</p>
<h5 id="other-resources">Other resources</h5>
<p>To advertise a new node-level extended resource, the cluster operator can
submit a <code>PATCH</code> HTTP request to the API server to specify the available
quantity in the <code>status.capacity</code> for a node in the cluster. After this
operation, the node&rsquo;s <code>status.capacity</code> will include a new resource. The
<code>status.allocatable</code> field is updated automatically with the new resource
asynchronously by the kubelet. Note that because the scheduler uses the	node
<code>status.allocatable</code> value when evaluating Pod fitness, there may be a short
delay between patching the node capacity with a new resource and the first Pod
that requests the resource to be scheduled on that node.</p>
<p><strong>Example:</strong></p>
<p>Here is an example showing how to use <code>curl</code> to form an HTTP request that
advertises five &ldquo;example.com/foo&rdquo; resources on node <code>k8s-node-1</code> whose master
is <code>k8s-master</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl --header <span style="color:#e6db74">&#34;Content-Type: application/json-patch+json&#34;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--request PATCH <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>--data <span style="color:#e6db74">&#39;[{&#34;op&#34;: &#34;add&#34;, &#34;path&#34;: &#34;/status/capacity/example.com~1foo&#34;, &#34;value&#34;: &#34;5&#34;}]&#39;</span> <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>http://k8s-master:8080/api/v1/nodes/k8s-node-1/status
</code></pre></div><blockquote class="note">
  <div><strong>说明：</strong> In the preceding request, <code>~1</code> is the encoding for the character <code>/</code>
in the patch path. The operation path value in JSON-Patch is interpreted as a
JSON-Pointer. For more details, see
<a href="https://tools.ietf.org/html/rfc6901#section-3">IETF RFC 6901, section 3</a>.</div>
</blockquote>

<h4 id="cluster-level-extended-resources">Cluster-level extended resources</h4>
<p>Cluster-level extended resources are not tied to nodes. They are usually managed
by scheduler extenders, which handle the resource consumption and resource quota.</p>
<p>You can specify the extended resources that are handled by scheduler extenders
in <a href="https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/scheduler/api/v1/types.go#L31">scheduler policy
configuration</a>.</p>
<p><strong>Example:</strong></p>
<p>The following configuration for a scheduler policy indicates that the
cluster-level extended resource &ldquo;example.com/foo&rdquo; is handled by the scheduler
extender.</p>
<ul>
<li>The scheduler sends a Pod to the scheduler extender only if the Pod requests
&ldquo;example.com/foo&rdquo;.</li>
<li>The <code>ignoredByScheduler</code> field specifies that the scheduler does not check
the &ldquo;example.com/foo&rdquo; resource in its <code>PodFitsResources</code> predicate.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#f92672">&#34;kind&#34;</span>: <span style="color:#e6db74">&#34;Policy&#34;</span>,
  <span style="color:#f92672">&#34;apiVersion&#34;</span>: <span style="color:#e6db74">&#34;v1&#34;</span>,
  <span style="color:#f92672">&#34;extenders&#34;</span>: [
    {
      <span style="color:#f92672">&#34;urlPrefix&#34;</span>:<span style="color:#e6db74">&#34;&lt;extender-endpoint&gt;&#34;</span>,
      <span style="color:#f92672">&#34;bindVerb&#34;</span>: <span style="color:#e6db74">&#34;bind&#34;</span>,
      <span style="color:#f92672">&#34;managedResources&#34;</span>: [
        {
          <span style="color:#f92672">&#34;name&#34;</span>: <span style="color:#e6db74">&#34;example.com/foo&#34;</span>,
          <span style="color:#f92672">&#34;ignoredByScheduler&#34;</span>: <span style="color:#66d9ef">true</span>
        }
      ]
    }
  ]
}
</code></pre></div><h3 id="consuming-extended-resources">Consuming extended resources</h3>
<p>Users can consume extended resources in Pod specs just like CPU and memory.
The scheduler takes care of the resource accounting so that no more than the
available amount is simultaneously allocated to Pods.</p>
<p>The API server restricts quantities of extended resources to whole numbers.
Examples of <em>valid</em> quantities are <code>3</code>, <code>3000m</code> and <code>3Ki</code>. Examples of
<em>invalid</em> quantities are <code>0.5</code> and <code>1500m</code>.</p>
<blockquote class="note">
  <div><strong>说明：</strong> Extended resources replace Opaque Integer Resources.
Users can use any domain name prefix other than <code>kubernetes.io</code> which is reserved.</div>
</blockquote>

<p>To consume an extended resource in a Pod, include the resource name as a key
in the <code>spec.containers[].resources.limits</code> map in the container spec.</p>
<blockquote class="note">
  <div><strong>说明：</strong> Extended resources cannot be overcommitted, so request and limit
must be equal if both are present in a container spec.</div>
</blockquote>

<p>A Pod is scheduled only if all of the resource requests are satisfied, including
CPU, memory and any extended resources. The Pod remains in the <code>PENDING</code> state
as long as the resource request cannot be satisfied.</p>
<p><strong>Example:</strong></p>
<p>The Pod below requests 2 CPUs and 1 &ldquo;example.com/foo&rdquo; (an extended resource).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
<span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
<span style="color:#f92672">metadata</span>:
  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">my-pod</span>
<span style="color:#f92672">spec</span>:
  <span style="color:#f92672">containers</span>:
  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">my-container</span>
    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">myimage</span>
    <span style="color:#f92672">resources</span>:
      <span style="color:#f92672">requests</span>:
        <span style="color:#f92672">cpu</span>: <span style="color:#ae81ff">2</span>
        <span style="color:#f92672">example.com/foo</span>: <span style="color:#ae81ff">1</span>
      <span style="color:#f92672">limits</span>:
        <span style="color:#f92672">example.com/foo</span>: <span style="color:#ae81ff">1</span>
</code></pre></div><h2 id="pid-limiting">PID limiting</h2>
<p>Process ID (PID) limits allow for the configuration of a kubelet to limit the number of PIDs that a given Pod can consume. See <a href="/docs/concepts/policy/pid-limiting/">Pid Limiting</a> for information.</p>
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="my-pods-are-pending-with-event-message-failedscheduling">My Pods are pending with event message failedScheduling</h3>
<p>If the scheduler cannot find any node where a Pod can fit, the Pod remains
unscheduled until a place can be found. An event is produced each time the
scheduler fails to find a place for the Pod, like this:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod frontend | grep -A <span style="color:#ae81ff">3</span> Events
</code></pre></div><pre><code>Events:
  FirstSeen LastSeen   Count  From          Subobject   PathReason      Message
  36s   5s     6      {scheduler }              FailedScheduling  Failed for reason PodExceedsFreeCPU and possibly others
</code></pre><p>In the preceding example, the Pod named &ldquo;frontend&rdquo; fails to be scheduled due to
insufficient CPU resource on the node. Similar error messages can also suggest
failure due to insufficient memory (PodExceedsFreeMemory). In general, if a Pod
is pending with a message of this type, there are several things to try:</p>
<ul>
<li>Add more nodes to the cluster.</li>
<li>Terminate unneeded Pods to make room for pending Pods.</li>
<li>Check that the Pod is not larger than all the nodes. For example, if all the
nodes have a capacity of <code>cpu: 1</code>, then a Pod with a request of <code>cpu: 1.1</code> will
never be scheduled.</li>
</ul>
<p>You can check node capacities and amounts allocated with the
<code>kubectl describe nodes</code> command. For example:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe nodes e2e-test-node-pool-4lw4
</code></pre></div><pre><code>Name:            e2e-test-node-pool-4lw4
[ ... lines removed for clarity ...]
Capacity:
 cpu:                               2
 memory:                            7679792Ki
 pods:                              110
Allocatable:
 cpu:                               1800m
 memory:                            7474992Ki
 pods:                              110
[ ... lines removed for clarity ...]
Non-terminated Pods:        (5 in total)
  Namespace    Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------    ----                                  ------------  ----------  ---------------  -------------
  kube-system  fluentd-gcp-v1.38-28bv1               100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)
  kube-system  kube-dns-3297075139-61lj3             260m (13%)    0 (0%)      100Mi (1%)       170Mi (2%)
  kube-system  kube-proxy-e2e-test-...               100m (5%)     0 (0%)      0 (0%)           0 (0%)
  kube-system  monitoring-influxdb-grafana-v4-z1m12  200m (10%)    200m (10%)  600Mi (8%)       600Mi (8%)
  kube-system  node-problem-detector-v0.1-fj7m3      20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests    CPU Limits    Memory Requests    Memory Limits
  ------------    ----------    ---------------    -------------
  680m (34%)      400m (20%)    920Mi (11%)        1070Mi (13%)
</code></pre><p>In the preceding output, you can see that if a Pod requests more than 1120m
CPUs or 6.23Gi of memory, it will not fit on the node.</p>
<p>By looking at the <code>Pods</code> section, you can see which Pods are taking up space on
the node.</p>
<p>The amount of resources available to Pods is less than the node capacity, because
system daemons use a portion of the available resources. The <code>allocatable</code> field
<a href="/docs/reference/generated/kubernetes-api/v1.19/#nodestatus-v1-core">NodeStatus</a>
gives the amount of resources that are available to Pods. For more information, see
<a href="https://git.k8s.io/community/contributors/design-proposals/node/node-allocatable.md">Node Allocatable Resources</a>.</p>
<p>The <a href="/docs/concepts/policy/resource-quotas/">resource quota</a> feature can be configured
to limit the total amount of resources that can be consumed. If used in conjunction
with namespaces, it can prevent one team from hogging all the resources.</p>
<h3 id="my-container-is-terminated">My Container is terminated</h3>
<p>Your Container might get terminated because it is resource-starved. To check
whether a Container is being killed because it is hitting a resource limit, call
<code>kubectl describe pod</code> on the Pod of interest:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl describe pod simmemleak-hra99
</code></pre></div><pre><code>Name:                           simmemleak-hra99
Namespace:                      default
Image(s):                       saadali/simmemleak
Node:                           kubernetes-node-tf0f/10.240.216.66
Labels:                         name=simmemleak
Status:                         Running
Reason:
Message:
IP:                             10.244.2.75
Replication Controllers:        simmemleak (1/1 replicas created)
Containers:
  simmemleak:
    Image:  saadali/simmemleak
    Limits:
      cpu:                      100m
      memory:                   50Mi
    State:                      Running
      Started:                  Tue, 07 Jul 2015 12:54:41 -0700
    Last Termination State:     Terminated
      Exit Code:                1
      Started:                  Fri, 07 Jul 2015 12:54:30 -0700
      Finished:                 Fri, 07 Jul 2015 12:54:33 -0700
    Ready:                      False
    Restart Count:              5
Conditions:
  Type      Status
  Ready     False
Events:
  FirstSeen                         LastSeen                         Count  From                              SubobjectPath                       Reason      Message
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {scheduler }                                                          scheduled   Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {kubelet kubernetes-node-tf0f}    implicitly required container POD   pulled      Pod container image &quot;k8s.gcr.io/pause:0.8.0&quot; already present on machine
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {kubelet kubernetes-node-tf0f}    implicitly required container POD   created     Created with docker id 6a41280f516d
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {kubelet kubernetes-node-tf0f}    implicitly required container POD   started     Started with docker id 6a41280f516d
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {kubelet kubernetes-node-tf0f}    spec.containers{simmemleak}         created     Created with docker id 87348f12526a
</code></pre><p>In the preceding example, the <code>Restart Count:  5</code> indicates that the <code>simmemleak</code>
Container in the Pod was terminated and restarted five times.</p>
<p>You can call <code>kubectl get pod</code> with the <code>-o go-template=...</code> option to fetch the status
of previously terminated Containers:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">kubectl get pod -o go-template<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;{{range.status.containerStatuses}}{{&#34;Container Name: &#34;}}{{.name}}{{&#34;\r\nLastState: &#34;}}{{.lastState}}{{end}}&#39;</span>  simmemleak-hra99
</code></pre></div><pre><code>Container Name: simmemleak
LastState: map[terminated:map[exitCode:137 reason:OOM Killed startedAt:2015-07-07T20:58:43Z finishedAt:2015-07-07T20:58:43Z containerID:docker://0e4095bba1feccdfe7ef9fb6ebffe972b4b14285d5acdec6f0d3ae8a22fad8b2]]
</code></pre><p>You can see that the Container was terminated because of <code>reason:OOM Killed</code>, where <code>OOM</code> stands for Out Of Memory.</p>
<h2 id="相关资料">相关资料</h2>
<ul>
<li>
<p>Get hands-on experience <a href="/docs/tasks/configure-pod-container/assign-memory-resource/">assigning Memory resources to Containers and Pods</a>.</p>
</li>
<li>
<p>Get hands-on experience <a href="/docs/tasks/configure-pod-container/assign-cpu-resource/">assigning CPU resources to Containers and Pods</a>.</p>
</li>
<li>
<p>For more details about the difference between requests and limits, see
<a href="https://git.k8s.io/community/contributors/design-proposals/node/resource-qos.md">Resource QoS</a>.</p>
</li>
<li>
<p>Read the <a href="/docs/reference/generated/kubernetes-api/v1.19/#container-v1-core">Container</a> API reference</p>
</li>
<li>
<p>Read the <a href="/docs/reference/generated/kubernetes-api/v1.19/#resourcerequirements-v1-core">ResourceRequirements</a> API reference</p>
</li>
<li>
<p>Read about <a href="https://xfs.org/docs/xfsdocs-xml-dev/XFS_User_Guide/tmp/en-US/html/xfs-quotas.html">project quotas</a> in XFS</p>
</li>
</ul>



            
            <div class="text-muted mt-5 pt-3 border-top">最后修改 2020-12-28 17:58:00: <a  href="https://github.com/lostsquirrel/k8sDocs/commit/bf37f0c316b2b97bbab7468535316ece3dd32ba1">[WIP] manage container resources day 1 (bf37f0c)</a>
</div>
            
          </main>
          <div class="d-none d-xl-block col-xl-2 td-toc d-print-none">
            






<div class="td-page-meta ml-2 pb-1 pt-2 mb-0">











<a href="https://github.com/lostsquirrel/k8sDocs/edit/master/content/zh/docs/concepts/configuration/manage-resources-containers.md" target="_blank"><i class="fa fa-edit fa-fw"></i> 编辑此页</a>
<a href="https://github.com/lostsquirrel/k8sDocs/new/master/content/zh/docs/concepts/configuration/manage-resources-containers.md?filename=change-me.md&amp;value=---%0Atitle%3A&#43;%22Long&#43;Page&#43;Title%22%0AlinkTitle%3A&#43;%22Short&#43;Nav&#43;Title%22%0Aweight%3A&#43;100%0Adescription%3A&#43;%3E-%0A&#43;&#43;&#43;&#43;&#43;Page&#43;description&#43;for&#43;heading&#43;and&#43;indexes.%0A---%0A%0A%23%23&#43;Heading%0A%0AEdit&#43;this&#43;template&#43;to&#43;create&#43;your&#43;new&#43;page.%0A%0A%2A&#43;Give&#43;it&#43;a&#43;good&#43;name%2C&#43;ending&#43;in&#43;%60.md%60&#43;-&#43;e.g.&#43;%60getting-started.md%60%0A%2A&#43;Edit&#43;the&#43;%22front&#43;matter%22&#43;section&#43;at&#43;the&#43;top&#43;of&#43;the&#43;page&#43;%28weight&#43;controls&#43;how&#43;its&#43;ordered&#43;amongst&#43;other&#43;pages&#43;in&#43;the&#43;same&#43;directory%3B&#43;lowest&#43;number&#43;first%29.%0A%2A&#43;Add&#43;a&#43;good&#43;commit&#43;message&#43;at&#43;the&#43;bottom&#43;of&#43;the&#43;page&#43;%28%3C80&#43;characters%3B&#43;use&#43;the&#43;extended&#43;description&#43;field&#43;for&#43;more&#43;detail%29.%0A%2A&#43;Create&#43;a&#43;new&#43;branch&#43;so&#43;you&#43;can&#43;preview&#43;your&#43;new&#43;file&#43;and&#43;request&#43;a&#43;review&#43;via&#43;Pull&#43;Request.%0A" target="_blank"><i class="fa fa-edit fa-fw"></i> 添加子页面</a>
<a href="https://github.com/lostsquirrel/k8sDocs/issues/new?title=%e7%ae%a1%e7%90%86%e5%ae%b9%e5%99%a8%e8%b5%84%e6%ba%90" target="_blank"><i class="fab fa-github fa-fw"></i> 提交文档问题</a>

</div>






<nav id="TableOfContents">
  <ul>
    <li><a href="#requests-and-limits">请求与限制</a></li>
    <li><a href="#resource-types">资源类型</a></li>
    <li><a href="#resource-requests-and-limits-of-pod-and-container">Pod 和 容器对资源的请求和限制</a></li>
    <li><a href="#resource-units-in-kubernetes">k8s 中的资源单元</a>
      <ul>
        <li><a href="#meaning-of-cpu">CPU 的含义</a></li>
        <li><a href="#meaning-of-memory">内存的含义</a></li>
      </ul>
    </li>
    <li><a href="#how-pods-with-resource-requests-are-scheduled">带有资源请求的 Pod 是怎么调度的</a></li>
    <li><a href="#how-pods-with-resource-limits-are-run">带有资源限制的 Pod 是怎么运行的</a>
      <ul>
        <li><a href="#monitoring-compute-memory-resource-usage">监控计算和内存资源使用</a></li>
      </ul>
    </li>
    <li><a href="#local-ephemeral-storage">本地临时存储</a>
      <ul>
        <li><a href="#configurations-for-local-ephemeral-storage">Configurations for local ephemeral storage</a></li>
        <li><a href="#setting-requests-and-limits-for-local-ephemeral-storage">Setting requests and limits for local ephemeral storage</a></li>
        <li><a href="#how-pods-with-ephemeral-storage-requests-are-scheduled">How Pods with ephemeral-storage requests are scheduled</a></li>
        <li><a href="#resource-emphemeralstorage-consumption">Ephemeral storage consumption management</a></li>
      </ul>
    </li>
    <li><a href="#extended-resources">Extended resources</a>
      <ul>
        <li><a href="#managing-extended-resources">Managing extended resources</a></li>
        <li><a href="#consuming-extended-resources">Consuming extended resources</a></li>
      </ul>
    </li>
    <li><a href="#pid-limiting">PID limiting</a></li>
    <li><a href="#troubleshooting">Troubleshooting</a>
      <ul>
        <li><a href="#my-pods-are-pending-with-event-message-failedscheduling">My Pods are pending with event message failedScheduling</a></li>
        <li><a href="#my-container-is-terminated">My Container is terminated</a></li>
      </ul>
    </li>
    <li><a href="#相关资料">相关资料</a></li>
  </ul>
</nav>



          </div>
        </div>
      </div>
      
<footer class="bg-dark py-5 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        
      </div>
      <div class="col-12 col-sm-4 text-center py-2 order-sm-2">
        
        
	
		
	
      </div>
    </div>
  </div>
</footer>


    </div>
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>











<script src="/k8sDocs/js/main.min.446a2e681f8167f44314691b77368dd561bf0b67514487f3f0fd999316c6da4a.js" integrity="sha256-RGouaB&#43;BZ/RDFGkbdzaN1WG/C2dRRIfz8P2ZkxbG2ko=" crossorigin="anonymous"></script>




  </body>
</html>