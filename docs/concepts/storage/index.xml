<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes – 存储(Storage)</title>
    <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/</link>
    <description>Recent content in 存储(Storage) on Kubernetes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    
	  <atom:link href="https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: 卷(Volume)</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/</link>
      <pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jsafrane
- saad-ali
- thockin
- msau42
title: Volumes
content_type: concept
weight: 10
--- --&gt;
&lt;!-- overview --&gt;
&lt;!--
On-disk files in a Container are ephemeral, which presents some problems for
non-trivial applications when running in Containers.  First, when a Container
crashes, kubelet will restart it, but the files will be lost - the
Container starts with a clean state.  Second, when running Containers together
in a `Pod` it is often necessary to share files between those Containers.  The
Kubernetes `Volume` abstraction solves both of these problems.

Familiarity with [Pods](/docs/concepts/workloads/pods/pod/) is suggested.
 --&gt;
&lt;p&gt;容器中写到硬盘的文件是临时的，这会导致有些需要写入硬盘文件的应用出现一些问题。第一个问题是当一个容器
崩溃后，kubelet 会将其重启，但其所写的文件会全部丢失 - 容器会以全新的状态启动。 第二个问题是
在一个 &lt;code&gt;Pod&lt;/code&gt; 中的不同容器之间需要共享文件。 k8s 的 &lt;code&gt;Volume&lt;/code&gt; 抽象概念就是解决这些问题的。&lt;/p&gt;
&lt;p&gt;建议先看看 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/pod/&#34;&gt;Pods&lt;/a&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Background

Docker also has a concept of
[volumes](https://docs.docker.com/storage/), though it is
somewhat looser and less managed.  In Docker, a volume is simply a directory on
disk or in another Container.  Lifetimes are not managed and until very
recently there were only local-disk-backed volumes.  Docker now provides volume
drivers, but the functionality is very limited for now (e.g. as of Docker 1.7
only one volume driver is allowed per Container and there is no way to pass
parameters to volumes).

A Kubernetes volume, on the other hand, has an explicit lifetime - the same as
the Pod that encloses it.  Consequently, a volume outlives any Containers that run
within the Pod, and data is preserved across Container restarts. Of course, when a
Pod ceases to exist, the volume will cease to exist, too.  Perhaps more
importantly than this, Kubernetes supports many types of volumes, and a Pod can
use any number of them simultaneously.

At its core, a volume is just a directory, possibly with some data in it, which
is accessible to the Containers in a Pod.  How that directory comes to be, the
medium that backs it, and the contents of it are determined by the particular
volume type used.

To use a volume, a Pod specifies what volumes to provide for the Pod (the
`.spec.volumes`
field) and where to mount those into Containers (the
`.spec.containers[*].volumeMounts`
field).

A process in a container sees a filesystem view composed from their Docker
image and volumes.  The [Docker
image](https://docs.docker.com/userguide/dockerimages/) is at the root of the
filesystem hierarchy, and any volumes are mounted at the specified paths within
the image.  Volumes can not mount onto other volumes or have hard links to
other volumes.  Each Container in the Pod must independently specify where to
mount each volume.
 --&gt;
&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;p&gt;Docker 同样有 &lt;a href=&#34;https://docs.docker.com/storage/&#34;&gt;volumes&lt;/a&gt; 概念，但是 Docker 的数据卷(volume)
不那么好管理。在 Docker 中，一个数据卷就是简单地在磁盘上或另一个容器中的一个目录。存在期不受管理，
并且直到最近(文档最早提交为 2018年5月6日)都只支持本地磁盘的数据卷。 现在 Docker 提供了数据卷
驱动， 但其功能也还是相当有限(例如，Docker v1.7 中每个容器只支持一个数据卷驱动，并且没办法向
数据卷传递参数)。&lt;/p&gt;
&lt;p&gt;k8s 的数据卷，相对来说就更加强大，拥有明确的生命期 - 与其绑定的 Pod 相同。 因此，一个数据卷可能
比 Pod 中所有的容器的命都长， 并且在容器重启之后数据依然存在。 当然，当 Pod 不存在进，与其绑定的
数据卷也就不存在了。 可能比这些更重要的是 k8s 支持许多类型的数据卷，并且一个 Pod 可以同时使用
任意类型任意数量的数据卷。&lt;/p&gt;
&lt;p&gt;数据卷的核心也只是一个目录，可能其中还有数据，它可以被 Pod 中的容器访问。 这个目录是怎么来的，
它所用的介质是什么，它其中的内存是什么，这些都是由数据卷所使用的类型所决定的。&lt;/p&gt;
&lt;p&gt;要使用一个数据卷，需要在 Pod 配置文件中 &lt;code&gt;.spec.volumes&lt;/code&gt; 字段上配置可用数据卷，并在容器的
&lt;code&gt;.spec.containers[*].volumeMounts&lt;/code&gt; 字段指定数据卷和在容器内的挂载点&lt;/p&gt;
&lt;p&gt;在一个容器内的进程看到的文件系统是由它们的 Docker 镜像和数据卷共同组成的。 &lt;a href=&#34;https://docs.docker.com/userguide/dockerimages/&#34;&gt;Docker
镜像&lt;/a&gt;位于文件系统层级的底层， 然后各个
数据卷挂载到镜像内相应的目录上。 数据卷不能再挂载到其它的数据卷上，也不创建硬连接到其它的数据卷。
Pod 中的每个容器都需要独立的指定每个数据卷的挂载点。&lt;/p&gt;
&lt;!--
## Types of Volumes

Kubernetes supports several types of Volumes:

   * [awsElasticBlockStore](#awselasticblockstore)
   * [azureDisk](#azuredisk)
   * [azureFile](#azurefile)
   * [cephfs](#cephfs)
   * [cinder](#cinder)
   * [configMap](#configmap)
   * [csi](#csi)
   * [downwardAPI](#downwardapi)
   * [emptyDir](#emptydir)
   * [fc (fibre channel)](#fc)
   * [flexVolume](#flexVolume)
   * [flocker](#flocker)
   * [gcePersistentDisk](#gcepersistentdisk)
   * [gitRepo (deprecated)](#gitrepo)
   * [glusterfs](#glusterfs)
   * [hostPath](#hostpath)
   * [iscsi](#iscsi)
   * [local](#local)
   * [nfs](#nfs)
   * [persistentVolumeClaim](#persistentvolumeclaim)
   * [projected](#projected)
   * [portworxVolume](#portworxvolume)
   * [quobyte](#quobyte)
   * [rbd](#rbd)
   * [scaleIO](#scaleio)
   * [secret](#secret)
   * [storageos](#storageos)
   * [vsphereVolume](#vspherevolume)

We welcome additional contributions.

 --&gt;
&lt;h2 id=&#34;types-of-volumes&#34;&gt;数据卷(Volume)的类型&lt;/h2&gt;
&lt;p&gt;以下为 k8s 支持的数据卷(Volume)类型:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#awselasticblockstore&#34;&gt;awsElasticBlockStore&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#azuredisk&#34;&gt;azureDisk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#azurefile&#34;&gt;azureFile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cephfs&#34;&gt;cephfs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cinder&#34;&gt;cinder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#configmap&#34;&gt;configMap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#csi&#34;&gt;csi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#downwardapi&#34;&gt;downwardAPI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#emptydir&#34;&gt;emptyDir&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fc&#34;&gt;fc (fibre channel)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#flexVolume&#34;&gt;flexVolume&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#flocker&#34;&gt;flocker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gcepersistentdisk&#34;&gt;gcePersistentDisk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gitrepo&#34;&gt;gitRepo (deprecated)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#glusterfs&#34;&gt;glusterfs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hostpath&#34;&gt;hostPath&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#iscsi&#34;&gt;iscsi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#local&#34;&gt;local&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nfs&#34;&gt;nfs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#persistentvolumeclaim&#34;&gt;persistentVolumeClaim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#projected&#34;&gt;projected&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#portworxvolume&#34;&gt;portworxVolume&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quobyte&#34;&gt;quobyte&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rbd&#34;&gt;rbd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scaleio&#34;&gt;scaleIO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#secret&#34;&gt;secret&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#storageos&#34;&gt;storageos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vspherevolume&#34;&gt;vsphereVolume&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;欢迎贡献更多类型.&lt;/p&gt;
&lt;!--
### awsElasticBlockStore {#awselasticblockstore}

An `awsElasticBlockStore` volume mounts an Amazon Web Services (AWS) [EBS
Volume](https://aws.amazon.com/ebs/) into your Pod.  Unlike
`emptyDir`, which is erased when a Pod is removed, the contents of an EBS
volume are preserved and the volume is merely unmounted.  This means that an
EBS volume can be pre-populated with data, and that data can be &#34;handed off&#34;
between Pods.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must create an EBS volume using &lt;code&gt;aws ec2 create-volume&lt;/code&gt; or the AWS API before you can use it.&lt;/div&gt;
&lt;/blockquote&gt;


There are some restrictions when using an `awsElasticBlockStore` volume:

* the nodes on which Pods are running must be AWS EC2 instances
* those instances need to be in the same region and availability-zone as the EBS volume
* EBS only supports a single EC2 instance mounting a volume
 --&gt;
&lt;h3 id=&#34;awselasticblockstore&#34;&gt;awsElasticBlockStore&lt;/h3&gt;
&lt;p&gt;一个 &lt;code&gt;awsElasticBlockStore&lt;/code&gt; 数据卷会挂载一个 AWS 的 &lt;a href=&#34;https://aws.amazon.com/ebs/&#34;&gt;EBS 卷&lt;/a&gt;
到 Pod 中。 与 &lt;code&gt;emptyDir&lt;/code&gt; 在 Pod 删除时清除数据不同， 在 EBS 卷中的内存会在卸载后依然会保存
其中的内容。也就是说一个 EBS 卷 可以预先存在数据，其中的数据也可以在不同的 Pod 之间传递。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 必须要先使用 &lt;code&gt;aws ec2 create-volume&lt;/code&gt; 或 AWS API 创建一个 EBS 卷 才能在 &lt;code&gt;awsElasticBlockStore&lt;/code&gt; 中使用。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;使用 &lt;code&gt;awsElasticBlockStore&lt;/code&gt; 卷有如下限制:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod 运行的节点必须是 AWS EC2 实例&lt;/li&gt;
&lt;li&gt;节点实例必须与 EBS 卷 在同一个地址或可用域&lt;/li&gt;
&lt;li&gt;EBS 只支持一个 EC2 实例挂载一个卷&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
#### Creating an EBS volume

Before you can use an EBS volume with a Pod, you need to create it.

```shell
aws ec2 create-volume --availability-zone=eu-west-1a --size=10 --volume-type=gp2
```

Make sure the zone matches the zone you brought up your cluster in.  (And also check that the size and EBS volume
type are suitable for your use!)
 --&gt;
&lt;h4 id=&#34;创建一个-ebs-卷&#34;&gt;创建一个 EBS 卷&lt;/h4&gt;
&lt;p&gt;在 Pod 中使用一个 EBS 卷 之前需要先创建。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;aws ec2 create-volume --availability-zone&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;eu-west-1a --size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt; --volume-type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;gp2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;需要确保卷所在的可用区域与集群所在的可用区域相同。(同时还要检查卷的大小和类型是合用的)&lt;/p&gt;
&lt;!--
#### AWS EBS Example configuration

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-ebs
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-ebs
      name: test-volume
  volumes:
  - name: test-volume
    # This AWS EBS volume must already exist.
    awsElasticBlockStore:
      volumeID: &lt;volume-id&gt;
      fsType: ext4
```
 --&gt;
&lt;h4 id=&#34;一个使用-aws-ebs-的配置示例&#34;&gt;一个使用 AWS EBS 的配置示例&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-ebs&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/test-ebs&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# This AWS EBS volume must already exist.&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;awsElasticBlockStore&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeID&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;&amp;lt;volume-id&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
#### CSI Migration






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;



The CSI Migration feature for awsElasticBlockStore, when enabled, shims all plugin operations
from the existing in-tree plugin to the `ebs.csi.aws.com` Container
Storage Interface (CSI) Driver. In order to use this feature, the [AWS EBS CSI
Driver](https://github.com/kubernetes-sigs/aws-ebs-csi-driver)
must be installed on the cluster and the `CSIMigration` and `CSIMigrationAWS`
Beta features must be enabled.
 --&gt;
&lt;h4 id=&#34;csi-迁移&#34;&gt;CSI 迁移&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;code&gt;awsElasticBlockStore&lt;/code&gt; 的 CSI 迁移特性，在启用时，会将来自已经存在的插件的插件操作转移到
&lt;code&gt;ebs.csi.aws.com&lt;/code&gt; 容器存储接口(CSI)驱动。 为了使用该特性， 需要在集群中先安装
&lt;a href=&#34;https://github.com/kubernetes-sigs/aws-ebs-csi-driver&#34;&gt;AWS EBS CSI 驱动&lt;/a&gt;
同时启用 &lt;code&gt;CSIMigration&lt;/code&gt; 和 &lt;code&gt;CSIMigrationAWS&lt;/code&gt; 两个 &lt;code&gt;beta&lt;/code&gt; 特性&lt;/p&gt;
&lt;!--
#### CSI Migration Complete





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [alpha]&lt;/code&gt;
&lt;/div&gt;



To turn off the awsElasticBlockStore storage plugin from being loaded by controller manager and kubelet, you need to set this feature flag to true. This requires `ebs.csi.aws.com` Container Storage Interface (CSI) driver being installed on all worker nodes.
 --&gt;
&lt;h4 id=&#34;csi-迁移完成&#34;&gt;CSI 迁移完成&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;为了关闭由控制器管理器和 kubelet 加载的 &lt;code&gt;awsElasticBlockStore&lt;/code&gt; 存储插件， 需要将这个标记设置
为 &lt;code&gt;true&lt;/code&gt;. 这需要在所有的工作节点安装 &lt;code&gt;ebs.csi.aws.com&lt;/code&gt; 容器存储接口(CSI)驱动&lt;/p&gt;
&lt;!--
### azureDisk {#azuredisk}

A `azureDisk` is used to mount a Microsoft Azure [Data Disk](https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-linux-about-disks-vhds/) into a Pod.

More details can be found [here](https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_disk/README.md).
 --&gt;
&lt;h3 id=&#34;azuredisk&#34;&gt;azureDisk&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;azureDisk&lt;/code&gt; 是用来将微软 Azure 的
&lt;a href=&#34;https://azure.microsoft.com/en-us/documentation/articles/virtual-machines-linux-about-disks-vhds/&#34;&gt;Data Disk&lt;/a&gt;
数据盘挂载到 Pod 中的。
更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_disk/README.md&#34;&gt;这里&lt;/a&gt;.&lt;/p&gt;
&lt;!--
#### CSI Migration






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;



The CSI Migration feature for azureDisk, when enabled, shims all plugin operations
from the existing in-tree plugin to the `disk.csi.azure.com` Container
Storage Interface (CSI) Driver. In order to use this feature, the [Azure Disk CSI
Driver](https://github.com/kubernetes-sigs/azuredisk-csi-driver)
must be installed on the cluster and the `CSIMigration` and `CSIMigrationAzureDisk`
features must be enabled.
 --&gt;
&lt;h4 id=&#34;csi-迁移-1&#34;&gt;CSI 迁移&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;code&gt;azureDisk&lt;/code&gt; 的 CSI 迁移特性，在启用时，会将来自已经存在的插件的插件操作转移到
&lt;code&gt;disk.csi.azure.com&lt;/code&gt;  容器存储接口(CSI)驱动。 为了使用该特性， 需要在集群中先安装
&lt;a href=&#34;https://github.com/kubernetes-sigs/azuredisk-csi-driver&#34;&gt;Azure Disk CSI Driver&lt;/a&gt;
同时启用 &lt;code&gt;CSIMigration&lt;/code&gt; 和 &lt;code&gt;CSIMigrationAzureDisk&lt;/code&gt; 两个特性&lt;/p&gt;
&lt;!--
### azureFile {#azurefile}

A `azureFile` is used to mount a Microsoft Azure File Volume (SMB 2.1 and 3.0)
into a Pod.

More details can be found [here](https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_file/README.md).
 --&gt;
&lt;h3 id=&#34;azurefile&#34;&gt;azureFile&lt;/h3&gt;
&lt;p&gt;A &lt;code&gt;azureFile&lt;/code&gt; is used to mount a Microsoft Azure File Volume (SMB 2.1 and 3.0)
into a Pod.
&lt;code&gt;azureFile&lt;/code&gt; 用于将微软 Azure 文件卷(SMB 2.1 and 3.0) 挂载到 Pod 中。
更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_file/README.md&#34;&gt;这里&lt;/a&gt;.&lt;/p&gt;
&lt;!--
#### CSI Migration






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.15 [alpha]&lt;/code&gt;
&lt;/div&gt;



The CSI Migration feature for azureFile, when enabled, shims all plugin operations
from the existing in-tree plugin to the `file.csi.azure.com` Container
Storage Interface (CSI) Driver. In order to use this feature, the [Azure File CSI
Driver](https://github.com/kubernetes-sigs/azurefile-csi-driver)
must be installed on the cluster and the `CSIMigration` and `CSIMigrationAzureFile`
Alpha features must be enabled.
 --&gt;
&lt;h4 id=&#34;csi-迁移-2&#34;&gt;CSI 迁移&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.15 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;code&gt;azureFile&lt;/code&gt; 的 CSI 迁移特性，在启用时，会将来自已经存在的插件的插件操作转移到
&lt;code&gt;file.csi.azure.com&lt;/code&gt;  容器存储接口(CSI)驱动。 为了使用该特性， 需要在集群中先安装
&lt;a href=&#34;https://github.com/kubernetes-sigs/azurefile-csi-driver&#34;&gt;Azure File CSI Driver&lt;/a&gt;
同时启用 &lt;code&gt;CSIMigration&lt;/code&gt; 和 &lt;code&gt;CSIMigrationAzureFile&lt;/code&gt; 两个 &lt;code&gt;Alpha&lt;/code&gt; 特性&lt;/p&gt;
&lt;h3 id=&#34;cephfs&#34;&gt;cephfs&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;cephfs&lt;/code&gt; 卷可以将一个已经存在的 CephFS 卷挂载到一个 Pod 中。
与 &lt;code&gt;emptyDir&lt;/code&gt; 在 Pod 删除时清除数据不同， 在 &lt;code&gt;cephfs&lt;/code&gt; 卷中的内存会在卸载后依然会保存
其中的内容。也就是说一个 &lt;code&gt;cephfs&lt;/code&gt; 卷可以预先存在数据，其中的数据也可以在不同的 Pod 之间传递。
CephFS 还可以被同时多个挂载拥有写权限&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在使用之前需要有自己的 Ceph 服务在运行，并且相应的分离已经配置好&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/volumes/cephfs/&#34;&gt;CephFS 示例&lt;/a&gt;&lt;/p&gt;
&lt;!--
### cinder {#cinder}

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Prerequisite: Kubernetes with OpenStack Cloud Provider configured.&lt;/div&gt;
&lt;/blockquote&gt;


`cinder` is used to mount OpenStack Cinder Volume into your Pod.
 --&gt;
&lt;h3 id=&#34;cinder&#34;&gt;cinder&lt;/h3&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 前置条件: 配置好 OpenStack 云提供商的 k8s 集群。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;cinder&lt;/code&gt; 用于将 OpenStack Cinder 卷挂载到 Pod.&lt;/p&gt;
&lt;!--
#### Cinder Volume Example configuration

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-cinder
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-cinder-container
    volumeMounts:
    - mountPath: /test-cinder
      name: test-volume
  volumes:
  - name: test-volume
    # This OpenStack volume must already exist.
    cinder:
      volumeID: &lt;volume-id&gt;
      fsType: ext4
```
--&gt;
&lt;h4 id=&#34;cinder-volume-配置示例&#34;&gt;Cinder Volume 配置示例&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-cinder&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-cinder-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/test-cinder&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# 这个 OpenStack 卷必须已经存在.&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;cinder&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeID&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;&amp;lt;volume-id&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;csi-迁移-3&#34;&gt;CSI 迁移&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;code&gt;Cinder&lt;/code&gt; 的 CSI 迁移特性，在启用时，会将来自已经存在的插件的插件操作转移到 &lt;code&gt;cinder.csi.openstack.org&lt;/code&gt;  容器存储接口(CSI)驱动。 为了使用该特性， 需要在集群中先安装
&lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-cinder-csi-plugin.md&#34;&gt;Openstack Cinder CSI
Driver&lt;/a&gt;
同时启用 &lt;code&gt;CSIMigration&lt;/code&gt; 和 &lt;code&gt;CSIMigrationOpenStack&lt;/code&gt; 两个 &lt;code&gt;beta&lt;/code&gt; 特性&lt;/p&gt;
&lt;!--
### configMap {#configmap}

The [`configMap`](/docs/tasks/configure-pod-container/configure-pod-configmap/) resource
provides a way to inject configuration data into Pods.
The data stored in a `ConfigMap` object can be referenced in a volume of type
`configMap` and then consumed by containerized applications running in a Pod.

When referencing a `configMap` object, you can simply provide its name in the
volume to reference it. You can also customize the path to use for a specific
entry in the ConfigMap.
For example, to mount the `log-config` ConfigMap onto a Pod called `configmap-pod`,
you might use the YAML below:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
    - name: test
      image: busybox
      volumeMounts:
        - name: config-vol
          mountPath: /etc/config
  volumes:
    - name: config-vol
      configMap:
        name: log-config
        items:
          - key: log_level
            path: log_level
```

The `log-config` ConfigMap is mounted as a volume, and all contents stored in
its `log_level` entry are mounted into the Pod at path &#34;`/etc/config/log_level`&#34;.
Note that this path is derived from the volume&#39;s `mountPath` and the `path`
keyed with `log_level`.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must create a &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/configure-pod-container/configure-pod-configmap/&#34;&gt;ConfigMap&lt;/a&gt; before you can use it.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A Container using a ConfigMap as a &lt;a href=&#34;#using-subpath&#34;&gt;subPath&lt;/a&gt; volume mount will not
receive ConfigMap updates.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Text data is exposed as files using the UTF-8 character encoding. To use some other character encoding, use binaryData.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;configmap&#34;&gt;configMap&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/configure-pod-container/configure-pod-configmap/&#34;&gt;&lt;code&gt;configMap&lt;/code&gt;&lt;/a&gt; 资源提供了一种将配置数据注入 Pod 的方式。&lt;/p&gt;
&lt;p&gt;存在于 &lt;code&gt;ConfigMap&lt;/code&gt; 对象中的数据可以通过 &lt;code&gt;configMap&lt;/code&gt; 类型的数据卷被 Pod 引用，然后被存在于 Pod 中的容器化应用所使用。&lt;/p&gt;
&lt;p&gt;在引用 &lt;code&gt;configMap&lt;/code&gt; 对象时，只需要提供这个对象的名称在引用它的卷上面。
也可以通过自定义路径的方式使用 ConfigMap 中指定的条目。
例如， 将一个叫 &lt;code&gt;log-config&lt;/code&gt; ConfigMap 挂载到一个 叫 &lt;code&gt;configmap-pod&lt;/code&gt; 的 Pod上，配置 YAML 如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;configmap-pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;config-vol&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/etc/config&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;config-vol&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;configMap&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;log-config&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;items&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;log_level&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;log_level&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;log-config&lt;/code&gt; ConfigMap 以卷的方式挂载， 其中所有存在于条目 &lt;code&gt;log_level&lt;/code&gt; 的内容会被挂载到 Pod 的 &lt;code&gt;/etc/config/log_level&lt;/code&gt; 上
其中这个路径继承了卷上面的 &lt;code&gt;mountPath&lt;/code&gt; 和 键名为 &lt;code&gt;log_level&lt;/code&gt; 的
&lt;code&gt;path&lt;/code&gt; 的值 -  &lt;code&gt;log_level&lt;/code&gt;&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在使用之前必须要先创建对应的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-pod-configmap/&#34;&gt;ConfigMap&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 一个以 &lt;a href=&#34;#using-subpath&#34;&gt;subPath&lt;/a&gt; 卷方式使用 ConfigMap 的容器是
不能接收到 ConfigMap 的更新的&lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 文本内存数据以 UTF-8编码的文件方式提供。 要使用其它的编码方式请使用
二进制数据(&lt;code&gt;binaryData&lt;/code&gt;)&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### downwardAPI {#downwardapi}

A `downwardAPI` volume is used to make downward API data available to applications.
It mounts a directory and writes the requested data in plain text files.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A Container using Downward API as a &lt;a href=&#34;#using-subpath&#34;&gt;subPath&lt;/a&gt; volume mount will not
receive Downward API updates.&lt;/div&gt;
&lt;/blockquote&gt;


See the [`downwardAPI` volume example](/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/)  for more details.

 --&gt;
&lt;h3 id=&#34;downwardapi&#34;&gt;downwardAPI&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;downwardAPI&lt;/code&gt; 卷用来让 downward API 的数据在应用中可用。
它通过挂载一个目录然后将请求的数据以纯文本文件的形式写在这个目录中。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 一个以 &lt;a href=&#34;#using-subpath&#34;&gt;subPath&lt;/a&gt; 卷方式使用 Downward API 的容器是 不能接收到 Downward API 的更新的&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;更多信息见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/&#34;&gt;&lt;code&gt;downwardAPI&lt;/code&gt; 卷示例&lt;/a&gt;&lt;/p&gt;
&lt;!--

### emptyDir {#emptydir}

An `emptyDir` volume is first created when a Pod is assigned to a Node, and
exists as long as that Pod is running on that node.  As the name says, it is
initially empty.  Containers in the Pod can all read and write the same
files in the `emptyDir` volume, though that volume can be mounted at the same
or different paths in each Container.  When a Pod is removed from a node for
any reason, the data in the `emptyDir` is deleted forever.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A Container crashing does &lt;em&gt;NOT&lt;/em&gt; remove a Pod from a node, so the data in an &lt;code&gt;emptyDir&lt;/code&gt; volume is safe across Container crashes.&lt;/div&gt;
&lt;/blockquote&gt;


Some uses for an `emptyDir` are:

* scratch space, such as for a disk-based merge sort
* checkpointing a long computation for recovery from crashes
* holding files that a content-manager Container fetches while a webserver
  Container serves the data

By default, `emptyDir` volumes are stored on whatever medium is backing the
node - that might be disk or SSD or network storage, depending on your
environment.  However, you can set the `emptyDir.medium` field to `&#34;Memory&#34;`
to tell Kubernetes to mount a tmpfs (RAM-backed filesystem) for you instead.
While tmpfs is very fast, be aware that unlike disks, tmpfs is cleared on
node reboot and any files you write will count against your Container&#39;s
memory limit.
 --&gt;
&lt;h3 id=&#34;emptydir&#34;&gt;emptyDir&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;emptyDir&lt;/code&gt; 卷在 Pod 分配到节点时创建，只要这个 Pod 还在这个节点上运行，
那么这个郑就会存在。 就像名称所示，它就是个空目录，所以初始化时是空的。
Pod 中 所有的容器都可以读写这个 &lt;code&gt;emptyDir&lt;/code&gt; 卷中的文件， 这个卷可以挂载到每个
容器的同一个路径或不同的路径。当 Pod 因为任何原因从节点移除时， 存在于  &lt;code&gt;emptyDir&lt;/code&gt;
中的数据会被永久删除。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 容器崩溃并 &lt;em&gt;不会&lt;/em&gt; 导致 Pod 从节点删除，所以容器崩溃并不会导致 &lt;code&gt;emptyDir&lt;/code&gt; 中的数据丢失。&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;Some uses for an &lt;code&gt;emptyDir&lt;/code&gt; are:
一些用到 &lt;code&gt;emptyDir&lt;/code&gt; 的地方:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;临时空间，例如基于磁盘的合并排序&lt;/li&gt;
&lt;li&gt;作为一个恢复崩溃的长时计划的检查点
Container serves the data&lt;/li&gt;
&lt;li&gt;存放由 内容管理容器抓取然后用于 web 服务器使用的数据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;默认情况下， &lt;code&gt;emptyDir&lt;/code&gt; 卷使用所在节点所使用的介质存储数据，基于环境可能是机械硬盘
SSD 或 网络存储。 但是，用户可以通过设置 &lt;code&gt;emptyDir.medium&lt;/code&gt; 字段值为 &lt;code&gt;&amp;quot;Memory&amp;quot;&lt;/code&gt;
让 k8s 将其挂载为 tmpfs (基于内存的文件系统)。
因为 tmpfs 的速度很快，但要注意与磁盘不同， tmpfs 会在节点重启时被清除，
可用空间的大小受内存大小限制。&lt;/p&gt;
&lt;h4 id=&#34;示例&#34;&gt;示例&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-pd&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/cache&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cache-volume&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cache-volume&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;emptyDir&lt;/span&gt;: {}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;fc&#34;&gt;fc (fibre channel)&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;fc&lt;/code&gt; 卷可以将光纤信道(fibre channel)卷挂载到 Pod 中。 可以在卷的配置中使用
&lt;code&gt;targetWWNs&lt;/code&gt; 参数指定一个或多个目标全局名称(World Wide Names).
如果指定了多个 WWNs， targetWWNs 使用的是那些来自 multi-path 连接的  WWNs
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 必须要预先配置 FC SAN Zoning 分配并且标记这些目标 WWN 的 LUNs (卷)
这样 k8s 主机才能访问它们&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/fibre_channel&#34;&gt;FC 示例&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;flocker&#34;&gt;flocker&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ClusterHQ/flocker&#34;&gt;Flocker&lt;/a&gt; 是一个开源的集群
容器数据卷管理器。 它提供了管理和编排多种存储后端的数据卷&lt;/p&gt;
&lt;p&gt;&lt;code&gt;flocker&lt;/code&gt; 让 Flocker 数据集可以被挂载到 Pod 中。 如果数据集还没有存在于 Flocker， 必须要使用 Flocker
CLI 或 Flocker API 创建。 如果数据集已经存在于 Flocker， 当 Pod 被调度到
节点时会被重新通过 Flocker 挂载。这就是说这些数据可以在需要的 Pod 传递。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 必须要在使用之前先安装运行 Flocker&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/flocker&#34;&gt;Flocker 示例&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;gcepersistentdisk&#34;&gt;gcePersistentDisk&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;gcePersistentDisk&lt;/code&gt; 卷可以将 Google 计算引擎(GCE)
&lt;a href=&#34;https://cloud.google.com/compute/docs/disks&#34;&gt;持久化磁盘(PD)&lt;/a&gt;
挂载到 Pod 中。 与 &lt;code&gt;emptyDir&lt;/code&gt; 在 Pod 删除时同时删除不同， PD 中的内存会依旧保留 仅仅卸载卷。 这就是说 PD 可以预先添加数据，并且可以在不同 Pod 之间传递。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 必须要先使用 &lt;code&gt;gcloud&lt;/code&gt; 或 GCE API 或 UI 创建 PD 才能使用。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;使用 &lt;code&gt;gcePersistentDisk&lt;/code&gt; 有如下限制:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod 运行的节点必须是 GCE 虚拟机&lt;/li&gt;
&lt;li&gt;这些虚拟机必须与 PD 在同一个 GCE 项目和区域&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PD 有一个特性是如果要同时挂载到多个 Pod 只能以只读方式挂载。
也就是可以预先将数据存入 PD 中，然后就可以在需要的任意个多 Pod 中使用。
不幸的是只能以读写方式挂载一次，不能以写方式多次挂载
在一个用 ReplicationController 管理的 Pod 中使用 PD 时，只有在 PD 为只读
或副本数为 0 或 1 时才能成功否则就会失败。&lt;/p&gt;
&lt;!--
#### Creating a PD

Before you can use a GCE PD with a Pod, you need to create it.

```shell
gcloud compute disks create --size=500GB --zone=us-central1-a my-data-disk
```

#### Example Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    # This GCE PD must already exist.
    gcePersistentDisk:
      pdName: my-data-disk
      fsType: ext4
```
 --&gt;
&lt;h4 id=&#34;创建-pd&#34;&gt;创建 PD&lt;/h4&gt;
&lt;p&gt;在 Pod 中使用 GCE PD 前需要先创建&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;gcloud compute disks create --size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;500GB --zone&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;us-central1-a my-data-disk
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;示例-pod&#34;&gt;示例 Pod&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-pd&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/test-pd&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# 这个 GCE PD 必须要先存在&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;gcePersistentDisk&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;pdName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-data-disk&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
#### Regional Persistent Disks
The [Regional Persistent Disks](https://cloud.google.com/compute/docs/disks/#repds) feature allows the creation of Persistent Disks that are available in two zones within the same region. In order to use this feature, the volume must be provisioned as a PersistentVolume; referencing the volume directly from a pod is not supported.
 --&gt;
&lt;h4 id=&#34;地区性持久化磁盘&#34;&gt;地区性持久化磁盘&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/compute/docs/disks/#repds&#34;&gt;地区性持久化磁盘&lt;/a&gt; 特性允许在同一个地区的两个区域创建 地区性持久化磁盘。
要使用这个特性， 这个卷必须由 PersistentVolume 管理；不支持在 Pod 直接使用&lt;/p&gt;
&lt;!--
#### Manually provisioning a Regional PD PersistentVolume
Dynamic provisioning is possible using a [StorageClass for GCE PD](/docs/concepts/storage/storage-classes/#gce).
Before creating a PersistentVolume, you must create the PD:
```shell
gcloud compute disks create --size=500GB my-data-disk
    --region us-central1
    --replica-zones us-central1-a,us-central1-b
```
Example PersistentVolume spec:

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: test-volume
spec:
  capacity:
    storage: 400Gi
  accessModes:
  - ReadWriteOnce
  gcePersistentDisk:
    pdName: my-data-disk
    fsType: ext4
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: failure-domain.beta.kubernetes.io/zone
          operator: In
          values:
          - us-central1-a
          - us-central1-b
```
 --&gt;
&lt;h4 id=&#34;手动管理一个地区性-pd-persistentvolume&#34;&gt;手动管理一个地区性 PD PersistentVolume&lt;/h4&gt;
&lt;p&gt;动态管理使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-classes/#gce&#34;&gt;StorageClass for GCE PD&lt;/a&gt;.
在创建 PersistentVolume 之前，需要先创建 PD:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;gcloud compute disks create --size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;500GB my-data-disk
    --region us-central1
    --replica-zones us-central1-a,us-central1-b
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;示例 PersistentVolume spec:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;capacity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;400Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;gcePersistentDisk&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;pdName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-data-disk&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;nodeAffinity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;required&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelectorTerms&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;failure-domain.beta.kubernetes.io/zone&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
          - &lt;span style=&#34;color:#ae81ff&#34;&gt;us-central1-a&lt;/span&gt;
          - &lt;span style=&#34;color:#ae81ff&#34;&gt;us-central1-b&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
#### CSI Migration






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;



The CSI Migration feature for GCE PD, when enabled, shims all plugin operations
from the existing in-tree plugin to the `pd.csi.storage.gke.io` Container
Storage Interface (CSI) Driver. In order to use this feature, the [GCE PD CSI
Driver](https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver)
must be installed on the cluster and the `CSIMigration` and `CSIMigrationGCE`
Beta features must be enabled.
 --&gt;
&lt;h4 id=&#34;csi-迁移-4&#34;&gt;CSI 迁移&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;GCE PD 的 CSI 迁移特性，在启用时，会将来自已经存在的插件的插件操作转移到 &lt;code&gt;pd.csi.storage.gke.io&lt;/code&gt;  容器存储接口(CSI)驱动。 为了使用该特性， 需要在集群中先安装
&lt;a href=&#34;https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver&#34;&gt;GCE PD CSI
Driver&lt;/a&gt;
同时启用 &lt;code&gt;CSIMigration&lt;/code&gt; 和 &lt;code&gt;CSIMigrationGCE&lt;/code&gt; 两个 &lt;code&gt;beta&lt;/code&gt; 特性&lt;/p&gt;
&lt;!--
### gitRepo (deprecated) {#gitrepo}

&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; The gitRepo volume type is deprecated. To provision a container with a git repo, mount an &lt;a href=&#34;#emptydir&#34;&gt;EmptyDir&lt;/a&gt; into an InitContainer that clones the repo using git, then mount the &lt;a href=&#34;#emptydir&#34;&gt;EmptyDir&lt;/a&gt; into the Pod&amp;rsquo;s container.&lt;/div&gt;
&lt;/blockquote&gt;


A `gitRepo` volume is an example of what can be done as a volume plugin.  It
mounts an empty directory and clones a git repository into it for your Pod to
use.  In the future, such volumes may be moved to an even more decoupled model,
rather than extending the Kubernetes API for every such use case.

Here is an example of gitRepo volume:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: server
spec:
  containers:
  - image: nginx
    name: nginx
    volumeMounts:
    - mountPath: /mypath
      name: git-volume
  volumes:
  - name: git-volume
    gitRepo:
      repository: &#34;git@somewhere:me/my-git-repository.git&#34;
      revision: &#34;22f1d8406d464b0c0874075539c1f2e96c253775&#34;
```
 --&gt;
&lt;h3 id=&#34;gitrepo&#34;&gt;gitRepo (废弃)&lt;/h3&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; gitRepo 卷类型已经被废弃。 要管理一个包含 git 创建的容器，可以在初始化容器
(InitContainer) 中挂载一个 &lt;a href=&#34;#emptydir&#34;&gt;EmptyDir&lt;/a&gt; 使用 git 克隆这个仓库，
然后在需要使用的容器的 Pod 上挂载这个 &lt;a href=&#34;#emptydir&#34;&gt;EmptyDir&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;gitRepo&lt;/code&gt; 卷是一个能够使用卷插件的示例。 它挂载一个空目录然后在其中克隆一个
git 创建到其中，供 Pod 使用。 在未来，这些卷可能被改为更加松耦合的模式，
而不是每次在这些情况下都通过扩展 k8s  API 来实现。
以下为 gitRepo 卷示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;server&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/mypath&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;git-volume&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;git-volume&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;gitRepo&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;repository&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;git@somewhere:me/my-git-repository.git&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;revision&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;22f1d8406d464b0c0874075539c1f2e96c253775&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### glusterfs {#glusterfs}

A `glusterfs` volume allows a [Glusterfs](https://www.gluster.org) (an open
source networked filesystem) volume to be mounted into your Pod.  Unlike
`emptyDir`, which is erased when a Pod is removed, the contents of a
`glusterfs` volume are preserved and the volume is merely unmounted.  This
means that a glusterfs volume can be pre-populated with data, and that data can
be &#34;handed off&#34; between Pods.  GlusterFS can be mounted by multiple writers
simultaneously.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must have your own GlusterFS installation running before you can use it.&lt;/div&gt;
&lt;/blockquote&gt;


See the [GlusterFS example](https://github.com/kubernetes/examples/tree/master/volumes/glusterfs) for more details.
 --&gt;
&lt;h3 id=&#34;glusterfs&#34;&gt;glusterfs&lt;/h3&gt;
&lt;p&gt;使用 &lt;code&gt;glusterfs&lt;/code&gt; 卷可以将 &lt;a href=&#34;https://www.gluster.org&#34;&gt;Glusterfs&lt;/a&gt;
(一个开源的网络文件系统)卷挂载到 Pod 中。
与 &lt;code&gt;emptyDir&lt;/code&gt; 在 Pod 删除时清除数据不同， 在 &lt;code&gt;glusterfs&lt;/code&gt; 卷中的内存会在卸载后依然会保存
其中的内容。也就是说一个 &lt;code&gt;glusterfs&lt;/code&gt; 卷可以预先存在数据，其中的数据也可以在不同的 Pod 之间传递。
GlusterFS 还可以被同时多个挂载拥有写权限
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在使用之前必须要先安装运行自己的 GlusterFS&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;更多个信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/volumes/glusterfs&#34;&gt;GlusterFS 示例&lt;/a&gt;&lt;/p&gt;
&lt;!--
### hostPath {#hostpath}

A `hostPath` volume mounts a file or directory from the host node&#39;s filesystem
into your Pod. This is not something that most Pods will need, but it offers a
powerful escape hatch for some applications.

For example, some uses for a `hostPath` are:

* running a Container that needs access to Docker internals; use a `hostPath`
  of `/var/lib/docker`
* running cAdvisor in a Container; use a `hostPath` of `/sys`
* allowing a Pod to specify whether a given `hostPath` should exist prior to the
  Pod running, whether it should be created, and what it should exist as

In addition to the required `path` property, user can optionally specify a `type` for a `hostPath` volume.

The supported values for field `type` are:


| Value | Behavior |
|:------|:---------|
| | Empty string (default) is for backward compatibility, which means that no checks will be performed before mounting the hostPath volume. |
| `DirectoryOrCreate` | If nothing exists at the given path, an empty directory will be created there as needed with permission set to 0755, having the same group and ownership with Kubelet. |
| `Directory` | A directory must exist at the given path |
| `FileOrCreate` | If nothing exists at the given path, an empty file will be created there as needed with permission set to 0644, having the same group and ownership with Kubelet. |
| `File` | A file must exist at the given path |
| `Socket` | A UNIX socket must exist at the given path |
| `CharDevice` | A character device must exist at the given path |
| `BlockDevice` | A block device must exist at the given path |

Watch out when using this type of volume, because:

* Pods with identical configuration (such as created from a podTemplate) may
  behave differently on different nodes due to different files on the nodes
* when Kubernetes adds resource-aware scheduling, as is planned, it will not be
  able to account for resources used by a `hostPath`
* the files or directories created on the underlying hosts are only writable by root. You
  either need to run your process as root in a
  [privileged Container](/docs/tasks/configure-pod-container/security-context/) or modify the file
  permissions on the host to be able to write to a `hostPath` volume
 --&gt;
&lt;h3 id=&#34;hostpath&#34;&gt;hostPath&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;hostPath&lt;/code&gt; 卷可以将节点上的一个文件或目录挂载到 Pod 中. 这种方式不是大多数
Pod 所需要要的， 但也能为有些应用提供强力支持。&lt;/p&gt;
&lt;p&gt;For example, some uses for a &lt;code&gt;hostPath&lt;/code&gt; are:
例如以下为可以用到 &lt;code&gt;hostPath&lt;/code&gt; 的地方:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;运行一个需要访问 Docker 内部的应用，使用 &lt;code&gt;hostPath&lt;/code&gt; 挂载 &lt;code&gt;/var/lib/docker&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在容器中运行 &lt;code&gt;cAdvisor&lt;/code&gt;； 使用 &lt;code&gt;hostPath&lt;/code&gt; 挂载 &lt;code&gt;/sys&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;允许 Pod 指定某个 &lt;code&gt;hostPath&lt;/code&gt; 在 Pod 运行之前需要存在，这个目录是否需要创建，以什么方式存在(在说啥？)
In addition to the required &lt;code&gt;path&lt;/code&gt; property, user can optionally specify a &lt;code&gt;type&lt;/code&gt; for a &lt;code&gt;hostPath&lt;/code&gt; volume.
在需要的 &lt;code&gt;path&lt;/code&gt; 属性外， 用户还可以为 &lt;code&gt;hostPath&lt;/code&gt; 指定一个可选的 &lt;code&gt;type&lt;/code&gt; 属性。
The supported values for field &lt;code&gt;type&lt;/code&gt; are:
&lt;code&gt;type&lt;/code&gt; 字段支持的值有:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;值&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;行为&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;空字符器(默认)为了向后兼容，含义是在执行挂载 &lt;code&gt;hostPath&lt;/code&gt; 卷之前不执行任何检查&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;DirectoryOrCreate&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;如果指定路径不存在，则创建一个空目录，权限为 0755，组和所属关系与 kubelet 相同&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Directory&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;路径必须是一个已经存在的目录&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;FileOrCreate&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;如果指定路径不存在，则创建一个空文件，权限为 0644，组和所属关系与 kubelet 相同&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;File&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;路径必须是一个已经存在的文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Socket&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;路径必须是一个已经存在的 UNIX 套接字&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;CharDevice&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;路径必须是一个已经存在的 字符设备(character device)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;BlockDevice&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;路径必须是一个已经存在的 块设备(block device)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Watch out when using this type of volume, because:
在使用带类型的卷时需要小心，因为:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拥有相同配置(如由同一个 podTemplate 创建的) Pod 可以会因为不同节点上拥有的文件不同为表现不同行为&lt;/li&gt;
&lt;li&gt;根据设计，当 k8s 添加资源感知(resource-aware) 调度时，不会计算 &lt;code&gt;hostPath&lt;/code&gt; 使用的资源。&lt;/li&gt;
&lt;li&gt;在底层主机上创建的文件或目录只有 root 拥有写权限。 所以必须要在一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/security-context/&#34;&gt;提权容器&lt;/a&gt;
以 root 权限运行进程或在主机上修改文件权限以便可以让 &lt;code&gt;hostPath&lt;/code&gt; 卷为可读&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
#### Example Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /data
      # this field is optional
      type: Directory
```

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; It should be noted that the &lt;code&gt;FileOrCreate&lt;/code&gt; mode does not create the parent directory of the file. If the parent directory of the mounted file does not exist, the pod fails to start. To ensure that this mode works, you can try to mount directories and files separately, as shown below.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h4 id=&#34;示例-pod-1&#34;&gt;示例 Pod&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-pd&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/test-pd&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
      &lt;span style=&#34;color:#75715e&#34;&gt;# 主机上的目录&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/data&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# 该字段可选&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Directory&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 要注意 &lt;code&gt;FileOrCreate&lt;/code&gt; 模式时，如果文件的父目录不存在是不会自动创建的。 如果挂载文件的父目录不
存在， Pod 就会启动失败。 为了保证该模式正常工作，可以参考下面的方式，将目录和文件分开挂载。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
#### Example Pod FileOrCreate

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-webserver
spec:
  containers:
  - name: test-webserver
    image: k8s.gcr.io/test-webserver:latest
    volumeMounts:
    - mountPath: /var/local/aaa
      name: mydir
    - mountPath: /var/local/aaa/1.txt
      name: myfile
  volumes:
  - name: mydir
    hostPath:
      # Ensure the file directory is created.
      path: /var/local/aaa
      type: DirectoryOrCreate
  - name: myfile
    hostPath:
      path: /var/local/aaa/1.txt
      type: FileOrCreate
```
 --&gt;
&lt;h4 id=&#34;fileorcreate-示例-pod&#34;&gt;FileOrCreate 示例 Pod&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-webserver&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver:latest&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/local/aaa&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mydir&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/local/aaa/1.txt&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myfile&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mydir&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
      &lt;span style=&#34;color:#75715e&#34;&gt;# 确保文件所在的目录是存在的&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/local/aaa&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DirectoryOrCreate&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myfile&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/local/aaa/1.txt&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;FileOrCreate&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### iscsi {#iscsi}

An `iscsi` volume allows an existing iSCSI (SCSI over IP) volume to be mounted
into your Pod.  Unlike `emptyDir`, which is erased when a Pod is removed, the
contents of an `iscsi` volume are preserved and the volume is merely
unmounted.  This means that an iscsi volume can be pre-populated with data, and
that data can be &#34;handed off&#34; between Pods.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must have your own iSCSI server running with the volume created before you can use it.&lt;/div&gt;
&lt;/blockquote&gt;


A feature of iSCSI is that it can be mounted as read-only by multiple consumers
simultaneously.  This means that you can pre-populate a volume with your dataset
and then serve it in parallel from as many Pods as you need.  Unfortunately,
iSCSI volumes can only be mounted by a single consumer in read-write mode - no
simultaneous writers allowed.

See the [iSCSI example](https://github.com/kubernetes/examples/tree/master/volumes/iscsi) for more details.
 --&gt;
&lt;h3 id=&#34;iscsi&#34;&gt;iscsi&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;iscsi&lt;/code&gt; 卷可以将已经存在的 iSCSI (SCSI over IP) 卷挂载到 Pod 中。
与 &lt;code&gt;emptyDir&lt;/code&gt; 在 Pod 删除时清除数据不同， 在 &lt;code&gt;iscsi&lt;/code&gt; 卷中的内存会在卸载后依然会保存
其中的内容。也就是说一个 &lt;code&gt;iscsi&lt;/code&gt; 卷可以预先存在数据，其中的数据也可以在不同的 Pod 之间传递。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在创建和使用卷之前必须为先有运行的 iSCSI 服务。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;iSCSI&lt;/code&gt; 一个特性是可以以只读的方式同时挂载到多个消费者中。 这就代表着可以预先将数据集放入卷中
然后根据需要使用任意数量的 Pod 来提供访问服务。 不过 iSCSI 卷以读写方式挂载时只能有一个消费者，
不允许并行写。
更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/volumes/iscsi&#34;&gt;iSCSI 示例&lt;/a&gt;&lt;/p&gt;
&lt;!--
### local {#local}






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [stable]&lt;/code&gt;
&lt;/div&gt;



A `local` volume represents a mounted local storage device such as a disk,
partition or directory.

Local volumes can only be used as a statically created PersistentVolume. Dynamic
provisioning is not supported yet.

Compared to `hostPath` volumes, local volumes can be used in a durable and
portable manner without manually scheduling Pods to nodes, as the system is aware
of the volume&#39;s node constraints by looking at the node affinity on the PersistentVolume.

However, local volumes are still subject to the availability of the underlying
node and are not suitable for all applications. If a node becomes unhealthy,
then the local volume will also become inaccessible, and a Pod using it will not
be able to run. Applications using local volumes must be able to tolerate this
reduced availability, as well as potential data loss, depending on the
durability characteristics of the underlying disk.

The following is an example of PersistentVolume spec using a `local` volume and
`nodeAffinity`:

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 100Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /mnt/disks/ssd1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - example-node
```

PersistentVolume `nodeAffinity` is required when using local volumes. It enables
the Kubernetes scheduler to correctly schedule Pods using local volumes to the
correct node.

PersistentVolume `volumeMode` can be set to &#34;Block&#34; (instead of the default
value &#34;Filesystem&#34;) to expose the local volume as a raw block device.

When using local volumes, it is recommended to create a StorageClass with
`volumeBindingMode` set to `WaitForFirstConsumer`. See the
[example](/docs/concepts/storage/storage-classes/#local). Delaying volume binding ensures
that the PersistentVolumeClaim binding decision will also be evaluated with any
other node constraints the Pod may have, such as node resource requirements, node
selectors, Pod affinity, and Pod anti-affinity.

An external static provisioner can be run separately for improved management of
the local volume lifecycle. Note that this provisioner does not support dynamic
provisioning yet. For an example on how to run an external local provisioner,
see the [local volume provisioner user
guide](https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner).

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The local PersistentVolume requires manual cleanup and deletion by the
user if the external static provisioner is not used to manage the volume
lifecycle.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;local&#34;&gt;local&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;code&gt;local&lt;/code&gt; 卷代表一个挂载的本地存储，例如磁盘，分区或目录
&lt;code&gt;local&lt;/code&gt; 卷只能以 静态创建 &lt;code&gt;PersistentVolume&lt;/code&gt; 的方式使用。 目前还不支持动态管理&lt;/p&gt;
&lt;p&gt;Compared to &lt;code&gt;hostPath&lt;/code&gt; volumes, local volumes can be used in a durable and
portable manner without manually scheduling Pods to nodes, as the system is aware
of the volume&amp;rsquo;s node constraints by looking at the node affinity on the PersistentVolume.
与 &lt;code&gt;hostPath&lt;/code&gt; 相比， &lt;code&gt;local&lt;/code&gt; 卷可以通过手动在节点上管理实现持久的和可移植的， 然后根据
PersistentVolume 的节点亲和性来打开对应节点上的卷。&lt;/p&gt;
&lt;p&gt;但是， &lt;code&gt;local&lt;/code&gt; 卷依然受底层节点的可用性影响，不是对所有应用都适用。 如果有一个节点应得不健康，
这个节点上的 &lt;code&gt;local&lt;/code&gt; 也会变得不可用，使用这个卷的 Pod 也不能运行。 使用 &lt;code&gt;local&lt;/code&gt; 卷的这些应用必要
要能忍受这样可用性降低的情况，同时还因为底层磁盘的持久性特性而产生的潜在的数据丢失的风险&lt;/p&gt;
&lt;p&gt;以下示例中使用了 &lt;code&gt;local&lt;/code&gt;卷和 &lt;code&gt;nodeAffinity&lt;/code&gt; 的 &lt;code&gt;PersistentVolume&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;example-pv&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;capacity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;100Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Filesystem&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeReclaimPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Delete&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;local-storage&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;local&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/mnt/disks/ssd1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;nodeAffinity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;required&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelectorTerms&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/hostname&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
          - &lt;span style=&#34;color:#ae81ff&#34;&gt;example-node&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在使用 &lt;code&gt;local&lt;/code&gt; 卷时 PersistentVolume 需要使用 &lt;code&gt;nodeAffinity&lt;/code&gt;。 这让 k8s 可以正确地将
使用 &lt;code&gt;local&lt;/code&gt; 卷的 Pod 调度到正确的节点上。&lt;/p&gt;
&lt;p&gt;PersistentVolume 的 &lt;code&gt;volumeMode&lt;/code&gt; 可以被设置为 &amp;ldquo;Block&amp;rdquo; (而不是默认值 &amp;ldquo;Filesystem&amp;rdquo;)
将这个 &lt;code&gt;local&lt;/code&gt; 卷作为块设备&lt;/p&gt;
&lt;p&gt;当使用 &lt;code&gt;local&lt;/code&gt; 卷时， 推荐在创建 StorageClass 时将 &lt;code&gt;volumeBindingMode&lt;/code&gt; 设置为 &lt;code&gt;WaitForFirstConsumer&lt;/code&gt;。
可以见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes/#local&#34;&gt;示例&lt;/a&gt;.
卷延迟绑定确定 PersistentVolumeClaim 在作绑定决策时检查 Pod 可能有的其它的节点约束，如果节点
的资源需求，节点的选择器，Pod 的亲和性和反亲和性。&lt;/p&gt;
&lt;p&gt;可以在外部独立运行一个静态提供者来改善 &lt;code&gt;local&lt;/code&gt; 卷生命周期管理。 要注意这个提供者目前还不支持动态管理。
运行外部提供者的示例见
&lt;a href=&#34;https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner&#34;&gt;local 卷提供者用户指南&lt;/a&gt;.
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果外部的静态提供都没有用来管理 &lt;code&gt;local&lt;/code&gt; PersistentVolume 的生命周期， 需要手动清理和删除。&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;!--
### nfs {#nfs}

An `nfs` volume allows an existing NFS (Network File System) share to be
mounted into your Pod. Unlike `emptyDir`, which is erased when a Pod is
removed, the contents of an `nfs` volume are preserved and the volume is merely
unmounted.  This means that an NFS volume can be pre-populated with data, and
that data can be &#34;handed off&#34; between Pods.  NFS can be mounted by multiple
writers simultaneously.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must have your own NFS server running with the share exported before you can use it.&lt;/div&gt;
&lt;/blockquote&gt;


See the [NFS example](https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs) for more details.
 --&gt;
&lt;h3 id=&#34;nfs&#34;&gt;nfs&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;nfs&lt;/code&gt; 卷可以将现有的 NFS (网络文件系统)挂载到 Pod 中。 与 &lt;code&gt;emptyDir&lt;/code&gt; 在 Pod 删除时清除数据不同，
在 &lt;code&gt;iscsi&lt;/code&gt; 卷中的内存会在卸载后依然会保存 其中的内容。也就是说一个 &lt;code&gt;iscsi&lt;/code&gt; 卷可以预先存在数据，
其中的数据也可以在不同的 Pod 之间传递。 NFS 可以以读写方式同时挂载到多个 Pod 中。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在使用之前必须要先搭建并运行好 NFS 服务器&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;更多详情见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs&#34;&gt;NFS 示例&lt;/a&gt;&lt;/p&gt;
&lt;!--
### persistentVolumeClaim {#persistentvolumeclaim}

A `persistentVolumeClaim` volume is used to mount a
[PersistentVolume](/docs/concepts/storage/persistent-volumes/) into a Pod. PersistentVolumeClaims
are a way for users to &#34;claim&#34; durable storage (such as a GCE PersistentDisk or an
iSCSI volume) without knowing the details of the particular cloud environment.

See the [PersistentVolumes example](/docs/concepts/storage/persistent-volumes/) for more
details.
 --&gt;
&lt;h3 id=&#34;persistentvolumeclaim&#34;&gt;persistentVolumeClaim&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;persistentVolumeClaim&lt;/code&gt; 卷用于将
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/&#34;&gt;PersistentVolume&lt;/a&gt; 挂载到 Pod 中。
&lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 是用户&amp;quot;声明&amp;quot;持久存储(如GCE PersistentDisk 或 iSCSI 卷)时不需要
知道具体云环境细节的一种方式
更多信息见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/&#34;&gt;PersistentVolumes 示例&lt;/a&gt;&lt;/p&gt;
&lt;!--
### projected {#projected}

A `projected` volume maps several existing volume sources into the same directory.

Currently, the following types of volume sources can be projected:

- [`secret`](#secret)
- [`downwardAPI`](#downwardapi)
- [`configMap`](#configmap)
- `serviceAccountToken`

All sources are required to be in the same namespace as the Pod. For more details,
see the [all-in-one volume design document](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/all-in-one-volume.md).

The projection of service account tokens is a feature introduced in Kubernetes
1.11 and promoted to Beta in 1.12.
To enable this feature on 1.11, you need to explicitly set the `TokenRequestProjection`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) to
True.
 --&gt;
&lt;h3 id=&#34;projected&#34;&gt;projected&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;projected&lt;/code&gt; 卷可以将几种现有的不同的卷作为源映射到同一个目录中&lt;/p&gt;
&lt;p&gt;目前， 支持以下几种卷作为 &lt;code&gt;projected&lt;/code&gt; 的源&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#secret&#34;&gt;&lt;code&gt;secret&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#downwardapi&#34;&gt;&lt;code&gt;downwardAPI&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#configmap&#34;&gt;&lt;code&gt;configMap&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;serviceAccountToken&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有的源都需要要与 Pod 在同一个命令空间。 更多细节见
&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/all-in-one-volume.md&#34;&gt;all-in-one 卷设计文档&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;对服务账号凭据(Service Account Token)的投射支持是从 k8s 1.11 加入的， v1.12 提升为 beta 版本。
在 v1.11 中要启用该特性，需要显示地将
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;
&lt;code&gt;TokenRequestProjection&lt;/code&gt; 设置为 &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
&lt;!--
#### Example Pod with a secret, a downward API, and a configmap.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts:
    - name: all-in-one
      mountPath: &#34;/projected-volume&#34;
      readOnly: true
  volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: mysecret
          items:
            - key: username
              path: my-group/my-username
      - downwardAPI:
          items:
            - path: &#34;labels&#34;
              fieldRef:
                fieldPath: metadata.labels
            - path: &#34;cpu_limit&#34;
              resourceFieldRef:
                containerName: container-test
                resource: limits.cpu
      - configMap:
          name: myconfigmap
          items:
            - key: config
              path: my-group/my-config
```
 --&gt;
&lt;h4 id=&#34;示例-pod-中包含-secret-downwardapi-configmap&#34;&gt;示例： Pod 中包含 Secret, DownwardAPI, Configmap.&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;volume-test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;container-test&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;all-in-one&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/projected-volume&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;all-in-one&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;projected&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;sources&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysecret&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;items&lt;/span&gt;:
            - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;username&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-group/my-username&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;downwardAPI&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;items&lt;/span&gt;:
            - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;labels&amp;#34;&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;fieldRef&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;fieldPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;metadata.labels&lt;/span&gt;
            - &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;cpu_limit&amp;#34;&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;resourceFieldRef&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;containerName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;container-test&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;resource&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;limits.cpu&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;configMap&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myconfigmap&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;items&lt;/span&gt;:
            - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;config&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-group/my-config&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
#### Example Pod with multiple secrets with a non-default permission mode set.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts:
    - name: all-in-one
      mountPath: &#34;/projected-volume&#34;
      readOnly: true
  volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: mysecret
          items:
            - key: username
              path: my-group/my-username
      - secret:
          name: mysecret2
          items:
            - key: password
              path: my-group/my-password
              mode: 511
```

Each projected volume source is listed in the spec under `sources`. The
parameters are nearly the same with two exceptions:

* For secrets, the `secretName` field has been changed to `name` to be consistent
  with ConfigMap naming.
* The `defaultMode` can only be specified at the projected level and not for each
  volume source. However, as illustrated above, you can explicitly set the `mode`
  for each individual projection.

When the `TokenRequestProjection` feature is enabled, you can inject the token
for the current [service account](/docs/reference/access-authn-authz/authentication/#service-account-tokens)
into a Pod at a specified path. Below is an example:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sa-token-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts:
    - name: token-vol
      mountPath: &#34;/service-account&#34;
      readOnly: true
  volumes:
  - name: token-vol
    projected:
      sources:
      - serviceAccountToken:
          audience: api
          expirationSeconds: 3600
          path: token
```

The example Pod has a projected volume containing the injected service account
token. This token can be used by Pod containers to access the Kubernetes API
server, for example. The `audience` field contains the intended audience of the
token. A recipient of the token must identify itself with an identifier specified
in the audience of the token, and otherwise should reject the token. This field
is optional and it defaults to the identifier of the API server.

The `expirationSeconds` is the expected duration of validity of the service account
token. It defaults to 1 hour and must be at least 10 minutes (600 seconds). An administrator
can also limit its maximum value by specifying the `--service-account-max-token-expiration`
option for the API server. The `path` field specifies a relative path to the mount point
of the projected volume.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A Container using a projected volume source as a &lt;a href=&#34;#using-subpath&#34;&gt;subPath&lt;/a&gt; volume mount will not
receive updates for those volume sources.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h4 id=&#34;示例-pod-包含多有设置非默认权限模式的-secret&#34;&gt;示例： Pod 包含多有设置非默认权限模式的 Secret&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;volume-test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;container-test&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;all-in-one&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/projected-volume&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;all-in-one&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;projected&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;sources&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysecret&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;items&lt;/span&gt;:
            - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;username&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-group/my-username&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;secret&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysecret2&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;items&lt;/span&gt;:
            - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;password&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-group/my-password&lt;/span&gt;
              &lt;span style=&#34;color:#f92672&#34;&gt;mode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;511&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;每一个投射(&lt;code&gt;projected&lt;/code&gt;)卷的源都在配置 &lt;code&gt;sources&lt;/code&gt; 下。 除了以下两个例外，其它的参数都基本相同:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于 Secret, &lt;code&gt;secretName&lt;/code&gt; 字段变为 &lt;code&gt;name&lt;/code&gt; 以便与 ConfigMap 的命名一致&lt;/li&gt;
&lt;li&gt;&lt;code&gt;defaultMode&lt;/code&gt; 只能在 projected 级别上设置不能在每个卷源上设置。 但是，就像上面例子中所示，
可以为每一个源设置各自独立的 &lt;code&gt;mode&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当 &lt;code&gt;TokenRequestProjection&lt;/code&gt; 特性开启时，可能将当前[服务账号(ServiceAccount)]的凭据(token)
注入到 Pod 的指定路径，下面为示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;sa-token-test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;container-test&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;token-vol&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/service-account&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;token-vol&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;projected&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;sources&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;serviceAccountToken&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;audience&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;api&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;expirationSeconds&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3600&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;token&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上面的例子中， &lt;code&gt;projected&lt;/code&gt; 卷中包含了被注入的 服务账号凭据。 这个凭据可以被 Pod 的容器用来
访问 k8s API 服务. &lt;code&gt;audience&lt;/code&gt; 字段中包含的是凭据的目标用户， 凭据的的接收者必须要验证凭据中
提供的 &lt;code&gt;audience&lt;/code&gt; 与其本身的标识进行验证，否则就应该拒绝这个凭据。 这个字段为可选，默认的标识符为 API 服务&lt;/p&gt;
&lt;p&gt;&lt;code&gt;expirationSeconds&lt;/code&gt; 为服务账号凭据的有效时长。 默认为1小时，最少为 10 分钟(即 600 秒)。
管理员也可能通过API 服务的 &lt;code&gt;--service-account-max-token-expiration&lt;/code&gt; 选项限制其最大值。
&lt;code&gt;path&lt;/code&gt; 字段指定 &lt;code&gt;projected&lt;/code&gt; 卷挂载点的相对路径。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果一个容器以将一个 &lt;code&gt;projected&lt;/code&gt; 卷以 &lt;a href=&#34;#using-subpath&#34;&gt;subPath&lt;/a&gt; 方式挂载，则不会接收到
卷源的更新&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;!--
### portworxVolume {#portworxvolume}

A `portworxVolume` is an elastic block storage layer that runs hyperconverged with
Kubernetes. [Portworx](https://portworx.com/use-case/kubernetes-storage/) fingerprints storage in a server, tiers based on capabilities,
and aggregates capacity across multiple servers. Portworx runs in-guest in virtual machines or on bare metal Linux nodes.

A `portworxVolume` can be dynamically created through Kubernetes or it can also
be pre-provisioned and referenced inside a Kubernetes Pod.
Here is an example Pod referencing a pre-provisioned PortworxVolume:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-portworx-volume-pod
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /mnt
      name: pxvol
  volumes:
  - name: pxvol
    # This Portworx volume must already exist.
    portworxVolume:
      volumeID: &#34;pxvol&#34;
      fsType: &#34;&lt;fs-type&gt;&#34;
```

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; Make sure you have an existing PortworxVolume with name &lt;code&gt;pxvol&lt;/code&gt;
before using it in the Pod.&lt;/div&gt;
&lt;/blockquote&gt;


More details and examples can be found [here](https://github.com/kubernetes/examples/tree/master/staging/volumes/portworx/README.md).
 --&gt;
&lt;h3 id=&#34;portworxvolume&#34;&gt;portworxVolume&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;portworxVolume&lt;/code&gt; 是一个弹性块存储层(elastic block storage layer), 它通过 k8s 运行
&lt;code&gt;hyperconverged&lt;/code&gt;。
&lt;a href=&#34;https://portworx.com/use-case/kubernetes-storage/&#34;&gt;Portworx&lt;/a&gt; (这句有点绕)
Portworx 可以运行在虚拟机或裸金属的 Linux 节点中。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;portworxVolume&lt;/code&gt; 可以通过 k8s 动态创建或预先创建然后在一个 k8s 的 Pod 中引用。
以下为在 Pod 中使用一个预先创建好的 &lt;code&gt;PortworxVolume&lt;/code&gt; 的示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-portworx-volume-pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/mnt&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pxvol&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pxvol&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# 这个 Portworx 卷必须要已经存在.&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;portworxVolume&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeID&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pxvol&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;lt;fs-type&amp;gt;&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在 Pod 中使用之前需要已经存在一个名叫 &lt;code&gt;pxvol&lt;/code&gt; 的 &lt;code&gt;PortworxVolume&lt;/code&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;更多详情及示例见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/portworx/README.md&#34;&gt;这里&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### quobyte {#quobyte}

A `quobyte` volume allows an existing [Quobyte](https://www.quobyte.com) volume to
be mounted into your Pod.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must have your own Quobyte setup running with the volumes
created before you can use it.&lt;/div&gt;
&lt;/blockquote&gt;


Quobyte supports the &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;Container Storage Interface&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt;.
CSI is the recommended plugin to use Quobyte volumes inside Kubernetes. Quobyte&#39;s
GitHub project has [instructions](https://github.com/quobyte/quobyte-csi#quobyte-csi) for deploying Quobyte using CSI, along with examples.
 --&gt;
&lt;h3 id=&#34;quobyte&#34;&gt;quobyte&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;quobyte&lt;/code&gt; 可以将已经存在的 &lt;a href=&#34;https://www.quobyte.com&#34;&gt;Quobyte&lt;/a&gt; 卷挂载到 Pod 中。
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在使用之前必须要配置好 Quobyte 并创建使用的卷&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;Quobyte 支持 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;容器存储接口 (CSI)&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt;.
CSI 是在 k8s 中使用 Quobyte 卷的推荐插件。 Quobyte 的 GitHub 项目中有介绍使用 CSI 部署
Quobyte 的介绍&lt;a href=&#34;https://github.com/quobyte/quobyte-csi#quobyte-csi&#34;&gt;介绍&lt;/a&gt;
和示例。&lt;/p&gt;
&lt;!--
### rbd {#rbd}

An `rbd` volume allows a
[Rados Block Device](https://ceph.com/docs/master/rbd/rbd/) volume to be mounted into your
Pod.  Unlike `emptyDir`, which is erased when a Pod is removed, the contents of
a `rbd` volume are preserved and the volume is merely unmounted.  This
means that a RBD volume can be pre-populated with data, and that data can
be &#34;handed off&#34; between Pods.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must have your own Ceph installation running before you can use RBD.&lt;/div&gt;
&lt;/blockquote&gt;


A feature of RBD is that it can be mounted as read-only by multiple consumers
simultaneously.  This means that you can pre-populate a volume with your dataset
and then serve it in parallel from as many Pods as you need.  Unfortunately,
RBD volumes can only be mounted by a single consumer in read-write mode - no
simultaneous writers allowed.

See the [RBD example](https://github.com/kubernetes/examples/tree/master/volumes/rbd) for more details.
 --&gt;
&lt;h3 id=&#34;rbd&#34;&gt;rbd&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;rbd&lt;/code&gt; 卷可以让 &lt;a href=&#34;https://ceph.com/docs/master/rbd/rbd/&#34;&gt;Rados 块设备&lt;/a&gt;卷挂载到 Pod 中。
与 &lt;code&gt;emptyDir&lt;/code&gt; 在 Pod 删除时清除数据不同， 在 &lt;code&gt;rbd&lt;/code&gt; 卷中的内存会在卸载后依然会保存
其中的内容。也就是说一个 &lt;code&gt;rbd&lt;/code&gt; 卷可以预先存在数据，其中的数据也可以在不同的 Pod 之间传递。
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在使用 RBD 之前需要先配置运行 Ceph&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;&lt;code&gt;RBD&lt;/code&gt; 一个特性是可以以只读的方式同时挂载到多个消费者中。 这就代表着可以预先将数据集放入卷中
然后根据需要使用任意数量的 Pod 来提供访问服务。 不过 &lt;code&gt;RBD&lt;/code&gt; 卷以读写方式挂载时只能有一个消费者，
不允许并行写。
更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/volumes/rbd&#34;&gt;RBD 示例&lt;/a&gt;&lt;/p&gt;
&lt;!--
### scaleIO {#scaleio}

ScaleIO is a software-based storage platform that can use existing hardware to
create clusters of scalable shared block networked storage. The `scaleIO` volume
plugin allows deployed Pods to access existing ScaleIO
volumes (or it can dynamically provision new volumes for persistent volume claims, see
[ScaleIO Persistent Volumes](/docs/concepts/storage/persistent-volumes/#scaleio)).

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must have an existing ScaleIO cluster already setup and
running with the volumes created before you can use them.&lt;/div&gt;
&lt;/blockquote&gt;


The following is an example of Pod configuration with ScaleIO:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-0
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: pod-0
    volumeMounts:
    - mountPath: /test-pd
      name: vol-0
  volumes:
  - name: vol-0
    scaleIO:
      gateway: https://localhost:443/api
      system: scaleio
      protectionDomain: sd0
      storagePool: sp1
      volumeName: vol-0
      secretRef:
        name: sio-secret
      fsType: xfs
```

For further detail, please see the [ScaleIO examples](https://github.com/kubernetes/examples/tree/master/staging/volumes/scaleio).
 --&gt;
&lt;h3 id=&#34;scaleio&#34;&gt;scaleIO&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;ScaleIO&lt;/code&gt; 是一个基于软件的存储平台，它可以使用已经存在的硬件来创建可扩展的共享块网络存储集群。
&lt;code&gt;scaleIO&lt;/code&gt; 卷插件让 Pod 可以使用 已经存在的 ScaleIO 卷(也可以为 持久化卷声明(PersistentVolumeClaim)
动态地创建新的卷， 见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/#scaleio&#34;&gt;ScaleIO 持久化卷&lt;/a&gt;)&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在使用 &lt;code&gt;ScaleIO&lt;/code&gt; 卷之前，需要先部署配置运行 ScaleIO 集群。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;以下为一个使用 ScaleIO 的 Pod 配置示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod-0&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod-0&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/test-pd&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vol-0&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vol-0&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;scaleIO&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;gateway&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://localhost:443/api&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;system&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;scaleio&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;protectionDomain&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;sd0&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;storagePool&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;sp1&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vol-0&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;secretRef&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;sio-secret&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;xfs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/scaleio&#34;&gt;ScaleIO 示例&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### secret {#secret}

A `secret` volume is used to pass sensitive information, such as passwords, to
Pods.  You can store secrets in the Kubernetes API and mount them as files for
use by Pods without coupling to Kubernetes directly.  `secret` volumes are
backed by tmpfs (a RAM-backed filesystem) so they are never written to
non-volatile storage.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must create a secret in the Kubernetes API before you can use it.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A Container using a Secret as a &lt;a href=&#34;#using-subpath&#34;&gt;subPath&lt;/a&gt; volume mount will not
receive Secret updates.&lt;/div&gt;
&lt;/blockquote&gt;


Secrets are described in more detail [here](/docs/concepts/configuration/secret/).
 --&gt;
&lt;h3 id=&#34;secret&#34;&gt;secret&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;secret&lt;/code&gt; 卷用于将如密码这样的敏感数据传入 Pod 中。 用户可以将 Secret 存在 k8s API 中，然后
将它们以文件的方式挂载到 Pod 中而不需要直接与 k8s 耦合在一起。 &lt;code&gt;secret&lt;/code&gt; 卷基于 tmpfs(
一个基于内存的文件系统)，所以这给永远不会写入持久性存储。
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在使用之前需要先在 k8s API 中先创建相应的 Secret&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 一个以 &lt;a href=&#34;#using-subpath&#34;&gt;subPath&lt;/a&gt; 挂载 Secret 的容器不会接收到 Secret 的更新。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;更多关于 Secret 的信息见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#34;&gt;这里&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### storageOS {#storageos}

A `storageos` volume allows an existing [StorageOS](https://www.storageos.com)
volume to be mounted into your Pod.

StorageOS runs as a Container within your Kubernetes environment, making local
or attached storage accessible from any node within the Kubernetes cluster.
Data can be replicated to protect against node failure. Thin provisioning and
compression can improve utilization and reduce cost.

At its core, StorageOS provides block storage to Containers, accessible via a file system.

The StorageOS Container requires 64-bit Linux and has no additional dependencies.
A free developer license is available.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must run the StorageOS Container on each node that wants to
access StorageOS volumes or that will contribute storage capacity to the pool.
For installation instructions, consult the
&lt;a href=&#34;https://docs.storageos.com&#34;&gt;StorageOS documentation&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;


```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: redis
    role: master
  name: test-storageos-redis
spec:
  containers:
    - name: master
      image: kubernetes/redis:v1
      env:
        - name: MASTER
          value: &#34;true&#34;
      ports:
        - containerPort: 6379
      volumeMounts:
        - mountPath: /redis-master-data
          name: redis-data
  volumes:
    - name: redis-data
      storageos:
        # The `redis-vol01` volume must already exist within StorageOS in the `default` namespace.
        volumeName: redis-vol01
        fsType: ext4
```

For more information including Dynamic Provisioning and Persistent Volume Claims, please see the
[StorageOS examples](https://github.com/kubernetes/examples/blob/master/volumes/storageos).
 --&gt;
&lt;h3 id=&#34;storageos&#34;&gt;storageOS&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;storageos&lt;/code&gt; 卷可以将现有的 &lt;a href=&#34;https://www.storageos.com&#34;&gt;StorageOS&lt;/a&gt; 卷挂载到 Pod 中。&lt;/p&gt;
&lt;p&gt;StorageOS 以容器方式运行在 k8s 环境中，可以将集群中任意节点的本地存储或附加存储变为可访问。
数据可以被复制以提高可用性。瘦供给或压缩可以改善利用率并减少开销。
StorageOS 核心是向容器提供块存储，可以通过文件系统访问。
StorageOS 容器需要 64 位 Linux 外不需要其它额外的信赖。
有一个免费的开发者许可可用。
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 需要在想让 StorageOS 访问的节点上运行 StorageOS 容器这样可以将它的存储容量加到池中。
更新安装指导见 &lt;a href=&#34;https://docs.storageos.com&#34;&gt;StorageOS 文档&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;role&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;master&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-storageos-redis&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;master&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes/redis:v1&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MASTER&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;6379&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/redis-master-data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis-data&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis-data&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;storageos&lt;/span&gt;:
        &lt;span style=&#34;color:#75715e&#34;&gt;# The `redis-vol01` volume must already exist within StorageOS in the `default` namespace.&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;redis-vol01&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;更多关于动态管理和 PersistentVolumeClaim 的信息见
&lt;a href=&#34;https://github.com/kubernetes/examples/blob/master/volumes/storageos&#34;&gt;StorageOS 示例&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### vsphereVolume {#vspherevolume}

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Prerequisite: Kubernetes with vSphere Cloud Provider configured. For cloudprovider
configuration please refer &lt;a href=&#34;https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/&#34;&gt;vSphere getting started guide&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;


A `vsphereVolume` is used to mount a vSphere VMDK Volume into your Pod.  The contents
of a volume are preserved when it is unmounted. It supports both VMFS and VSAN datastore.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; You must create VMDK using one of the following methods before using with Pod.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;vspherevolume&#34;&gt;vsphereVolume&lt;/h3&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 前置条件: k8s 需要配置好 vSphere 云提供商。 cloudprovider 配置见
&lt;a href=&#34;https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/&#34;&gt;vSphere 上手指导&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;vsphereVolume&lt;/code&gt; 用于将  vSphere VMDK 卷挂载到 Pod 中。 卷中的数据在卸载后依然存在。
支持 VMFS 和 VSAN 数据存储&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 需要使用以下方式中的其中一种创建 VMDK 后才能在 Pod 中使用。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
#### Creating a VMDK volume

Choose one of the following methods to create a VMDK.

&lt;div id=&#34;tabs_volumes&#34;&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;#tabs_volumes-0&#34;&gt;Create using vmkfstools&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#tabs_volumes-1&#34;&gt;Create using vmware-vdiskmanager&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div id=&#34;tabs_volumes-0&#34;&gt;&lt;p&gt;First ssh into ESX, then use the following command to create a VMDK:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;vmkfstools -c 2G /vmfs/volumes/DatastoreName/volumes/myDisk.vmdk
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&#34;tabs_volumes-1&#34;&gt;&lt;p&gt;Use the following command to create a VMDK:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;vmware-vdiskmanager -c -t &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; -s 40GB -a lsilogic myDisk.vmdk
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;script&gt;$(function(){$(&#34;#tabs_volumes&#34;).tabs();});&lt;/script&gt;
 --&gt;
&lt;h4 id=&#34;创建-vmdk-卷&#34;&gt;创建 VMDK 卷&lt;/h4&gt;
&lt;p&gt;选择以下方式中的一种创建 VMDK&lt;/p&gt;
&lt;div id=&#34;tabs_volumes&#34;&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;#tabs_volumes-0&#34;&gt;Create using vmkfstools&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;#tabs_volumes-1&#34;&gt;Create using vmware-vdiskmanager&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div id=&#34;tabs_volumes-0&#34;&gt;&lt;p&gt;先 ssh 到 ESX 中，然后使用用下面的命令创建一个 VMDK&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;vmkfstools -c 2G /vmfs/volumes/DatastoreName/volumes/myDisk.vmdk
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;div id=&#34;tabs_volumes-1&#34;&gt;&lt;p&gt;使用下面的命令创建一个 VMDK&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;vmware-vdiskmanager -c -t &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; -s 40GB -a lsilogic myDisk.vmdk
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;script&gt;$(function(){$(&#34;#tabs_volumes&#34;).tabs();});&lt;/script&gt;
&lt;!--
#### vSphere VMDK Example configuration

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-vmdk
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-vmdk
      name: test-volume
  volumes:
  - name: test-volume
    # This VMDK volume must already exist.
    vsphereVolume:
      volumePath: &#34;[DatastoreName] volumes/myDisk&#34;
      fsType: ext4
```
 --&gt;
&lt;h4 id=&#34;vsphere-vmdk-配置示例&#34;&gt;vSphere VMDK 配置示例&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-vmdk&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/test-webserver&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/test-vmdk&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-volume&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# 这个 VMDK 必须要已经存在.&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;vsphereVolume&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;volumePath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;[DatastoreName] volumes/myDisk&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;更多信息见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere&#34;&gt;这里&lt;/a&gt;.&lt;/p&gt;
&lt;!--
#### CSI migration






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;



The CSI Migration feature for vsphereVolume, when enabled, shims all plugin operations
from the existing in-tree plugin to the `csi.vsphere.vmware.com` &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;CSI&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt; driver. In order to use this feature, the [vSphere CSI
Driver](https://github.com/kubernetes-sigs/vsphere-csi-driver)
must be installed on the cluster and the `CSIMigration` and `CSIMigrationvSphere`
[feature gates](/docs/reference/command-line-tools-reference/feature-gates/) must be enabled.

This also requires minimum vSphere vCenter/ESXi Version to be 7.0u1 and minimum HW Version to be VM version 15.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;The following StorageClass parameters from the built-in vsphereVolume plugin are not supported by the vSphere CSI driver:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;diskformat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hostfailurestotolerate&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;forceprovisioning&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cachereservation&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;diskstripes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;objectspacereservation&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;iopslimit&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Existing volumes created using these parameters will be migrated to the vSphere CSI driver, but new volumes created by the vSphere CSI driver will not be honoring these parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h4 id=&#34;csi-migration-5&#34;&gt;CSI 迁移&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;的 CSI 迁移特性，在启用时，会将来自已经存在的插件的插件操作转移到
&lt;code&gt;csi.vsphere.vmware.com&lt;/code&gt; &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;CSI&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt;(CSI)驱动。
为了使用该特性， 需要在集群中先安装
&lt;a href=&#34;https://github.com/kubernetes-sigs/vsphere-csi-driver&#34;&gt;vSphere CSI Driver&lt;/a&gt;
同时在
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;
启用 &lt;code&gt;CSIMigration&lt;/code&gt; 和 &lt;code&gt;CSIMigrationAzureFile&lt;/code&gt; 两个 &lt;code&gt;Alpha&lt;/code&gt; 特性
This also requires minimum vSphere vCenter/ESXi Version to be 7.0u1 and minimum HW Version to be VM version 15.
同时还需要 vSphere vCenter/ESXi v7.0u1+ 和 VM v15+
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;以下来自 vsphereVolume 插件内置的 StorageClass 参数不受 vSphere CSI 驱动支持:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;diskformat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hostfailurestotolerate&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;forceprovisioning&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cachereservation&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;diskstripes&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;objectspacereservation&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;iopslimit&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;已经存在使用这些参数创建的卷会被迁移到 vSphere CSI 驱动， 但是使用 vSphere CSI 驱动创建新
的卷时就不能再使用这些参数了&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;!--
#### CSI Migration Complete





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;



To turn off the vsphereVolume plugin from being loaded by controller manager and kubelet, you need to set this feature flag to true. This requires `csi.vsphere.vmware.com` &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;CSI&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt; driver being installed on all worker nodes.
 --&gt;
&lt;h4 id=&#34;csi-migration-complete&#34;&gt;CSI Migration Complete&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;为了关闭由控制器管理器和 kubelet 加载的 vsphereVolume 插件， 需要将这具功能特性标记为 true
同时还需要在每个节点上安装 &lt;code&gt;csi.vsphere.vmware.com&lt;/code&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;容器存储接口 (CSI)&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt;
驱动&lt;/p&gt;
&lt;!--
## Using subPath

Sometimes, it is useful to share one volume for multiple uses in a single Pod. The `volumeMounts.subPath`
property can be used to specify a sub-path inside the referenced volume instead of its root.

Here is an example of a Pod with a LAMP stack (Linux Apache Mysql PHP) using a single, shared volume.
The HTML contents are mapped to its `html` folder, and the databases will be stored in its `mysql` folder:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-lamp-site
spec:
    containers:
    - name: mysql
      image: mysql
      env:
      - name: MYSQL_ROOT_PASSWORD
        value: &#34;rootpasswd&#34;
      volumeMounts:
      - mountPath: /var/lib/mysql
        name: site-data
        subPath: mysql
    - name: php
      image: php:7.0-apache
      volumeMounts:
      - mountPath: /var/www/html
        name: site-data
        subPath: html
    volumes:
    - name: site-data
      persistentVolumeClaim:
        claimName: my-lamp-site-data
```
 --&gt;
&lt;h2 id=&#34;使用-subpath&#34;&gt;使用 subPath&lt;/h2&gt;
&lt;p&gt;有时，在同一个 Pod 中的多个用户分享同一个卷是相当有用的。 &lt;code&gt;volumeMounts.subPath&lt;/code&gt; 属性可以用来
在引用卷时不使用其根路径而是指定子路径&lt;/p&gt;
&lt;p&gt;以下示例 Pod 为一个使用一个共享卷的 LAMP (Linux Apache Mysql PHP) 栈。
HTML 内容映射到 &lt;code&gt;html&lt;/code&gt; 目录， 数据库会使用 &lt;code&gt;mysql&lt;/code&gt; 目录。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-lamp-site&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;MYSQL_ROOT_PASSWORD&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;value&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rootpasswd&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/lib/mysql&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;site-data&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;subPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mysql&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;php&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;php:7.0-apache&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/www/html&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;site-data&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;subPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;html&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;site-data&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaim&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;claimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-lamp-site-data&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Using subPath with expanded environment variables






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [stable]&lt;/code&gt;
&lt;/div&gt;




Use the `subPathExpr` field to construct `subPath` directory names from Downward API environment variables.
The `subPath` and `subPathExpr` properties are mutually exclusive.

In this example, a Pod uses `subPathExpr` to create a directory `pod1` within the hostPath volume `/var/log/pods`, using the pod name from the Downward API.  The host directory `/var/log/pods/pod1` is mounted at `/logs` in the container.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
  - name: container1
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    image: busybox
    command: [ &#34;sh&#34;, &#34;-c&#34;, &#34;while [ true ]; do echo &#39;Hello&#39;; sleep 10; done | tee -a /logs/hello.txt&#34; ]
    volumeMounts:
    - name: workdir1
      mountPath: /logs
      subPathExpr: $(POD_NAME)
  restartPolicy: Never
  volumes:
  - name: workdir1
    hostPath:
      path: /var/log/pods
```
 --&gt;
&lt;h3 id=&#34;在-subpath-配置上使用环境变量&#34;&gt;在 subPath 配置上使用环境变量&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;使用 &lt;code&gt;subPathExpr&lt;/code&gt; 字段来通过 Downward API 的环境变量构建 &lt;code&gt;subPath&lt;/code&gt; 的名称。
&lt;code&gt;subPath&lt;/code&gt; 和 &lt;code&gt;subPathExpr&lt;/code&gt; 相互独立。&lt;/p&gt;
&lt;p&gt;在这个例子中， 一个 Pod 使用 &lt;code&gt;subPathExpr&lt;/code&gt; 创建一个 &lt;code&gt;pod1&lt;/code&gt; 目录在 hostPath 卷 &lt;code&gt;/var/log/pods&lt;/code&gt; 中，
使用来自 Downward API 的 Pod 名称。 主机目录 &lt;code&gt;/var/log/pods/pod1&lt;/code&gt; 挂载到容器中的 &lt;code&gt;/logs&lt;/code&gt; 上。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;container1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;env&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;POD_NAME&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;valueFrom&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;fieldRef&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;fieldPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;metadata.name&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sh&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;while [ true ]; do echo &amp;#39;Hello&amp;#39;; sleep 10; done | tee -a /logs/hello.txt&amp;#34;&lt;/span&gt; ]
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;workdir1&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/logs&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;subPathExpr&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;$(POD_NAME)&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;workdir1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/log/pods&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Resources

The storage media (Disk, SSD, etc.) of an `emptyDir` volume is determined by the
medium of the filesystem holding the kubelet root dir (typically
`/var/lib/kubelet`).  There is no limit on how much space an `emptyDir` or
`hostPath` volume can consume, and no isolation between Containers or between
Pods.

In the future, we expect that `emptyDir` and `hostPath` volumes will be able to
request a certain amount of space using a [resource](/docs/concepts/configuration/manage-resources-containers/)
specification, and to select the type of media to use, for clusters that have
several media types.
 --&gt;
&lt;h2 id=&#34;资源&#34;&gt;资源&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;emptyDir&lt;/code&gt; 卷的存储介质(Disk, SSD, etc.)是由 kubelet 根目录(通常为 &lt;code&gt;/var/lib/kubelet&lt;/code&gt;)
所在的文件系统的存储介质决定的。 &lt;code&gt;emptyDir&lt;/code&gt; 和 &lt;code&gt;hostPath&lt;/code&gt; 卷没有使用容量限制，容器和 Pod
之间也没有限离。&lt;/p&gt;
&lt;p&gt;在将来，我们预计将通过
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/manage-resources-containers/&#34;&gt;资源&lt;/a&gt;
来确定 &lt;code&gt;emptyDir&lt;/code&gt; 和 &lt;code&gt;hostPath&lt;/code&gt; 请求确定数量的空间，如果集群中有多种介质也 选择要使用的介质&lt;/p&gt;
&lt;!--
## Out-of-Tree Volume Plugins

The Out-of-tree volume plugins include the Container Storage Interface (CSI)
and FlexVolume. They enable storage vendors to create custom storage plugins
without adding them to the Kubernetes repository.

Before the introduction of CSI and FlexVolume, all volume plugins (like
volume types listed above) were &#34;in-tree&#34; meaning they were built, linked,
compiled, and shipped with the core Kubernetes binaries and extend the core
Kubernetes API. This meant that adding a new storage system to Kubernetes (a
volume plugin) required checking code into the core Kubernetes code repository.

Both CSI and FlexVolume allow volume plugins to be developed independent of
the Kubernetes code base, and deployed (installed) on Kubernetes clusters as
extensions.

For storage vendors looking to create an out-of-tree volume plugin, please refer
to [this FAQ](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md).
 --&gt;
&lt;h2 id=&#34;外部卷插件&#34;&gt;外部卷插件&lt;/h2&gt;
&lt;p&gt;外部卷插件包括 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;容器存储接口 (CSI)&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt; 和 FlexVolume。它们使得存储提供商
可能创建自定义的存储插件而不需要将其添加到 k8s 代码库中。&lt;/p&gt;
&lt;p&gt;在引入 CSI 和 FlexVolume 之前， 所有的卷插件(像所有前面列举的那些)都是内置的，它们在构建，连接
编译，发布都是与 k8s 核心二进制文件在一起的并且扩展了 k8s 核心 API。 这意味着如果我们要添加
一种新的存储系统(一个卷插件)到 k8s 中时，需要将其代码检入到 k8s 核心代码库中。&lt;/p&gt;
&lt;p&gt;CSI 和 FlexVolume 都允许独立于 k8s 核心代码独立开发卷插件，并且独立以插件的方式部署(安装)
到 k8s 集群中。&lt;/p&gt;
&lt;p&gt;要看看有哪些外部卷插件，请见
&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md&#34;&gt;这个 FAQ&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### CSI

[Container Storage Interface](https://github.com/container-storage-interface/spec/blob/master/spec.md) (CSI)
defines a standard interface for container orchestration systems (like
Kubernetes) to expose arbitrary storage systems to their container workloads.

Please read the [CSI design proposal](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md) for more information.

CSI support was introduced as alpha in Kubernetes v1.9, moved to beta in
Kubernetes v1.10, and is GA in Kubernetes v1.13.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Support for CSI spec versions 0.2 and 0.3 are deprecated in Kubernetes
v1.13 and will be removed in a future release.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; CSI drivers may not be compatible across all Kubernetes releases.
Please check the specific CSI driver&amp;rsquo;s documentation for supported
deployments steps for each Kubernetes release and a compatibility matrix.&lt;/div&gt;
&lt;/blockquote&gt;


Once a CSI compatible volume driver is deployed on a Kubernetes cluster, users
may use the `csi` volume type to attach, mount, etc. the volumes exposed by the
CSI driver.

A `csi` volume can be used in a pod in three different ways:
- through a reference to a [`persistentVolumeClaim`](#persistentvolumeclaim)
- with a [generic ephemeral volume](/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volume) (alpha feature)
- with a [CSI ephemeral volume](/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volume) if the driver
  supports that (beta feature)

The following fields are available to storage administrators to configure a CSI
persistent volume:

- `driver`: A string value that specifies the name of the volume driver to use.
  This value must correspond to the value returned in the `GetPluginInfoResponse`
  by the CSI driver as defined in the [CSI spec](https://github.com/container-storage-interface/spec/blob/master/spec.md#getplugininfo).
  It is used by Kubernetes to identify which CSI driver to call out to, and by
  CSI driver components to identify which PV objects belong to the CSI driver.
- `volumeHandle`: A string value that uniquely identifies the volume. This value
  must correspond to the value returned in the `volume.id` field of the
  `CreateVolumeResponse` by the CSI driver as defined in the [CSI spec](https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume).
  The value is passed as `volume_id` on all calls to the CSI volume driver when
  referencing the volume.
- `readOnly`: An optional boolean value indicating whether the volume is to be
  &#34;ControllerPublished&#34; (attached) as read only. Default is false. This value is
  passed to the CSI driver via the `readonly` field in the
  `ControllerPublishVolumeRequest`.
- `fsType`: If the PV&#39;s `VolumeMode` is `Filesystem` then this field may be used
  to specify the filesystem that should be used to mount the volume. If the
  volume has not been formatted and formatting is supported, this value will be
  used to format the volume.
  This value is passed to the CSI driver via the `VolumeCapability` field of
  `ControllerPublishVolumeRequest`, `NodeStageVolumeRequest`, and
  `NodePublishVolumeRequest`.
- `volumeAttributes`: A map of string to string that specifies static properties
  of a volume. This map must correspond to the map returned in the
  `volume.attributes` field of the `CreateVolumeResponse` by the CSI driver as
  defined in the [CSI spec](https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume).
  The map is passed to the CSI driver via the `volume_context` field in the
  `ControllerPublishVolumeRequest`, `NodeStageVolumeRequest`, and
  `NodePublishVolumeRequest`.
- `controllerPublishSecretRef`: A reference to the secret object containing
  sensitive information to pass to the CSI driver to complete the CSI
  `ControllerPublishVolume` and `ControllerUnpublishVolume` calls. This field is
  optional, and may be empty if no secret is required. If the secret object
  contains more than one secret, all secrets are passed.
- `nodeStageSecretRef`: A reference to the secret object containing
  sensitive information to pass to the CSI driver to complete the CSI
  `NodeStageVolume` call. This field is optional, and may be empty if no secret
  is required. If the secret object contains more than one secret, all secrets
  are passed.
- `nodePublishSecretRef`: A reference to the secret object containing
  sensitive information to pass to the CSI driver to complete the CSI
  `NodePublishVolume` call. This field is optional, and may be empty if no
  secret is required. If the secret object contains more than one secret, all
  secrets are passed.
--&gt;
&lt;h3 id=&#34;csi&#34;&gt;CSI&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34;&gt;容器存储接口&lt;/a&gt; (CSI)
defines a standard interface for container orchestration systems (like
Kubernetes) to expose arbitrary storage systems to their container workloads.
定义一个标准接口用于容器编排系统(像 k8s)暴露任意存储系统到它们的容器工作负载中。
更多信息请见 &lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md&#34;&gt;CSI 设计方案&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;CSI 支持是在 k8s v1.9 中引入为 alpha， 在 k8s v1.10 进变为 beta 版本， k8s v1.13 变为 GA
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 对 CSI 0.2 和 0.3 标准的支持在 k8s v1.13 中被废弃并会在将来被移除&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; CSI 驱动可能不能与所有的 k8s 发行版本兼容。请查阅对应 CSI 驱动 文档，了解其在每一个 k8s 版本
中的部署方式和对应的版本兼容表格&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;当一个 CSI 兼容的卷驱动被部署到 k8s 集群中时， 用户可以使用 &lt;code&gt;csi&lt;/code&gt; 卷类型为装载，挂载那些
通过 CSI 驱动暴露的卷。&lt;/p&gt;
&lt;p&gt;一个 &lt;code&gt;csi&lt;/code&gt; 卷可以以下三种方式在 Pod 中使用:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过一个 &lt;a href=&#34;#persistentvolumeclaim&#34;&gt;&lt;code&gt;persistentVolumeClaim&lt;/code&gt;&lt;/a&gt; 引用&lt;/li&gt;
&lt;li&gt;通过&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volume&#34;&gt;generic ephemeral volume&lt;/a&gt;(alpha 特性)&lt;/li&gt;
&lt;li&gt;如果驱动支持，通过 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volume&#34;&gt;CSI ephemeral volume&lt;/a&gt;  (beta 特性)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下字段可以用于存储管理员对 CSI 持久卷的配置:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;driver&lt;/code&gt;: 一个字符串，用于指定卷所使用的驱动的名称。 取值范围是
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#getplugininfo&#34;&gt;CSI spec&lt;/a&gt;
中定义的 CSI 驱动 &lt;code&gt;GetPluginInfoResponse&lt;/code&gt; 的返回值。 它被 k8s 用来识别，应该调用哪个 CSI 驱动
我大兵由 CSI 驱动组件来识别哪些 PV 对象是属于该 CSI 驱动的&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;volumeHandle&lt;/code&gt;: 一个字符串， 用于唯一标识该卷。 这个值必须是在
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume&#34;&gt;CSI spec&lt;/a&gt;.
定义的 CSI 驱动 &lt;code&gt;CreateVolumeResponse&lt;/code&gt; 返回值中的 &lt;code&gt;volume.id&lt;/code&gt; 的值。 在所有引用这个
卷时会该这个值 以 &lt;code&gt;volume_id&lt;/code&gt; 调用参数发给 CSI 卷驱动&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;readOnly&lt;/code&gt;: 一个可选布尔值，用于指定该卷是否以只读方式挂载(ControllerPublished)。
默认值为 false. 这个值通过 &lt;code&gt;ControllerPublishVolumeRequest&lt;/code&gt; 中 &lt;code&gt;readonly&lt;/code&gt; 传递给
CSI 驱动&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;fsType&lt;/code&gt;: 如果 PV 的 &lt;code&gt;VolumeMode&lt;/code&gt; 是 &lt;code&gt;Filesystem&lt;/code&gt;，这个字段就可以用来指定挂载该卷所使用
的文件系统。 如果这个卷还没有被格式化并支持格式化，则这个被被用来格式化这个卷。 这个值通过
&lt;code&gt;ControllerPublishVolumeRequest&lt;/code&gt;, &lt;code&gt;NodeStageVolumeRequest&lt;/code&gt;, &lt;code&gt;NodePublishVolumeRequest&lt;/code&gt;
的 &lt;code&gt;VolumeCapability&lt;/code&gt; 字段传递给 CSI 驱动。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;volumeAttributes&lt;/code&gt;: 一个键值都是字符串的字典，用于指定卷的静态属性。这个字段的键值必须是
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume&#34;&gt;CSI spec&lt;/a&gt;
中定义 CSI 驱动 &lt;code&gt;CreateVolumeResponse&lt;/code&gt; 返回值的 &lt;code&gt;volume.attributes&lt;/code&gt; 字段的键值的子集
这个字典通过 &lt;code&gt;ControllerPublishVolumeRequest&lt;/code&gt;, &lt;code&gt;NodeStageVolumeRequest&lt;/code&gt;,
&lt;code&gt;NodePublishVolumeRequest&lt;/code&gt; 的 &lt;code&gt;volume_context&lt;/code&gt; 字段传递给 CSI 驱动&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;controllerPublishSecretRef&lt;/code&gt;:  完成 &lt;code&gt;ControllerPublishVolume&lt;/code&gt; 和
&lt;code&gt;ControllerUnpublishVolume&lt;/code&gt; 调用向 CSI 驱动传递敏感信息的 Secret 对象的引用。该字段为
可选， 如果没使用 Secret 则留空。 如果 Secret 对象中有多个 Secret, 则所有的都会传递。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;nodeStageSecretRef&lt;/code&gt;: 为了完成 &lt;code&gt;NodeStageVolume&lt;/code&gt; 调用， 向 CSI 驱动传递敏感信息的
Secret 对象的引用。该字段为 可选， 如果没使用 Secret 则留空。
如果 Secret 对象中有多个 Secret, 则所有的都会传递。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;nodePublishSecretRef&lt;/code&gt;:  为了完成 &lt;code&gt;NodePublishVolume&lt;/code&gt; 调用， 向 CSI 驱动传递敏感信息的
Secret 对象的引用。该字段为 可选， 如果没使用 Secret 则留空。
如果 Secret 对象中有多个 Secret, 则所有的都会传递。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
#### CSI raw block volume support






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;



Vendors with external CSI drivers can implement raw block volumes support
in Kubernetes workloads.

You can [setup your PV/PVC with raw block volume support](/docs/concepts/storage/persistent-volumes/#raw-block-volume-support)
as usual, without any CSI specific changes.
 --&gt;
&lt;h4 id=&#34;csi-原生块卷支持&#34;&gt;CSI 原生块卷支持&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;有外部 CSI 驱动的供应商可以实现对 k8s 工作负载原生块卷的支持&lt;/p&gt;
&lt;p&gt;用户可以在不作任何 CSI 配置修改的情况下
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/#raw-block-volume-support&#34;&gt;设置支持原生块卷的 PV/PVC&lt;/a&gt;&lt;/p&gt;
&lt;!--
#### CSI ephemeral volumes






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [beta]&lt;/code&gt;
&lt;/div&gt;



You can directly configure CSI volumes within the Pod
specification. Volumes specified in this way are ephemeral and do not
persist across Pod restarts. See [Ephemeral
Volumes](/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volume)
for more information.
 --&gt;
&lt;h4 id=&#34;csi-临时卷&#34;&gt;CSI 临时卷&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;You can directly configure CSI volumes within the Pod
specification. Volumes specified in this way are ephemeral and do not
persist across Pod restarts. See &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volume&#34;&gt;Ephemeral
Volumes&lt;/a&gt;
for more information.
用户可以在 Pod 定义中直接配置 CSI 卷。 但是通过这种方式创建的卷是临时，Pod 重启后就不会存在。
更多信息请见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volume&#34;&gt;临时卷&lt;/a&gt;&lt;/p&gt;
&lt;!--
#### 相关资料

For more information on how to develop a CSI driver, refer to the [kubernetes-csi
documentation](https://kubernetes-csi.github.io/docs/)
 --&gt;
&lt;h4 id=&#34;相关资料&#34;&gt;相关资料&lt;/h4&gt;
&lt;p&gt;更多关于如何开发 CSI 驱动的信息，请见
&lt;a href=&#34;https://kubernetes-csi.github.io/docs/&#34;&gt;kubernetes-csi 文档&lt;/a&gt;&lt;/p&gt;
&lt;!--
#### Migrating to CSI drivers from in-tree plugins






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [alpha]&lt;/code&gt;
&lt;/div&gt;



The CSI Migration feature, when enabled, directs operations against existing in-tree
plugins to corresponding CSI plugins (which are expected to be installed and configured).
The feature implements the necessary translation logic and shims to re-route the
operations in a seamless fashion. As a result, operators do not have to make any
configuration changes to existing Storage Classes, PVs or PVCs (referring to
in-tree plugins) when transitioning to a CSI driver that supersedes an in-tree plugin.

In the alpha state, the operations and features that are supported include
provisioning/delete, attach/detach, mount/unmount and resizing of volumes.

In-tree plugins that support CSI Migration and have a corresponding CSI driver implemented
are listed in the &#34;Types of Volumes&#34; section above.
 --&gt;
&lt;h4 id=&#34;从内部插件迁移到-csi-驱动&#34;&gt;从内部插件迁移到 CSI 驱动&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;当启用 CSI 迁移特性后，当内部插件有对应的 CSI 插件(已经完成安装配置)时。这个特性实现了必要的翻译
逻辑将操作无缝地转移。 最终，操作者不需要对已经存在的 StorageClass， PV， PVC(使用内部插件的)
配置做任何修改而实现 从内部插件迁移到 CSI 驱动上。&lt;/p&gt;
&lt;p&gt;在 alpha 状态，支持的操作和特性包括 管理/删除 attach/detach, mount/unmount 和修改卷大小。&lt;/p&gt;
&lt;p&gt;支持 CSI 迁移并且有对应 CSI 驱动实现的内部插件见上面的 &lt;a href=&#34;#types-of-volumes&#34;&gt;卷类型&lt;/a&gt; 章节&lt;/p&gt;
&lt;h3 id=&#34;flexVolume&#34;&gt;FlexVolume&lt;/h3&gt;
&lt;p&gt;FlexVolume 是一个外部插件接口，它从 k8v v1.2 (在 CSI 之间)就存在了。 它使用一个基于 exec
的模式与驱动交互。 &lt;code&gt;FlexVolume&lt;/code&gt; 驱动的二进制必须要预先安装在节点上一个预定义插件路径中(有些情况 master 也要装)&lt;/p&gt;
&lt;p&gt;Pod 通过内部插件 &lt;code&gt;flexvolume&lt;/code&gt; 与 FlexVolume 驱动交互，更多信息请见
&lt;a href=&#34;https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md&#34;&gt;这里&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Mount propagation

Mount propagation allows for sharing volumes mounted by a Container to
other Containers in the same Pod, or even to other Pods on the same node.

Mount propagation of a volume is controlled by `mountPropagation` field in Container.volumeMounts.
Its values are:

 * `None` - This volume mount will not receive any subsequent mounts
   that are mounted to this volume or any of its subdirectories by the host.
   In similar fashion, no mounts created by the Container will be visible on
   the host. This is the default mode.

   This mode is equal to `private` mount propagation as described in the
   [Linux kernel documentation](https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt)

 * `HostToContainer` - This volume mount will receive all subsequent mounts
   that are mounted to this volume or any of its subdirectories.

   In other words, if the host mounts anything inside the volume mount, the
   Container will see it mounted there.

   Similarly, if any Pod with `Bidirectional` mount propagation to the same
   volume mounts anything there, the Container with `HostToContainer` mount
   propagation will see it.

   This mode is equal to `rslave` mount propagation as described in the
   [Linux kernel documentation](https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt)

 * `Bidirectional` - This volume mount behaves the same the `HostToContainer` mount.
   In addition, all volume mounts created by the Container will be propagated
   back to the host and to all Containers of all Pods that use the same volume.

   A typical use case for this mode is a Pod with a FlexVolume or CSI driver or
   a Pod that needs to mount something on the host using a `hostPath` volume.

   This mode is equal to `rshared` mount propagation as described in the
   [Linux kernel documentation](https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt)

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; &lt;code&gt;Bidirectional&lt;/code&gt; mount propagation can be dangerous. It can damage
the host operating system and therefore it is allowed only in privileged
Containers. Familiarity with Linux kernel behavior is strongly recommended.
In addition, any volume mounts created by Containers in Pods must be destroyed
(unmounted) by the Containers on termination.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;挂载传播&#34;&gt;挂载传播&lt;/h2&gt;
&lt;p&gt;挂载传播可以让同一个 Pod 中的一个容器上挂载的卷可以共享给另一个容器，甚至可以共享给同一个节点上
的其它 Pod&lt;/p&gt;
&lt;p&gt;一个卷的挂载传播受容 &lt;code&gt;Container.volumeMounts&lt;/code&gt; 的 &lt;code&gt;mountPropagation&lt;/code&gt; 字段控制。可选值有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;None&lt;/code&gt; 这个卷挂载不会接收到任何后续由主机挂载到这个卷或任意其子目录的挂载。
类似地，由容器创建的挂载在主机上不可见。 这是默认模式。&lt;/p&gt;
&lt;p&gt;这个模式与
&lt;a href=&#34;https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt&#34;&gt;Linux 内核文档&lt;/a&gt;
中描述的 &lt;code&gt;private&lt;/code&gt; 挂载传播等效。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;HostToContainer&lt;/code&gt; 这个卷挂载会接收所有后继到这个卷或其子目录的挂载。&lt;/p&gt;
&lt;p&gt;换种方式来说， 如果主机挂载了任意内容到这个卷挂载，容器中就会看到挂载的内容。&lt;/p&gt;
&lt;p&gt;相似地，如果任意使用 &lt;code&gt;Bidirectional&lt;/code&gt; 挂载传播的 Pod 挂载了任意内容， 使用 &lt;code&gt;HostToContainer&lt;/code&gt;
挂载传播的容器也会看到这个挂载。&lt;/p&gt;
&lt;p&gt;这种模式与
&lt;a href=&#34;https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt&#34;&gt;Linux 内核文档&lt;/a&gt;
中描述的 &lt;code&gt;rslave&lt;/code&gt; 挂载传播等效。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Bidirectional&lt;/code&gt; 这种卷挂载的表现方式与 &lt;code&gt;HostToContainer&lt;/code&gt; 挂载相同。 同时，所以由容器创建
卷挂载也会反向传播给主机和所以使用这个卷的所有 Pod 的所有容器。&lt;/p&gt;
&lt;p&gt;使用这种模式的一个典型场景是一个使用 &lt;code&gt;FlexVolume&lt;/code&gt; 或 CSI 驱动容器 或 一个需要使用 &lt;code&gt;hostPath&lt;/code&gt;
卷挂载一些主机内容的容器。
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; &lt;code&gt;Bidirectional&lt;/code&gt; 挂载传播可能会相当危险。 它可能危害主机操作系统并且只能在提权容器上使用。
强烈建议操作使用前要相当熟悉  Linux 内核行为。 另外，由 Pod 中容器创建的卷挂载必须要在容器被终结时销毁
(卸载)&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Configuration
Before mount propagation can work properly on some deployments (CoreOS,
RedHat/Centos, Ubuntu) mount share must be configured correctly in
Docker as shown below.

Edit your Docker&#39;s `systemd` service file.  Set `MountFlags` as follows:
```shell
MountFlags=shared
```
Or, remove `MountFlags=slave` if present.  Then restart the Docker daemon:
```shell
sudo systemctl daemon-reload
sudo systemctl restart docker
```
 --&gt;
&lt;h3 id=&#34;配置&#34;&gt;配置&lt;/h3&gt;
&lt;p&gt;Before mount propagation can work properly on some deployments (CoreOS,
RedHat/Centos, Ubuntu) mount share must be configured correctly in
Docker as shown below.
想要让挂载传播在某些部署(CoreOS, RedHat/Centos, Ubuntu)上正常工作，需要在 Docker 上
进行正确的共享挂载配置，具体如下:
编辑 Docker &lt;code&gt;systemd&lt;/code&gt; 服务配置文件，将 &lt;code&gt;MountFlags&lt;/code&gt; 设置如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MountFlags=shared
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;或者，如果有配置为 &lt;code&gt;MountFlags=slave&lt;/code&gt; 则将其移除。然后重启 Docker 守护进程:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sudo systemctl daemon-reload
sudo systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;相关资料-1&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
* Follow an example of [deploying WordPress and MySQL with Persistent Volumes](/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/).
--&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/&#34;&gt;部署带有持久卷的 WordPress 和 MySQL&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 持久化卷(PV)</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/persistent-volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/persistent-volumes/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jsafrane
- saad-ali
- thockin
- msau42
- xing-yang
title: Persistent Volumes
feature:
  title: Storage orchestration
  description: &gt;
    Automatically mount the storage system of your choice, whether from local storage, a public cloud provider such as &lt;a href=&#34;https://cloud.google.com/storage/&#34;&gt;GCP&lt;/a&gt; or &lt;a href=&#34;https://aws.amazon.com/products/storage/&#34;&gt;AWS&lt;/a&gt;, or a network storage system such as NFS, iSCSI, Gluster, Ceph, Cinder, or Flocker.

content_type: concept
weight: 20
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This document describes the current state of _persistent volumes_ in Kubernetes. Familiarity with [volumes](/docs/concepts/storage/volumes/) is suggested.
 --&gt;
&lt;p&gt;本文主要介绍当前 k8s 中 &lt;em&gt;持久化卷(persistent volume)&lt;/em&gt; 的状态。
建议先熟悉 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#34;&gt;volumes&lt;/a&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Introduction

Managing storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed. To do this, we introduce two new API resources:  PersistentVolume and PersistentVolumeClaim.

A _PersistentVolume_ (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using [Storage Classes](/docs/concepts/storage/storage-classes/). It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.

A _PersistentVolumeClaim_ (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory).  Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see [AccessModes](#access-modes)).

While PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than just size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the _StorageClass_ resource.

See the [detailed walkthrough with working examples](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/).
 --&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;管理存储和管理实例是两个不同的问题。 PersistentVolume 子系统为用户和管理员提供了一套 API 将
存储是怎么提供的细节从存储使用中抽象出来。 而为了做到这一个我们要介绍两个新的 API 资源:
PersistentVolume 和 PersistentVolumeClaim.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PersistentVolume&lt;/em&gt; (PV) 是集群中的一块存储，它可能由管理员管理或使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-classes/&#34;&gt;StorageClass&lt;/a&gt;
动态管理。它是集群中的一个资源，就像节点是集群中的一个资源一样。 PV 是与卷(Volume)类似的卷插件，
但它的生命周期与使用它的 Pod 的生命周期是相互独立的。这个 API 对象中包含了存储的实现细节，包含
NFS, iSCSI, 云服务提供的存储系统。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PersistentVolumeClaim&lt;/em&gt; (PVC) 是一个用户对存储的请求。 它就像是一个 Pod。Pod 使用的是节点上的资源
而 PVC 使用的是 PV 资源。 Pod 可以申请指定级别的资源(CPU 和 Memory)。 PVC 可以申请指定容量的存储
和访问模式 (如，它们可以以 &lt;code&gt;ReadWriteOnce&lt;/code&gt;, &lt;code&gt;ReadOnlyMany&lt;/code&gt; 或 &lt;code&gt;ReadWriteMany&lt;/code&gt; 方式挂载，
见 &lt;a href=&#34;#access-modes&#34;&gt;AccessMode&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;因为 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 允许用户使用抽象的存储资源，通常用户都需要 PersistentVolume
中包含许多属性，如性能，来应对不同的问题。 集群管理员需要能够提供多样的 PersistentVolume，而不
仅仅是容量和访问模式，还需要向用户提供这些卷的实现细节。 为了满足这些需求，我们就有了 &lt;em&gt;StorageClass&lt;/em&gt; 资源。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-persistent-volume-storage/&#34;&gt;亲自上手试试&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Lifecycle of a volume and claim

PVs are resources in the cluster. PVCs are requests for those resources and also act as claim checks to the resource. The interaction between PVs and PVCs follows this lifecycle:
 --&gt;
&lt;h2 id=&#34;pv-和-pvc-的生命周期&#34;&gt;PV 和 PVC 的生命周期&lt;/h2&gt;
&lt;p&gt;PV 是集群中的资源。 PVC 是对这些资源的申请同时也表现为对这些资源的检查声明。PV 和 PVC 之间的
相互影响遵守以下生命周期:&lt;/p&gt;
&lt;!--
### Provisioning

There are two ways PVs may be provisioned: statically or dynamically.
 --&gt;
&lt;h3 id=&#34;provisioning&#34;&gt;供给&lt;/h3&gt;
&lt;p&gt;PV 可以被两个方式提供: 静态供给 或 动态供给&lt;/p&gt;
&lt;!--
#### Static

A cluster administrator creates a number of PVs. They carry the details of the real storage, which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.
 --&gt;
&lt;h4 id=&#34;静态供给&#34;&gt;静态供给&lt;/h4&gt;
&lt;p&gt;集群管理创建一系列 PV。 它们包含真实存储的细节，可以被集群用户使用。
它们存在于 k8s API 中，可以被取用。&lt;/p&gt;
&lt;!--
#### Dynamic

When none of the static PVs the administrator created match a user&#39;s PersistentVolumeClaim,
the cluster may try to dynamically provision a volume specially for the PVC.
This provisioning is based on StorageClasses: the PVC must request a
[storage class](/docs/concepts/storage/storage-classes/) and
the administrator must have created and configured that class for dynamic
provisioning to occur. Claims that request the class `&#34;&#34;` effectively disable
dynamic provisioning for themselves.

To enable dynamic storage provisioning based on storage class, the cluster administrator
needs to enable the `DefaultStorageClass` [admission controller](/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)
on the API server. This can be done, for example, by ensuring that `DefaultStorageClass` is
among the comma-delimited, ordered list of values for the `--enable-admission-plugins` flag of
the API server component. For more information on API server command-line flags,
check [kube-apiserver](/docs/admin/kube-apiserver/) documentation.
 --&gt;
&lt;h4 id=&#34;动态供给&#34;&gt;动态供给&lt;/h4&gt;
&lt;p&gt;当管理创建的 PV 不能满足用户的 PersistentVolumeClaim 时，集群可能就会尝试为这个 PVC 动态
提供一个卷。这种供给基于 &lt;code&gt;StorageClass&lt;/code&gt;: PVC 必须要申请一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes/&#34;&gt;StorageClass&lt;/a&gt;
并且在动态供给发生之前管理员必须要完成该 &lt;code&gt;StorageClass&lt;/code&gt; 的创建和配置。
如果 PVC 的 &lt;code&gt;StorageClass&lt;/code&gt; 被设置为 &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt; 实际上表示关闭自身的动态供给。&lt;/p&gt;
&lt;p&gt;要启用基于 &lt;code&gt;StorageClass&lt;/code&gt; 的动态存储供给需要集群管理在 api-server 中的
&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass&#34;&gt;准入控制&lt;/a&gt;
中启用 &lt;code&gt;DefaultStorageClass&lt;/code&gt;。 具体的配置方法就是确认 api-server 组件的 &lt;code&gt;--enable-admission-plugins&lt;/code&gt; 选项
的值中有没有 &lt;code&gt;DefaultStorageClass&lt;/code&gt;。 更多关于 api-server 命令行参数见
&lt;a href=&#34;https://kubernetes.io/docs/admin/kube-apiserver/&#34;&gt;kube-apiserver&lt;/a&gt; 文档&lt;/p&gt;
&lt;!--
### Binding

A user creates, or in the case of dynamic provisioning, has already created, a PersistentVolumeClaim with a specific amount of storage requested and with certain access modes. A control loop in the master watches for new PVCs, finds a matching PV (if possible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim.

Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster.
 --&gt;
&lt;h3 id=&#34;binding&#34;&gt;绑定&lt;/h3&gt;
&lt;p&gt;当一个指定容量和访问模式的 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 被创建后。 主控中心中的一个控制回环会监控新创建的 PVC，
如果有匹配的 PV 存在，则将它们绑定在一起。 如果这个 PV 是动态供给给这个新建的 PVC 的， 那么控制
回环将会始终将这个 PV 绑定到这个 PVC。 否则用户会得到至少满足其请求的卷，但这个卷容量可能实际是超出请求的。
当绑定完成，就会确定绑定关系，不管它们是怎么绑定的。 PVC 到 PV 的绑定是一对一关系，PV 使用的 &lt;code&gt;spec.claimRef&lt;/code&gt;
实现 &lt;code&gt;PersistentVolume&lt;/code&gt; 与 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 的双向绑定。&lt;/p&gt;
&lt;p&gt;如果没有匹配的 PV 存在，PVC 会永远保持在未绑定状态。 当有可用的匹配 PV 出现时 PVC 就会绑定。
例如， 集群中提供许多 50Gi PV 是不会被一个申请 100Gi 的 PVC 匹配到的。当集群中添加了一个
100Gi PV 时，这个 PVC 就可以绑定了。&lt;/p&gt;
&lt;!--
### Using

Pods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a Pod. For volumes that support multiple access modes, the user specifies which mode is desired when using their claim as a volume in a Pod.

Once a user has a claim and that claim is bound, the bound PV belongs to the user for as long as they need it. Users schedule Pods and access their claimed PVs by including a `persistentVolumeClaim` section in a Pod&#39;s `volumes` block. See [Claims As Volumes](#claims-as-volumes) for more details on this.
 --&gt;
&lt;h3 id=&#34;使用&#34;&gt;使用&lt;/h3&gt;
&lt;p&gt;Pod 会将 PVC 当作卷来使用。 集群会检视这个 PVC 找到它绑定的 PV 然后将这个 PV 挂载到 Pod 中。
对于支持多次访问模式的卷，用户在 Pod 当作卷的 PVC 中指定需要的访问模式。&lt;/p&gt;
&lt;p&gt;当一个用户拥有一个 PVC 并且这个 PVC 完成绑定，则这个被绑定的 PV 在用户需要时始终属于该用户。
用户通过 Pod 中的 &lt;code&gt;volumes&lt;/code&gt; 配置区中添加 &lt;code&gt;persistentVolumeClaim&lt;/code&gt; 配置区来访问这些由
PVC 管理的 PV。
更多信息见 &lt;a href=&#34;#claims-as-volumes&#34;&gt;将 PVC 当作卷(PV)&lt;/a&gt;&lt;/p&gt;
&lt;!--
### Storage Object in Use Protection
The purpose of the Storage Object in Use Protection feature is to ensure that PersistentVolumeClaims (PVCs) in active use by a Pod and PersistentVolume (PVs) that are bound to PVCs are not removed from the system, as this may result in data loss.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; PVC is in active use by a Pod when a Pod object exists that is using the PVC.&lt;/div&gt;
&lt;/blockquote&gt;


If a user deletes a PVC in active use by a Pod, the PVC is not removed immediately. PVC removal is postponed until the PVC is no longer actively used by any Pods. Also, if an admin deletes a PV that is bound to a PVC, the PV is not removed immediately. PV removal is postponed until the PV is no longer bound to a PVC.

You can see that a PVC is protected when the PVC&#39;s status is `Terminating` and the `Finalizers` list includes `kubernetes.io/pvc-protection`:

```shell
kubectl describe pvc hostpath
Name:          hostpath
Namespace:     default
StorageClass:  example-hostpath
Status:        Terminating
Volume:
Labels:        &lt;none&gt;
Annotations:   volume.beta.kubernetes.io/storage-class=example-hostpath
               volume.beta.kubernetes.io/storage-provisioner=example.com/hostpath
Finalizers:    [kubernetes.io/pvc-protection]
...
```

You can see that a PV is protected when the PV&#39;s status is `Terminating` and the `Finalizers` list includes `kubernetes.io/pv-protection` too:

```shell
kubectl describe pv task-pv-volume
Name:            task-pv-volume
Labels:          type=local
Annotations:     &lt;none&gt;
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    standard
Status:          Terminating
Claim:
Reclaim Policy:  Delete
Access Modes:    RWO
Capacity:        1Gi
Message:
Source:
    Type:          HostPath (bare host directory volume)
    Path:          /tmp/data
    HostPathType:
Events:            &lt;none&gt;
```
 --&gt;
&lt;h3 id=&#34;使用保护模式的存储对象&#34;&gt;使用保护模式的存储对象&lt;/h3&gt;
&lt;p&gt;保护模式的存储对象特性的目的是保证被 Pod 使用的有效的 PVC 和与 PVC 绑定的 PV 能会被系统删除，
从而可能导致数据丢失。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 当一个使用 PVC 的 Pod 对象存在时就表示 PVC 被 Pod 有效使用。&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;如果用户删除了一个被 Pod 有效使用的 PVC， 这个 PVC 不会立马被删除。 PVC 会延迟到没有被任何
Pod 有效使用后才会删除。 同样，如果管理员删除了一个与 PVC 绑定的 PV， 这个 PV 也不会被立马删除，
PV 的删除行为会被延迟到不再与 PVC 绑定时。&lt;/p&gt;
&lt;p&gt;当 PVC 的状态是 &lt;code&gt;Terminating&lt;/code&gt; 并且 &lt;code&gt;Finalizers&lt;/code&gt; 列表中包含 &lt;code&gt;kubernetes.io/pvc-protection&lt;/code&gt;
就表示这个 PVC 是被保护的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe pvc hostpath
Name:          hostpath
Namespace:     default
StorageClass:  example-hostpath
Status:        Terminating
Volume:
Labels:        &amp;lt;none&amp;gt;
Annotations:   volume.beta.kubernetes.io/storage-class&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;example-hostpath
               volume.beta.kubernetes.io/storage-provisioner&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;example.com/hostpath
Finalizers:    &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;kubernetes.io/pvc-protection&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;同样，当 PV 的状态是 &lt;code&gt;Terminating&lt;/code&gt; 并且 &lt;code&gt;Finalizers&lt;/code&gt; 列表中包含 &lt;code&gt;kubernetes.io/pvc-protection&lt;/code&gt;
就表示这个 PV 是被保护的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe pv task-pv-volume
Name:            task-pv-volume
Labels:          type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;local
Annotations:     &amp;lt;none&amp;gt;
Finalizers:      &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;kubernetes.io/pv-protection&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
StorageClass:    standard
Status:          Terminating
Claim:
Reclaim Policy:  Delete
Access Modes:    RWO
Capacity:        1Gi
Message:
Source:
    Type:          HostPath &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;bare host directory volume&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
    Path:          /tmp/data
    HostPathType:
Events:            &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Reclaiming

When a user is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained, Recycled, or Deleted.
 --&gt;
&lt;h3 id=&#34;回收&#34;&gt;回收&lt;/h3&gt;
&lt;p&gt;当用于不再需要一个卷时，可以通过 API 删除这个 PVC 对象，这样就允许对对应资源的回收。 PV 的回收
策略让集群可以在 PV 释放以后使用对应的方式回收。 目前，卷的回收策略有 保留(&lt;code&gt;Retain&lt;/code&gt;),
循环使用(&lt;code&gt;Recycle&lt;/code&gt;), 删除 (&lt;code&gt;Delete&lt;/code&gt;).&lt;/p&gt;
&lt;!--
#### Retain

The `Retain` reclaim policy allows for manual reclamation of the resource. When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered &#34;released&#34;. But it is not yet available for another claim because the previous claimant&#39;s data remains on the volume. An administrator can manually reclaim the volume with the following steps.

1. Delete the PersistentVolume. The associated storage asset in external infrastructure (such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume) still exists after the PV is deleted.
1. Manually clean up the data on the associated storage asset accordingly.
1. Manually delete the associated storage asset, or if you want to reuse the same storage asset, create a new PersistentVolume with the storage asset definition.
 --&gt;
&lt;h4 id=&#34;保留retained&#34;&gt;保留(&lt;code&gt;Retained&lt;/code&gt;)&lt;/h4&gt;
&lt;p&gt;保留(&lt;code&gt;Retained&lt;/code&gt;) 回收策略允许对资源手动回收。 当 PVC 被删除后，其之前绑定的 PV 依然存在并被
认为是已经释放。 但是它还不被另一个 PVC 使用，因为其中还有上一个 PVC 时的数据。 管理员可以
通过以下步骤手动回收这个卷:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;删除 PV 对象。 它所关联的外部存储设施(如 AWS EBS, GCE PD, Azure Disk, Cinder 卷)依然存在。&lt;/li&gt;
&lt;li&gt;根据需要手动清理对应存储资源上的数据&lt;/li&gt;
&lt;li&gt;手动删除对应的存储资源，如果想要重新使用该存储资源，可以再创建一个新的 PV 对象与之关联。&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
#### Delete

For volume plugins that support the `Delete` reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume. Volumes that were dynamically provisioned inherit the [reclaim policy of their StorageClass](#reclaim-policy), which defaults to `Delete`. The administrator should configure the StorageClass according to users&#39; expectations; otherwise, the PV must be edited or patched after it is created. See [Change the Reclaim Policy of a PersistentVolume](/docs/tasks/administer-cluster/change-pv-reclaim-policy/).
 --&gt;
&lt;h4 id=&#34;删除-delete&#34;&gt;删除 (&lt;code&gt;Delete&lt;/code&gt;)&lt;/h4&gt;
&lt;p&gt;对于支持 删除 (&lt;code&gt;Delete&lt;/code&gt;) 回收策略的卷插件，在删除 PVC 对象之后与其郑的 PV 对象也会被删除，
而且对应的外部存储资源，如 AWS EBS, GCE PD, Azure Disk, Cinder 卷也会一同被删除。
动态提供的卷的回收策略继承自 &lt;a href=&#34;#reclaim-policy&#34;&gt;它们的 StorageClass 的回收策略&lt;/a&gt;，默认为 删除 (&lt;code&gt;Delete&lt;/code&gt;)
管理应该根据用户期望设置 StorageClass 的回收策略，否则 PV 在创建后再需要再次修改才行。
见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/change-pv-reclaim-policy/&#34;&gt;修改 PersistentVolume 的回收策略&lt;/a&gt;.&lt;/p&gt;
&lt;!--
#### Recycle

&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; The &lt;code&gt;Recycle&lt;/code&gt; reclaim policy is deprecated. Instead, the recommended approach is to use dynamic provisioning.&lt;/div&gt;
&lt;/blockquote&gt;


If supported by the underlying volume plugin, the `Recycle` reclaim policy performs a basic scrub (`rm -rf /thevolume/*`) on the volume and makes it available again for a new claim.

However, an administrator can configure a custom recycler Pod template using
the Kubernetes controller manager command line arguments as described in the
[reference](/docs/reference/command-line-tools-reference/kube-controller-manager/).
The custom recycler Pod template must contain a `volumes` specification, as
shown in the example below:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pv-recycler
  namespace: default
spec:
  restartPolicy: Never
  volumes:
  - name: vol
    hostPath:
      path: /any/path/it/will/be/replaced
  containers:
  - name: pv-recycler
    image: &#34;k8s.gcr.io/busybox&#34;
    command: [&#34;/bin/sh&#34;, &#34;-c&#34;, &#34;test -e /scrub &amp;&amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  &amp;&amp; test -z \&#34;$(ls -A /scrub)\&#34; || exit 1&#34;]
    volumeMounts:
    - name: vol
      mountPath: /scrub
```

However, the particular path specified in the custom recycler Pod template in the `volumes` part is replaced with the particular path of the volume that is being recycled.
--&gt;
&lt;h4 id=&#34;循环使用recycle&#34;&gt;循环使用(&lt;code&gt;Recycle&lt;/code&gt;)&lt;/h4&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; 循环使用(&lt;code&gt;Recycle&lt;/code&gt;) 回收策略已经废弃。 推荐使用动态供给方式。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果底层卷插件支持， 循环使用(&lt;code&gt;Recycle&lt;/code&gt;) 回收策略在卷上执行基础的清理操作 (&lt;code&gt;rm -rf /thevolume/*&lt;/code&gt;)
然后它就可以再次被新的 PVC 使用了。&lt;/p&gt;
&lt;p&gt;但是，管理也可以使用 k8s 控制管理器的命令行参数配置一个自定义的回收器 Pod 模板。具体见
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/
&#34;&gt;这里&lt;/a&gt;.
这个自定义的回收器 Pod 模板必须要包含 &lt;code&gt;volumes&lt;/code&gt; 定义，下面就是一个示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pv-recycler&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vol&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/any/path/it/will/be/replaced&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pv-recycler&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;k8s.gcr.io/busybox&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/bin/sh&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;test -e /scrub &amp;amp;&amp;amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  &amp;amp;&amp;amp; test -z \&amp;#34;$(ls -A /scrub)\&amp;#34; || exit 1&amp;#34;&lt;/span&gt;]
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vol&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/scrub&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;回收器 Pod 模板中的 &lt;code&gt;volumes&lt;/code&gt; 部分指定的路径，就是要被回收的卷&lt;/p&gt;
&lt;!--
### Reserving a PersistentVolume

The control plane can [bind PersistentVolumeClaims to matching PersistentVolumes](#binding) in the
cluster. However, if you want a PVC to bind to a specific PV, you need to pre-bind them.

By specifying a PersistentVolume in a PersistentVolumeClaim, you declare a binding between that specific PV and PVC.
If the PersistentVolume exists and has not reserved PersistentVolumeClaims through its `claimRef` field, then the PersistentVolume and PersistentVolumeClaim will be bound.

The binding happens regardless of some volume matching criteria, including node affinity.
The control plane still checks that [storage class](/docs/concepts/storage/storage-classes/), access modes, and requested storage size are valid.

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: foo-pvc
  namespace: foo
spec:
  storageClassName: &#34;&#34; # Empty string must be explicitly set otherwise default StorageClass will be set
  volumeName: foo-pv
  ...
```

This method does not guarantee any binding privileges to the PersistentVolume. If other PersistentVolumeClaims could use the PV that you specify, you first need to reserve that storage volume. Specify the relevant PersistentVolumeClaim in the `claimRef` field of the PV so that other PVCs can not bind to it.

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: foo-pv
spec:
  storageClassName: &#34;&#34;
  claimRef:
    name: foo-pvc
    namespace: foo
  ...
```

This is useful if you want to consume PersistentVolumes that have their `claimPolicy` set
to `Retain`, including cases where you are reusing an existing PV.
 --&gt;
&lt;h3 id=&#34;保留-persistentvolume&#34;&gt;保留 &lt;code&gt;PersistentVolume&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;控制中心可以在集群中将&lt;a href=&#34;#binding&#34;&gt; &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 与匹配的 &lt;code&gt;PersistentVolume&lt;/code&gt; 绑定&lt;/a&gt;。
但是，如果想要将 PVC 与指定 PV 绑定， 则需要预先绑定(而不是自动绑定)。&lt;/p&gt;
&lt;p&gt;通过在 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 中指定一个 &lt;code&gt;PersistentVolume&lt;/code&gt;，就可以将 PV 绑定到这个 PVC 上。
如果 PV 已经存在， 可能通过设置它的 &lt;code&gt;claimRef&lt;/code&gt; 字段指定一个 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;，
这样 PV 和 PVC 就会绑定。&lt;/p&gt;
&lt;p&gt;这种绑定会忽略一些卷匹配条件，包括节点亲和性。 控制中心还是会检测，
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes/&#34;&gt;StorageClass&lt;/a&gt;, 访问模式，申请容量是否是有效的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo-pvc&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# 这里必须要显示地设置，否则就会使用默认的 StorageClass&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo-pv&lt;/span&gt;
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这种方式不对绑定 PV 的优先级做任何保证。 如果其它的 PVC 能够使用过这个 PV，必须要先保留存储卷。
在需要绑定的 PV &lt;code&gt;claimRef&lt;/code&gt; 的 PVC 这样其它的 PVC 才不能绑定这个 PV。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo-pv&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;claimRef&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo-pvc&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;foo&lt;/span&gt;
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果想要使用 &lt;code&gt;claimPolicy&lt;/code&gt; 是 &lt;code&gt;Retain&lt;/code&gt; 的 PV 这一招很有用， 包括重复使用那些已经存在的 PV。&lt;/p&gt;
&lt;!--
### Expanding Persistent Volumes Claims






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [beta]&lt;/code&gt;
&lt;/div&gt;



Support for expanding PersistentVolumeClaims (PVCs) is now enabled by default. You can expand
the following types of volumes:

* gcePersistentDisk
* awsElasticBlockStore
* Cinder
* glusterfs
* rbd
* Azure File
* Azure Disk
* Portworx
* FlexVolumes
* CSI

You can only expand a PVC if its storage class&#39;s `allowVolumeExpansion` field is set to true.

``` yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gluster-vol-default
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: &#34;http://192.168.10.100:8080&#34;
  restuser: &#34;&#34;
  secretNamespace: &#34;&#34;
  secretName: &#34;&#34;
allowVolumeExpansion: true
```

To request a larger volume for a PVC, edit the PVC object and specify a larger
size. This triggers expansion of the volume that backs the underlying PersistentVolume. A
new PersistentVolume is never created to satisfy the claim. Instead, an existing volume is resized.
 --&gt;
&lt;h3 id=&#34;扩展-pvc&#34;&gt;扩展 PVC&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;对 PVC 的扩展默认是启用的。 可以对以下类型的卷进行扩展:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;gcePersistentDisk&lt;/li&gt;
&lt;li&gt;awsElasticBlockStore&lt;/li&gt;
&lt;li&gt;Cinder&lt;/li&gt;
&lt;li&gt;glusterfs&lt;/li&gt;
&lt;li&gt;rbd&lt;/li&gt;
&lt;li&gt;Azure File&lt;/li&gt;
&lt;li&gt;Azure Disk&lt;/li&gt;
&lt;li&gt;Portworx&lt;/li&gt;
&lt;li&gt;FlexVolumes&lt;/li&gt;
&lt;li&gt;CSI&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;只能对 StorageClass 的 &lt;code&gt;allowVolumeExpansion&lt;/code&gt; 字段为 &lt;code&gt;true&lt;/code&gt; PVC 进行扩展&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gluster-vol-default&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/glusterfs&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;resturl&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://192.168.10.100:8080&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;restuser&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;secretNamespace&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;allowVolumeExpansion&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;想要为 PVC 申请一个更大的卷，只需要修改 PVC 对象，设置一个更大的容量。 这会触发卷底层的 PV 的扩充。
这样做不会创建一个新的 PV 来满足这个 PVC， 而是通过修改原来的容量来实现。&lt;/p&gt;
&lt;!--
#### CSI Volume expansion






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [beta]&lt;/code&gt;
&lt;/div&gt;



Support for expanding CSI volumes is enabled by default but it also requires a specific CSI driver to support volume expansion. Refer to documentation of the specific CSI driver for more information.
 --&gt;
&lt;h4 id=&#34;csi-卷扩容&#34;&gt;CSI 卷扩容&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;对 CSI 卷扩展 扩容默认是开启的，但也是需要相应的 CSI 驱动支持卷扩容。 具体信息请参阅相应
CSI 驱动的文档。&lt;/p&gt;
&lt;!--
#### Resizing a volume containing a file system

You can only resize volumes containing a file system if the file system is XFS, Ext3, or Ext4.

When a volume contains a file system, the file system is only resized when a new Pod is using
the PersistentVolumeClaim in `ReadWrite` mode. File system expansion is either done when a Pod is starting up
or when a Pod is running and the underlying file system supports online expansion.

FlexVolumes allow resize if the driver is set with the `RequiresFSResize` capability to `true`.
The FlexVolume can be resized on Pod restart.
 --&gt;
&lt;h4 id=&#34;修改包含文件系统的卷的容量&#34;&gt;修改包含文件系统的卷的容量&lt;/h4&gt;
&lt;p&gt;如果郑文件系统是  XFS, Ext3, Ext4 之一，则可以修改其容量。&lt;/p&gt;
&lt;p&gt;当卷中包含文件文件系统时，只有在
新的 Pod 使用 &lt;code&gt;ReadWrite&lt;/code&gt; 模式的 PVC 时才会变更容量。文件系统的扩容只有在 Pod 启动或
正在运行的 Pod 底层的文件系统支持在线扩容时才能生效。&lt;/p&gt;
&lt;p&gt;FlexVolume 允许在 驱动的 &lt;code&gt;RequiresFSResize&lt;/code&gt; 设置为 &lt;code&gt;true&lt;/code&gt; 时变更容量。
FlexVolume 的容量变更只能在 Pod 重启时生效。&lt;/p&gt;
&lt;!--
#### Resizing an in-use PersistentVolumeClaim






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.15 [beta]&lt;/code&gt;
&lt;/div&gt;



&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Expanding in-use PVCs is available as beta since Kubernetes 1.15, and as alpha since 1.11. The &lt;code&gt;ExpandInUsePersistentVolumes&lt;/code&gt; feature must be enabled, which is the case automatically for many clusters for beta features. Refer to the &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;feature gate&lt;/a&gt; documentation for more information.&lt;/div&gt;
&lt;/blockquote&gt;


In this case, you don&#39;t need to delete and recreate a Pod or deployment that is using an existing PVC.
Any in-use PVC automatically becomes available to its Pod as soon as its file system has been expanded.
This feature has no effect on PVCs that are not in use by a Pod or deployment. You must create a Pod that
uses the PVC before the expansion can complete.


Similar to other volume types - FlexVolume volumes can also be expanded when in-use by a Pod.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; FlexVolume resize is possible only when the underlying driver supports resize.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Expanding EBS volumes is a time-consuming operation. Also, there is a per-volume quota of one modification every 6 hours.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h4 id=&#34;对使用中的-persistentvolumeclaim-变更容量&#34;&gt;对使用中的 PersistentVolumeClaim 变更容量&lt;/h4&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.15 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 对使用中的 PersistentVolumeClaim 变更容量 在 k8s &lt;code&gt;v1.15&lt;/code&gt; 是 &lt;code&gt;beta&lt;/code&gt; 状态，
&lt;code&gt;v1.11&lt;/code&gt; 是 &lt;code&gt;alpha&lt;/code&gt; 状态, &lt;code&gt;ExpandInUsePersistentVolumes&lt;/code&gt; 在 &lt;code&gt;beta&lt;/code&gt; 时是默认打开的。
更多信息请见
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;在这种情况下，不需要删除或重新创建使用现有 PVC 的 Pod 或 Deployment. 任意使用中的 PVC 在
文件系统扩容后在 Pod 中自动变得可用。 这个特性对那些没有被 Pod 或 Deployment 的 PVC 无效。
要想扩容生效必须要有一个使用 PVC 的 Pod。&lt;/p&gt;
&lt;p&gt;与其它的卷类型类似， FlexVolume 也可以在被 Pod 使用时扩容。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; FlexVolume 变量容量只能在底层驱动支持的情况下才行。&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 对 EBS 卷扩容是一个耗时的操作。并且还有一个每个卷每 6 个小时只能扩容的限制&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
#### Recovering from Failure when Expanding Volumes

If expanding underlying storage fails, the cluster administrator can manually recover the Persistent Volume Claim (PVC) state and cancel the resize requests. Otherwise, the resize requests are continuously retried by the controller without administrator intervention.

1. Mark the PersistentVolume(PV) that is bound to the PersistentVolumeClaim(PVC) with `Retain` reclaim policy.
2. Delete the PVC. Since PV has `Retain` reclaim policy - we will not lose any data when we recreate the PVC.
3. Delete the `claimRef` entry from PV specs, so as new PVC can bind to it. This should make the PV `Available`.
4. Re-create the PVC with smaller size than PV and set `volumeName` field of the PVC to the name of the PV. This should bind new PVC to existing PV.
5. Don&#39;t forget to restore the reclaim policy of the PV.
 --&gt;
&lt;h4 id=&#34;从卷扩展失败中恢复&#34;&gt;从卷扩展失败中恢复&lt;/h4&gt;
&lt;p&gt;如果底层存储扩容的失败，集群管理可以通过手动恢复 PVC 的状态，取消扩容请求。否则，在没管理员中止
的情况下控制器会不断地重试容量变更请求。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将与 PersistentVolumeClaim(PVC) 绑定的 PersistentVolume(PV) 回收策略改为 &lt;code&gt;Retain&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;删除 PVC。 因为 PV 的回收策略是 &lt;code&gt;Retain&lt;/code&gt;，所以在重新创建 PVC 之前数据不会丢失。&lt;/li&gt;
&lt;li&gt;删除 PV 对象中的 &lt;code&gt;claimRef&lt;/code&gt; 实体， 这样新的 PVC 才可以与它绑定， 可以可以将 PV 变为 &lt;code&gt;Available&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;以小于 PV 的容量重新创建一个 PVC，这个 PVC 的 &lt;code&gt;volumeName&lt;/code&gt; 设置 PV 的名称。这样就可以将
原来的 PV 绑定到新的 PVC 上&lt;/li&gt;
&lt;li&gt;不要忘记将 PV 的回收策略也改加原来的值&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
## Types of Persistent Volumes

PersistentVolume types are implemented as plugins.  Kubernetes currently supports the following plugins:

* GCEPersistentDisk
* AWSElasticBlockStore
* AzureFile
* AzureDisk
* CSI
* FC (Fibre Channel)
* FlexVolume
* Flocker
* NFS
* iSCSI
* RBD (Ceph Block Device)
* CephFS
* Cinder (OpenStack block storage)
* Glusterfs
* VsphereVolume
* Quobyte Volumes
* HostPath (Single node testing only -- local storage is not supported in any way and WILL NOT WORK in a multi-node cluster)
* Portworx Volumes
* ScaleIO Volumes
* StorageOS
 --&gt;
&lt;h2 id=&#34;持久化卷的类型&#34;&gt;持久化卷的类型&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;PersistentVolume&lt;/code&gt; 的类型随同实现插件。 目前 k8s 支持以下插件:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GCEPersistentDisk&lt;/li&gt;
&lt;li&gt;AWSElasticBlockStore&lt;/li&gt;
&lt;li&gt;AzureFile&lt;/li&gt;
&lt;li&gt;AzureDisk&lt;/li&gt;
&lt;li&gt;CSI&lt;/li&gt;
&lt;li&gt;FC (Fibre Channel)&lt;/li&gt;
&lt;li&gt;FlexVolume&lt;/li&gt;
&lt;li&gt;Flocker&lt;/li&gt;
&lt;li&gt;NFS&lt;/li&gt;
&lt;li&gt;iSCSI&lt;/li&gt;
&lt;li&gt;RBD (Ceph Block Device)&lt;/li&gt;
&lt;li&gt;CephFS&lt;/li&gt;
&lt;li&gt;Cinder (OpenStack block storage)&lt;/li&gt;
&lt;li&gt;Glusterfs&lt;/li&gt;
&lt;li&gt;VsphereVolume&lt;/li&gt;
&lt;li&gt;Quobyte Volumes&lt;/li&gt;
&lt;li&gt;HostPath (只在单节点上测试过 &amp;ndash; 本地存储现在不支持多节点集群，将来也永远不会支持)&lt;/li&gt;
&lt;li&gt;Portworx Volumes&lt;/li&gt;
&lt;li&gt;ScaleIO Volumes&lt;/li&gt;
&lt;li&gt;StorageOS&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Persistent Volumes

Each PV contains a spec and status, which is the specification and status of the volume.
The name of a PersistentVolume object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /tmp
    server: 172.17.0.2
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Helper programs relating to the volume type may be required for consumption of a PersistentVolume within a cluster.  In this example, the PersistentVolume is of type NFS and the helper program /sbin/mount.nfs is required to support the mounting of NFS filesystems.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;persistentvolume&#34;&gt;PersistentVolume&lt;/h2&gt;
&lt;p&gt;每个 PV 包含一个 &lt;code&gt;spec&lt;/code&gt; 和 &lt;code&gt;status&lt;/code&gt;, 分别是对这个卷的配置定义和状态。 &lt;code&gt;PersistentVolume&lt;/code&gt;
对象的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pv0003&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;capacity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Filesystem&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeReclaimPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Recycle&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;mountOptions&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;hard&lt;/span&gt;
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;nfsvers=4.1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;nfs&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/tmp&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;server&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;172.17.0.2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Helper programs relating to the volume type may be required for consumption of a PersistentVolume within a cluster.  In this example, the PersistentVolume is of type NFS and the helper program /sbin/mount.nfs is required to support the mounting of NFS filesystems.
集群中辅助程序应该能够正确处理相关类型的卷。 在上面的例子中，PersistentVolume 的类型是 NFS
辅助程序 &lt;code&gt;/sbin/mount.nfs&lt;/code&gt; 就需要支持挂载 NFS 文件系统&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Capacity

Generally, a PV will have a specific storage capacity.  This is set using the PV&#39;s `capacity` attribute.  See the Kubernetes [Resource Model](https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md) to understand the units expected by `capacity`.

Currently, storage size is the only resource that can be set or requested.  Future attributes may include IOPS, throughput, etc.
 --&gt;
&lt;h3 id=&#34;容量capacity&#34;&gt;容量(Capacity)&lt;/h3&gt;
&lt;p&gt;通常，一个 PV 是有指定存储容量的。 这是通过 PV 的 &lt;code&gt;capacity&lt;/code&gt; 属性设置的。 理解受 &lt;code&gt;capacity&lt;/code&gt;
见 &lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md&#34;&gt;Resource Model&lt;/a&gt;
Currently, storage size is the only resource that can be set or requested.  Future attributes may include IOPS, throughput, etc.&lt;/p&gt;
&lt;!--
### Volume Mode






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;



Kubernetes supports two `volumeModes` of PersistentVolumes: `Filesystem` and `Block`.

`volumeMode` is an optional API parameter.
`Filesystem` is the default mode used when `volumeMode` parameter is omitted.

A volume with `volumeMode: Filesystem` is *mounted* into Pods into a directory. If the volume
is backed by a block device and the device is empty, Kuberneretes creates a filesystem
on the device before mounting it for the first time.

You can set the value of `volumeMode` to `Block` to use a volume as a raw block device.
Such volume is presented into a Pod as a block device, without any filesystem on it.
This mode is useful to provide a Pod the fastest possible way to access a volume, without
any filesystem layer between the Pod and the volume. On the other hand, the application
running in the Pod must know how to handle a raw block device.
See [Raw Block Volume Support](#raw-block-volume-support)
for an example on how to use a volume with `volumeMode: Block` in a Pod.
 --&gt;
&lt;h3 id=&#34;卷模式&#34;&gt;卷模式&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;k8s 支持两种 PersistentVolume 卷模式(&lt;code&gt;volumeModes&lt;/code&gt;): 文件系统(&lt;code&gt;Filesystem&lt;/code&gt;) 和 块设备(&lt;code&gt;Block&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;volumeMode&lt;/code&gt; 是一个可选 API 参数
如果没有设置 &lt;code&gt;volumeMode&lt;/code&gt;， 则默认使用 &lt;code&gt;Filesystem&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;一个卷模式为 &lt;code&gt;volumeMode: Filesystem&lt;/code&gt; 的卷 &lt;em&gt;挂载&lt;/em&gt; 到 Pod 中的一个目录。 如果卷的后端是
一个块设备并且这个设备是空的， k8s 会在第一次挂载之前在块设备上创建一个文件系统。&lt;/p&gt;
&lt;p&gt;也可以将 &lt;code&gt;volumeMode&lt;/code&gt; 的值设置为 &lt;code&gt;Block&lt;/code&gt; 以块设备的方式来使用这个卷。 这个卷就会以块设备的
形式存在于 Pod 中， 其中不会有任何文件系统。 这种方式在为 Pod 提供该卷可能最佳的访问速度，
在 Pod 和 卷之间没有任何文件系统层。 另一方面， Pod 中运行的应用必须要能正确使用块设备。&lt;/p&gt;
&lt;p&gt;可以在
&lt;a href=&#34;#raw-block-volume-support&#34;&gt;块设备卷支持&lt;/a&gt;
查看使用 &lt;code&gt;volumeMode: Block&lt;/code&gt; 卷模式卷的 Pod 。&lt;/p&gt;
&lt;!--
### Access Modes

A PersistentVolume can be mounted on a host in any way supported by the resource provider. As shown in the table below, providers will have different capabilities and each PV&#39;s access modes are set to the specific modes supported by that particular volume.  For example, NFS can support multiple read/write clients, but a specific NFS PV might be exported on the server as read-only. Each PV gets its own set of access modes describing that specific PV&#39;s capabilities.

The access modes are:

* ReadWriteOnce -- the volume can be mounted as read-write by a single node
* ReadOnlyMany -- the volume can be mounted read-only by many nodes
* ReadWriteMany -- the volume can be mounted as read-write by many nodes

In the CLI, the access modes are abbreviated to:

* RWO - ReadWriteOnce
* ROX - ReadOnlyMany
* RWX - ReadWriteMany

&gt; __Important!__ A volume can only be mounted using one access mode at a time, even if it supports many.  For example, a GCEPersistentDisk can be mounted as ReadWriteOnce by a single node or ReadOnlyMany by many nodes, but not at the same time.


| Volume Plugin        | ReadWriteOnce          | ReadOnlyMany          | ReadWriteMany|
| :---                 | :---:                  | :---:                 | :---:        |
| AWSElasticBlockStore | &amp;#x2713;               | -                     | -            |
| AzureFile            | &amp;#x2713;               | &amp;#x2713;              | &amp;#x2713;     |
| AzureDisk            | &amp;#x2713;               | -                     | -            |
| CephFS               | &amp;#x2713;               | &amp;#x2713;              | &amp;#x2713;     |
| Cinder               | &amp;#x2713;               | -                     | -            |
| CSI                  | depends on the driver  | depends on the driver | depends on the driver |
| FC                   | &amp;#x2713;               | &amp;#x2713;              | -            |
| FlexVolume           | &amp;#x2713;               | &amp;#x2713;              | depends on the driver |
| Flocker              | &amp;#x2713;               | -                     | -            |
| GCEPersistentDisk    | &amp;#x2713;               | &amp;#x2713;              | -            |
| Glusterfs            | &amp;#x2713;               | &amp;#x2713;              | &amp;#x2713;     |
| HostPath             | &amp;#x2713;               | -                     | -            |
| iSCSI                | &amp;#x2713;               | &amp;#x2713;              | -            |
| Quobyte              | &amp;#x2713;               | &amp;#x2713;              | &amp;#x2713;     |
| NFS                  | &amp;#x2713;               | &amp;#x2713;              | &amp;#x2713;     |
| RBD                  | &amp;#x2713;               | &amp;#x2713;              | -            |
| VsphereVolume        | &amp;#x2713;               | -                     | - (works when Pods are collocated)  |
| PortworxVolume       | &amp;#x2713;               | -                     | &amp;#x2713;     |
| ScaleIO              | &amp;#x2713;               | &amp;#x2713;              | -            |
| StorageOS            | &amp;#x2713;               | -                     | -            |
 --&gt;
&lt;h3 id=&#34;访问模式&#34;&gt;访问模式&lt;/h3&gt;
&lt;p&gt;A PersistentVolume can be mounted on a host in any way supported by the resource provider. As shown in the table below, providers will have different capabilities and each PV&amp;rsquo;s access modes are set to the specific modes supported by that particular volume.  For example, NFS can support multiple read/write clients, but a specific NFS PV might be exported on the server as read-only. Each PV gets its own set of access modes describing that specific PV&amp;rsquo;s capabilities.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PersistentVolume&lt;/code&gt; 可以以任意资源提供者支持的方式挂载到主机中。 如下表所示， 不同的提供者
拥有不同能力，每个 PV 的模式可以在对应卷上设置支持的模式。 例如， NFS 支持多客户端读写,但是可以
将一个 NFS PV 以只读方式提供。 每个 PV 都可以独立设置各自的方式模式。&lt;/p&gt;
&lt;p&gt;访问模式有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ReadWriteOnce &amp;ndash; 卷只能被一个消费者以只读方式挂载&lt;/li&gt;
&lt;li&gt;ReadOnlyMany &amp;ndash; 卷能被多个消费者以只读方式挂载&lt;/li&gt;
&lt;li&gt;ReadWriteMany &amp;ndash; 卷能被多个消费者以读写方式挂载&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在命令，这些模式的简写如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RWO - ReadWriteOnce&lt;/li&gt;
&lt;li&gt;ROX - ReadOnlyMany&lt;/li&gt;
&lt;li&gt;RWX - ReadWriteMany&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;重要!&lt;/strong&gt; 一个卷一次只能以一种访问模式挂载，即便它是支持多种的。 例如， 一个 GCEPersistentDisk 可以
在一个消费者挂载为 ReadWriteOnce 或可以在多个消费者挂载为 ReadOnlyMany， 但不能同时挂载
ReadWriteOnce 和 ReadOnlyMany。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Volume Plugin&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteOnce&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadOnlyMany&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;ReadWriteMany&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AWSElasticBlockStore&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AzureFile&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AzureDisk&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CephFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Cinder&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CSI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;基于驱动&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;基于驱动&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;基于驱动&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FC&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FlexVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;基于驱动&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Flocker&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;GCEPersistentDisk&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Glusterfs&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;HostPath&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;iSCSI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Quobyte&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;RBD&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;VsphereVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;- (works when Pods are collocated)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PortworxVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ScaleIO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;StorageOS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
### Class

A PV can have a class, which is specified by setting the
`storageClassName` attribute to the name of a
[StorageClass](/docs/concepts/storage/storage-classes/).
A PV of a particular class can only be bound to PVCs requesting
that class. A PV with no `storageClassName` has no class and can only be bound
to PVCs that request no particular class.

In the past, the annotation `volume.beta.kubernetes.io/storage-class` was used instead
of the `storageClassName` attribute. This annotation is still working; however,
it will become fully deprecated in a future Kubernetes release.
 --&gt;
&lt;h3 id=&#34;class&#34;&gt;类别&lt;/h3&gt;
&lt;p&gt;一个 PV 可以有一个类别， 可以通过 &lt;code&gt;storageClassName&lt;/code&gt; 属性来设置一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes/&#34;&gt;StorageClass&lt;/a&gt;.
一个指定类别的 PV 只能与请求对应类的 PVC 相绑定。 没有设置 &lt;code&gt;storageClassName&lt;/code&gt; 的 PV 是没有类别的
并且只能与没有请求类别的 PVC 绑定。&lt;/p&gt;
&lt;p&gt;在过去是通过 &lt;code&gt;volume.beta.kubernetes.io/storage-class&lt;/code&gt; 注解设置类别而不是 &lt;code&gt;storageClassName&lt;/code&gt;。
这个注解目前还能用；但在未来的版本中会完全废弃。&lt;/p&gt;
&lt;!--
### Reclaim Policy

Current reclaim policies are:

* Retain -- manual reclamation
* Recycle -- basic scrub (`rm -rf /thevolume/*`)
* Delete -- associated storage asset such as AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder volume is deleted

Currently, only NFS and HostPath support recycling. AWS EBS, GCE PD, Azure Disk, and Cinder volumes support deletion.
 --&gt;
&lt;h3 id=&#34;reclain-policy&#34;&gt;回收策略&lt;/h3&gt;
&lt;p&gt;目前支持的回收策略:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Retain &amp;ndash; 手动回收&lt;/li&gt;
&lt;li&gt;Recycle &amp;ndash; 基础清理 (&lt;code&gt;rm -rf /thevolume/*&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Delete &amp;ndash; 关联存储资源如 AWS EBS, GCE PD, Azure Disk, OpenStack Cinder 卷也会被删除。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前只有 NFS 和 HostPath 支持 &lt;code&gt;Recycle&lt;/code&gt;
AWS EBS, GCE PD, Azure Disk, and Cinder 卷支持 &lt;code&gt;Delete&lt;/code&gt;&lt;/p&gt;
&lt;!--
### Mount Options

A Kubernetes administrator can specify additional mount options for when a Persistent Volume is mounted on a node.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Not all Persistent Volume types support mount options.&lt;/div&gt;
&lt;/blockquote&gt;


The following volume types support mount options:

* AWSElasticBlockStore
* AzureDisk
* AzureFile
* CephFS
* Cinder (OpenStack block storage)
* GCEPersistentDisk
* Glusterfs
* NFS
* Quobyte Volumes
* RBD (Ceph Block Device)
* StorageOS
* VsphereVolume
* iSCSI

Mount options are not validated, so mount will simply fail if one is invalid.

In the past, the annotation `volume.beta.kubernetes.io/mount-options` was used instead
of the `mountOptions` attribute. This annotation is still working; however,
it will become fully deprecated in a future Kubernetes release.
 --&gt;
&lt;h3 id=&#34;挂载选项&#34;&gt;挂载选项&lt;/h3&gt;
&lt;p&gt;k8s 管理员可以在 PV 挂载时指定额外的挂载选项。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 不是所有的 PV 类型都支持挂载选项&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;以下卷类型支持挂载选项:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AWSElasticBlockStore&lt;/li&gt;
&lt;li&gt;AzureDisk&lt;/li&gt;
&lt;li&gt;AzureFile&lt;/li&gt;
&lt;li&gt;CephFS&lt;/li&gt;
&lt;li&gt;Cinder (OpenStack block storage)&lt;/li&gt;
&lt;li&gt;GCEPersistentDisk&lt;/li&gt;
&lt;li&gt;Glusterfs&lt;/li&gt;
&lt;li&gt;NFS&lt;/li&gt;
&lt;li&gt;Quobyte Volumes&lt;/li&gt;
&lt;li&gt;RBD (Ceph Block Device)&lt;/li&gt;
&lt;li&gt;StorageOS&lt;/li&gt;
&lt;li&gt;VsphereVolume&lt;/li&gt;
&lt;li&gt;iSCSI&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;挂载选项不会验证，如果其中有无效选项就会挂载失败。&lt;/p&gt;
&lt;p&gt;在过去 使用的是 &lt;code&gt;volume.beta.kubernetes.io/mount-options&lt;/code&gt; 注解设置挂载选项，而不是 &lt;code&gt;mountOptions&lt;/code&gt;
这个注解目前还可以用，但在未来的版本中会完全废弃。&lt;/p&gt;
&lt;!--
### Node Affinity

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; For most volume types, you do not need to set this field. It is automatically populated for &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/#awselasticblockstore&#34;&gt;AWS EBS&lt;/a&gt;, &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/#gcepersistentdisk&#34;&gt;GCE PD&lt;/a&gt; and &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/#azuredisk&#34;&gt;Azure Disk&lt;/a&gt; volume block types. You need to explicitly set this for &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volumes/#local&#34;&gt;local&lt;/a&gt; volumes.&lt;/div&gt;
&lt;/blockquote&gt;


A PV can specify [node affinity](/docs/reference/generated/kubernetes-api/v1.19/#volumenodeaffinity-v1-core) to define constraints that limit what nodes this volume can be accessed from. Pods that use a PV will only be scheduled to nodes that are selected by the node affinity.
 --&gt;
&lt;h3 id=&#34;节点亲和性&#34;&gt;节点亲和性&lt;/h3&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 对于大多数卷类型， 是不需要设置这个字段的。
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#awselasticblockstore&#34;&gt;AWS EBS&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#gcepersistentdisk&#34;&gt;GCE PD&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#azuredisk&#34;&gt;Azure Disk&lt;/a&gt;
卷块设备会自动添加。
但需要为
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#local&#34;&gt;local&lt;/a&gt;
卷需要显示设置该字段&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;PV 可以通过设置
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#volumenodeaffinity-v1-core&#34;&gt;节点亲和性&lt;/a&gt;
来定义限制哪些节点能够访问这个卷。
Pod 只会被调度到节点亲和性选择的节点上。&lt;/p&gt;
&lt;!--
### Phase

A volume will be in one of the following phases:

* Available -- a free resource that is not yet bound to a claim
* Bound -- the volume is bound to a claim
* Released -- the claim has been deleted, but the resource is not yet reclaimed by the cluster
* Failed -- the volume has failed its automatic reclamation

The CLI will show the name of the PVC bound to the PV.
 --&gt;
&lt;h3 id=&#34;阶段&#34;&gt;阶段&lt;/h3&gt;
&lt;p&gt;一个卷会处于以下阶段中的一个:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Available &amp;ndash; 一个空闲资源，还没有被绑定到 PVC&lt;/li&gt;
&lt;li&gt;Bound &amp;ndash; 这个卷被绑定了一个 PVC&lt;/li&gt;
&lt;li&gt;Released &amp;ndash; 绑定的 PVC 已经被删除，但资源还没有被集群回收&lt;/li&gt;
&lt;li&gt;Failed &amp;ndash; 这个卷自动回收失败&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;命令行可以显示与 PV 绑定的 PVC 的名称&lt;/p&gt;
&lt;!--
## PersistentVolumeClaims

Each PVC contains a spec and status, which is the specification and status of the claim.
The name of a PersistentVolumeClaim object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi
  storageClassName: slow
  selector:
    matchLabels:
      release: &#34;stable&#34;
    matchExpressions:
      - {key: environment, operator: In, values: [dev]}
```
 --&gt;
&lt;h2 id=&#34;persistentvolumeclaims&#34;&gt;PersistentVolumeClaim&lt;/h2&gt;
&lt;p&gt;每个 PVC 包含 &lt;code&gt;spec&lt;/code&gt; 和 &lt;code&gt;status&lt;/code&gt;, 其中包含 PVC 的配置定义和状态。
&lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 对象的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myclaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Filesystem&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;8Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;release&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stable&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
      - {&lt;span style=&#34;color:#f92672&#34;&gt;key: environment, operator: In, values&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;dev]}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Access Modes

Claims use the same conventions as volumes when requesting storage with specific access modes.
 --&gt;
&lt;h3 id=&#34;访问模式-1&#34;&gt;访问模式&lt;/h3&gt;
&lt;p&gt;Claims use the same conventions as volumes when requesting storage with specific access modes.
PVC 的访问模式与卷在请求存储时指定的访问模式一至&lt;/p&gt;
&lt;!--
### Volume Modes

Claims use the same convention as volumes to indicate the consumption of the volume as either a filesystem or block device.
 --&gt;
&lt;h3 id=&#34;卷模式-1&#34;&gt;卷模式&lt;/h3&gt;
&lt;p&gt;PVC 的卷模式与卷指定的卷模式一至，可以是 文件系统 或 块设备&lt;/p&gt;
&lt;!--
### Resources

Claims, like Pods, can request specific quantities of a resource. In this case, the request is for storage. The same [resource model](https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md) applies to both volumes and claims.
 --&gt;
&lt;h3 id=&#34;资源&#34;&gt;资源&lt;/h3&gt;
&lt;p&gt;PVC 与 Pod 类似， 可以请求指定数量的资源。 在这种情况下， 请求是提存储。
&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/scheduling/resources.md&#34;&gt;资源模式&lt;/a&gt;
可应用到卷和 PVC&lt;/p&gt;
&lt;!--
### Selector

Claims can specify a [label selector](/docs/concepts/overview/working-with-objects/labels/#label-selectors) to further filter the set of volumes. Only the volumes whose labels match the selector can be bound to the claim. The selector can consist of two fields:

* `matchLabels` - the volume must have a label with this value
* `matchExpressions` - a list of requirements made by specifying key, list of values, and operator that relates the key and values. Valid operators include In, NotIn, Exists, and DoesNotExist.

All of the requirements, from both `matchLabels` and `matchExpressions`, are ANDed together – they must all be satisfied in order to match.
 --&gt;
&lt;h3 id=&#34;选择器&#34;&gt;选择器&lt;/h3&gt;
&lt;p&gt;PVC 可以指定一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/labels/#label-selectors&#34;&gt;标签选择器&lt;/a&gt;
来选择卷集合。 只有与选择器匹配的卷可以与 PVC 绑定。 选择器可以包含以下两个字段&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;matchLabels&lt;/code&gt; - 卷必须要拥有与其对应的标签&lt;/li&gt;
&lt;li&gt;&lt;code&gt;matchExpressions&lt;/code&gt; - 一个由指定键和值列表，以及键和值相应的操作符组成的条件列表，
可用的操作符包含 &lt;code&gt;In&lt;/code&gt;, &lt;code&gt;NotIn&lt;/code&gt;, &lt;code&gt;Exists&lt;/code&gt;, &lt;code&gt;DoesNotExist&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;来自 &lt;code&gt;matchLabels&lt;/code&gt; 和 &lt;code&gt;matchExpressions&lt;/code&gt; 的所有条件以逻辑与方式组合。
必须要满足所有条件才能匹配到。&lt;/p&gt;
&lt;!--
### Class

A claim can request a particular class by specifying the name of a
[StorageClass](/docs/concepts/storage/storage-classes/)
using the attribute `storageClassName`.
Only PVs of the requested class, ones with the same `storageClassName` as the PVC, can
be bound to the PVC.

PVCs don&#39;t necessarily have to request a class. A PVC with its `storageClassName` set
equal to `&#34;&#34;` is always interpreted to be requesting a PV with no class, so it
can only be bound to PVs with no class (no annotation or one set equal to
`&#34;&#34;`). A PVC with no `storageClassName` is not quite the same and is treated differently
by the cluster, depending on whether the
[`DefaultStorageClass` admission plugin](/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)
is turned on.

* If the admission plugin is turned on, the administrator may specify a
  default StorageClass. All PVCs that have no `storageClassName` can be bound only to
  PVs of that default. Specifying a default StorageClass is done by setting the
  annotation `storageclass.kubernetes.io/is-default-class` equal to `true` in
  a StorageClass object. If the administrator does not specify a default, the
  cluster responds to PVC creation as if the admission plugin were turned off. If
  more than one default is specified, the admission plugin forbids the creation of
  all PVCs.
* If the admission plugin is turned off, there is no notion of a default
  StorageClass. All PVCs that have no `storageClassName` can be bound only to PVs that
  have no class. In this case, the PVCs that have no `storageClassName` are treated the
  same way as PVCs that have their `storageClassName` set to `&#34;&#34;`.

Depending on installation method, a default StorageClass may be deployed
to a Kubernetes cluster by addon manager during installation.

When a PVC specifies a `selector` in addition to requesting a StorageClass,
the requirements are ANDed together: only a PV of the requested class and with
the requested labels may be bound to the PVC.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Currently, a PVC with a non-empty &lt;code&gt;selector&lt;/code&gt; can&amp;rsquo;t have a PV dynamically provisioned for it.&lt;/div&gt;
&lt;/blockquote&gt;


In the past, the annotation `volume.beta.kubernetes.io/storage-class` was used instead
of `storageClassName` attribute. This annotation is still working; however,
it won&#39;t be supported in a future Kubernetes release.
 --&gt;
&lt;h3 id=&#34;类别&#34;&gt;类别&lt;/h3&gt;
&lt;p&gt;PVC 可以通过设置 &lt;code&gt;storageClassName&lt;/code&gt; 的值为一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-classes/&#34;&gt;StorageClass&lt;/a&gt;
的名称来指定其使用该类别。
PV 与 PVC 只有在拥有相同 &lt;code&gt;storageClassName&lt;/code&gt; 的情况下才能绑定。&lt;/p&gt;
&lt;p&gt;PVC 并不是必须要设置一个类别。 PVC 可以将 &lt;code&gt;storageClassName&lt;/code&gt; 设置为  &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;， 表示请求一个
没有类别的 PV，因此它也只能与没有类别的 PV 绑定(没有类别注解或类别注解值为 &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;)。
如果 PVC 没有设置 &lt;code&gt;storageClassName&lt;/code&gt;，则会基于是否开启
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass&#34;&gt;&lt;code&gt;DefaultStorageClass&lt;/code&gt; admission plugin&lt;/a&gt;
以下情况而有不同的表现行为:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果准入插件是开启的， 则管理员可以设置一个默认的 &lt;code&gt;StorageClass&lt;/code&gt;。 所有没有设置 &lt;code&gt;storageClassName&lt;/code&gt;
的 PVC 就只能与这个默认类别的 PV 绑定。 通过将 &lt;code&gt;StorageClass&lt;/code&gt; 对象的
&lt;code&gt;storageclass.kubernetes.io/is-default-class&lt;/code&gt; 注解值设置为 &lt;code&gt;true&lt;/code&gt; 可以将其设置默认。
如果管理不有设置默认类别， 集群应答 PVC 创建操作与准入插件关闭相同。 如果设置了不只一个默认
类别，则准入插件会阻止所有 PVC 的创建&lt;/li&gt;
&lt;li&gt;如果准入插件没有打开， 那就没有默认 &lt;code&gt;StorageClass&lt;/code&gt; 这回事。 所有没有设置 &lt;code&gt;storageClassName&lt;/code&gt;
的 PVC 就只能与没有设置类型的 PV 绑定。 在这种情况下， 没有设置 &lt;code&gt;storageClassName&lt;/code&gt; 与
将 &lt;code&gt;storageClassName&lt;/code&gt; 设置为 &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt; 的处理方式是一样的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;基于安装方式， 默认 &lt;code&gt;StorageClass&lt;/code&gt; 可以被插件管理器在安装时加入集群。&lt;/p&gt;
&lt;p&gt;当 PVC 设置 &lt;code&gt;selector&lt;/code&gt; 来请求 StorageClass， 所有条件是逻辑与关系: 只有包含所有选择器需要
的标签的类型才能被绑定的 PVC。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 目前，如果 PVC 的 &lt;code&gt;selector&lt;/code&gt; 是空，则不能实现动态管理 PV&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;在过去， 用的是 &lt;code&gt;volume.beta.kubernetes.io/storage-class&lt;/code&gt; 注解而不是 &lt;code&gt;storageClassName&lt;/code&gt; 属性。
这个注解目前还有用，但在未来版本中会完全废弃。&lt;/p&gt;
&lt;!--
## Claims As Volumes

Pods access storage by using the claim as a volume. Claims must exist in the same namespace as the Pod using the claim. The cluster finds the claim in the Pod&#39;s namespace and uses it to get the PersistentVolume backing the claim. The volume is then mounted to the host and into the Pod.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: &#34;/var/www/html&#34;
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim
```
 --&gt;
&lt;h2 id=&#34;claims-as-volumes&#34;&gt;将 PVC 当作卷(PV)&lt;/h2&gt;
&lt;p&gt;Pod 可以将 PVC 当作卷(PV) 来用作存储。 被引用 PVC 必须要要与 Pod 在同一个命名空间。
集群会在 Pod 所在的命名空间中寻找 PVC 然后使用与其绑定的 PV。 最后将 PV 挂载到主机，最终挂载到
Pod 中。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myfrontend&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/var/www/html&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypd&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypd&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaim&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;claimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myclaim&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### A Note on Namespaces

PersistentVolumes binds are exclusive, and since PersistentVolumeClaims are namespaced objects, mounting claims with &#34;Many&#34; modes (`ROX`, `RWX`) is only possible within one namespace.
 --&gt;
&lt;h3 id=&#34;一个需要注意命名空间的问题&#34;&gt;一个需要注意命名空间的问题&lt;/h3&gt;
&lt;p&gt;PersistentVolume 的绑定是独占的，而又因为 PVC 是命名空间级别的对象，因此在对 PVC 进行多节点
模式(&lt;code&gt;ROX&lt;/code&gt;, &lt;code&gt;RWX&lt;/code&gt;)挂载时只能针对同一个命名空间的节点。&lt;/p&gt;
&lt;!--
## Raw Block Volume Support






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;



The following volume plugins support raw block volumes, including dynamic provisioning where
applicable:

* AWSElasticBlockStore
* AzureDisk
* CSI
* FC (Fibre Channel)
* GCEPersistentDisk
* iSCSI
* Local volume
* OpenStack Cinder
* RBD (Ceph Block Device)
* VsphereVolume
 --&gt;
&lt;h2 id=&#34;raw-block-volume-support&#34;&gt;块设备卷支持&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;以下卷插件支持块设备卷，包含适配的动态管理:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AWSElasticBlockStore&lt;/li&gt;
&lt;li&gt;AzureDisk&lt;/li&gt;
&lt;li&gt;CSI&lt;/li&gt;
&lt;li&gt;FC (Fibre Channel)&lt;/li&gt;
&lt;li&gt;GCEPersistentDisk&lt;/li&gt;
&lt;li&gt;iSCSI&lt;/li&gt;
&lt;li&gt;Local volume&lt;/li&gt;
&lt;li&gt;OpenStack Cinder&lt;/li&gt;
&lt;li&gt;RBD (Ceph Block Device)&lt;/li&gt;
&lt;li&gt;VsphereVolume&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### PersistentVolume using a Raw Block Volume {#persistent-volume-using-a-raw-block-volume}

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: block-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  persistentVolumeReclaimPolicy: Retain
  fc:
    targetWWNs: [&#34;50060e801049cfd1&#34;]
    lun: 0
    readOnly: false
```
 --&gt;
&lt;h3 id=&#34;persistent-volume-using-a-raw-block-volume&#34;&gt;使用块设备卷的 PersistentVolume&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;block-pv&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;capacity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Block&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeReclaimPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Retain&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;fc&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;targetWWNs&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;50060e801049cfd1&amp;#34;&lt;/span&gt;]
    &lt;span style=&#34;color:#f92672&#34;&gt;lun&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### PersistentVolumeClaim requesting a Raw Block Volume {#persistent-volume-claim-requesting-a-raw-block-volume}

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 10Gi
```
 --&gt;
&lt;h3 id=&#34;persistent-volume-claim-requesting-a-raw-block-volume&#34;&gt;申请块设备卷的 PersistentVolumeClaim&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;block-pvc&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Block&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10Gi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Pod specification adding Raw Block Device path in container

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: [&#34;/bin/sh&#34;, &#34;-c&#34;]
      args: [ &#34;tail -f /dev/null&#34; ]
      volumeDevices:
        - name: data
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: block-pvc
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; When adding a raw block device for a Pod, you specify the device path in the container instead of a mount path.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;在-pod-定义中添加容器中的块设备路径&#34;&gt;在 Pod 定义中添加容器中的块设备路径&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod-with-block-volume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fc-container&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fedora:26&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/bin/sh&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-c&amp;#34;&lt;/span&gt;]
      &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tail -f /dev/null&amp;#34;&lt;/span&gt; ]
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeDevices&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;data&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;devicePath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/dev/xvda&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;data&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaim&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;claimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;block-pvc&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 在为一个 Pod 添加块设备时，在容器中指定的是设备路径，而不是挂载路径&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Binding Block Volumes

If a user requests a raw block volume by indicating this using the `volumeMode` field in the PersistentVolumeClaim spec, the binding rules differ slightly from previous releases that didn&#39;t consider this mode as part of the spec.
Listed is a table of possible combinations the user and admin might specify for requesting a raw block device. The table indicates if the volume will be bound or not given the combinations:
Volume binding matrix for statically provisioned volumes:

| PV volumeMode | PVC volumeMode  | Result           |
| --------------|:---------------:| ----------------:|
|   unspecified | unspecified     | BIND             |
|   unspecified | Block           | NO BIND          |
|   unspecified | Filesystem      | BIND             |
|   Block       | unspecified     | NO BIND          |
|   Block       | Block           | BIND             |
|   Block       | Filesystem      | NO BIND          |
|   Filesystem  | Filesystem      | BIND             |
|   Filesystem  | Block           | NO BIND          |
|   Filesystem  | unspecified     | BIND             |

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Only statically provisioned volumes are supported for alpha release. Administrators should take care to consider these values when working with raw block devices.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;绑定块设备&#34;&gt;绑定块设备&lt;/h3&gt;
&lt;p&gt;如果用户在 PVC 配置中使用 &lt;code&gt;volumeMode&lt;/code&gt; 字段指定申请一个块设备卷，则绑定规则与之前版本中
没有在配置中指定 &lt;code&gt;volumeMode&lt;/code&gt; 是有点不一样的。&lt;/p&gt;
&lt;p&gt;以下为用户/管理员在申请块设备时可能的组合列表。 这个表展示了这个组合卷是否能绑定
静态管理卷的绑定矩阵:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;PV volumeMode&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;PVC volumeMode&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Result&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NO BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NO BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NO BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Block&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;NO BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Filesystem&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;未指定&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;BIND&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 在 alpha 特性中只有静态管理的卷被支持。 管理员在操作块设备需要仔细考虑这些值。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Volume Snapshot and Restore Volume from Snapshot Support






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;



Volume snapshot feature was added to support CSI Volume Plugins only. For details, see [volume snapshots](/docs/concepts/storage/volume-snapshots/).

To enable support for restoring a volume from a volume snapshot data source, enable the
`VolumeSnapshotDataSource` feature gate on the apiserver and controller-manager.
 --&gt;
&lt;h2 id=&#34;volume-snapshot-and-restore-volume-from-snapshot-support&#34;&gt;卷快照和快照恢复支持&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;卷快照特性只对 CSI 卷插件支持。详细信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volume-snapshots/&#34;&gt;卷快照&lt;/a&gt;.
要启用对从卷快照恢复的支持，需要在 &lt;code&gt;apiserver&lt;/code&gt; &lt;code&gt;controller-manager&lt;/code&gt; 打开
&lt;code&gt;VolumeSnapshotDataSource&lt;/code&gt; 功能阀&lt;/p&gt;
&lt;!--
### Create a PersistentVolumeClaim from a Volume Snapshot {#create-persistent-volume-claim-from-volume-snapshot}

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: restore-pvc
spec:
  storageClassName: csi-hostpath-sc
  dataSource:
    name: new-snapshot-test
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```
 --&gt;
&lt;h3 id=&#34;create-persistent-volume-claim-from-volume-snapshot&#34;&gt;基于卷快照创建 PVC&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;restore-pvc&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;csi-hostpath-sc&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dataSource&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;new-snapshot-test&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshot&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;apiGroup&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10Gi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Volume Cloning

[Volume Cloning](/docs/concepts/storage/volume-pvc-datasource/) only available for CSI volume plugins.
 --&gt;
&lt;h2 id=&#34;卷克隆&#34;&gt;卷克隆&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volume-pvc-datasource/&#34;&gt;卷克隆&lt;/a&gt; 只存在于 CSI 卷插件&lt;/p&gt;
&lt;!--
### Create PersistentVolumeClaim from an existing PVC {#create-persistent-volume-claim-from-an-existing-pvc}

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: cloned-pvc
spec:
  storageClassName: my-csi-plugin
  dataSource:
    name: existing-src-pvc-name
    kind: PersistentVolumeClaim
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```
 --&gt;
&lt;h3 id=&#34;create-persistent-volume-claim-from-an-existing-pvc&#34;&gt;基于存在的 PVC 创建新的 PVC&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cloned-pvc&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-csi-plugin&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dataSource&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;existing-src-pvc-name&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10Gi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Writing Portable Configuration

If you&#39;re writing configuration templates or examples that run on a wide range of clusters
and need persistent storage, it is recommended that you use the following pattern:

- Include PersistentVolumeClaim objects in your bundle of config (alongside
  Deployments, ConfigMaps, etc).
- Do not include PersistentVolume objects in the config, since the user instantiating
  the config may not have permission to create PersistentVolumes.
- Give the user the option of providing a storage class name when instantiating
  the template.
  - If the user provides a storage class name, put that value into the
    `persistentVolumeClaim.storageClassName` field.
    This will cause the PVC to match the right storage
    class if the cluster has StorageClasses enabled by the admin.
  - If the user does not provide a storage class name, leave the
    `persistentVolumeClaim.storageClassName` field as nil. This will cause a
    PV to be automatically provisioned for the user with the default StorageClass
    in the cluster. Many cluster environments have a default StorageClass installed,
    or administrators can create their own default StorageClass.
- In your tooling, watch for PVCs that are not getting bound after some time
  and surface this to the user, as this may indicate that the cluster has no
  dynamic storage support (in which case the user should create a matching PV)
  or the cluster has no storage system (in which case the user cannot deploy
  config requiring PVCs).

  ## 相关资料


* Learn more about [Creating a PersistentVolume](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume).
* Learn more about [Creating a PersistentVolumeClaim](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim).
* Read the [Persistent Storage design document](https://git.k8s.io/community/contributors/design-proposals/storage/persistent-storage.md).
 --&gt;
&lt;h2 id=&#34;编写可移植的配置&#34;&gt;编写可移植的配置&lt;/h2&gt;
&lt;p&gt;如果要编写运行在大范围集群中并且需要使用到持久化存储的模板或示例配置，建议依照以下模式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在配置 (随同 Deployments, ConfigMaps, 等)时在同一个配置文件中包含其使用的 PVC 对象。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果使用配置的用户可能没有创建 PV 的权限，则不要在配置中包含 PV 对象。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在用户使用模板时，提供设置 StorageClass 名称的选项&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果用户通过 &lt;code&gt;persistentVolumeClaim.storageClassName&lt;/code&gt;  字段设置了 StorageClass 名称。
当集群管理员启用了该 StorageClasses 时， PVC 就能正确使用存储类别。&lt;/li&gt;
&lt;li&gt;如果用户没提供 &lt;code&gt;StorageClass&lt;/code&gt; 名称。 这会导致 &lt;code&gt;persistentVolumeClaim.storageClassName&lt;/code&gt;
值为空。 这样集群中 PV 就会使用默认 &lt;code&gt;StorageClass&lt;/code&gt; 自动管理。 许多集群环境中都都有安装一个
默认的 &lt;code&gt;StorageClass&lt;/code&gt;或管理可能创建自己的默认 &lt;code&gt;StorageClass&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在工具中，在一个时间后观测未绑定的 PVC 并将其传递给用户。 这可能是集群不支持动态存储(这样用户
就要自己创建相应的 PV) 或者集群中没有存储系统(这种情况用户就不能部署包含 PVC 的配置)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
* Learn more about [Creating a PersistentVolume](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume).
* Learn more about [Creating a PersistentVolumeClaim](/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim).
* Read the [Persistent Storage design document](https://git.k8s.io/community/contributors/design-proposals/storage/persistent-storage.md).
 --&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume&#34;&gt;创建 PersistentVolume&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim&#34;&gt;创建 PersistentVolumeClaim&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;参阅 &lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/storage/persistent-storage.md&#34;&gt;持久化存储设计文稿&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#persistentvolume-v1-core&#34;&gt;PersistentVolume&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#persistentvolumespec-v1-core&#34;&gt;PersistentVolumeSpec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#persistentvolumeclaim-v1-core&#34;&gt;PersistentVolumeClaim&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#persistentvolumeclaimspec-v1-core&#34;&gt;PersistentVolumeClaimSpec&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 卷快照</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volume-snapshots/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volume-snapshots/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- saad-ali
- thockin
- msau42
- jingxu97
- xing-yang
- yuxiangqian
title: Volume Snapshots
content_type: concept
weight: 20
---
--&gt;
&lt;!-- overview --&gt;
&lt;!--





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;


In Kubernetes, a _VolumeSnapshot_ represents a snapshot of a volume on a storage system. This document assumes that you are already familiar with Kubernetes [persistent volumes](/docs/concepts/storage/persistent-volumes/).
 --&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;在 k8s 中， &lt;em&gt;VolumeSnapshot&lt;/em&gt; 代表存储系统中一个卷的快照。 在阅读此文之间需要熟悉 k8s
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/&#34;&gt;持久化卷(PV)&lt;/a&gt;.&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Introduction

Similar to how API resources `PersistentVolume` and `PersistentVolumeClaim` are used to provision volumes for users and administrators, `VolumeSnapshotContent` and `VolumeSnapshot` API resources are provided to create volume snapshots for users and administrators.

A `VolumeSnapshotContent` is a snapshot taken from a volume in the cluster that has been provisioned by an administrator. It is a resource in the cluster just like a PersistentVolume is a cluster resource.

A `VolumeSnapshot` is a request for snapshot of a volume by a user. It is similar to a PersistentVolumeClaim.

`VolumeSnapshotClass` allows you to specify different attributes belonging to a `VolumeSnapshot`. These attributes may differ among snapshots taken from the same volume on the storage system and therefore cannot be expressed by using the same `StorageClass` of a `PersistentVolumeClaim`.

Volume snapshots provide Kubernetes users with a standardized way to copy a volume&#39;s contents at a particular point in time without creating an entirely new volume. This functionality enables, for example, database administrators to backup databases before performing edit or delete modifications.

Users need to be aware of the following when using this feature:

* API Objects `VolumeSnapshot`, `VolumeSnapshotContent`, and `VolumeSnapshotClass` are &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/&#39; target=&#39;_blank&#39;&gt;CRDs&lt;span class=&#39;tooltip-text&#39;&gt;Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server.&lt;/span&gt;
&lt;/a&gt;, not part of the core API.
* `VolumeSnapshot` support is only available for CSI drivers.
* As part of the deployment process in the beta version of `VolumeSnapshot`, the Kubernetes team provides a snapshot controller to be deployed into the control plane, and a sidecar helper container called csi-snapshotter to be deployed together with the CSI driver.  The snapshot controller watches `VolumeSnapshot` and `VolumeSnapshotContent` objects and is responsible for the creation and deletion of `VolumeSnapshotContent` object in dynamic provisioning.  The sidecar csi-snapshotter watches `VolumeSnapshotContent` objects and triggers `CreateSnapshot` and `DeleteSnapshot` operations against a CSI endpoint.
* CSI drivers may or may not have implemented the volume snapshot functionality. The CSI drivers that have provided support for volume snapshot will likely use the csi-snapshotter. See [CSI Driver documentation](https://kubernetes-csi.github.io/docs/) for details.
* The CRDs and snapshot controller installations are the responsibility of the Kubernetes distribution.
 --&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;与 &lt;code&gt;PersistentVolume&lt;/code&gt; 和 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; API 资源用来为用户和管理员管理卷相似，
&lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 和 &lt;code&gt;VolumeSnapshot&lt;/code&gt; API 资源是用来为用户和管理员创建卷快照的。&lt;/p&gt;
&lt;p&gt;一个 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 是一个由管理员管理的从集群中一个卷创建的快照。
它是一个与  &lt;code&gt;PersistentVolume&lt;/code&gt; 类似的集群资源。&lt;/p&gt;
&lt;p&gt;一个 &lt;code&gt;VolumeSnapshot&lt;/code&gt; 是由用户申请的对一个卷的快照。 与 PVC 类似。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;VolumeSnapshotClass&lt;/code&gt; 可以指定属于 &lt;code&gt;VolumeSnapshot&lt;/code&gt; 的许多属性。 这些属性可能与其它来自
同一个卷打的快照不一样， 因此不能由使用同样 &lt;code&gt;StorageClass&lt;/code&gt; 的 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 表现。&lt;/p&gt;
&lt;p&gt;卷快照为 k8s 用户提供了一种拷贝特定时间点的卷内容，而不用创建新的卷的标准方式。 这个功能可以
用于数据库管理员在做变更或删除操作之前备份数据库。&lt;/p&gt;
&lt;p&gt;用户在使用该特性时要注意以下几点:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;VolumeSnapshot&lt;/code&gt;, &lt;code&gt;VolumeSnapshotContent&lt;/code&gt;, &lt;code&gt;VolumeSnapshotClass&lt;/code&gt; API 对象是
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/&#39; target=&#39;_blank&#39;&gt;CRDs&lt;span class=&#39;tooltip-text&#39;&gt;Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server.&lt;/span&gt;
&lt;/a&gt;，
不是核心 API 的对象。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VolumeSnapshot&lt;/code&gt; 只支持 CSI 驱动。&lt;/li&gt;
&lt;li&gt;作为 &lt;code&gt;VolumeSnapshot&lt;/code&gt; beta 版本的部署里程， k8s 团队提供了一个部署到控制中心的快照控制器，
和一个叫做 &lt;code&gt;csi-snapshotter&lt;/code&gt; 的边车工具容器和 CSI 驱动部署在一起。 快照控制器监测
&lt;code&gt;VolumeSnapshot&lt;/code&gt; 和 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 对象，并负责动态管理的 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt;
对象的创建和删除。 csi-snapshotter 边车，则监测 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 对象并触发
到 CSI 接口的 &lt;code&gt;CreateSnapshot&lt;/code&gt; 和 &lt;code&gt;DeleteSnapshot&lt;/code&gt; 操作。&lt;/li&gt;
&lt;li&gt;CSI 驱动有可能实现也有可能没有实现快照功能。 提供了卷快照支持的 CSI 驱动通常会使用 &lt;code&gt;csi-snapshotter&lt;/code&gt;。
详细信息见 &lt;a href=&#34;https://kubernetes-csi.github.io/docs/&#34;&gt;CSI 驱动文档&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CRD 和 快照控制器的安装由 k8s 发行版本负责&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Lifecycle of a volume snapshot and volume snapshot content

`VolumeSnapshotContents` are resources in the cluster. `VolumeSnapshots` are requests for those resources. The interaction between `VolumeSnapshotContents` and `VolumeSnapshots` follow this lifecycle:
 --&gt;
&lt;h2 id=&#34;volumesnapshot-和-volumesnapshotcontent-的生命周期&#34;&gt;&lt;code&gt;VolumeSnapshot&lt;/code&gt; 和 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 的生命周期&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;VolumeSnapshotContents&lt;/code&gt; 是集群中的资源。 &lt;code&gt;VolumeSnapshots&lt;/code&gt; 是对这些资源的申请。
&lt;code&gt;VolumeSnapshotContents&lt;/code&gt; 与 &lt;code&gt;VolumeSnapshots&lt;/code&gt; 交互遵循以下周期:&lt;/p&gt;
&lt;!--
### Provisioning Volume Snapshot

There are two ways snapshots may be provisioned: pre-provisioned or dynamically provisioned.
 --&gt;
&lt;h3 id=&#34;卷快照管理&#34;&gt;卷快照管理&lt;/h3&gt;
&lt;p&gt;卷快照管理有两种方式: 静态管理 或 动态管理&lt;/p&gt;
&lt;!--
#### Pre-provisioned {#static}
A cluster administrator creates a number of `VolumeSnapshotContents`. They carry the details of the real volume snapshot on the storage system which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.
 --&gt;
&lt;h4 id=&#34;static&#34;&gt;静态管理&lt;/h4&gt;
&lt;p&gt;由集群管理员创建一定数据的 &lt;code&gt;VolumeSnapshotContents&lt;/code&gt;。 其中包含存储系统中可以被集群用户使用的
真实卷快照的详细信息。 它们存在于 k8s API 并可被消费。&lt;/p&gt;
&lt;!--
#### Dynamic
Instead of using a pre-existing snapshot, you can request that a snapshot to be dynamically taken from a PersistentVolumeClaim. The [VolumeSnapshotClass](/docs/concepts/storage/volume-snapshot-classes/) specifies storage provider-specific parameters to use when taking a snapshot.
 --&gt;
&lt;h4 id=&#34;动态管理&#34;&gt;动态管理&lt;/h4&gt;
&lt;p&gt;与使用静态快照管理不同，可以申请从 PVC 动态创建快照。
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volume-snapshot-classes/&#34;&gt;VolumeSnapshotClass&lt;/a&gt; 可以在
做快照时指定存储提供者相关的参数&lt;/p&gt;
&lt;!--
### Binding

The snapshot controller handles the binding of a `VolumeSnapshot` object with an appropriate `VolumeSnapshotContent` object, in both pre-provisioned and dynamically provisioned scenarios. The binding is a one-to-one mapping.

In the case of pre-provisioned binding, the VolumeSnapshot will remain unbound until the requested VolumeSnapshotContent object is created.
 --&gt;
&lt;h3 id=&#34;绑定&#34;&gt;绑定&lt;/h3&gt;
&lt;p&gt;在静态管理和动态管理的情况下，快照控制器会处理 &lt;code&gt;VolumeSnapshot&lt;/code&gt; 对象与恰当的 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt;
的绑定。 这种绑定关系是一对一的。&lt;/p&gt;
&lt;p&gt;在静态管理绑定时， &lt;code&gt;VolumeSnapshot&lt;/code&gt; 在 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 创建之前都是未绑定状态&lt;/p&gt;
&lt;!--
### Persistent Volume Claim as Snapshot Source Protection

The purpose of this protection is to ensure that in-use
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/persistent-volumes/&#39; target=&#39;_blank&#39;&gt;PersistentVolumeClaim&lt;span class=&#39;tooltip-text&#39;&gt;索要定义在 PersistentVolume 中的存储资源，然后就可以将其以卷的方式挂载到容器中。&lt;/span&gt;
&lt;/a&gt;
API objects are not removed from the system while a snapshot is being taken from it (as this may result in data loss).

While a snapshot is being taken of a PersistentVolumeClaim, that PersistentVolumeClaim is in-use. If you delete a PersistentVolumeClaim API object in active use as a snapshot source, the PersistentVolumeClaim object is not removed immediately. Instead, removal of the PersistentVolumeClaim object is postponed until the snapshot is readyToUse or aborted.
 --&gt;
&lt;h3 id=&#34;当-pvc-作为快照源的保护机制&#34;&gt;当 PVC 作为快照源的保护机制&lt;/h3&gt;
&lt;p&gt;这个保护机制的目的是确保使用中的
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/persistent-volumes/&#39; target=&#39;_blank&#39;&gt;PersistentVolumeClaim&lt;span class=&#39;tooltip-text&#39;&gt;索要定义在 PersistentVolume 中的存储资源，然后就可以将其以卷的方式挂载到容器中。&lt;/span&gt;
&lt;/a&gt;
API 对象在做快照的过程中不会被从系统中删除(因为这可能会导致数据丢失)。&lt;/p&gt;
&lt;p&gt;当一个 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 在用于做快照，则它就在使用中状态。 如果删除了一个正在作为快照
源的 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 对象，它不会马上被删除。 这个删除行为会被延迟到快照的状态
变为 &lt;code&gt;readyToUse&lt;/code&gt; 或 中止(&lt;code&gt;aborted&lt;/code&gt;) 之后。&lt;/p&gt;
&lt;!--
### Delete

Deletion is triggered by deleting the `VolumeSnapshot` object, and the `DeletionPolicy` will be followed. If the `DeletionPolicy` is `Delete`, then the underlying storage snapshot will be deleted along with the `VolumeSnapshotContent` object. If the `DeletionPolicy` is `Retain`, then both the underlying snapshot and `VolumeSnapshotContent` remain.
 --&gt;
&lt;h3 id=&#34;删除&#34;&gt;删除&lt;/h3&gt;
&lt;p&gt;删除由删除 &lt;code&gt;VolumeSnapshot&lt;/code&gt; 对象触发， 会依照 &lt;code&gt;DeletionPolicy&lt;/code&gt; 进行。 如果 &lt;code&gt;DeletionPolicy&lt;/code&gt;
是删除(&lt;code&gt;Delete&lt;/code&gt;)， 则底层的存储快照会连同 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 对象一起删除。 如果
&lt;code&gt;DeletionPolicy&lt;/code&gt; 是保留(&lt;code&gt;Retain&lt;/code&gt;), 则 底层的存储快照 和 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 对象都保留&lt;/p&gt;
&lt;!--
## VolumeSnapshots

Each VolumeSnapshot contains a spec and a status.

```yaml
apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshot
metadata:
  name: new-snapshot-test
spec:
  volumeSnapshotClassName: csi-hostpath-snapclass
  source:
    persistentVolumeClaimName: pvc-test
```

`persistentVolumeClaimName` is the name of the PersistentVolumeClaim data source for the snapshot. This field is required for dynamically provisioning a snapshot.

A volume snapshot can request a particular class by specifying the name of a
[VolumeSnapshotClass](/docs/concepts/storage/volume-snapshot-classes/)
using the attribute `volumeSnapshotClassName`. If nothing is set, then the default class is used if available.

For pre-provisioned snapshots, you need to specify a `volumeSnapshotContentName` as the source for the snapshot as shown in the following example. The `volumeSnapshotContentName` source field is required for pre-provisioned snapshots.

```yaml
apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshot
metadata:
  name: test-snapshot
spec:
  source:
    volumeSnapshotContentName: test-content
```
 --&gt;
&lt;h2 id=&#34;volumesnapshot&#34;&gt;&lt;code&gt;VolumeSnapshot&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;每个 &lt;code&gt;VolumeSnapshot&lt;/code&gt; 包含一个 &lt;code&gt;spec&lt;/code&gt; 和一个 &lt;code&gt;status&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshot&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;new-snapshot-test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeSnapshotClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;csi-hostpath-snapclass&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;source&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;persistentVolumeClaimName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pvc-test&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;persistentVolumeClaimName&lt;/code&gt; is the name of the PersistentVolumeClaim data source for the snapshot. This field is required for dynamically provisioning a snapshot.
&lt;code&gt;persistentVolumeClaimName&lt;/code&gt; 是作为快照数据源的 PVC 的名称。 这个字段在动态管理快照时是必要的。&lt;/p&gt;
&lt;p&gt;A volume snapshot can request a particular class by specifying the name of a
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/docs/concepts/storage/volume-snapshot-classes/&#34;&gt;VolumeSnapshotClass&lt;/a&gt;
using the attribute &lt;code&gt;volumeSnapshotClassName&lt;/code&gt;. If nothing is set, then the default class is used if available.
卷快照时可以通过 &lt;code&gt;volumeSnapshotClassName&lt;/code&gt; 设置一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volume-snapshot-classes/&#34;&gt;VolumeSnapshotClass&lt;/a&gt;
名称来指定特定的类别。
如果没有设置，在有默认的类别时就使用默认的&lt;/p&gt;
&lt;p&gt;对于静态管理的快照，需要向下面例子中所示的指定一个 &lt;code&gt;volumeSnapshotContentName&lt;/code&gt; 作为快照源。
在静态管理快照时 &lt;code&gt;volumeSnapshotContentName&lt;/code&gt; 源字段是必要的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshot&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-snapshot&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;source&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeSnapshotContentName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;test-content&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Volume Snapshot Contents

Each VolumeSnapshotContent contains a spec and status. In dynamic provisioning, the snapshot common controller creates `VolumeSnapshotContent` objects. Here is an example:

```yaml
apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshotContent
metadata:
  name: snapcontent-72d9a349-aacd-42d2-a240-d775650d2455
spec:
  deletionPolicy: Delete
  driver: hostpath.csi.k8s.io
  source:
    volumeHandle: ee0cfb94-f8d4-11e9-b2d8-0242ac110002
  volumeSnapshotClassName: csi-hostpath-snapclass
  volumeSnapshotRef:
    name: new-snapshot-test
    namespace: default
    uid: 72d9a349-aacd-42d2-a240-d775650d2455
```

`volumeHandle` is the unique identifier of the volume created on the storage backend and returned by the CSI driver during the volume creation. This field is required for dynamically provisioning a snapshot. It specifies the volume source of the snapshot.

For pre-provisioned snapshots, you (as cluster administrator) are responsible for creating the `VolumeSnapshotContent` object as follows.

```yaml
apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshotContent
metadata:
  name: new-snapshot-content-test
spec:
  deletionPolicy: Delete
  driver: hostpath.csi.k8s.io
  source:
    snapshotHandle: 7bdd0de3-aaeb-11e8-9aae-0242ac110002
  volumeSnapshotRef:
    name: new-snapshot-test
    namespace: default
```

`snapshotHandle` is the unique identifier of the volume snapshot created on the storage backend. This field is required for the pre-provisioned snapshots. It specifies the CSI snapshot id on the storage system that this `VolumeSnapshotContent` represents.
 --&gt;
&lt;h2 id=&#34;volumesnapshotcontent&#34;&gt;&lt;code&gt;VolumeSnapshotContent&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;每个 VolumeSnapshotContent 都包含一个 &lt;code&gt;spec&lt;/code&gt; 和一个 &lt;code&gt;status&lt;/code&gt;， 在动态管理时，
快照控制器创建 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 对象，下面是一个示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshotContent&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapcontent-72d9a349-aacd-42d2-a240-d775650d2455&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;deletionPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Delete&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;driver&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hostpath.csi.k8s.io&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;source&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;volumeHandle&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ee0cfb94-f8d4-11e9-b2d8-0242ac110002&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeSnapshotClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;csi-hostpath-snapclass&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeSnapshotRef&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;new-snapshot-test&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;uid&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;72d9a349-aacd-42d2-a240-d775650d2455&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;volumeHandle&lt;/code&gt; 是卷在存储后台上的唯一标识，由 CSI 创建卷时返回。 这个字段在动态管理快照时是必要的。
它指定了快照的源(卷)&lt;/p&gt;
&lt;p&gt;对于静态管理的乜是， 集群管理员负责创建如下 &lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 对象。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshotContent&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;new-snapshot-content-test&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;deletionPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Delete&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;driver&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hostpath.csi.k8s.io&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;source&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;snapshotHandle&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;7bdd0de3-aaeb-11e8-9aae-0242ac110002&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeSnapshotRef&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;new-snapshot-test&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;volumeHandle&lt;/code&gt; 是卷在存储后台上的唯一标识， 这个字段在静态管理快照时是必要的。 它指定这个
&lt;code&gt;VolumeSnapshotContent&lt;/code&gt; 在存储系统所代表的 CSI 快照的ID&lt;/p&gt;
&lt;!--
## Provisioning Volumes from Snapshots

You can provision a new volume, pre-populated with data from a snapshot, by using
the *dataSource* field in the `PersistentVolumeClaim` object.

For more details, see
[Volume Snapshot and Restore Volume from Snapshot](/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support).
 --&gt;
&lt;h2 id=&#34;基于快照创建卷&#34;&gt;基于快照创建卷&lt;/h2&gt;
&lt;p&gt;You can provision a new volume, pre-populated with data from a snapshot, by using
the &lt;em&gt;dataSource&lt;/em&gt; field in the &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; object.
可以通过 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 对象的 &lt;em&gt;dataSource&lt;/em&gt; 指定一个快照，将快照的数据预先添加
到新创建的卷中。&lt;/p&gt;
&lt;p&gt;更多详细信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support&#34;&gt;卷快照和从快照中恢复卷&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: CSI 卷克隆</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volume-pvc-datasource/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volume-pvc-datasource/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jsafrane
- saad-ali
- thockin
- msau42
title: CSI Volume Cloning
content_type: concept
weight: 30
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This document describes the concept of cloning existing CSI Volumes in Kubernetes.  Familiarity with [Volumes](/docs/concepts/storage/volumes) is suggested.
 --&gt;
&lt;p&gt;本文主要介绍在 k8s 中克隆已有的 CSI 卷的概念. 建议先熟悉
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes&#34;&gt;Volumes&lt;/a&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Introduction

The &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;CSI&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt; Volume Cloning feature adds support for specifying existing &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/persistent-volumes/&#39; target=&#39;_blank&#39;&gt;PVC&lt;span class=&#39;tooltip-text&#39;&gt;索要定义在 PersistentVolume 中的存储资源，然后就可以将其以卷的方式挂载到容器中。&lt;/span&gt;
&lt;/a&gt;s in the `dataSource` field to indicate a user would like to clone a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;卷(Volume)&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;.

A Clone is defined as a duplicate of an existing Kubernetes Volume that can be consumed as any standard Volume would be.  The only difference is that upon provisioning, rather than creating a &#34;new&#34; empty Volume, the back end device creates an exact duplicate of the specified Volume.

The implementation of cloning, from the perspective of the Kubernetes API, simply adds the ability to specify an existing PVC as a dataSource during new PVC creation. The source PVC must be bound and available (not in use).

Users need to be aware of the following when using this feature:

* Cloning support (`VolumePVCDataSource`) is only available for CSI drivers.
* Cloning support is only available for dynamic provisioners.
* CSI drivers may or may not have implemented the volume cloning functionality.
* You can only clone a PVC when it exists in the same namespace as the destination PVC (source and destination must be in the same namespace).
* Cloning is only supported within the same Storage Class.
    - Destination volume must be the same storage class as the source
    - Default storage class can be used and storageClassName omitted in the spec
* Cloning can only be performed between two volumes that use the same VolumeMode setting (if you request a block mode volume, the source MUST also be block mode)
 --&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;CSI&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt; 卷克隆的特性。它支持在 &lt;code&gt;dataSource&lt;/code&gt;
字段指定现有的
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/persistent-volumes/&#39; target=&#39;_blank&#39;&gt;PVC&lt;span class=&#39;tooltip-text&#39;&gt;索要定义在 PersistentVolume 中的存储资源，然后就可以将其以卷的方式挂载到容器中。&lt;/span&gt;
&lt;/a&gt;
表示用户希望克隆一个
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;卷(Volume)&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这个克隆体是对现有的 k8s 卷的复制，可以用于任意标准卷可用的地方。 区别只有提供时不是创建一个新
的空卷，而对源卷底层存储设备完全复制的卷&lt;/p&gt;
&lt;p&gt;以 k8s API 的角度来看，克隆的实现就是，在创建新的 PVC 时，可以在 &lt;code&gt;dataSource&lt;/code&gt; 字段指定一个
现有的 PVC 作为数据源。 源 PVC 必须绑定且可用(不能在使用中)&lt;/p&gt;
&lt;p&gt;用户在使用该特性时需要注意以下几点:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只有 CSI 驱动才支持克隆 (&lt;code&gt;VolumePVCDataSource&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;只支持在动态供应中使用克隆&lt;/li&gt;
&lt;li&gt;CSI 驱动可能实现或可能没实现克隆功能&lt;/li&gt;
&lt;li&gt;只有源 PVC 必须与目标 PVC 在同一个命名空间才能克隆(源和目标必须在一个命名空间)&lt;/li&gt;
&lt;li&gt;克隆只能发生一同一个存储类别中(StorageClass)
&lt;ul&gt;
&lt;li&gt;目标卷的 存储类别必须与源相同&lt;/li&gt;
&lt;li&gt;当定义中没有指定 &lt;code&gt;storageClassName&lt;/code&gt; 时使用默认存储类别(StorageClass)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;克隆只能在两个相同卷类别(VolumeMode)的卷之间操作(如果请求的是一个块设备卷，源就必须是一个块设备卷)&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Provisioning

Clones are provisioned just like any other PVC with the exception of adding a dataSource that references an existing PVC in the same namespace.

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
    name: clone-of-pvc-1
    namespace: myns
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: cloning
  resources:
    requests:
      storage: 5Gi
  dataSource:
    kind: PersistentVolumeClaim
    name: pvc-1
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You must specify a capacity value for &lt;code&gt;spec.resources.requests.storage&lt;/code&gt;, and the value you specify must be the same or larger than the capacity of the source volume.&lt;/div&gt;
&lt;/blockquote&gt;


The result is a new PVC with the name `clone-of-pvc-1` that has the exact same content as the specified source `pvc-1`.
 --&gt;
&lt;h2 id=&#34;供应&#34;&gt;供应&lt;/h2&gt;
&lt;p&gt;克隆体除了在 &lt;code&gt;dataSource&lt;/code&gt; 指定一个同一命名空间的 PVC 外与任意其它的 PVC 的供应完全相同，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;clone-of-pvc-1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myns&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;cloning&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5Gi&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;dataSource&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pvc-1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 必须要为 &lt;code&gt;spec.resources.requests.storage&lt;/code&gt; 指定一个值，并且这个值必须大于或等于源卷的值&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;最终结果就是新创建一个叫  &lt;code&gt;clone-of-pvc-1&lt;/code&gt; 的PVC 其内容与它的源 &lt;code&gt;pvc-1&lt;/code&gt; 完全一样。&lt;/p&gt;
&lt;!--
## Usage

Upon availability of the new PVC, the cloned PVC is consumed the same as other PVC.  It&#39;s also expected at this point that the newly created PVC is an independent object.  It can be consumed, cloned, snapshotted, or deleted independently and without consideration for it&#39;s original dataSource PVC.  This also implies that the source is not linked in any way to the newly created clone, it may also be modified or deleted without affecting the newly created clone.
 --&gt;
&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;
&lt;p&gt;在可用性上，克隆体的 PVC 与其它的 PVC 一样。 新创建的 PVC 是一个独立的对象。 它可以消费，克隆，创建快照，
并在不影响它源 PVC 的情况下独立的删除。 这就是说克隆的源和目标之间没有任意形式的链接，对源的
修改或删除也不会影响它克隆体&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: StorageClass</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-classes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-classes/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jsafrane
- saad-ali
- thockin
- msau42
title: Storage Classes
content_type: concept
weight: 30
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This document describes the concept of a StorageClass in Kubernetes. Familiarity
with [volumes](/docs/concepts/storage/volumes/) and
[persistent volumes](/docs/concepts/storage/persistent-volumes) is suggested.
 --&gt;
&lt;p&gt;本文介绍 k8s 中的 StorageClass 这个概念。 建议先熟悉
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#34;&gt;卷(Volume)&lt;/a&gt;
和
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes&#34;&gt;持久化卷(PV)&lt;/a&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Introduction

A StorageClass provides a way for administrators to describe the &#34;classes&#34; of
storage they offer. Different classes might map to quality-of-service levels,
or to backup policies, or to arbitrary policies determined by the cluster
administrators. Kubernetes itself is unopinionated about what classes
represent. This concept is sometimes called &#34;profiles&#34; in other storage
systems.
--&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;StorageClass 为管理员提供了一种描述不同存储类别的方式. 不同的类别与 服务质量(QoS)级别，备份策略，
或其它由管理决定的任意策略关联。k8s 本身没不固化出现的类别。 这个概念在某些存储系统中被称为 &amp;ldquo;profiles&amp;rdquo;&lt;/p&gt;
&lt;!--
## The StorageClass Resource

Each StorageClass contains the fields `provisioner`, `parameters`, and
`reclaimPolicy`, which are used when a PersistentVolume belonging to the
class needs to be dynamically provisioned.

The name of a StorageClass object is significant, and is how users can
request a particular class. Administrators set the name and other parameters
of a class when first creating StorageClass objects, and the objects cannot
be updated once they are created.

Administrators can specify a default StorageClass just for PVCs that don&#39;t
request any particular class to bind to: see the
[PersistentVolumeClaim section](/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims)
for details.

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
reclaimPolicy: Retain
allowVolumeExpansion: true
mountOptions:
  - debug
volumeBindingMode: Immediate
```
 --&gt;
&lt;h2 id=&#34;storageclass-资源&#34;&gt;&lt;code&gt;StorageClass&lt;/code&gt; 资源&lt;/h2&gt;
&lt;p&gt;每个 &lt;code&gt;StorageClass&lt;/code&gt; 对象包含 &lt;code&gt;provisioner&lt;/code&gt;, &lt;code&gt;parameters&lt;/code&gt;, &lt;code&gt;reclaimPolicy&lt;/code&gt; 字段，
当有被该类别的 PV 需要被动态供给时会用到。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;StorageClass&lt;/code&gt; 对象的名称是有意义的，它会被用户在申请该类别存储时用到。 管理员在第一次创建
&lt;code&gt;StorageClass&lt;/code&gt; 对象时设置名称和其它的参数， 这些对象在创建后将不可修改。&lt;/p&gt;
&lt;p&gt;管理员可以指定一个默认的 &lt;code&gt;StorageClass&lt;/code&gt;，那些没有指定类别的 PVC 就会使用这个默认的类别:
详细信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims&#34;&gt;PersistentVolumeClaim 章节&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;standard&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/aws-ebs&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gp2&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;reclaimPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Retain&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;allowVolumeExpansion&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;mountOptions&lt;/span&gt;:
  - &lt;span style=&#34;color:#ae81ff&#34;&gt;debug&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;volumeBindingMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Immediate&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Provisioner

Each StorageClass has a provisioner that determines what volume plugin is used
for provisioning PVs. This field must be specified.

| Volume Plugin        | Internal Provisioner| Config Example                       |
| :---                 |     :---:           |    :---:                             |
| AWSElasticBlockStore | &amp;#x2713;            | [AWS EBS](#aws-ebs)                          |
| AzureFile            | &amp;#x2713;            | [Azure File](#azure-file)            |
| AzureDisk            | &amp;#x2713;            | [Azure Disk](#azure-disk)            |
| CephFS               | -                   | -                                    |
| Cinder               | &amp;#x2713;            | [OpenStack Cinder](#openstack-cinder)|
| FC                   | -                   | -                                    |
| FlexVolume           | -                   | -                                    |
| Flocker              | &amp;#x2713;            | -                                    |
| GCEPersistentDisk    | &amp;#x2713;            | [GCE PD](#gce-pd)                          |
| Glusterfs            | &amp;#x2713;            | [Glusterfs](#glusterfs)              |
| iSCSI                | -                   | -                                    |
| Quobyte              | &amp;#x2713;            | [Quobyte](#quobyte)                  |
| NFS                  | -                   | -                                    |
| RBD                  | &amp;#x2713;            | [Ceph RBD](#ceph-rbd)                |
| VsphereVolume        | &amp;#x2713;            | [vSphere](#vsphere)                  |
| PortworxVolume       | &amp;#x2713;            | [Portworx Volume](#portworx-volume)  |
| ScaleIO              | &amp;#x2713;            | [ScaleIO](#scaleio)                  |
| StorageOS            | &amp;#x2713;            | [StorageOS](#storageos)              |
| Local                | -                   | [Local](#local)              |

You are not restricted to specifying the &#34;internal&#34; provisioners
listed here (whose names are prefixed with &#34;kubernetes.io&#34; and shipped
alongside Kubernetes). You can also run and specify external provisioners,
which are independent programs that follow a [specification](https://git.k8s.io/community/contributors/design-proposals/storage/volume-provisioning.md)
defined by Kubernetes. Authors of external provisioners have full discretion
over where their code lives, how the provisioner is shipped, how it needs to be
run, what volume plugin it uses (including Flex), etc. The repository
[kubernetes-sigs/sig-storage-lib-external-provisioner](https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner)
houses a library for writing external provisioners that implements the bulk of
the specification. Some external provisioners are listed under the repository
[kubernetes-sigs/sig-storage-lib-external-provisioner](https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner).

For example, NFS doesn&#39;t provide an internal provisioner, but an external
provisioner can be used. There are also cases when 3rd party storage
vendors provide their own external provisioner.
 --&gt;
&lt;h3 id=&#34;供应者provisioner&#34;&gt;供应者(Provisioner)&lt;/h3&gt;
&lt;p&gt;每个 &lt;code&gt;StorageClass&lt;/code&gt; 都有一个供应者，这个供应者决定供给 PV 的卷插件。 这个字段必须指定。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Volume Plugin&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Internal Provisioner&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Config Example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AWSElasticBlockStore&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#aws-ebs&#34;&gt;AWS EBS&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AzureFile&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#azure-file&#34;&gt;Azure File&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;AzureDisk&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#azure-disk&#34;&gt;Azure Disk&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CephFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Cinder&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#openstack-cinder&#34;&gt;OpenStack Cinder&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FC&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FlexVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Flocker&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;GCEPersistentDisk&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#gce-pd&#34;&gt;GCE PD&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Glusterfs&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#glusterfs&#34;&gt;Glusterfs&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;iSCSI&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Quobyte&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#quobyte&#34;&gt;Quobyte&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;NFS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;RBD&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#ceph-rbd&#34;&gt;Ceph RBD&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;VsphereVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#vsphere&#34;&gt;vSphere&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PortworxVolume&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#portworx-volume&#34;&gt;Portworx Volume&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;ScaleIO&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#scaleio&#34;&gt;ScaleIO&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;StorageOS&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#storageos&#34;&gt;StorageOS&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Local&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;-&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;a href=&#34;#local&#34;&gt;Local&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;用户并不仅限于上面列举的&amp;quot;内部&amp;quot;供应者(这些名称以 &amp;ldquo;kubernetes.io&amp;rdquo; 前缀的是随同 k8s 发行一起的)。
也可以运行和指定外部的供应者，这些是依照由 k8s 定义的
&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/storage/volume-provisioning.md&#34;&gt;specification&lt;/a&gt;
独立程序。 外部供应者的开发者可以完全自主地决定代码存入在哪， 供应者程序是什么发布的， 运行需要什么，
使用什么卷插件(包括 Flex)，等等。
&lt;a href=&#34;https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner&#34;&gt;kubernetes-sigs/sig-storage-lib-external-provisioner&lt;/a&gt;
仓库中包含了编写外部供应者需要实现的一系列规格说明。 一些外部提供也列举在这个仓库中&lt;/p&gt;
&lt;p&gt;例如， NFS 没有提供内部的供应者，就可以使用一个外部的供应都。 还有种情况是第三方存储提供都
也会提供自己的外部供应者。&lt;/p&gt;
&lt;!--
### Reclaim Policy

PersistentVolumes that are dynamically created by a StorageClass will have the
reclaim policy specified in the `reclaimPolicy` field of the class, which can be
either `Delete` or `Retain`. If no `reclaimPolicy` is specified when a
StorageClass object is created, it will default to `Delete`.

PersistentVolumes that are created manually and managed via a StorageClass will have
whatever reclaim policy they were assigned at creation.
 --&gt;
&lt;h3 id=&#34;回收策略&#34;&gt;回收策略&lt;/h3&gt;
&lt;p&gt;由 &lt;code&gt;StorageClass&lt;/code&gt; 动态创建的持久化卷(PV)将通过 &lt;code&gt;StorageClass&lt;/code&gt; 的 &lt;code&gt;reclaimPolicy&lt;/code&gt; 字段
设备回收策略，这些策略可以是 &lt;code&gt;Delete&lt;/code&gt; 或 &lt;code&gt;Retain&lt;/code&gt;。 如果在创建 &lt;code&gt;StorageClass&lt;/code&gt; 对象的时候
没有指定 &lt;code&gt;reclaimPolicy&lt;/code&gt;， 默认回收策略为 &lt;code&gt;Delete&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;由手动创建并通过 &lt;code&gt;StorageClass&lt;/code&gt; 管理的 持久化卷(PV) 会在创建的时候指定回收策略&lt;/p&gt;
&lt;!--
### Allow Volume Expansion






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [beta]&lt;/code&gt;
&lt;/div&gt;



PersistentVolumes can be configured to be expandable. This feature when set to `true`,
allows the users to resize the volume by editing the corresponding PVC object.

The following types of volumes support volume expansion, when the underlying
StorageClass has the field `allowVolumeExpansion` set to true.






&lt;table&gt;&lt;caption style=&#34;display: none;&#34;&gt;Table of Volume types and the version of Kubernetes they require&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Volume type&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Required Kubernetes version&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;gcePersistentDisk&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;awsElasticBlockStore&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Cinder&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;glusterfs&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;rbd&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Azure File&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Azure Disk&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Portworx&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FlexVolume&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CSI&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.14 (alpha), 1.16 (beta)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;



&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You can only use the volume expansion feature to grow a Volume, not to shrink it.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;允许卷扩容&#34;&gt;允许卷扩容&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;持久化卷(PV) 可以设置为可扩展的。当开启该特性后允许用户通过修改对应 PVC 对象的方式修改卷的容量。&lt;/p&gt;
&lt;p&gt;以下类型的卷在底层 &lt;code&gt;StorageClass&lt;/code&gt; 的 &lt;code&gt;allowVolumeExpansion&lt;/code&gt; 字段设置为 &lt;code&gt;true&lt;/code&gt;,时支持卷扩展。&lt;/p&gt;





&lt;table&gt;&lt;caption style=&#34;display: none;&#34;&gt;Table of Volume types and the version of Kubernetes they require&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;卷类型&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;需要的 k8s 版本&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;gcePersistentDisk&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;awsElasticBlockStore&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Cinder&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;glusterfs&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;rbd&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Azure File&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Azure Disk&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Portworx&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;FlexVolume&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;CSI&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;1.14 (alpha), 1.16 (beta)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 只能使用卷扩展特性扩充卷，不能缩小&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Mount Options

PersistentVolumes that are dynamically created by a StorageClass will have the
mount options specified in the `mountOptions` field of the class.

If the volume plugin does not support mount options but mount options are
specified, provisioning will fail. Mount options are not validated on either
the class or PV, so mount of the PV will simply fail if one is invalid.
 --&gt;
&lt;h3 id=&#34;挂载选项&#34;&gt;挂载选项&lt;/h3&gt;
&lt;p&gt;由 &lt;code&gt;StorageClass&lt;/code&gt; 动态创建的持久化卷(PV)会拥有由 &lt;code&gt;StorageClass&lt;/code&gt; &lt;code&gt;mountOptions&lt;/code&gt; 字段指定
的挂载选项。&lt;/p&gt;
&lt;p&gt;如果卷插件不支持挂载选项但又指定了挂载选项，供应就会失败。 &lt;code&gt;StorageClass&lt;/code&gt; 或 PV 挂载选项
不是有效的， 如果其中有一个无效则挂载就会失败。&lt;/p&gt;
&lt;!--
### Volume Binding Mode

The `volumeBindingMode` field controls when [volume binding and dynamic
provisioning](/docs/concepts/storage/persistent-volumes/#provisioning) should occur.

By default, the `Immediate` mode indicates that volume binding and dynamic
provisioning occurs once the PersistentVolumeClaim is created. For storage
backends that are topology-constrained and not globally accessible from all Nodes
in the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod&#39;s scheduling
requirements. This may result in unschedulable Pods.

A cluster administrator can address this issue by specifying the `WaitForFirstConsumer` mode which
will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created.
PersistentVolumes will be selected or provisioned conforming to the topology that is
specified by the Pod&#39;s scheduling constraints. These include, but are not limited to, [resource
requirements](/docs/concepts/configuration/manage-resources-containers/),
[node selectors](/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector),
[pod affinity and
anti-affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity),
and [taints and tolerations](/docs/concepts/scheduling-eviction/taint-and-toleration).

The following plugins support `WaitForFirstConsumer` with dynamic provisioning:

* [AWSElasticBlockStore](#aws-ebs)
* [GCEPersistentDisk](#gce-pd)
* [AzureDisk](#azure-disk)

The following plugins support `WaitForFirstConsumer` with pre-created PersistentVolume binding:

* All of the above
* [Local](#local)






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [stable]&lt;/code&gt;
&lt;/div&gt;


[CSI volumes](/docs/concepts/storage/volumes/#csi) are also supported with dynamic provisioning
and pre-created PVs, but you&#39;ll need to look at the documentation for a specific CSI driver
to see its supported topology keys and examples.
 --&gt;
&lt;h3 id=&#34;volume-binding-mode&#34;&gt;卷绑定模式&lt;/h3&gt;
&lt;p&gt;当
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/#provisioning&#34;&gt;卷绑定和动态供应&lt;/a&gt;
发生时由 &lt;code&gt;volumeBindingMode&lt;/code&gt; 字段控制。&lt;/p&gt;
&lt;p&gt;默认情况下使用的是 &lt;code&gt;Immediate&lt;/code&gt; 模式，这种模式表示在 PersistentVolumeClaim 对象创建后立即
进行卷绑定和动态供应。对于有拓扑限制的存储后台和并不是集群中所有节点都可以访问的存储时，
持久化卷(PV) 会在不知道 Pod 调度要求的情况下绑定或供应。这可能会导致 Pod 不可调度。&lt;/p&gt;
&lt;p&gt;要避免这个问题，管理员可以将卷模式设置为 &lt;code&gt;WaitForFirstConsumer&lt;/code&gt;，这样持久化卷(PV)的绑定和
供应会推迟到使用这个 PVC 的 Pod 创建之后。此时持久化卷(PV)在供应时会确认由 Pod 指定的调度约束。
这些限制包括但不限于
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/manage-resources-containers/&#34;&gt;资源需求&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector&#34;&gt;节点选择器&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity&#34;&gt;Pod 亲和性和反亲和性&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration&#34;&gt;毒点和耐受&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;以下插件支持带动态供应的 &lt;code&gt;WaitForFirstConsumer&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#aws-ebs&#34;&gt;AWSElasticBlockStore&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gce-pd&#34;&gt;GCEPersistentDisk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#azure-disk&#34;&gt;AzureDisk&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下插件支持在预先创建 持久化卷(PV)绑定的 &lt;code&gt;WaitForFirstConsumer&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;上面所有&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#local&#34;&gt;Local&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#34;&gt;CSI 卷&lt;/a&gt; 也支持动态供应和预创建 PV，但需要先
查看对应 CSI 驱动的文档，看看支持的拓扑键和示例。&lt;/p&gt;
&lt;!--
### Allowed Topologies

When a cluster operator specifies the `WaitForFirstConsumer` volume binding mode, it is no longer necessary
to restrict provisioning to specific topologies in most situations. However,
if still required, `allowedTopologies` can be specified.

This example demonstrates how to restrict the topology of provisioned volumes to specific
zones and should be used as a replacement for the `zone` and `zones` parameters for the
supported plugins.

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
volumeBindingMode: WaitForFirstConsumer
allowedTopologies:
- matchLabelExpressions:
  - key: failure-domain.beta.kubernetes.io/zone
    values:
    - us-central1-a
    - us-central1-b
```
 --&gt;
&lt;h3 id=&#34;allowed-topologies&#34;&gt;允许的拓扑&lt;/h3&gt;
&lt;p&gt;当集群中的卷绑定模式被设置为 &lt;code&gt;WaitForFirstConsumer&lt;/code&gt; 时，在大多数情况下就不供应严格受限于
指定拓扑。 但如果仍然需要这些限制，可以通过 &lt;code&gt;allowedTopologies&lt;/code&gt; 指定。&lt;/p&gt;
&lt;p&gt;以下的示例中展示的是怎么通过设置区域来限制供应的卷拓扑，如果插件支持，这些限制会用来替换插件中的 &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 参数&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;standard&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/gce-pd&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pd-standard&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;volumeBindingMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;WaitForFirstConsumer&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;allowedTopologies&lt;/span&gt;:
- &lt;span style=&#34;color:#f92672&#34;&gt;matchLabelExpressions&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;failure-domain.beta.kubernetes.io/zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;us-central1-a&lt;/span&gt;
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;us-central1-b&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Parameters

Storage Classes have parameters that describe volumes belonging to the storage
class. Different parameters may be accepted depending on the `provisioner`. For
 example, the value `io1`, for the parameter `type`, and the parameter
`iopsPerGB` are specific to EBS. When a parameter is omitted, some default is
used.

There can be at most 512 parameters defined for a StorageClass.
The total length of the parameters object including its keys and values cannot
exceed 256 KiB.
 --&gt;
&lt;h2 id=&#34;参数&#34;&gt;参数&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;StorageClass&lt;/code&gt; 有些参数，这些参数描述属于该存储类别的卷。 基于不同的 &lt;code&gt;provisioner&lt;/code&gt; 可以接受
不同的参数。 例如， 对于 &lt;code&gt;type&lt;/code&gt; 的参数值为 &lt;code&gt;io1&lt;/code&gt;， &lt;code&gt;iopsPerGB&lt;/code&gt; 参数的值为 &lt;code&gt;EBS&lt;/code&gt;。 当一个参数
没有设置时，就会使用默认值。&lt;/p&gt;
&lt;p&gt;对于每个 &lt;code&gt;StorageClass&lt;/code&gt; 最多可以定义 &lt;code&gt;512&lt;/code&gt; 个参数。参数对象的总长度，包含其键和值不能超过
256 KiB&lt;/p&gt;
&lt;!--
### AWS EBS

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/aws-ebs
parameters:
  type: io1
  iopsPerGB: &#34;10&#34;
  fsType: ext4
```

* `type`: `io1`, `gp2`, `sc1`, `st1`. See
  [AWS docs](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html)
  for details. Default: `gp2`.
* `zone` (Deprecated): AWS zone. If neither `zone` nor `zones` is specified, volumes are
  generally round-robin-ed across all active zones where Kubernetes cluster
  has a node. `zone` and `zones` parameters must not be used at the same time.
* `zones` (Deprecated): A comma separated list of AWS zone(s). If neither `zone` nor `zones`
  is specified, volumes are generally round-robin-ed across all active zones
  where Kubernetes cluster has a node. `zone` and `zones` parameters must not
  be used at the same time.
* `iopsPerGB`: only for `io1` volumes. I/O operations per second per GiB. AWS
  volume plugin multiplies this with size of requested volume to compute IOPS
  of the volume and caps it at 20 000 IOPS (maximum supported by AWS, see
  [AWS docs](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html).
  A string is expected here, i.e. `&#34;10&#34;`, not `10`.
* `fsType`: fsType that is supported by kubernetes. Default: `&#34;ext4&#34;`.
* `encrypted`: denotes whether the EBS volume should be encrypted or not.
  Valid values are `&#34;true&#34;` or `&#34;false&#34;`. A string is expected here,
  i.e. `&#34;true&#34;`, not `true`.
* `kmsKeyId`: optional. The full Amazon Resource Name of the key to use when
  encrypting the volume. If none is supplied but `encrypted` is true, a key is
  generated by AWS. See AWS docs for valid ARN value.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;code&gt;zone&lt;/code&gt; and &lt;code&gt;zones&lt;/code&gt; parameters are deprecated and replaced with
&lt;a href=&#34;#allowed-topologies&#34;&gt;allowedTopologies&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;aws-ebs&#34;&gt;AWS EBS&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/aws-ebs&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;io1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;iopsPerGB&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;type&lt;/code&gt;: &lt;code&gt;io1&lt;/code&gt;, &lt;code&gt;gp2&lt;/code&gt;, &lt;code&gt;sc1&lt;/code&gt;, &lt;code&gt;st1&lt;/code&gt;. 默认: &lt;code&gt;gp2&lt;/code&gt;
详细信息见 &lt;a href=&#34;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html&#34;&gt;AWS 文档&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;zone&lt;/code&gt; (废弃): AWS 区域。如果 &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 都没有设置， 卷会在 k8s 集群中所有有节点的
活跃区别之间随机调度&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;zones&lt;/code&gt; (废弃): 一个用逗号分隔的 AWS 区域列表。 如果没有设置，卷会在 k8s 集群中所有有节点的
活跃区别之间随机调度。 &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 参数一定不要同时使用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;iopsPerGB&lt;/code&gt;: 仅限 &lt;code&gt;io1&lt;/code&gt; 卷。每秒每GiB I/O 操作数。AWS 会将这个值乘以申请的卷大小得出
卷的 IOPS， 最高为 20 000 IOPS (AWS 支持的最大值, 见
&lt;a href=&#34;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html&#34;&gt;AWS 文档&lt;/a&gt;.
这个的值是一个字段串，也就是这样 &lt;code&gt;&amp;quot;10&amp;quot;&lt;/code&gt;， 而不是 &lt;code&gt;10&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;fsType&lt;/code&gt;: k8s 支持的文件系统类型。 默认: &lt;code&gt;&amp;quot;ext4&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;encrypted&lt;/code&gt;: 表示这个 EBS 卷是否使用加密。 有效的值是 &lt;code&gt;&amp;quot;true&amp;quot;&lt;/code&gt; 或 &lt;code&gt;&amp;quot;false&amp;quot;&lt;/code&gt;
这里的值也是字符串，也就是 &lt;code&gt;&amp;quot;true&amp;quot;&lt;/code&gt;, 而不是 &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;kmsKeyId&lt;/code&gt;: 可选。 在对卷加密时且的亚马逊资源全名的键。 如果这个值没提供但 &lt;code&gt;encrypted&lt;/code&gt;
设置为 &amp;ldquo;true&amp;rdquo;, AWS 就会生成一个键。 关于有效的 ARN 值见 AWS 文档。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 参数已经废弃，被 &lt;a href=&#34;#allowed-topologies&#34;&gt;允许的拓扑&lt;/a&gt; 替换&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### GCE PD

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
  fstype: ext4
  replication-type: none
```

* `type`: `pd-standard` or `pd-ssd`. Default: `pd-standard`
* `zone` (Deprecated): GCE zone. If neither `zone` nor `zones` is specified, volumes are
  generally round-robin-ed across all active zones where Kubernetes cluster has
  a node. `zone` and `zones` parameters must not be used at the same time.
* `zones` (Deprecated): A comma separated list of GCE zone(s). If neither `zone` nor `zones`
  is specified, volumes are generally round-robin-ed across all active zones
  where Kubernetes cluster has a node. `zone` and `zones` parameters must not
  be used at the same time.
* `fstype`: `ext4` or `xfs`. Default: `ext4`. The defined filesystem type must be supported by the host operating system.

* `replication-type`: `none` or `regional-pd`. Default: `none`.

If `replication-type` is set to `none`, a regular (zonal) PD will be provisioned.

If `replication-type` is set to `regional-pd`, a
[Regional Persistent Disk](https://cloud.google.com/compute/docs/disks/#repds)
will be provisioned. It&#39;s highly recommended to have
`volumeBindingMode: WaitForFirstConsumer` set, in which case when you create
a Pod that consumes a PersistentVolumeClaim which uses this StorageClass, a
Regional Persistent Disk is provisioned with two zones. One zone is the same
as the zone that the Pod is scheduled in. The other zone is randomly picked
from the zones available to the cluster. Disk zones can be further constrained
using `allowedTopologies`.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;code&gt;zone&lt;/code&gt; and &lt;code&gt;zones&lt;/code&gt; parameters are deprecated and replaced with
&lt;a href=&#34;#allowed-topologies&#34;&gt;allowedTopologies&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;gce-pd&#34;&gt;GCE PD&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/gce-pd&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pd-standard&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;fstype&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replication-type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;none&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;type&lt;/code&gt;: &lt;code&gt;pd-standard&lt;/code&gt; 或 &lt;code&gt;pd-ssd&lt;/code&gt;. 默认: &lt;code&gt;pd-standard&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;zone&lt;/code&gt; (废弃): GCE 区域. 如果 &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 都没有设置， 卷会在 k8s 集群中所有有节点的
活跃区别之间随机调度， &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 参数一定不要同时使用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;zones&lt;/code&gt; (废弃): 一个用逗号分隔的 GCE 区域列表。 如果没有设置，卷会在 k8s 集群中所有有节点的
活跃区别之间随机调度。 &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 参数一定不要同时使用。&lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 参数一定不要同时使用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;fstype&lt;/code&gt;: &lt;code&gt;ext4&lt;/code&gt; 或 &lt;code&gt;xfs&lt;/code&gt;. 默认: &lt;code&gt;ext4&lt;/code&gt;. 定义的文件系统类型必须被主机操作系统支持&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;replication-type&lt;/code&gt;: &lt;code&gt;none&lt;/code&gt; 或 &lt;code&gt;regional-pd&lt;/code&gt;. 默认: &lt;code&gt;none&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果 &lt;code&gt;replication-type&lt;/code&gt; 设置为 &lt;code&gt;none&lt;/code&gt;， 会供应一个常规的 PD。&lt;/p&gt;
&lt;p&gt;如果 &lt;code&gt;replication-type&lt;/code&gt; 设置为 &lt;code&gt;regional-pd&lt;/code&gt;， 会供应一个
&lt;a href=&#34;https://cloud.google.com/compute/docs/disks/#repds&#34;&gt;Regional Persistent Disk&lt;/a&gt;
强烈推荐同时设置 &lt;code&gt;volumeBindingMode: WaitForFirstConsumer&lt;/code&gt;。 在这种情况下，当创建一个
消费使用该 &lt;code&gt;StorageClass&lt;/code&gt; 的 PVC 时， 会供应两个区域持久化盘。 一个区域与 Pod 调度的区域相同，
另一个则随机到集群中其它的可用区域。 硬盘区域还可以使用  &lt;code&gt;allowedTopologies&lt;/code&gt; 添加更多多限制。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;code&gt;zone&lt;/code&gt; 和 &lt;code&gt;zones&lt;/code&gt; 参数已经废弃，被 &lt;a href=&#34;#allowed-topologies&#34;&gt;允许的拓扑&lt;/a&gt; 替换&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Glusterfs

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: &#34;http://127.0.0.1:8081&#34;
  clusterid: &#34;630372ccdc720a92c681fb928f27b53f&#34;
  restauthenabled: &#34;true&#34;
  restuser: &#34;admin&#34;
  secretNamespace: &#34;default&#34;
  secretName: &#34;heketi-secret&#34;
  gidMin: &#34;40000&#34;
  gidMax: &#34;50000&#34;
  volumetype: &#34;replicate:3&#34;
```

* `resturl`: Gluster REST service/Heketi service url which provision gluster
  volumes on demand. The general format should be `IPaddress:Port` and this is
  a mandatory parameter for GlusterFS dynamic provisioner. If Heketi service is
  exposed as a routable service in openshift/kubernetes setup, this can have a
  format similar to `http://heketi-storage-project.cloudapps.mystorage.com`
  where the fqdn is a resolvable Heketi service url.
* `restauthenabled` : Gluster REST service authentication boolean that enables
  authentication to the REST server. If this value is `&#34;true&#34;`, `restuser` and
  `restuserkey` or `secretNamespace` + `secretName` have to be filled. This
  option is deprecated, authentication is enabled when any of `restuser`,
  `restuserkey`, `secretName` or `secretNamespace` is specified.
* `restuser` : Gluster REST service/Heketi user who has access to create volumes
  in the Gluster Trusted Pool.
* `restuserkey` : Gluster REST service/Heketi user&#39;s password which will be used
  for authentication to the REST server. This parameter is deprecated in favor
  of `secretNamespace` + `secretName`.
* `secretNamespace`, `secretName` : Identification of Secret instance that
  contains user password to use when talking to Gluster REST service. These
  parameters are optional, empty password will be used when both
  `secretNamespace` and `secretName` are omitted. The provided secret must have
  type `&#34;kubernetes.io/glusterfs&#34;`, for example created in this way:

    ```
    kubectl create secret generic heketi-secret \
      --type=&#34;kubernetes.io/glusterfs&#34; --from-literal=key=&#39;opensesame&#39; \
      --namespace=default
    ```

    Example of a secret can be found in
    [glusterfs-provisioning-secret.yaml](https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/glusterfs/glusterfs-secret.yaml).

* `clusterid`: `630372ccdc720a92c681fb928f27b53f` is the ID of the cluster
  which will be used by Heketi when provisioning the volume. It can also be a
  list of clusterids, for example:
  `&#34;8452344e2becec931ece4e33c4674e4e,42982310de6c63381718ccfa6d8cf397&#34;`. This
  is an optional parameter.
* `gidMin`, `gidMax` : The minimum and maximum value of GID range for the
  StorageClass. A unique value (GID) in this range ( gidMin-gidMax ) will be
  used for dynamically provisioned volumes. These are optional values. If not
  specified, the volume will be provisioned with a value between 2000-2147483647
  which are defaults for gidMin and gidMax respectively.
* `volumetype` : The volume type and its parameters can be configured with this
  optional value. If the volume type is not mentioned, it&#39;s up to the provisioner
  to decide the volume type.

    For example:
    * Replica volume: `volumetype: replicate:3` where &#39;3&#39; is replica count.
    * Disperse/EC volume: `volumetype: disperse:4:2` where &#39;4&#39; is data and &#39;2&#39; is the redundancy count.
    * Distribute volume: `volumetype: none`

    For available volume types and administration options, refer to the
    [Administration Guide](https://access.redhat.com/documentation/en-US/Red_Hat_Storage/3.1/html/Administration_Guide/part-Overview.html).

    For further reference information, see
    [How to configure Heketi](https://github.com/heketi/heketi/wiki/Setting-up-the-topology).

    When persistent volumes are dynamically provisioned, the Gluster plugin
    automatically creates an endpoint and a headless service in the name
    `gluster-dynamic-&lt;claimname&gt;`. The dynamic endpoint and service are automatically
    deleted when the persistent volume claim is deleted.
 --&gt;
&lt;h3 id=&#34;glusterfs&#34;&gt;Glusterfs&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/glusterfs&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;resturl&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://127.0.0.1:8081&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;clusterid&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;630372ccdc720a92c681fb928f27b53f&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;restauthenabled&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;restuser&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;admin&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;secretNamespace&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;secretName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;heketi-secret&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;gidMin&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;40000&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;gidMax&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;50000&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumetype&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;replicate:3&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;resturl&lt;/code&gt;: Gluster REST 服务/Heketi 服务 url, 用来根据需要供应 gluster 卷。 通常格式为
&lt;code&gt;IPaddress:Port&lt;/code&gt; 这是 GlusterFS 动态供应的必要参数。 如果 Heketi 服务是可路由的服务
在 openshift/kubernetes 设置中提供。 这会有一个类似 &lt;code&gt;http://heketi-storage-project.cloudapps.mystorage.com&lt;/code&gt;
格式的地址，其中的 fqdn 是 Heketi 服务的可解析 url.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;restauthenabled&lt;/code&gt; :Gluster REST 服务是否开启认证。 如果这个值是 &lt;code&gt;&amp;quot;true&amp;quot;&lt;/code&gt;， 则必须要提供
&lt;code&gt;restuser&lt;/code&gt; 和 &lt;code&gt;restuserkey&lt;/code&gt; 或 &lt;code&gt;secretNamespace&lt;/code&gt; + &lt;code&gt;secretName&lt;/code&gt;。 这个选项已经废弃
当 &lt;code&gt;restuser&lt;/code&gt;, &lt;code&gt;restuserkey&lt;/code&gt;, &lt;code&gt;secretName&lt;/code&gt; 或 &lt;code&gt;secretNamespace&lt;/code&gt; 有值时，默认就启用&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;restuser&lt;/code&gt; : Gluster REST 服务/Heketi 中可以访问并在 Gluster Trusted Pool 中创建卷的用户。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;restuserkey&lt;/code&gt; : Gluster REST 服务/Heketi 用户的密码，用于 REST 服务认证。 这个参数
已经废弃，推荐使用 &lt;code&gt;secretNamespace&lt;/code&gt; + &lt;code&gt;secretName&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;secretNamespace&lt;/code&gt;, &lt;code&gt;secretName&lt;/code&gt; :  确定在访问 Gluster REST 服务用户密码的 Secret
实例是哪个。这两个参数为可选， 如果它们都没有设置，则会使用空密码。 提供的 Secret 必要是
&lt;code&gt;&amp;quot;kubernetes.io/glusterfs&amp;quot;&lt;/code&gt; 类型的。 创建示例&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create secret generic heketi-secret \
  --type=&amp;quot;kubernetes.io/glusterfs&amp;quot; --from-literal=key=&#39;opensesame&#39; \
  --namespace=default
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;使用 Secret 的示例在这里
&lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/glusterfs/glusterfs-secret.yaml&#34;&gt;glusterfs-provisioning-secret.yaml&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;clusterid&lt;/code&gt;: &lt;code&gt;630372ccdc720a92c681fb928f27b53f&lt;/code&gt; 集群 ID，会在 Heketi 供应卷时用到。
也可以是集群 ID 的列表，例如: &lt;code&gt;&amp;quot;8452344e2becec931ece4e33c4674e4e,42982310de6c63381718ccfa6d8cf397&amp;quot;&lt;/code&gt;
这个参数为可选&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;gidMin&lt;/code&gt;, &lt;code&gt;gidMax&lt;/code&gt; : StorageClass GID 范围的最小值和最大值， 在这个范围(&lt;code&gt;gidMin&lt;/code&gt;-&lt;code&gt;gidMax&lt;/code&gt;)
中的一个唯一值(GID) 会用于动态供应卷。 这两个参数为可选。 如果没有指定，被供应卷的范围值为
&lt;code&gt;2000-2147483647&lt;/code&gt; 也就对应着默认的最小值(&lt;code&gt;gidMin&lt;/code&gt;)和最大值(&lt;code&gt;gidMax&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;volumetype&lt;/code&gt; : 卷类型及其参数可以使用这个参数配置，这是一个可选参数。 如果卷类型没有指定，
则其类型由供应者决定。&lt;/p&gt;
&lt;p&gt;例如:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;复制型卷: &lt;code&gt;volumetype: replicate:3&lt;/code&gt; 其中 3 是副本数&lt;/li&gt;
&lt;li&gt;Disperse/EC 卷 : &lt;code&gt;volumetype: disperse:4:2&lt;/code&gt; 其中 4 是数据 2 是冗余数&lt;/li&gt;
&lt;li&gt;分布式卷: &lt;code&gt;volumetype: none&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于可用的卷类型及其管理选项，见
&lt;a href=&#34;https://access.redhat.com/documentation/en-US/Red_Hat_Storage/3.1/html/Administration_Guide/part-Overview.html&#34;&gt;Administration Guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;更多信息见
&lt;a href=&#34;https://github.com/heketi/heketi/wiki/Setting-up-the-topology&#34;&gt;怎么配置 Heketi&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;当持久化卷是被动态供应时， Gluster 会自动创建一个 Endpoint 和一个无头 Service, 名称都是
&lt;code&gt;gluster-dynamic-&amp;lt;claimname&amp;gt;&lt;/code&gt;. 当 PVC 被删除时，对应的 Endpoint 和 Service 也会
动态自动删除。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### OpenStack Cinder

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gold
provisioner: kubernetes.io/cinder
parameters:
  availability: nova
```

* `availability`: Availability Zone. If not specified, volumes are generally
  round-robin-ed across all active zones where Kubernetes cluster has a node.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [deprecated]&lt;/code&gt;
&lt;/div&gt;
&lt;p&gt;This internal provisioner of OpenStack is deprecated. Please use &lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack&#34;&gt;the external cloud provider for OpenStack&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

--&gt;
&lt;h3 id=&#34;openstack-cinder&#34;&gt;OpenStack Cinder&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;gold&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/cinder&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;availability&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nova&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;availability&lt;/code&gt;: 可用区域。 如果没有设置， 卷会在所有有 k8s 节点的区域中随机&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.11 [deprecated]&lt;/code&gt;
&lt;/div&gt;
&lt;p&gt;这个 OpenStack 内部供应都已经废弃。 请使用
&lt;a href=&#34;https://github.com/kubernetes/cloud-provider-openstack&#34;&gt;OpenStack 外部云提供者&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### vSphere

There are two types of provisioners for vSphere storage classes:

- [CSI provisioner](#csi-provisioner): `csi.vsphere.vmware.com`
- [vCP provisioner](#vcp-provisioner): `kubernetes.io/vsphere-volume`

In-tree provisioners are [deprecated](/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/#why-are-we-migrating-in-tree-plugins-to-csi). For more information on the CSI provisioner, see [Kubernetes vSphere CSI Driver](https://vsphere-csi-driver.sigs.k8s.io/) and [vSphereVolume CSI migration](/docs/concepts/storage/volumes/#csi-migration-5).
 --&gt;
&lt;h3 id=&#34;vsphere&#34;&gt;vSphere&lt;/h3&gt;
&lt;p&gt;vSphere 存储类别有两种类型的供应者:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#vsphere-provisioner-csi&#34;&gt;CSI 供应者&lt;/a&gt;: &lt;code&gt;csi.vsphere.vmware.com&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vcp-provisioner&#34;&gt;vCP 供应者&lt;/a&gt;: &lt;code&gt;kubernetes.io/vsphere-volume&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;内部供应都已经
&lt;a href=&#34;https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/#why-are-we-migrating-in-tree-plugins-to-csi&#34;&gt;废弃&lt;/a&gt;
更多关于 CSI 供应者的信息见
&lt;a href=&#34;https://vsphere-csi-driver.sigs.k8s.io/&#34;&gt;Kubernetes vSphere CSI Driver&lt;/a&gt; and &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi-migration-5&#34;&gt;vSphereVolume CSI migration&lt;/a&gt;.&lt;/p&gt;
&lt;!--
#### CSI Provisioner {#vsphere-provisioner-csi}

The vSphere CSI StorageClass provisioner works with Tanzu Kubernetes clusters. For an example, refer to the [vSphere CSI repository](https://raw.githubusercontent.com/kubernetes-sigs/vsphere-csi-driver/master/example/vanilla-k8s-file-driver/example-sc.yaml).
 --&gt;
&lt;h4 id=&#34;vsphere-provisioner-csi&#34;&gt;CSI 供应者&lt;/h4&gt;
&lt;p&gt;vSphere CSI StorageClass 供应者工作于 Tanzu k8s 集群中。 示例见
&lt;a href=&#34;https://raw.githubusercontent.com/kubernetes-sigs/vsphere-csi-driver/master/example/vanilla-k8s-file-driver/example-sc.yaml&#34;&gt;vSphere CSI 仓库&lt;/a&gt;.&lt;/p&gt;
&lt;!--
#### vCP Provisioner

The following examples use the VMware Cloud Provider (vCP) StorageClass provisioner.  

1. Create a StorageClass with a user specified disk format.

    ```yaml
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: fast
    provisioner: kubernetes.io/vsphere-volume
    parameters:
      diskformat: zeroedthick
    ```

    `diskformat`: `thin`, `zeroedthick` and `eagerzeroedthick`. Default: `&#34;thin&#34;`.

2. Create a StorageClass with a disk format on a user specified datastore.

    ```yaml
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: fast
    provisioner: kubernetes.io/vsphere-volume
    parameters:
        diskformat: zeroedthick
        datastore: VSANDatastore
    ```

    `datastore`: The user can also specify the datastore in the StorageClass.
    The volume will be created on the datastore specified in the StorageClass,
    which in this case is `VSANDatastore`. This field is optional. If the
    datastore is not specified, then the volume will be created on the datastore
    specified in the vSphere config file used to initialize the vSphere Cloud
    Provider.

3. Storage Policy Management inside kubernetes

    * Using existing vCenter SPBM policy

        One of the most important features of vSphere for Storage Management is
        policy based Management. Storage Policy Based Management (SPBM) is a
        storage policy framework that provides a single unified control plane
        across a broad range of data services and storage solutions. SPBM enables
        vSphere administrators to overcome upfront storage provisioning challenges,
        such as capacity planning, differentiated service levels and managing
        capacity headroom.

        The SPBM policies can be specified in the StorageClass using the
        `storagePolicyName` parameter.

    * Virtual SAN policy support inside Kubernetes

        Vsphere Infrastructure (VI) Admins will have the ability to specify custom
        Virtual SAN Storage Capabilities during dynamic volume provisioning. You
        can now define storage requirements, such as performance and availability,
        in the form of storage capabilities during dynamic volume provisioning.
        The storage capability requirements are converted into a Virtual SAN
        policy which are then pushed down to the Virtual SAN layer when a
        persistent volume (virtual disk) is being created. The virtual disk is
        distributed across the Virtual SAN datastore to meet the requirements.

        You can see [Storage Policy Based Management for dynamic provisioning of volumes](https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/policy-based-mgmt.html)
        for more details on how to use storage policies for persistent volumes
        management.

There are few
[vSphere examples](https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere)
which you try out for persistent volume management inside Kubernetes for vSphere.
 --&gt;
&lt;h4 id=&#34;vcp-provisioner&#34;&gt;vCP 供应者&lt;/h4&gt;
&lt;p&gt;The following examples use the VMware Cloud Provider (vCP) StorageClass provisioner.&lt;br&gt;
以下示例使用 VMware Cloud Provider (vCP) StorageClass 应用者。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;创建一个用户指定硬盘模式的 StorageClass&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fast&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/vsphere-volume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;diskformat&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zeroedthick&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;diskformat&lt;/code&gt;: &lt;code&gt;thin&lt;/code&gt;, &lt;code&gt;zeroedthick&lt;/code&gt; 和 &lt;code&gt;eagerzeroedthick&lt;/code&gt;. 默认: &lt;code&gt;&amp;quot;thin&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建一个用于指定数据源和基于该数据库的磁盘模式的 StorageClass&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fast&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/vsphere-volume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;diskformat&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zeroedthick&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;datastore&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VSANDatastore&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;datastore&lt;/code&gt;:  用户也可以在 StorageClass 中指定数据源(datastore). 卷会在 StorageClass
指定的数据源上创建，本例中就是 &lt;code&gt;VSANDatastore&lt;/code&gt;. 该字段为可选。 如果没有指定数据源，
则会使用 vSphere Cloud Provider 初始化时使用的 vSphere 配置文件中指定的数据源来创建卷。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 k8s 中的存储策略管理&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;使用已经存在的 vCenter SPBM 策略&lt;/p&gt;
&lt;p&gt;vSphere 对于存储管理的重要特性之一就是基于策略的管理。 基于策略的存储管理(SPBM)是一个
存储策略框架， 它为大范围的数据服务和存储方案提供一个统一的控制台。 SPBM 让 vSphere
管理员能够应对存储供应的挑战，如 容量计划，细分服务级别，可用空间管理(capacity headroom)&lt;/p&gt;
&lt;p&gt;SPBM 可能通过 StorageClass 中的 &lt;code&gt;storagePolicyName&lt;/code&gt; 来指定。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;k8s 内部支持的虚拟 SAN 策略&lt;/p&gt;
&lt;p&gt;Vsphere 基础设施 (VI) 管理员可以在动态卷供应时指定自定义的虚拟 SAN 存储能力。
这时候可以在动态供应时以存储能力的形式定义存储要求， 如性能和可用性。 在一个持久化卷(虚拟磁盘)
被创建时，存储能力要求会被转换成虚拟 SAN 策略，再被推到虚拟 SAN 层。虚拟磁盘会发布在
满足要求的虚拟 SAN 数据中。&lt;/p&gt;
&lt;p&gt;更多关于怎么使用持久化卷管理的存储策略见
&lt;a href=&#34;https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/policy-based-mgmt.html&#34;&gt;Storage Policy Based Management for dynamic provisioning of volumes&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些
&lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere&#34;&gt;vSphere 示例&lt;/a&gt;
可以用来熟悉用于 vSphere 的 k8s 集群中的持久化卷管理&lt;/p&gt;
&lt;!--
### Ceph RBD

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/rbd
parameters:
  monitors: 10.16.153.105:6789
  adminId: kube
  adminSecretName: ceph-secret
  adminSecretNamespace: kube-system
  pool: kube
  userId: kube
  userSecretName: ceph-secret-user
  userSecretNamespace: default
  fsType: ext4
  imageFormat: &#34;2&#34;
  imageFeatures: &#34;layering&#34;
```

* `monitors`: Ceph monitors, comma delimited. This parameter is required.
* `adminId`: Ceph client ID that is capable of creating images in the pool.
  Default is &#34;admin&#34;.
* `adminSecretName`: Secret Name for `adminId`. This parameter is required.
  The provided secret must have type &#34;kubernetes.io/rbd&#34;.
* `adminSecretNamespace`: The namespace for `adminSecretName`. Default is &#34;default&#34;.
* `pool`: Ceph RBD pool. Default is &#34;rbd&#34;.
* `userId`: Ceph client ID that is used to map the RBD image. Default is the
  same as `adminId`.
* `userSecretName`: The name of Ceph Secret for `userId` to map RBD image. It
  must exist in the same namespace as PVCs. This parameter is required.
  The provided secret must have type &#34;kubernetes.io/rbd&#34;, for example created in this
  way:

    ```shell
    kubectl create secret generic ceph-secret --type=&#34;kubernetes.io/rbd&#34; \
      --from-literal=key=&#39;QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ==&#39; \
      --namespace=kube-system
    ```
* `userSecretNamespace`: The namespace for `userSecretName`.
* `fsType`: fsType that is supported by kubernetes. Default: `&#34;ext4&#34;`.
* `imageFormat`: Ceph RBD image format, &#34;1&#34; or &#34;2&#34;. Default is &#34;2&#34;.
* `imageFeatures`: This parameter is optional and should only be used if you
  set `imageFormat` to &#34;2&#34;. Currently supported features are `layering` only.
  Default is &#34;&#34;, and no features are turned on.
 --&gt;
&lt;h3 id=&#34;ceph-rbd&#34;&gt;Ceph RBD&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fast&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/rbd&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;monitors&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10.16.153.105&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;6789&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;adminId&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;adminSecretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ceph-secret&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;adminSecretNamespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube-system&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;pool&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;userId&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;userSecretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ceph-secret-user&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;userSecretNamespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;imageFormat&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;imageFeatures&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;layering&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;monitors&lt;/code&gt;: Ceph monitor, 逗号分隔，这是个必要参数&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;adminId&lt;/code&gt;: 能够在 pool 中创建镜像的 Ceph 客户 ID， 默认为 &lt;code&gt;&amp;quot;admin&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;adminSecretName&lt;/code&gt;: &lt;code&gt;adminId&lt;/code&gt; 的 &lt;code&gt;Secret&lt;/code&gt; 名称。 这是个必要参数。 这个 &lt;code&gt;Secret&lt;/code&gt;
必须包含 &amp;ldquo;kubernetes.io/rbd&amp;rdquo; 类型&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;adminSecretNamespace&lt;/code&gt;: &lt;code&gt;adminSecretName&lt;/code&gt; 对应的命名空间。 默认为 &lt;code&gt;&amp;quot;default&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;pool&lt;/code&gt;: Ceph RBD pool。 默认是 &lt;code&gt;&amp;quot;rbd&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;userId&lt;/code&gt;: 用来与 RBD 镜像关联的 Ceph 客户 ID。 默认与 &lt;code&gt;adminId&lt;/code&gt; 相同。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;userSecretName&lt;/code&gt;: 与 RBD image 镜像关联的 Ceph 客户 的 &lt;code&gt;Secret&lt;/code&gt;。 这个  &lt;code&gt;Secret&lt;/code&gt; 必须
与 PVC 在同一个命名空间。 这是一个必要参数。这个  &lt;code&gt;Secret&lt;/code&gt;必须包含 &amp;ldquo;kubernetes.io/rbd&amp;rdquo; 类型。
以下为创建示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret generic ceph-secret --type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/rbd&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ==&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  --namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;userSecretNamespace&lt;/code&gt;: &lt;code&gt;userSecretName&lt;/code&gt; 的命名空间&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;fsType&lt;/code&gt;: 受 k8s 支持的文件系统。 默认 &lt;code&gt;&amp;quot;ext4&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;imageFormat&lt;/code&gt;: Ceph RBD image 模式，&amp;ldquo;1&amp;rdquo; or &amp;ldquo;2&amp;rdquo;. 默认为 &amp;ldquo;2&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;imageFeatures&lt;/code&gt;: 这个参数为可选，但只用在 &lt;code&gt;imageFormat&lt;/code&gt; 设置为 &amp;ldquo;2&amp;rdquo; 时。 目前支持的特性
只有 &lt;code&gt;layering&lt;/code&gt;， 默认是 &lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;，表示没有开启任何特性。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Quobyte

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: slow
provisioner: kubernetes.io/quobyte
parameters:
    quobyteAPIServer: &#34;http://138.68.74.142:7860&#34;
    registry: &#34;138.68.74.142:7861&#34;
    adminSecretName: &#34;quobyte-admin-secret&#34;
    adminSecretNamespace: &#34;kube-system&#34;
    user: &#34;root&#34;
    group: &#34;root&#34;
    quobyteConfig: &#34;BASE&#34;
    quobyteTenant: &#34;DEFAULT&#34;
```

* `quobyteAPIServer`: API Server of Quobyte in the format
  `&#34;http(s)://api-server:7860&#34;`
* `registry`: Quobyte registry to use to mount the volume. You can specify the
  registry as ``&lt;host&gt;:&lt;port&gt;`` pair or if you want to specify multiple
  registries you just have to put a comma between them e.q.
  ``&lt;host1&gt;:&lt;port&gt;,&lt;host2&gt;:&lt;port&gt;,&lt;host3&gt;:&lt;port&gt;``.
  The host can be an IP address or if you have a working DNS you can also
  provide the DNS names.
* `adminSecretNamespace`: The namespace for `adminSecretName`.
  Default is &#34;default&#34;.
* `adminSecretName`: secret that holds information about the Quobyte user and
  the password to authenticate against the API server. The provided secret
  must have type &#34;kubernetes.io/quobyte&#34; and the keys `user` and `password`,
  for example:

    ```shell
    kubectl create secret generic quobyte-admin-secret \
      --type=&#34;kubernetes.io/quobyte&#34; --from-literal=user=&#39;admin&#39; --from-literal=password=&#39;opensesame&#39; \
      --namespace=kube-system
    ```

* `user`: maps all access to this user. Default is &#34;root&#34;.
* `group`: maps all access to this group. Default is &#34;nfsnobody&#34;.
* `quobyteConfig`: use the specified configuration to create the volume. You
  can create a new configuration or modify an existing one with the Web
  console or the quobyte CLI. Default is &#34;BASE&#34;.
* `quobyteTenant`: use the specified tenant ID to create/delete the volume.
  This Quobyte tenant has to be already present in Quobyte.
  Default is &#34;DEFAULT&#34;.
 --&gt;
&lt;h3 id=&#34;quobyte&#34;&gt;Quobyte&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
   &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/quobyte&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;quobyteAPIServer&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://138.68.74.142:7860&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;registry&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;138.68.74.142:7861&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;adminSecretName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;quobyte-admin-secret&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;adminSecretNamespace&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kube-system&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;user&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;root&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;group&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;root&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;quobyteConfig&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;BASE&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;quobyteTenant&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;DEFAULT&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;quobyteAPIServer&lt;/code&gt;:  &lt;code&gt;&amp;quot;http(s)://api-server:7860&amp;quot;&lt;/code&gt; 格式的 Quobyte API 服务&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;registry&lt;/code&gt;: 用于挂载卷的 Quobyte 注册中心。 注册中心的格式为 &lt;code&gt;&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;&lt;/code&gt;， 如果要
设置多个则用逗号分隔，如 &lt;code&gt;&amp;lt;host1&amp;gt;:&amp;lt;port&amp;gt;,&amp;lt;host2&amp;gt;:&amp;lt;port&amp;gt;,&amp;lt;host3&amp;gt;:&amp;lt;port&amp;gt;&lt;/code&gt;.
其中 host 是一个 IP 地址或如果有 DNS 服务，则可以使用 DNS 名称。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;adminSecretName&lt;/code&gt;: 用于存放 Quobyte API 服务对应用户密码信息的 &lt;code&gt;Secret&lt;/code&gt;. 这个 &lt;code&gt;Secret&lt;/code&gt;
必须包含 &amp;ldquo;kubernetes.io/quobyte&amp;rdquo; 类型和 &lt;code&gt;user&lt;/code&gt; 和 &lt;code&gt;password&lt;/code&gt; 键。
示例创建命令如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret generic quobyte-admin-secret &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  --type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/quobyte&amp;#34;&lt;/span&gt; --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;user&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;admin&amp;#39;&lt;/span&gt; --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;password&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;opensesame&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  --namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;adminSecretNamespace&lt;/code&gt;: &lt;code&gt;adminSecretName&lt;/code&gt; 所属的命名空间，默认为 &lt;code&gt;&amp;quot;default&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;user&lt;/code&gt;: 将所有访问指向这个用户。 默认为 &lt;code&gt;&amp;quot;root&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;group&lt;/code&gt;: 将所有访问指向这个用户组。 默认为 &lt;code&gt;&amp;quot;nfsnobody&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;quobyteConfig&lt;/code&gt;: 使用这个配置创建卷。 可通过控制台 quobyte 命令行或以创建一个新的配置
或修改已经存在的配置。 默认为 &amp;ldquo;BASE&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;quobyteTenant&lt;/code&gt;: 使用指定租户 ID 来创建/删除卷。 这个 Quobyte 租户必须是已经存在于 Quobyte 中。
默认为 &amp;ldquo;DEFAULT&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Azure Disk

#### Azure Unmanaged Disk storage class {#azure-unmanaged-disk-storage-class}

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/azure-disk
parameters:
  skuName: Standard_LRS
  location: eastus
  storageAccount: azure_storage_account_name
```

* `skuName`: Azure storage account Sku tier. Default is empty.
* `location`: Azure storage account location. Default is empty.
* `storageAccount`: Azure storage account name. If a storage account is provided,
  it must reside in the same resource group as the cluster, and `location` is
  ignored. If a storage account is not provided, a new storage account will be
  created in the same resource group as the cluster.
 --&gt;
&lt;h3 id=&#34;azure-disk&#34;&gt;Azure 磁盘&lt;/h3&gt;
&lt;h4 id=&#34;azure-unmanaged-disk-storage-class&#34;&gt;Azure 非托管磁盘 StorageClass&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/azure-disk&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;skuName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Standard_LRS&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;location&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;eastus&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageAccount&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;azure_storage_account_name&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;skuName&lt;/code&gt;: Azure 存储账户 Sku 层。 默认为空&lt;/li&gt;
&lt;li&gt;&lt;code&gt;location&lt;/code&gt;: Azure 存储账户位置。 默认为空&lt;/li&gt;
&lt;li&gt;&lt;code&gt;storageAccount&lt;/code&gt;: Azure 存储账户名称。 如果提供了存储账户，必须与集群在同一个资源组，
同时 &lt;code&gt;location&lt;/code&gt; 会被忽略。 如果没有提供存储账户，会在集群所在资源组创建一个新的存储账户。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;azure-disk-storage-class&#34;&gt;Azure 磁盘 StorageClass (starting from v1.7.2)&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/azure-disk&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;storageaccounttype&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Standard_LRS&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Shared&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;storageaccounttype&lt;/code&gt;: Azure 存储账户类型。 默认值 (原文档有问题，暂不知道默认值是啥)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kind&lt;/code&gt;: 可用值有 &lt;code&gt;shared&lt;/code&gt; (默认), &lt;code&gt;dedicated&lt;/code&gt;, &lt;code&gt;managed&lt;/code&gt;.
当 &lt;code&gt;kind&lt;/code&gt; 是 &lt;code&gt;shared&lt;/code&gt; 时，所有非托管的硬盘会在与集群同一个资源组创建几个分享存储账户。
当 &lt;code&gt;kind&lt;/code&gt; 是 &lt;code&gt;dedicated&lt;/code&gt; 时，在与集群同一个资源组中会为每一个新创建的非托管磁盘创建一个独立
的存储账户。
当 &lt;code&gt;kind&lt;/code&gt; 是 &lt;code&gt;managed&lt;/code&gt; 时，所有托管的磁盘都会创建在与集群同一个资源组&lt;/li&gt;
&lt;li&gt;&lt;code&gt;resourceGroup&lt;/code&gt;: 指定 Azure 磁盘创建的资源组。 必须是一个已经存在的资源组名称。 如果没有指定，
磁盘会放在与当前 k8s 集群所在的这个资源组。&lt;/li&gt;
&lt;li&gt;高级的 VM 可以挂载 Standard_LRS 和 Premium_LRS 磁盘，标准 VM 只可以挂载 Standard_LRS 磁盘。&lt;/li&gt;
&lt;li&gt;托管 VM 只能挂载托管磁盘，非托管 VM 只能挂载 非托管&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Azure File

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: azurefile
provisioner: kubernetes.io/azure-file
parameters:
  skuName: Standard_LRS
  location: eastus
  storageAccount: azure_storage_account_name
```

* `skuName`: Azure storage account Sku tier. Default is empty.
* `location`: Azure storage account location. Default is empty.
* `storageAccount`: Azure storage account name.  Default is empty. If a storage
  account is not provided, all storage accounts associated with the resource
  group are searched to find one that matches `skuName` and `location`. If a
  storage account is provided, it must reside in the same resource group as the
  cluster, and `skuName` and `location` are ignored.
* `secretNamespace`: the namespace of the secret that contains the Azure Storage
  Account Name and Key. Default is the same as the Pod.
* `secretName`: the name of the secret that contains the Azure Storage Account Name and
  Key. Default is `azure-storage-account-&lt;accountName&gt;-secret`
* `readOnly`: a flag indicating whether the storage will be mounted as read only.
  Defaults to false which means a read/write mount. This setting will impact the
  `ReadOnly` setting in VolumeMounts as well.

During storage provisioning, a secret named by `secretName` is created for the
mounting credentials. If the cluster has enabled both
[RBAC](/docs/reference/access-authn-authz/rbac/) and
[Controller Roles](/docs/reference/access-authn-authz/rbac/#controller-roles),
add the `create` permission of resource `secret` for clusterrole
`system:controller:persistent-volume-binder`.

In a multi-tenancy context, it is strongly recommended to set the value for
`secretNamespace` explicitly, otherwise the storage account credentials may
be read by other users.
 --&gt;
&lt;h3 id=&#34;azure-file&#34;&gt;Azure 文件&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;azurefile&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/azure-file&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;skuName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Standard_LRS&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;location&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;eastus&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageAccount&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;azure_storage_account_name&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;skuName&lt;/code&gt;: Azure 存储账户 Sku 层。 默认为空&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;location&lt;/code&gt;: Azure 存储账户位置。 默认为空&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;storageAccount&lt;/code&gt;: Azure 存储账户名称。 默认为空。 如果没有提供存储账户，会搜索与资源组
关联所有的存储账户中查找一个匹配 &lt;code&gt;skuName&lt;/code&gt; 和 &lt;code&gt;location&lt;/code&gt; 的那个。如果提供了存储账户
则必须是与集群在同一个资源组，并且 &lt;code&gt;skuName&lt;/code&gt; 和 &lt;code&gt;location&lt;/code&gt; 会被忽略。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;secretNamespace&lt;/code&gt;: 包含 Azure 存储账户名称的 key 的 Secret 所在的命名空间。 默认与 Pod
相同&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;secretName&lt;/code&gt;: 包含 Azure 存储账户名称的 key 的 Secret 的名称。 默认为
&lt;code&gt;azure-storage-account-&amp;lt;accountName&amp;gt;-secret&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;readOnly&lt;/code&gt;: 一个标记这个存储是否以只读方式挂载的标记。默认为 false 也就是以读写方式挂载。
这个配置会与 VolumeMount 的 &lt;code&gt;ReadOnly&lt;/code&gt; 设置有冲突&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在存储供应时会为挂载凭证创建一个名字为 &lt;code&gt;secretName&lt;/code&gt; 字段值的 Secret。如果集群启用了
&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/rbac/&#34;&gt;RBAC&lt;/a&gt; 和
&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/rbac/#controller-roles&#34;&gt;Controller Roles&lt;/a&gt;,
为要 &lt;code&gt;clusterrole&lt;/code&gt; &lt;code&gt;system:controller:persistent-volume-binder&lt;/code&gt; 中的 &lt;code&gt;secret&lt;/code&gt; 资源添加
&lt;code&gt;create&lt;/code&gt; 权限。&lt;/p&gt;
&lt;p&gt;在一个多租户的上下文下， 强烈建议显示地设置 &lt;code&gt;secretNamespace&lt;/code&gt; 值， 否则存储账户的凭据可能被
其它用户访问。&lt;/p&gt;
&lt;!--
### Portworx Volume

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: portworx-io-priority-high
provisioner: kubernetes.io/portworx-volume
parameters:
  repl: &#34;1&#34;
  snap_interval:   &#34;70&#34;
  priority_io:  &#34;high&#34;

```

* `fs`: filesystem to be laid out: `none/xfs/ext4` (default: `ext4`).
* `block_size`: block size in Kbytes (default: `32`).
* `repl`: number of synchronous replicas to be provided in the form of
  replication factor `1..3` (default: `1`) A string is expected here i.e.
  `&#34;1&#34;` and not `1`.
* `priority_io`: determines whether the volume will be created from higher
  performance or a lower priority storage `high/medium/low` (default: `low`).
* `snap_interval`: clock/time interval in minutes for when to trigger snapshots.
  Snapshots are incremental based on difference with the prior snapshot, 0
  disables snaps (default: `0`). A string is expected here i.e.
  `&#34;70&#34;` and not `70`.
* `aggregation_level`: specifies the number of chunks the volume would be
  distributed into, 0 indicates a non-aggregated volume (default: `0`). A string
  is expected here i.e. `&#34;0&#34;` and not `0`
* `ephemeral`: specifies whether the volume should be cleaned-up after unmount
  or should be persistent. `emptyDir` use case can set this value to true and
  `persistent volumes` use case such as for databases like Cassandra should set
  to false, `true/false` (default `false`). A string is expected here i.e.
  `&#34;true&#34;` and not `true`.
 --&gt;
&lt;h3 id=&#34;portworx-volume&#34;&gt;Portworx 卷&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;portworx-io-priority-high&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/portworx-volume&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;repl&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;snap_interval&lt;/span&gt;:   &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;70&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;priority_io&lt;/span&gt;:  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;high&amp;#34;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fs&lt;/code&gt;: 文件系统 &lt;code&gt;none/xfs/ext4&lt;/code&gt; (默认: &lt;code&gt;ext4&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;block_size&lt;/code&gt;: 块大小，单位千字节 (默认: &lt;code&gt;32&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;repl&lt;/code&gt;: 并发副本的数量，副本倍率格式为 &lt;code&gt;1..3&lt;/code&gt; (默认: &lt;code&gt;1&lt;/code&gt;) 这里值是字符串，也就是 &lt;code&gt;&amp;quot;1&amp;quot;&lt;/code&gt; 而不是 &lt;code&gt;1&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;priority_io&lt;/code&gt;: 决定卷以什么级别的性能创建。&lt;code&gt;high/medium/low&lt;/code&gt; (默认: &lt;code&gt;low&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;snap_interval&lt;/code&gt;: 触发快照的时间间隔，快照是基于之前的快照增量创建，0 就是禁用快照(默认: &lt;code&gt;0&lt;/code&gt;).
这里值是字符串，也就是 &lt;code&gt;&amp;quot;70&amp;quot;&lt;/code&gt; 而不是 &lt;code&gt;70&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;aggregation_level&lt;/code&gt;: 指定卷分布的块数量， 0 表示非聚合卷(默认为: &lt;code&gt;0&lt;/code&gt;)
这里值是字符串，也就是 &lt;code&gt;&amp;quot;0&amp;quot;&lt;/code&gt; 而不是 &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ephemeral&lt;/code&gt;: 指定这个卷是否在卸载时清查或应该持久存在。 &lt;code&gt;emptyDir&lt;/code&gt; 情况下可以将该值设置为 true
&lt;code&gt;persistent volumes&lt;/code&gt; 情况下，如例如 Cassandra 这些数据库，应该设置为 false,  &lt;code&gt;true/false&lt;/code&gt; (默认为 &lt;code&gt;false&lt;/code&gt;).
这个字段的值是一个字段串，是&lt;code&gt;&amp;quot;true&amp;quot;&lt;/code&gt; 而不是 &lt;code&gt;true&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### ScaleIO

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/scaleio
parameters:
  gateway: https://192.168.99.200:443/api
  system: scaleio
  protectionDomain: pd0
  storagePool: sp1
  storageMode: ThinProvisioned
  secretRef: sio-secret
  readOnly: false
  fsType: xfs
```

* `provisioner`: attribute is set to `kubernetes.io/scaleio`
* `gateway`: address to a ScaleIO API gateway (required)
* `system`: the name of the ScaleIO system (required)
* `protectionDomain`: the name of the ScaleIO protection domain (required)
* `storagePool`: the name of the volume storage pool (required)
* `storageMode`: the storage provision mode: `ThinProvisioned` (default) or
  `ThickProvisioned`
* `secretRef`: reference to a configured Secret object (required)
* `readOnly`: specifies the access mode to the mounted volume (default false)
* `fsType`: the file system to use for the volume (default ext4)

The ScaleIO Kubernetes volume plugin requires a configured Secret object.
The secret must be created with type `kubernetes.io/scaleio` and use the same
namespace value as that of the PVC where it is referenced
as shown in the following command:

```shell
kubectl create secret generic sio-secret --type=&#34;kubernetes.io/scaleio&#34; \
--from-literal=username=sioadmin --from-literal=password=d2NABDNjMA== \
--namespace=default
```
 --&gt;
&lt;h3 id=&#34;scaleio&#34;&gt;ScaleIO&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/scaleio&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;gateway&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://192.168.99.200:443/api&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;system&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;scaleio&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;protectionDomain&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pd0&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storagePool&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;sp1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ThinProvisioned&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;secretRef&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;sio-secret&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;xfs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;provisioner&lt;/code&gt;: 该属性设置为 &lt;code&gt;kubernetes.io/scaleio&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gateway&lt;/code&gt;: ScaleIO API 网关地址(必要)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;system&lt;/code&gt;: ScaleIO 系统名称(必要)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;protectionDomain&lt;/code&gt;: ScaleIO 保护域名名称 (必要)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;storagePool&lt;/code&gt;: 卷存储池名称(必要)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;storageMode&lt;/code&gt;: 存储供应模式 &lt;code&gt;ThinProvisioned&lt;/code&gt; (默认) 或 &lt;code&gt;ThickProvisioned&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;secretRef&lt;/code&gt;: 配置的 Secret 对象(必要)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;readOnly&lt;/code&gt;: 指定挂载卷访问模式(默认为 &lt;code&gt;&amp;quot;false&amp;quot;&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fsType&lt;/code&gt;: 卷使用的文件系统(默认 &lt;code&gt;ext4&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ScaleIO k8s 卷插件需要一个配置 &lt;code&gt;Secret&lt;/code&gt; 对象。 &lt;code&gt;Secret&lt;/code&gt; 必须要以 &lt;code&gt;kubernetes.io/scaleio&lt;/code&gt;
类型创建并且必须与 PVC 在同一个命名空间。 以下为创建示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret generic sio-secret --type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/scaleio&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;username&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;sioadmin --from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;password&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;d2NABDNjMA&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;default
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### StorageOS

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/storageos
parameters:
  pool: default
  description: Kubernetes volume
  fsType: ext4
  adminSecretNamespace: default
  adminSecretName: storageos-secret
```

* `pool`: The name of the StorageOS distributed capacity pool to provision the
  volume from.  Uses the `default` pool which is normally present if not specified.
* `description`: The description to assign to volumes that were created dynamically.
  All volume descriptions will be the same for the storage class, but different
  storage classes can be used to allow descriptions for different use cases.
  Defaults to `Kubernetes volume`.
* `fsType`: The default filesystem type to request. Note that user-defined rules
  within StorageOS may override this value.  Defaults to `ext4`.
* `adminSecretNamespace`: The namespace where the API configuration secret is
  located. Required if adminSecretName set.
* `adminSecretName`: The name of the secret to use for obtaining the StorageOS
  API credentials. If not specified, default values will be attempted.

The StorageOS Kubernetes volume plugin can use a Secret object to specify an
endpoint and credentials to access the StorageOS API. This is only required when
the defaults have been changed.
The secret must be created with type `kubernetes.io/storageos` as shown in the
following command:

```shell
kubectl create secret generic storageos-secret \
--type=&#34;kubernetes.io/storageos&#34; \
--from-literal=apiAddress=tcp://localhost:5705 \
--from-literal=apiUsername=storageos \
--from-literal=apiPassword=storageos \
--namespace=default
```

Secrets used for dynamically provisioned volumes may be created in any namespace
and referenced with the `adminSecretNamespace` parameter. Secrets used by
pre-provisioned volumes must be created in the same namespace as the PVC that
references it.
 --&gt;
&lt;h3 id=&#34;storageos&#34;&gt;StorageOS &lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fast&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/storageos&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;pool&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;description&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Kubernetes volume&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;fsType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ext4&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;adminSecretNamespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;adminSecretName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storageos-secret&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;pool&lt;/code&gt;: 供应卷的 StorageOS 分布式容量池的名称 ?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;description&lt;/code&gt;: 分配给动态供应的卷的描述。 同样 StorageClass 的所有卷的描述都是一样的，
不同的 StorageClass 可以根据不现的应用场景使用不同的描述。 默认为 &lt;code&gt;Kubernetes volume&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;fsType&lt;/code&gt;: 申请的默认文件系统类型。 StorageOS 中的用户定义规则可能会覆盖该值。 默认为 &lt;code&gt;ext4&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;adminSecretNamespace&lt;/code&gt;: API 配置的 Secret 所在的命名空间。 如果设置了 adminSecretName
则需要设置该字段&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;adminSecretName&lt;/code&gt;: 获取 StorageOS API 凭据的 Secret 名称。如果没有设置，则会尝试使用
默认值&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;StorageOS 的 k8s 卷插件可以使用一个 Secret 对象来指定访问StorageOS API 的地址和凭据。
只有在修改默认情况需要配置该值。
Secret 必须要以 &lt;code&gt;kubernetes.io/storageos&lt;/code&gt; 类型创建。创建命令示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl create secret generic storageos-secret &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kubernetes.io/storageos&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;apiAddress&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;tcp://localhost:5705 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;apiUsername&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;storageos &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--from-literal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;apiPassword&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;storageos &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;default
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;卷动态供应使用的 Secret 可以创建在任意命名空间中，通过 &lt;code&gt;adminSecretNamespace&lt;/code&gt; 参数引用。
预创建卷所使用的 Secret 必须与使用它的 PVC 在同一个命名空间&lt;/p&gt;
&lt;!--
### Local






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [stable]&lt;/code&gt;
&lt;/div&gt;



```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```

Local volumes do not currently support dynamic provisioning, however a StorageClass
should still be created to delay volume binding until Pod scheduling. This is
specified by the `WaitForFirstConsumer` volume binding mode.

Delaying volume binding allows the scheduler to consider all of a Pod&#39;s
scheduling constraints when choosing an appropriate PersistentVolume for a
PersistentVolumeClaim.
 --&gt;
&lt;h3 id=&#34;local&#34;&gt;Local&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;local-storage&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/no-provisioner&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;volumeBindingMode&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;WaitForFirstConsumer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Local 卷目前不支持动态供应， 但 StorageClass 也可以用来让卷绑定发生在 Pod 调度之后。
可以通过设置卷绑定模式为 &lt;code&gt;WaitForFirstConsumer&lt;/code&gt; 达到这个目的&lt;/p&gt;
&lt;p&gt;延迟绑定可以在为 PVC 选择恰当的 PV 时考虑 Pod 所有约束。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: VolumeSnapshotClass</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volume-snapshot-classes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/volume-snapshot-classes/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- saad-ali
- thockin
- msau42
- jingxu97
- xing-yang
- yuxiangqian
title: Volume Snapshot Classes
content_type: concept
weight: 30
---
--&gt;
&lt;!-- overview --&gt;
&lt;!--
This document describes the concept of VolumeSnapshotClass in Kubernetes. Familiarity
with [volume snapshots](/docs/concepts/storage/volume-snapshots/) and
[storage classes](/docs/concepts/storage/storage-classes) is suggested.
--&gt;
&lt;p&gt;本文介绍 k8s 中的 VolumeSnapshotClass 概念。建议先熟悉
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volume-snapshots/&#34;&gt;卷快照&lt;/a&gt; 和
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes&#34;&gt;StorageClass&lt;/a&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Introduction

Just like StorageClass provides a way for administrators to describe the &#34;classes&#34;
of storage they offer when provisioning a volume, VolumeSnapshotClass provides a
way to describe the &#34;classes&#34; of storage when provisioning a volume snapshot.
 --&gt;
&lt;!--
## Introduction
Just like StorageClass provides a way for administrators to describe the &#34;classes&#34;
of storage they offer when provisioning a volume, VolumeSnapshotClass provides a
way to describe the &#34;classes&#34; of storage when provisioning a volume snapshot.
--&gt;
&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;
&lt;p&gt;与 StorageClass 为管理员提供了一种描述他们供应的存储的类别(class)一样。
VolumeSnapshotClass 提供了一种描述他们供应的卷快照的类别(class)&lt;/p&gt;
&lt;!--
## The VolumeSnapshotClass Resource

Each VolumeSnapshotClass contains the fields `driver`, `deletionPolicy`, and `parameters`,
which are used when a VolumeSnapshot belonging to the class needs to be
dynamically provisioned.

The name of a VolumeSnapshotClass object is significant, and is how users can
request a particular class. Administrators set the name and other parameters
of a class when first creating VolumeSnapshotClass objects, and the objects cannot
be updated once they are created.

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: csi-hostpath-snapclass
driver: hostpath.csi.k8s.io
deletionPolicy: Delete
parameters:
```

Administrators can specify a default VolumeSnapshotClass for VolumeSnapshots
that don&#39;t request any particular class to bind to by adding the
`snapshot.storage.kubernetes.io/is-default-class: &#34;true&#34;` annotation:

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: csi-hostpath-snapclass
  annotations:
    snapshot.storage.kubernetes.io/is-default-class: &#34;true&#34;
driver: hostpath.csi.k8s.io
deletionPolicy: Delete
parameters:
```
 --&gt;
&lt;h2 id=&#34;volumesnapshotclass-资源&#34;&gt;VolumeSnapshotClass 资源&lt;/h2&gt;
&lt;p&gt;每个 &lt;code&gt;VolumeSnapshotClass&lt;/code&gt; 包含的字段有 &lt;code&gt;driver&lt;/code&gt;, &lt;code&gt;deletionPolicy&lt;/code&gt;, 和 &lt;code&gt;parameters&lt;/code&gt;,
这个对象被属于该类别的 VolumeSnapshot 动态供应时用到。&lt;/p&gt;
&lt;p&gt;VolumeSnapshotClass 对象的名称是有意义的，它是用户可以怎么申请一个指定的类别。 管理员在第一次创建
VolumeSnapshotClass 指定该类别的名称和其它参数，这个对象在创建之后将不能被修改。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshotClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;csi-hostpath-snapclass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;driver&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hostpath.csi.k8s.io&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;deletionPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Delete&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;管理员可以能过如下设置注解
&lt;code&gt;snapshot.storage.kubernetes.io/is-default-class: &amp;quot;true&amp;quot;&lt;/code&gt;
为那些没有指定任何类别的 VolumeSnapshot 指定默认的 VolumeSnapshotClass，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;snapshot.storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;VolumeSnapshotClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;csi-hostpath-snapclass&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;snapshot.storage.kubernetes.io/is-default-class&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;driver&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hostpath.csi.k8s.io&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;deletionPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Delete&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Driver

Volume snapshot classes have a driver that determines what CSI volume plugin is
used for provisioning VolumeSnapshots. This field must be specified.
 --&gt;
&lt;h3 id=&#34;驱动driver&#34;&gt;驱动(&lt;code&gt;driver&lt;/code&gt;)&lt;/h3&gt;
&lt;p&gt;卷快照类别有一个 driver 字段来决定使用哪个 CSI 卷插件来供应卷快照(VolumeSnapshot).
这是一相必要字段。&lt;/p&gt;
&lt;!--
### DeletionPolicy

Volume snapshot classes have a deletionPolicy. It enables you to configure what happens to a VolumeSnapshotContent when the VolumeSnapshot object it is bound to is to be deleted. The deletionPolicy of a volume snapshot can either be `Retain` or `Delete`. This field must be specified.

If the deletionPolicy is `Delete`, then the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. If the deletionPolicy is `Retain`, then both the underlying snapshot and VolumeSnapshotContent remain.
 --&gt;
&lt;h3 id=&#34;删除策略deletionpolicy&#34;&gt;删除策略(&lt;code&gt;deletionPolicy&lt;/code&gt;)&lt;/h3&gt;
&lt;p&gt;卷快照类别有一个 &lt;code&gt;deletionPolicy&lt;/code&gt;。 它让用户可以配置当与 VolumeSnapshotContent 绑定
VolumeSnapshot 对象被删除时，怎么处理 VolumeSnapshotContent。 卷快照类别的删除策略(&lt;code&gt;deletionPolicy&lt;/code&gt;)
可以是 &lt;code&gt;Retain&lt;/code&gt; 或 &lt;code&gt;Delete&lt;/code&gt;， 这是一个必要字段。&lt;/p&gt;
&lt;p&gt;如果删除策略(&lt;code&gt;deletionPolicy&lt;/code&gt;) 是 &lt;code&gt;Delete&lt;/code&gt;，底层的存储快照会随同 VolumeSnapshotContent
对象的删除一起删除。 如果删除策略(&lt;code&gt;deletionPolicy&lt;/code&gt;) 是 &lt;code&gt;Retain&lt;/code&gt;，
底层的存储快照 和 VolumeSnapshotContent 都会保留。&lt;/p&gt;
&lt;!--
## Parameters

Volume snapshot classes have parameters that describe volume snapshots belonging to
the volume snapshot class. Different parameters may be accepted depending on the
`driver`.
 --&gt;
&lt;h2 id=&#34;其它参数-parameters&#34;&gt;其它参数 (&lt;code&gt;parameters&lt;/code&gt;)&lt;/h2&gt;
&lt;p&gt;卷快照类别有描述属于该类别的卷快照的参数。 可以接受哪些的参数基于不同的驱动 &lt;code&gt;driver&lt;/code&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 卷动态供应</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/dynamic-provisioning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/dynamic-provisioning/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- saad-ali
- jsafrane
- thockin
- msau42
title: Dynamic Volume Provisioning
content_type: concept
weight: 70
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
Dynamic volume provisioning allows storage volumes to be created on-demand.
Without dynamic provisioning, cluster administrators have to manually make
calls to their cloud or storage provider to create new storage volumes, and
then create [`PersistentVolume` objects](/docs/concepts/storage/persistent-volumes/)
to represent them in Kubernetes. The dynamic provisioning feature eliminates
the need for cluster administrators to pre-provision storage. Instead, it
automatically provisions storage when it is requested by users.
 --&gt;
&lt;p&gt;卷动态供应允许存储卷可以根据需要创建。 如果没有动态供应，集群管理员必须要手动调用云存储或其它存储
提供者来创建新的存储卷，然后再创建
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/&#34;&gt;&lt;code&gt;PersistentVolume&lt;/code&gt; 对象&lt;/a&gt;来
在 k8s 中代表他们。动态供应特性就是将管理员从这些预创建的任务中解放出来。 系统会根据用户的请求
自动供应存储&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Background

The implementation of dynamic volume provisioning is based on the API object `StorageClass`
from the API group `storage.k8s.io`. A cluster administrator can define as many
`StorageClass` objects as needed, each specifying a *volume plugin* (aka
*provisioner*) that provisions a volume and the set of parameters to pass to
that provisioner when provisioning.
A cluster administrator can define and expose multiple flavors of storage (from
the same or different storage systems) within a cluster, each with a custom set
of parameters. This design also ensures that end users don&#39;t have to worry
about the complexity and nuances of how storage is provisioned, but still
have the ability to select from multiple storage options.

More information on storage classes can be found
[here](/docs/concepts/storage/storage-classes/).
 --&gt;
&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;p&gt;卷动态供应是基于来自 &lt;code&gt;storage.k8s.io&lt;/code&gt; API 组的 &lt;code&gt;StorageClass&lt;/code&gt; API 对象。 集群管理员可以
根据需要创建任意数量的 &lt;code&gt;StorageClass&lt;/code&gt; 对象， 每个对象中都会指定一个 &lt;em&gt;卷插件&lt;/em&gt;(也就是 &lt;em&gt;供应者&lt;/em&gt;)
来提供卷并且包含供应卷时需要传递的参数集。集群管理员可以在集群中定义并暴露多个偏好的存储(来自相同
或不同的存储系统)，每种都有一套自定义的参数。 这种设计也保证终端用户不需要关心存储供应的复杂性和差异性
的同时还能提供多种存储选项的选择。&lt;/p&gt;
&lt;p&gt;更多关于存储类别的信息见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes/&#34;&gt;这里&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Enabling Dynamic Provisioning

To enable dynamic provisioning, a cluster administrator needs to pre-create
one or more StorageClass objects for users.
StorageClass objects define which provisioner should be used and what parameters
should be passed to that provisioner when dynamic provisioning is invoked.
The name of a StorageClass object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

The following manifest creates a storage class &#34;slow&#34; which provisions standard
disk-like persistent disks.

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
```

The following manifest creates a storage class &#34;fast&#34; which provisions
SSD-like persistent disks.

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
```
 --&gt;
&lt;h2 id=&#34;enabling-dynamic-provisioning&#34;&gt;启用动态供应&lt;/h2&gt;
&lt;p&gt;为启用动态供应，集群管理员需要预先为用户创建好一个或多个 &lt;code&gt;StorageClass&lt;/code&gt; 对象。&lt;code&gt;StorageClass&lt;/code&gt;
对象定义需要使用哪个供应者和在动态供应调用时需要传递给供应者的参数。 &lt;code&gt;StorageClass&lt;/code&gt; 对象的名称
必须是一个有效的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;以下的配置中定义了一个叫 &amp;ldquo;slow&amp;rdquo; &lt;code&gt;StorageClass&lt;/code&gt;， 它提供了一个标准的类磁盘的持久下磁盘。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;slow&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/gce-pd&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pd-standard&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;以下的配置中定义了一个叫 &amp;ldquo;fast&amp;rdquo; &lt;code&gt;StorageClass&lt;/code&gt;， 它提供了一个类SSD盘的持久下磁盘。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;storage.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StorageClass&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fast&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;provisioner&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubernetes.io/gce-pd&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;parameters&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pd-ssd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Using Dynamic Provisioning

Users request dynamically provisioned storage by including a storage class in
their `PersistentVolumeClaim`. Before Kubernetes v1.6, this was done via the
`volume.beta.kubernetes.io/storage-class` annotation. However, this annotation
is deprecated since v1.6. Users now can and should instead use the
`storageClassName` field of the `PersistentVolumeClaim` object. The value of
this field must match the name of a `StorageClass` configured by the
administrator (see [below](#enabling-dynamic-provisioning)).

To select the &#34;fast&#34; storage class, for example, a user would create the
following PersistentVolumeClaim:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim1
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast
  resources:
    requests:
      storage: 30Gi
```

This claim results in an SSD-like Persistent Disk being automatically
provisioned. When the claim is deleted, the volume is destroyed.
--&gt;
&lt;h2 id=&#34;using-dynamic-provisioning&#34;&gt;使用动态供应&lt;/h2&gt;
&lt;p&gt;用户通过在 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 引用一个 &lt;code&gt;StorageClass&lt;/code&gt; 申请动态供应存储。 在 k8s
v1.6 之前， 这是通过添加注解 &lt;code&gt;volume.beta.kubernetes.io/storage-class&lt;/code&gt; 实现的。但是
这个注解在 k8s v1.6 之后已经废弃。在这之后的版本用户可以在 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 对象
中使用 &lt;code&gt;storageClassName&lt;/code&gt; 代替这个注解。这个字段值必须与管理员配置的 &lt;code&gt;StorageClass&lt;/code&gt; 中的一个
(见 &lt;a href=&#34;#enabling-dynamic-provisioning&#34;&gt;上面&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;例如，想要使用 &amp;ldquo;fast&amp;rdquo; &lt;code&gt;StorageClass&lt;/code&gt;， 用户可以创建以下 PersistentVolumeClaim:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;claim1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;:
    - &lt;span style=&#34;color:#ae81ff&#34;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fast&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;30Gi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个配置的 PVC 会自动供应一个类 SSD 的持久化磁盘。 当 PVC 被删除后，对应卷也会被删除。&lt;/p&gt;
&lt;!--
## Defaulting Behavior

Dynamic provisioning can be enabled on a cluster such that all claims are
dynamically provisioned if no storage class is specified. A cluster administrator
can enable this behavior by:

- Marking one `StorageClass` object as *default*;
- Making sure that the [`DefaultStorageClass` admission controller](/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass)
  is enabled on the API server.

An administrator can mark a specific `StorageClass` as default by adding the
`storageclass.kubernetes.io/is-default-class` annotation to it.
When a default `StorageClass` exists in a cluster and a user creates a
`PersistentVolumeClaim` with `storageClassName` unspecified, the
`DefaultStorageClass` admission controller automatically adds the
`storageClassName` field pointing to the default storage class.

Note that there can be at most one *default* storage class on a cluster, or
a `PersistentVolumeClaim` without `storageClassName` explicitly specified cannot
be created.
 --&gt;
&lt;h2 id=&#34;默认行为&#34;&gt;默认行为&lt;/h2&gt;
&lt;p&gt;可以在集群中启用动态供应，这样所有的PVC 都可以在没有指定 StorageClass 的情况下也是动态供应的。
集群管理员可以通过以下方式启动:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将一个 &lt;code&gt;StorageClass&lt;/code&gt; 对象设置为 &lt;em&gt;默认&lt;/em&gt;;
is enabled on the API server.&lt;/li&gt;
&lt;li&gt;确保 API server 上的
&lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass&#34;&gt;&lt;code&gt;DefaultStorageClass&lt;/code&gt; 准入控制&lt;/a&gt;
启用了&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;管理员可以通过在一个 &lt;code&gt;StorageClass&lt;/code&gt; 添加 &lt;code&gt;storageclass.kubernetes.io/is-default-class&lt;/code&gt;
注解的方式将其设置为默认。 当集群中有默认的 &lt;code&gt;StorageClass&lt;/code&gt; 存在时，用户创建 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt;
没有指定 &lt;code&gt;storageClassName&lt;/code&gt; ，&lt;code&gt;DefaultStorageClass&lt;/code&gt; 准入控制自动添加 &lt;code&gt;storageClassName&lt;/code&gt;
字段，并将其指向默认的 &lt;code&gt;StorageClass&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;要注意一个集群中最多只能有一个 &lt;em&gt;默认&lt;/em&gt; &lt;code&gt;StorageClass&lt;/code&gt;， 否则没有显示指定 &lt;code&gt;storageClassName&lt;/code&gt;
的 &lt;code&gt;PersistentVolumeClaim&lt;/code&gt; 是不能创建的。&lt;/p&gt;
&lt;!--
## Topology Awareness

In [Multi-Zone](/docs/setup/multiple-zones) clusters, Pods can be spread across
Zones in a Region. Single-Zone storage backends should be provisioned in the Zones where
Pods are scheduled. This can be accomplished by setting the [Volume Binding
Mode](/docs/concepts/storage/storage-classes/#volume-binding-mode).
 --&gt;
&lt;h2 id=&#34;拓扑自洽&#34;&gt;拓扑自洽&lt;/h2&gt;
&lt;p&gt;在 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/setup/multiple-zones&#34;&gt;多区域&lt;/a&gt; 集群中，Pod 可以在同一个地区的不同区域中分布。
单区域存储后端应该供应到 Pod 被调度到的区域中。这可以通过设置
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes/#volume-binding-mode&#34;&gt;卷绑定模式&lt;/a&gt;
实现。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 存储容量</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-capacity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-capacity/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jsafrane
- saad-ali
- msau42
- xing-yang
- pohly
title: Storage Capacity
content_type: concept
weight: 45
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
Storage capacity is limited and may vary depending on the node on
which a pod runs: network-attached storage might not be accessible by
all nodes, or storage is local to a node to begin with.






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [alpha]&lt;/code&gt;
&lt;/div&gt;



This page describes how Kubernetes keeps track of storage capacity and
how the scheduler uses that information to schedule Pods onto nodes
that have access to enough storage capacity for the remaining missing
volumes. Without storage capacity tracking, the scheduler may choose a
node that doesn&#39;t have enough capacity to provision a volume and
multiple scheduling retries will be needed.

Tracking storage capacity is supported for &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;Container Storage Interface&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt; (CSI) drivers and
[needs to be enabled](#enabling-storage-capacity-tracking) when installing a CSI driver.
 --&gt;
&lt;p&gt;存储容量是受限的并且还基于 Pod 运行的节点: 网络存储可以不能在所有的节点上访问，或者存储只属于
Pod 启动的节点上。&lt;/p&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;本文主要介绍 k8s 是怎么保持对存储容量的跟踪并且调度器使用这些信息来把 Pod 调度到能够访问到
剩余没使用的卷有足够存储容量。 如果没有对存储容量的跟踪， 造成调度器因调度到一个没有足够存储容量
的节点而造成多次尝试重新调度。&lt;/p&gt;
&lt;p&gt;跟踪存储容量是支持
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#39; target=&#39;_blank&#39;&gt;容器存储接口 (CSI)&lt;span class=&#39;tooltip-text&#39;&gt;容器存储接口 (CSI)定义一个标准接口用于暴露存储系统到容器中&lt;/span&gt;
&lt;/a&gt; (CSI) 驱动并且在安装 CSI 驱动时
&lt;a href=&#34;#enabling-storage-capacity-tracking&#34;&gt;需要开启&lt;/a&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## API

There are two API extensions for this feature:
- [CSIStorageCapacity](/docs/reference/generated/kubernetes-api/v1.19/#csistoragecapacity-v1alpha1-storage-k8s-io) objects:
  these get produced by a CSI driver in the namespace
  where the driver is installed. Each object contains capacity
  information for one storage class and defines which nodes have
  access to that storage.
- [The `CSIDriverSpec.StorageCapacity` field](/docs/reference/generated/kubernetes-api/v1.19/#csidriverspec-v1-storage-k8s-io):
  when set to `true`, the Kubernetes scheduler will consider storage
  capacity for volumes that use the CSI driver.
 --&gt;
&lt;h2 id=&#34;api&#34;&gt;API&lt;/h2&gt;
&lt;p&gt;对于这个特性有两个 API 扩展:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#csistoragecapacity-v1alpha1-storage-k8s-io&#34;&gt;CSIStorageCapacity&lt;/a&gt; 对象:
这些是当驱动被安装时由 CSI 驱动在命名空间中生成的。 每个对象包含一个 StorageClass 的容量信息
并定义哪个节点可以访问这个存储。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#csidriverspec-v1-storage-k8s-io&#34;&gt;&lt;code&gt;CSIDriverSpec.StorageCapacity&lt;/code&gt; 字段&lt;/a&gt;:
当设置为 &lt;code&gt;true&lt;/code&gt;， k8s 调度器会考量使用 CSI 驱动的卷的存储容量&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Scheduling

Storage capacity information is used by the Kubernetes scheduler if:
- the `CSIStorageCapacity` feature gate is true,
- a Pod uses a volume that has not been created yet,
- that volume uses a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes&#39; target=&#39;_blank&#39;&gt;StorageClass&lt;span class=&#39;tooltip-text&#39;&gt;StorageClass 为管理员提供了一种描述不同存储类别的方式&lt;/span&gt;
&lt;/a&gt; which references a CSI driver and
  uses `WaitForFirstConsumer` [volume binding
  mode](/docs/concepts/storage/storage-classes/#volume-binding-mode),
  and
- the `CSIDriver` object for the driver has `StorageCapacity` set to
  true.

In that case, the scheduler only considers nodes for the Pod which
have enough storage available to them. This check is very
simplistic and only compares the size of the volume against the
capacity listed in `CSIStorageCapacity` objects with a topology that
includes the node.

For volumes with `Immediate` volume binding mode, the storage driver
decides where to create the volume, independently of Pods that will
use the volume. The scheduler then schedules Pods onto nodes where the
volume is available after the volume has been created.

For [CSI ephemeral volumes](/docs/concepts/storage/volumes/#csi),
scheduling always happens without considering storage capacity. This
is based on the assumption that this volume type is only used by
special CSI drivers which are local to a node and do not need
significant resources there.
 --&gt;
&lt;h2 id=&#34;scheduling&#34;&gt;调度&lt;/h2&gt;
&lt;p&gt;k8s 调度器会使用存储容量信息如果:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CSIStorageCapacity&lt;/code&gt; 功能阀启用&lt;/li&gt;
&lt;li&gt;一个 Pod 使用了一个还没有被创建的卷&lt;/li&gt;
&lt;li&gt;这卷使用一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes&#39; target=&#39;_blank&#39;&gt;StorageClass&lt;span class=&#39;tooltip-text&#39;&gt;StorageClass 为管理员提供了一种描述不同存储类别的方式&lt;/span&gt;
&lt;/a&gt;
它使用 CSI 驱动并设置 &lt;code&gt;WaitForFirstConsumer&lt;/code&gt;
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes/#volume-binding-mode&#34;&gt;卷绑定模式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;驱动的 &lt;code&gt;CSIDriver&lt;/code&gt; 对象上的 &lt;code&gt;StorageCapacity&lt;/code&gt; 被设置为 true&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在那种情况下，调度器只考虑那些对于该 Pod 有足够存储的节点。这种检测是相当简单的也就是只对卷的大小
和 包含节点拓扑的 &lt;code&gt;CSIStorageCapacity&lt;/code&gt; 对象中列举的容量进行对比。&lt;/p&gt;
&lt;p&gt;对于那些使用 &lt;code&gt;Immediate&lt;/code&gt; 卷绑定模式的卷，由存储驱动决定在哪创建卷， Pod 会独立地使用这些卷。
然后调度器再把 Pod 调度到那些已经创建好卷的节点上。&lt;/p&gt;
&lt;p&gt;对于
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#csi&#34;&gt;CSI 临时卷&lt;/a&gt;,
在调度时始终不考虑存储容量的。 这么做是因为有这么一个假设: 这个卷类型只被一个在节点范围内，不
需要用到很多资源的特殊 CSI 驱动。&lt;/p&gt;
&lt;!--
## Rescheduling

When a node has been selected for a Pod with `WaitForFirstConsumer`
volumes, that decision is still tentative. The next step is that the
CSI storage driver gets asked to create the volume with a hint that the
volume is supposed to be available on the selected node.

Because Kubernetes might have chosen a node based on out-dated
capacity information, it is possible that the volume cannot really be
created. The node selection is then reset and the Kubernetes scheduler
tries again to find a node for the Pod.
 --&gt;
&lt;h2 id=&#34;rescheduling&#34;&gt;再调度&lt;/h2&gt;
&lt;p&gt;当一个节点被一个使用 &lt;code&gt;WaitForFirstConsumer&lt;/code&gt; 卷的 Pod 选中时，这个决策也是选择性的。 下一步
就是 CSI 存储驱动被告知选择的节点应该是有足够的空间来创建这相卷&lt;/p&gt;
&lt;p&gt;因为 k8s 可能基于过期的容量信息选择一个节点，这就可能这个卷不能被真正创建。然后这次节点选择就
被重围，k8s 调度器再次尝试为 Pod 找一个节点。&lt;/p&gt;
&lt;!--
## Limitations

Storage capacity tracking increases the chance that scheduling works
on the first try, but cannot guarantee this because the scheduler has
to decide based on potentially out-dated information. Usually, the
same retry mechanism as for scheduling without any storage capacity
information handles scheduling failures.

One situation where scheduling can fail permanently is when a Pod uses
multiple volumes: one volume might have been created already in a
topology segment which then does not have enough capacity left for
another volume. Manual intervention is necessary to recover from this,
for example by increasing capacity or deleting the volume that was
already created. [Further
work](https://github.com/kubernetes/enhancements/pull/1703) is needed
to handle this automatically.
 --&gt;
&lt;h2 id=&#34;limitations&#34;&gt;限制&lt;/h2&gt;
&lt;p&gt;存储容量的跟踪会增加调度在第一次调度时成功的机会，但不能保证，因为调度器可能是基于潜在的过期信息。
通常，这与没有容量信息调度时失败后的重试机制是一样的。&lt;/p&gt;
&lt;p&gt;有一种情况是当一个 Pod 使用两个卷时调度可能会永久失： 一个卷可能已经在拓扑信息中创建，但这里
没有足够的空间来创建另一个卷。 这时间就需要人工介入来修复这种问题， 例如增加容量或删除已经创建的卷。
&lt;a href=&#34;https://github.com/kubernetes/enhancements/pull/1703&#34;&gt;剩下的工件&lt;/a&gt; 就会自动地被处理。&lt;/p&gt;
&lt;!--
## Enabling storage capacity tracking

Storage capacity tracking is an *alpha feature* and only enabled when
the `CSIStorageCapacity` [feature
gate](/docs/reference/command-line-tools-reference/feature-gates/) and
the `storage.k8s.io/v1alpha1` &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/kubernetes-api/#api-groups&#39; target=&#39;_blank&#39;&gt;API group&lt;span class=&#39;tooltip-text&#39;&gt;A set of related paths in the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; are enabled. For details on
that, see the `--feature-gates` and `--runtime-config` [kube-apiserver
parameters](/docs/reference/command-line-tools-reference/kube-apiserver/).

A quick check
whether a Kubernetes cluster supports the feature is to list
CSIStorageCapacity objects with:
```shell
kubectl get csistoragecapacities --all-namespaces
```

If your cluster supports CSIStorageCapacity, the response is either a list of CSIStorageCapacity objects or:
```
No resources found
```

If not supported, this error is printed instead:
```
error: the server doesn&#39;t have a resource type &#34;csistoragecapacities&#34;
```

In addition to enabling the feature in the cluster, a CSI
driver also has to
support it. Please refer to the driver&#39;s documentation for
details.
 --&gt;
&lt;h2 id=&#34;enabling-storage-capacity-tracking&#34;&gt;启用容量跟踪&lt;/h2&gt;
&lt;p&gt;存储容量跟踪是一个 &lt;em&gt;alpha 特性&lt;/em&gt;， 只有在
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/kubernetes-api/#api-groups&#39; target=&#39;_blank&#39;&gt;API group&lt;span class=&#39;tooltip-text&#39;&gt;A set of related paths in the Kubernetes API.&lt;/span&gt;
&lt;/a&gt;
中的 &lt;code&gt;storage.k8s.io/v1alpha1&lt;/code&gt; 被启用，同时还启用了 &lt;code&gt;CSIStorageCapacity&lt;/code&gt;
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;
更多相关信息，见
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/&#34;&gt;kube-apiserver 参数&lt;/a&gt;
中的  &lt;code&gt;--feature-gates&lt;/code&gt; 和 &lt;code&gt;--runtime-config&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;快速检查 k8s 集群是否支持该特性的办法就是列举 &lt;code&gt;CSIStorageCapacity&lt;/code&gt; 对象:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get csistoragecapacities --all-namespaces
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果集群支持 CSIStorageCapacity， 返回内容可能是一个 CSIStorageCapacity 列表或者:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;No resources found
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果不支持则会出现以下错误:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;error: the server doesn&#39;t have a resource type &amp;quot;csistoragecapacities&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;另外，要在集群中启用该功能还需要所使用的 CSI 驱动也是支持。 具体请参阅驱动的文档。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
- For more information on the design, see the
[Storage Capacity Constraints for Pod Scheduling KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1472-storage-capacity-tracking/README.md).
- For more information on further development of this feature, see the [enhancement tracking issue #1472](https://github.com/kubernetes/enhancements/issues/1472).
- Learn about [Kubernetes Scheduler](/docs/concepts/scheduling-eviction/kube-scheduler/)
 --&gt;
&lt;ul&gt;
&lt;li&gt;更多关于设计的信息见
&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1472-storage-capacity-tracking/README.md&#34;&gt;Storage Capacity Constraints for Pod Scheduling KEP&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;更多关于该特性的开发展望见
&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1472&#34;&gt;enhancement tracking issue #1472&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;概念学习 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/kube-scheduler/&#34;&gt;k8s 调度器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 临时卷</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/ephemeral-volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/ephemeral-volumes/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jsafrane
- saad-ali
- msau42
- xing-yang
- pohly
title: Ephemeral Volumes
content_type: concept
weight: 50
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This document describes _ephemeral volumes_ in Kubernetes. Familiarity
with [volumes](/docs/concepts/storage/volumes/) is suggested, in
particular PersistentVolumeClaim and PersistentVolume.
 --&gt;
&lt;p&gt;本文主要介绍 k8s 中的 &lt;em&gt;临时卷(ephemeral volume)&lt;/em&gt;。 建议先熟悉
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#34;&gt;卷(Volume)&lt;/a&gt;
特别是 PersistentVolumeClaim 和 持久化卷(PV)&lt;/p&gt;
&lt;!-- body --&gt;
&lt;p&gt;Some application need additional storage but don&amp;rsquo;t care whether that
data is stored persistently across restarts. For example, caching
services are often limited by memory size and can move infrequently
used data into storage that is slower than memory with little impact
on overall performance.&lt;/p&gt;
&lt;p&gt;有些应用需要额外的存储，但不关心其中的数据在重启之后是不是还在。 例如，缓存服务经常受限于内存大小
可以将不常用的数据移到其它比内存慢的存储中，这样对全局情况影响比较小。
Other applications expect some read-only input data to be present in
files, like configuration data or secret keys.&lt;/p&gt;
&lt;p&gt;另一些应用可能需要一个存放在文件中的只读输入数据， 例如配置数据或 Secret 的键。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ephemeral volumes&lt;/em&gt; are designed for these use cases. Because volumes
follow the Pod&amp;rsquo;s lifetime and get created and deleted along with the
Pod, Pods can be stopped and restarted without being limited to where
some persistent volume is available.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;临时卷&lt;/em&gt; 就是为这些应用场景设计。因为卷跟随 Pod 的生命周期，随同 Pod 一起创建和删除， Pod 可以
在不再受限于那些只有可用卷的地方停止或重启。&lt;/p&gt;
&lt;p&gt;Ephemeral volumes are specified &lt;em&gt;inline&lt;/em&gt; in the Pod spec, which
simplifies application deployment and management.&lt;/p&gt;
&lt;p&gt;临时卷是在 Pod 定义中单行配置，这样可以简化应用的部署和管理。&lt;/p&gt;
&lt;!--
### Types of ephemeral volumes

Kubernetes supports several different kinds of ephemeral volumes for
different purposes:
- [emptyDir](/docs/concepts/storage/volumes/#emptydir): empty at Pod startup,
  with storage coming locally from the kubelet base directory (usually
  the root disk) or RAM
- [configMap](/docs/concepts/storage/volumes/#configmap),
  [downwardAPI](/docs/concepts/storage/volumes/#downwardapi),
  [secret](/docs/concepts/storage/volumes/#secret): inject different
  kinds of Kubernetes data into a Pod
- [CSI ephemeral volumes](#csi-ephemeral-volumes):
  similar to the previous volume kinds, but provided by special
  [CSI drivers](https://github.com/container-storage-interface/spec/blob/master/spec.md)
  which specifically [support this feature](https://kubernetes-csi.github.io/docs/drivers.html)
- [generic ephemeral volumes](#generic-ephemeral-volumes), which
  can be provided by all storage drivers that also support persistent volumes

`emptyDir`, `configMap`, `downwardAPI`, `secret` are provided as
[local ephemeral
storage](/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage).
They are managed by kubelet on each node.

CSI ephemeral volumes *must* be provided by third-party CSI storage
drivers.

Generic ephemeral volumes *can* be provided by third-party CSI storage
drivers, but also by any other storage driver that supports dynamic
provisioning. Some CSI drivers are written specifically for CSI
ephemeral volumes and do not support dynamic provisioning: those then
cannot be used for generic ephemeral volumes.

The advantage of using third-party drivers is that they can offer
functionality that Kubernetes itself does not support, for example
storage with different performance characteristics than the disk that
is managed by kubelet, or injecting different data.
 --&gt;
&lt;h3 id=&#34;临时卷的类型&#34;&gt;临时卷的类型&lt;/h3&gt;
&lt;p&gt;k8s 支持几种不同类型的临时卷用于不同目的:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#emptydir&#34;&gt;emptyDir&lt;/a&gt;: 在 Pod 启动时是空的，来自
kubelet 基础目录的本地存储(通常是系统盘)或内存&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#configmap&#34;&gt;configMap&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#downwardapi&#34;&gt;downwardAPI&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/#secret&#34;&gt;secret&lt;/a&gt;: 注入不同类型的 k8s 数据到 Pod 中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;#csi-ephemeral-volumes&#34;&gt;CSI 临时卷&lt;/a&gt;: 与上一个类型相似， 但由特殊的
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md&#34;&gt;CSI 驱动&lt;/a&gt;
提供，并且要
&lt;a href=&#34;https://kubernetes-csi.github.io/docs/drivers.html&#34;&gt;支持该特性&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;#generic-ephemeral-volumes&#34;&gt;通用临时卷&lt;/a&gt;， 可以由所有可以支持持久化卷(PV)的存储驱动提供&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;emptyDir&lt;/code&gt;, &lt;code&gt;configMap&lt;/code&gt;, &lt;code&gt;downwardAPI&lt;/code&gt;, &lt;code&gt;secret&lt;/code&gt;  是以
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage&#34;&gt;本地临时卷&lt;/a&gt;
提供。
它们由每个节点上的 kubelet 管理。&lt;/p&gt;
&lt;p&gt;CSI 临时卷&lt;em&gt;必须&lt;/em&gt;由第三方 CSI 存储驱动提供的。&lt;/p&gt;
&lt;p&gt;通用临时卷&lt;em&gt;可以&lt;/em&gt;由第三方 CSI 存储驱动提供的，但也可以由其它支持动态供应的存储驱动提供。
有些 CSI 驱动是 CSI 临时卷专用的，并不支持动态供应: 这些就不能用于通用临时卷。&lt;/p&gt;
&lt;p&gt;用第三方驱动的好处是它们能提供一个 k8s 本身不支持的功能， 例如除由 kubelet 管理外的磁盘外的
其它拥有不同性能特性的存储或插入不同的数据。&lt;/p&gt;
&lt;!--
### CSI ephemeral volumes






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [beta]&lt;/code&gt;
&lt;/div&gt;



This feature requires the `CSIInlineVolume` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) to be enabled. It
is enabled by default starting with Kubernetes 1.16.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; CSI ephemeral volumes are only supported by a subset of CSI drivers.
The Kubernetes CSI &lt;a href=&#34;https://kubernetes-csi.github.io/docs/drivers.html&#34;&gt;Drivers list&lt;/a&gt;
shows which drivers support ephemeral volumes.&lt;/div&gt;
&lt;/blockquote&gt;


Conceptually, CSI ephemeral volumes are similar to `configMap`,
`downwardAPI` and `secret` volume types: the storage is managed locally on each
 node and is created together with other local resources after a Pod has been
scheduled onto a node. Kubernetes has no concept of rescheduling Pods
anymore at this stage. Volume creation has to be unlikely to fail,
otherwise Pod startup gets stuck. In particular, [storage capacity
aware Pod scheduling](/docs/concepts/storage/storage-capacity/) is *not*
supported for these volumes. They are currently also not covered by
the storage resource usage limits of a Pod, because that is something
that kubelet can only enforce for storage that it manages itself.


Here&#39;s an example manifest for a Pod that uses CSI ephemeral storage:

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: my-csi-app
spec:
  containers:
    - name: my-frontend
      image: busybox
      volumeMounts:
      - mountPath: &#34;/data&#34;
        name: my-csi-inline-vol
      command: [ &#34;sleep&#34;, &#34;1000000&#34; ]
  volumes:
    - name: my-csi-inline-vol
      csi:
        driver: inline.storage.kubernetes.io
        volumeAttributes:
          foo: bar
```

The `volumeAttributes` determine what volume is prepared by the
driver. These attributes are specific to each driver and not
standardized. See the documentation of each CSI driver for further
instructions.

As a cluster administrator, you can use a [PodSecurityPolicy](/docs/concepts/policy/pod-security-policy/) to control which CSI drivers can be used in a Pod, specified with the
[`allowedCSIDrivers` field](/docs/reference/generated/kubernetes-api/v1.19/#podsecuritypolicyspec-v1beta1-policy).
 --&gt;
&lt;h3 id=&#34;csi-ephemeral-volumes&#34;&gt;CSI 临时卷&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;这个特性需要启用 &lt;code&gt;CSIInlineVolume&lt;/code&gt;
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;。
从 k8s v1.16 开始该功能默认是开启的。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; CSI 临时卷只被一部分 CSI 驱动支持。 这个 k8s CSI
&lt;a href=&#34;https://kubernetes-csi.github.io/docs/drivers.html&#34;&gt;驱动列表&lt;/a&gt;
中显示了哪些驱动支持临时卷。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;在概念上， CSI 临时卷与 &lt;code&gt;configMap&lt;/code&gt;, &lt;code&gt;downwardAPI&lt;/code&gt; 和 &lt;code&gt;secret&lt;/code&gt; 这些类型的卷相似:
存储由每个节点本地管理并且在 Pod 被调度到节点上时随同其它本地资源一起创建。在这种情况下 k8s
就没有对 Pod 重新调度的概念。卷创建看下来是不可能失败的，否则 Pod 启动就会卡住。 特别是
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-capacity/&#34;&gt;Pod 调度存储容量感知&lt;/a&gt; 对这些卷是不受支持。
这种不支持目前还包含 Pod 所使用的存储资源的使用限制，因为 kubelet 只能执行那些能自己对自己进行管理存储的管理。
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;以下这个示例配置是一个使用 CSI 临时卷的 Pod:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-csi-app&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-frontend&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/data&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-csi-inline-vol&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sleep&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1000000&amp;#34;&lt;/span&gt; ]
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-csi-inline-vol&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;csi&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;driver&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;inline.storage.kubernetes.io&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeAttributes&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;volumeAttributes&lt;/code&gt; 决定驱动需要准备什么卷。 这些属性因驱动的不同而不同，没有标准。 具体配置
请参阅对应驱动的文档&lt;/p&gt;
&lt;p&gt;作为一个集群管理员，可能使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/policy/pod-security-policy/&#34;&gt;PodSecurityPolicy&lt;/a&gt;
Pod 中可以使用哪些 CSI 驱动， 通过
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#podsecuritypolicyspec-v1beta1-policy&#34;&gt;&lt;code&gt;allowedCSIDrivers&lt;/code&gt; 字段&lt;/a&gt;
来指定&lt;/p&gt;
&lt;!--
### Generic ephemeral volumes






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [alpha]&lt;/code&gt;
&lt;/div&gt;



This feature requires the `GenericEphemeralVolume` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) to be
enabled. Because this is an alpha feature, it is disabled by default.

Generic ephemeral volumes are similar to `emptyDir` volumes, just more
flexible:
- Storage can be local or network-attached.
- Volumes can have a fixed size that Pods are not able to exceed.
- Volumes may have some initial data, depending on the driver and
  parameters.
- Typical operations on volumes are supported assuming that the driver
  supports them, including
  ([snapshotting](/docs/concepts/storage/volume-snapshots/),
  [cloning](/docs/concepts/storage/volume-pvc-datasource/),
  [resizing](/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims),
  and [storage capacity tracking](/docs/concepts/storage/storage-capacity/).

Example:

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: my-app
spec:
  containers:
    - name: my-frontend
      image: busybox
      volumeMounts:
      - mountPath: &#34;/scratch&#34;
        name: scratch-volume
      command: [ &#34;sleep&#34;, &#34;1000000&#34; ]
  volumes:
    - name: scratch-volume
      ephemeral:
        volumeClaimTemplate:
          metadata:
            labels:
              type: my-frontend-volume
          spec:
            accessModes: [ &#34;ReadWriteOnce&#34; ]
            storageClassName: &#34;scratch-storage-class&#34;
            resources:
              requests:
                storage: 1Gi
```
 --&gt;
&lt;h3 id=&#34;通用临时卷&#34;&gt;通用临时卷&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;这个特性需要启用 &lt;code&gt;GenericEphemeralVolume&lt;/code&gt;
&lt;a href=&#34;https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;
来开启。 因为这是一个 alpha 特性，默认是关闭的。&lt;/p&gt;
&lt;p&gt;通用临时卷与 &lt;code&gt;emptyDir&lt;/code&gt; 卷相似，只是更加灵活:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;存储可以在本地或网络挂载&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;卷可以有固定容量， Pod 不能使用超限&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;卷可以基于驱动和参数拥有初始数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果驱动支持它们也支持对于卷的典型操作，包含
(&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volume-snapshots/&#34;&gt;快照&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volume-pvc-datasource/&#34;&gt;克隆&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims&#34;&gt;扩容&lt;/a&gt;,
和 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-capacity/&#34;&gt;存储容量跟踪&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-app&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-frontend&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/scratch&amp;#34;&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;scratch-volume&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sleep&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1000000&amp;#34;&lt;/span&gt; ]
  &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;scratch-volume&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;ephemeral&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeClaimTemplate&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-frontend-volume&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ReadWriteOnce&amp;#34;&lt;/span&gt; ]
            &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;scratch-storage-class&amp;#34;&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
              &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
                &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1Gi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Lifecycle and PersistentVolumeClaim

The key design idea is that the
[parameters for a volume claim](/docs/reference/generated/kubernetes-api/v1.19/#ephemeralvolumesource-v1alpha1-core)
are allowed inside a volume source of the Pod. Labels, annotations and
the whole set of fields for a PersistentVolumeClaim are supported. When such a Pod gets
created, the ephemeral volume controller then creates an actual PersistentVolumeClaim
object in the same namespace as the Pod and ensures that the PersistentVolumeClaim
gets deleted when the Pod gets deleted.

That triggers volume binding and/or provisioning, either immediately if
the &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes&#39; target=&#39;_blank&#39;&gt;StorageClass&lt;span class=&#39;tooltip-text&#39;&gt;StorageClass 为管理员提供了一种描述不同存储类别的方式&lt;/span&gt;
&lt;/a&gt; uses immediate volume binding or when the Pod is
tentatively scheduled onto a node (`WaitForFirstConsumer` volume
binding mode). The latter is recommended for generic ephemeral volumes
because then the scheduler is free to choose a suitable node for
the Pod. With immediate binding, the scheduler is forced to select a node that has
access to the volume once it is available.

In terms of [resource ownership](/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents),
a Pod that has generic ephemeral storage is the owner of the PersistentVolumeClaim(s)
that provide that ephemeral storage. When the Pod is deleted,
the Kubernetes garbage collector deletes the PVC, which then usually
triggers deletion of the volume because the default reclaim policy of
storage classes is to delete volumes. You can create quasi-ephemeral local storage
using a StorageClass with a reclaim policy of `retain`: the storage outlives the Pod,
and in this case you need to ensure that volume clean up happens separately.

While these PVCs exist, they can be used like any other PVC. In
particular, they can be referenced as data source in volume cloning or
snapshotting. The PVC object also holds the current status of the
volume.
 --&gt;
&lt;h3 id=&#34;生命周期-和-persistentvolumeclaim&#34;&gt;生命周期 和 PersistentVolumeClaim&lt;/h3&gt;
&lt;p&gt;该设计主旨就是
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#ephemeralvolumesource-v1alpha1-core&#34;&gt;卷申领的参数&lt;/a&gt;
可以在 Pod 的卷源中。 标签，注解，PersistentVolumeClaim 的一堆字段都是支持的。 当这样一个
Pod 被创建时， 临时卷控制器实际是创建一个与 Pod 在同一个命名空间的 PersistentVolumeClaim，
并且确保这个 PersistentVolumeClaim 在 Pod 删除时也会被删除。&lt;/p&gt;
&lt;p&gt;在触发卷绑定和/或供应时，可能是即时的，如果
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/storage-classes&#39; target=&#39;_blank&#39;&gt;StorageClass&lt;span class=&#39;tooltip-text&#39;&gt;StorageClass 为管理员提供了一种描述不同存储类别的方式&lt;/span&gt;
&lt;/a&gt;
使用的绑定模式是即时绑定， 也可能是在 Pod 调度到节点时(&lt;code&gt;WaitForFirstConsumer&lt;/code&gt; 卷绑定模式)。
对于通用临时卷推荐使用后者，因为这样调度器可以自由的选择合适的节点。 如果使用即时绑定，调度器就会
限制在卷变为可用时，可以访问到卷的那些节点中选择一个。&lt;/p&gt;
&lt;p&gt;在术语
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents&#34;&gt;资源所有权&lt;/a&gt;,
中， 拥有这个通用临时存储的 Pod 是这个(些)提供这些临时存储的 PersistentVolumeClaim 的拥有者。
当 Pod 被删除后， k8s 垃圾回收器就会删除 PVC，通常这就会触发卷的删除，因为 StorageClass
默认的回收策略就是删除卷。 也可以使用回收策略为保留(&lt;code&gt;retain&lt;/code&gt;)的 StorageClass 创建一个准临时本地存储：
存储会比使用它的 Pod 存在时间更久， 在这种情况下需要保证独立执行卷清理工作。&lt;/p&gt;
&lt;p&gt;当这种 PVC 存在时，它们可以当做任意其它的 PVC 来用。 特别是它们可以被用作克隆或快照所引用的数据
源。 这些 PVC 对象也包含了卷的当前状态。&lt;/p&gt;
&lt;!--
### PersistentVolumeClaim naming

Naming of the automatically created PVCs is deterministic: the name is
a combination of Pod name and volume name, with a hyphen (`-`) in the
middle. In the example above, the PVC name will be
`my-app-scratch-volume`.  This deterministic naming makes it easier to
interact with the PVC because one does not have to search for it once
the Pod name and volume name are known.

The deterministic naming also introduces a potential conflict between different
Pods (a Pod &#34;pod-a&#34; with volume &#34;scratch&#34; and another Pod with name
&#34;pod&#34; and volume &#34;a-scratch&#34; both end up with the same PVC name
&#34;pod-a-scratch&#34;) and between Pods and manually created PVCs.

Such conflicts are detected: a PVC is only used for an ephemeral
volume if it was created for the Pod. This check is based on the
ownership relationship. An existing PVC is not overwritten or
modified. But this does not resolve the conflict because without the
right PVC, the Pod cannot start.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; Take care when naming Pods and volumes inside the
same namespace, so that these conflicts can&amp;rsquo;t occur.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;persistentvolumeclaim-命名&#34;&gt;PersistentVolumeClaim 命名&lt;/h3&gt;
&lt;p&gt;自动创建的 PVC 的名称的组成规则为: Pod 名称和 卷名称的组合，中间通过连字符(&lt;code&gt;-&lt;/code&gt;)连接。
在上面的示例中， PVC 的名称就是 &lt;code&gt;my-app-scratch-volume&lt;/code&gt;. 这种决定名称的方式可以简单与
PVC 的交互， 因为当知道 Pod 名称和卷名称时就相当于直接知道了 PVC 的名称，没有必要再查找。&lt;/p&gt;
&lt;p&gt;这种命名方式也会在不同 Pod 间引入潜在的命名冲突(一个名称为 &amp;ldquo;pod-a&amp;rdquo; 的 Pod 有一个名称为 &amp;ldquo;scratch&amp;rdquo;
与一个名称为 &amp;ldquo;pod&amp;rdquo; 的Pod 有一个名称为 &amp;ldquo;a-scratch&amp;rdquo; 卷，最终都会使用同一个 PVC 名称， 就是
&amp;ldquo;pod-a-scratch&amp;rdquo;) 并且与手动创建 PVC 的 Pod 也有类似风险。&lt;/p&gt;
&lt;p&gt;这类冲突的发现机制: 如果一个 PVC 是为一个 Pod 创建的它就只能用作一个临时卷。 这种检查是基于
所有权关系的。 一个存储的 PVC 是不能被覆盖或修改的。 但是这样并不能解决冲突，因为没正确的 PVC，
Pod 是不能启动的。&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 在同一个命名空间计划好 Pod 和 卷的命名就能避免这种冲突的发生。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Security

Enabling the GenericEphemeralVolume feature allows users to create
PVCs indirectly if they can create Pods, even if they do not have
permission to create PVCs directly. Cluster administrators must be
aware of this. If this does not fit their security model, they have
two choices:
- Explicitly disable the feature through the feature gate, to avoid
  being surprised when some future Kubernetes version enables it
  by default.
- Use a [Pod Security
  Policy](/docs/concepts/policy/pod-security-policy/) where the
  `volumes` list does not contain the `ephemeral` volume type.

The normal namespace quota for PVCs in a namespace still applies, so
even if users are allowed to use this new mechanism, they cannot use
it to circumvent other policies.
 --&gt;
&lt;h3 id=&#34;安全&#34;&gt;安全&lt;/h3&gt;
&lt;p&gt;启用 GenericEphemeralVolume 特性允许用户在创建 Pod 时间接地创建 PVC， 即便这些用户有直接
创建 PVC 的权限. 集群管理员一定要知道这点。 如果这不符合需要的安全模型，有两种选择:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过功能阀显示地禁用这个特性，防止如果在将来的 k8s 版本中默认开启了该特性而被吓到。&lt;/li&gt;
&lt;li&gt;使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/policy/pod-security-policy/&#34;&gt;Pod 安全策略&lt;/a&gt; 保证
&lt;code&gt;volumes&lt;/code&gt; 列表中没有 &lt;code&gt;ephemeral&lt;/code&gt; 这个卷类型。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常规的命名空间的限额对于这个命名空间中的 PVC 依然是起作用的，所以即便允许用户使用这个新的机制，
也不可能使用这个特性规避其它的策略。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
### Ephemeral volumes managed by kubelet

See [local ephemeral storage](/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage).
 --&gt;
&lt;h3 id=&#34;由-kubelet-管理的临时卷&#34;&gt;由 kubelet 管理的临时卷&lt;/h3&gt;
&lt;p&gt;见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage&#34;&gt;本地临时存储&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### CSI ephemeral volumes

- For more information on the design, see the [Ephemeral Inline CSI
  volumes KEP](https://github.com/kubernetes/enhancements/blob/ad6021b3d61a49040a3f835e12c8bb5424db2bbb/keps/sig-storage/20190122-csi-inline-volumes.md).
- For more information on further development of this feature, see the [enhancement tracking issue #596](https://github.com/kubernetes/enhancements/issues/596).
 --&gt;
&lt;h3 id=&#34;csi-临时卷&#34;&gt;CSI 临时卷&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;要了解更多设计上的信息，见
&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/ad6021b3d61a49040a3f835e12c8bb5424db2bbb/keps/sig-storage/20190122-csi-inline-volumes.md&#34;&gt;Ephemeral Inline CSI volumes KEP&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;要了解该功能的未来开发情况见
&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/596&#34;&gt;enhancement tracking issue #596&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Generic ephemeral volumes

- For more information on the design, see the
[Generic ephemeral inline volumes KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1698-generic-ephemeral-volumes/README.md).
- For more information on further development of this feature, see the [enhancement tracking issue #1698](https://github.com/kubernetes/enhancements/issues/1698).
 --&gt;
&lt;h3 id=&#34;通用临时卷-1&#34;&gt;通用临时卷&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;要了解更多设计上的信息，见
&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1698-generic-ephemeral-volumes/README.md&#34;&gt;Generic ephemeral inline volumes KEP&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;要了解该功能的未来开发情况见
&lt;a href=&#34;https://github.com/kubernetes/enhancements/issues/1698&#34;&gt;enhancement tracking issue #1698&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 节点级别的卷限制</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-limits/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/storage/storage-limits/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jsafrane
- saad-ali
- thockin
- msau42
title: Node-specific Volume Limits
content_type: concept
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This page describes the maximum number of volumes that can be attached
to a Node for various cloud providers.

Cloud providers like Google, Amazon, and Microsoft typically have a limit on
how many volumes can be attached to a Node. It is important for Kubernetes to
respect those limits. Otherwise, Pods scheduled on a Node could get stuck
waiting for volumes to attach.
--&gt;
&lt;p&gt;本文主要介绍在各家云提供商上每个节点能够挂载的卷的最大数量。
像 Google, Amazon, 和 Microsoft 这样典型的云提供上对于每个节点上能挂载的卷数量是有限制的。
让 k8s 遵守这个限制是比较重要的。 否则，Pod 在调度到一个节点后就会卡在等待卷挂载上。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Kubernetes default limits

The Kubernetes scheduler has default limits on the number of volumes
that can be attached to a Node:

&lt;table&gt;
  &lt;tr&gt;&lt;th&gt;Cloud service&lt;/th&gt;&lt;th&gt;Maximum volumes per Node&lt;/th&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://aws.amazon.com/ebs/&#34;&gt;Amazon Elastic Block Store (EBS)&lt;/a&gt;&lt;/td&gt;&lt;td&gt;39&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://cloud.google.com/persistent-disk/&#34;&gt;Google Persistent Disk&lt;/a&gt;&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/services/storage/main-disks/&#34;&gt;Microsoft Azure Disk Storage&lt;/a&gt;&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;
 --&gt;
&lt;h2 id=&#34;k8s-默认的限制&#34;&gt;k8s 默认的限制&lt;/h2&gt;
&lt;p&gt;k8s 调度器对每个节点上能够挂载的卷的数量是有限制的:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;云服务&lt;/th&gt;
&lt;th&gt;每个节点挂载卷的最大值&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://aws.amazon.com/ebs/&#34;&gt;Amazon Elastic Block Store (EBS)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;39&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://cloud.google.com/persistent-disk/&#34;&gt;Google Persistent Disk&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/services/storage/main-disks/&#34;&gt;Microsoft Azure Disk Storage&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
## Custom limits

You can change these limits by setting the value of the
`KUBE_MAX_PD_VOLS` environment variable, and then starting the scheduler.
CSI drivers might have a different procedure, see their documentation
on how to customize their limits.

Use caution if you set a limit that is higher than the default limit. Consult
the cloud provider&#39;s documentation to make sure that Nodes can actually support
the limit you set.

The limit applies to the entire cluster, so it affects all Nodes.
--&gt;
&lt;!--
## Custom limits

You can change these limits by setting the value of the
`KUBE_MAX_PD_VOLS` environment variable, and then starting the scheduler.
CSI drivers might have a different procedure, see their documentation
on how to customize their limits.

Use caution if you set a limit that is higher than the default limit. Consult
the cloud provider&#39;s documentation to make sure that Nodes can actually support
the limit you set.

The limit applies to the entire cluster, so it affects all Nodes.
 --&gt;
&lt;h2 id=&#34;自定义限制&#34;&gt;自定义限制&lt;/h2&gt;
&lt;p&gt;可以通过设置 &lt;code&gt;KUBE_MAX_PD_VOLS&lt;/code&gt; 环境变量的值来修改这个限制， 修改后重新启动调度器。
CSI 驱动可能需要不同的步骤，具体怎么修改这个自定义配置见他们的文档。&lt;/p&gt;
&lt;p&gt;要小心设置了一个比默认值大的限制。 参照云提供商的文档来决定节点实际支持的限制再设置这个值。&lt;/p&gt;
&lt;p&gt;这个限制会应用到整个集群，所以它会影响所有的节点。&lt;/p&gt;
&lt;!--
## Dynamic volume limits






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [stable]&lt;/code&gt;
&lt;/div&gt;



Dynamic volume limits are supported for following volume types.

- Amazon EBS
- Google Persistent Disk
- Azure Disk
- CSI

For volumes managed by in-tree volume plugins, Kubernetes automatically determines the Node
type and enforces the appropriate maximum number of volumes for the node. For example:

* On
&lt;a href=&#34;https://cloud.google.com/compute/&#34;&gt;Google Compute Engine&lt;/a&gt;,
up to 127 volumes can be attached to a node, [depending on the node
type](https://cloud.google.com/compute/docs/disks/#pdnumberlimits).

* For Amazon EBS disks on M5,C5,R5,T3 and Z1D instance types, Kubernetes allows only 25
volumes to be attached to a Node. For other instance types on
&lt;a href=&#34;https://aws.amazon.com/ec2/&#34;&gt;Amazon Elastic Compute Cloud (EC2)&lt;/a&gt;,
Kubernetes allows 39 volumes to be attached to a Node.

* On Azure, up to 64 disks can be attached to a node, depending on the node type. For more details, refer to [Sizes for virtual machines in Azure](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes).

* If a CSI storage driver advertises a maximum number of volumes for a Node (using `NodeGetInfo`), the &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-scheduler/&#39; target=&#39;_blank&#39;&gt;kube-scheduler&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.&lt;/span&gt;
&lt;/a&gt; honors that limit.
Refer to the [CSI specifications](https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetinfo) for details.

* For volumes managed by in-tree plugins that have been migrated to a CSI driver, the maximum number of volumes will be the one reported by the CSI driver.
--&gt;
&lt;h2 id=&#34;动态卷限制&#34;&gt;动态卷限制&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.17 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;以下类型的卷支持动态卷限制：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amazon EBS&lt;/li&gt;
&lt;li&gt;Google Persistent Disk&lt;/li&gt;
&lt;li&gt;Azure Disk&lt;/li&gt;
&lt;li&gt;CSI&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于由内部卷插件管理的卷， k8s 自动决定节点的类型并为节点设置合适的最大卷数量限制。 例如:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在
&lt;a href=&#34;https://cloud.google.com/compute/&#34;&gt;Google Compute Engine&lt;/a&gt;
上
&lt;a href=&#34;https://cloud.google.com/compute/docs/disks/#pdnumberlimits&#34;&gt;基于节点的类型&lt;/a&gt;.
每个节点最多可以挂载 127 个卷。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于 Amazon EBS 磁盘， 在 M5,C5,R5,T3 和 Z1D 类型的实例上，k8s 允许挂载到每个节点上的
卷的最大值是 25. 对于
&lt;a href=&#34;https://aws.amazon.com/ec2/&#34;&gt;Amazon Elastic Compute Cloud (EC2)&lt;/a&gt;
上其它类型的实例类型， k8s 允许挂载到每个节点上能挂载卷的最大值是 39&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 Azure, 基于节点类型的不同，每个节点上能挂载磁盘数量是 64， 更多信息见
&lt;a href=&#34;https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes&#34;&gt;Azure 中虚拟机的大小&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果一个 CSI 存储驱动指定了每个节点的最大卷数量(通过 &lt;code&gt;NodeGetInfo&lt;/code&gt;)，
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-scheduler/&#39; target=&#39;_blank&#39;&gt;kube-scheduler&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.&lt;/span&gt;
&lt;/a&gt;， 也会
接受这个限制。
详见
&lt;a href=&#34;https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetinfo&#34;&gt;CSI specifications&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于由内部插件管理的卷，但已经迁移到了 CSI 驱动的，节点能够挂载的卷的最大数量会由 CSI 驱动
来报告这个值。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
