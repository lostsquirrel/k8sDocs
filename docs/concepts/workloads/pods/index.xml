<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes – Pod</title>
    <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/</link>
    <description>Recent content in Pod on Kubernetes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 02 Jul 2020 00:00:00 +0000</lastBuildDate>
    
	  <atom:link href="https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Pod 生命周期</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/</guid>
      <description>
        
        
        &lt;!--
---
title: Pod Lifecycle
content_type: concept
weight: 30
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This page describes the lifecycle of a Pod. Pods follow a defined lifecycle, starting
in the `Pending` [phase](#pod-phase), moving through `Running` if at least one
of its primary containers starts OK, and then through either the `Succeeded` or
`Failed` phases depending on whether any container in the Pod terminated in failure.

Whilst a Pod is running, the kubelet is able to restart containers to handle some
kind of faults. Within a Pod, Kubernetes tracks different container
[states](#container-states) and handles

In the Kubernetes API, Pods have both a specification and an actual status. The
status for a Pod object consists of a set of [Pod conditions](#pod-conditions).
You can also inject [custom readiness information](#pod-readiness-gate) into the
condition data for a Pod, if that is useful to your application.

Pods are only [scheduled](/k8sDocs/docs/concepts/scheduling-eviction/) once in their lifetime.
Once a Pod is scheduled (assigned) to a Node, the Pod runs on that Node until it stops
or is [terminated](#pod-termination).


 --&gt;
&lt;p&gt;本文介经 Pod 的生命周期。 Pod 有一个既定的生命周期，开启后进行 &lt;code&gt;Pending&lt;/code&gt; &lt;a href=&#34;#pod-phase&#34;&gt;阶段&lt;/a&gt;，
当其中至少有一个主要容器正常启动后变更为 &lt;code&gt;Running&lt;/code&gt; 阶段， 如果所有容器全部正常启动则进入 &lt;code&gt;Succeeded&lt;/code&gt; 阶段，
如果有任意容器启动失败则进行 &lt;code&gt;Failed&lt;/code&gt; 阶段。&lt;/p&gt;
&lt;p&gt;当一个 Pod 在运行中， kubelet 可以在某些情况下容器挂掉后将其重启。 在 Pod 中， k8s 会跟踪和处理容器的 &lt;a href=&#34;#container-states&#34;&gt;状态&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在 k8s 的 API 对象中， Pod 对象拥有定义明细和实时状态。 Pod 对象的状态上包含一系列 &lt;a href=&#34;#pod-conditions&#34;&gt;Pod 条件&lt;/a&gt;
如果应用有需要，可以向 Pod 中加入 &lt;a href=&#34;#pod-readiness-gate&#34;&gt;自定义就绪信息&lt;/a&gt; 到条件子对象。&lt;/p&gt;
&lt;p&gt;在 Pod 的整个生命周期中只会被&lt;a href=&#34;../../../scheduling-eviction/&#34;&gt;调度&lt;/a&gt;一次，
当一个 Pod 被调度(分配)到一个节点后，就会一直运行在这个节点上，直接被停止或被&lt;a href=&#34;#pod-termination&#34;&gt;终止&lt;/a&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Pod lifetime

Like individual application containers, Pods are considered to be relatively
ephemeral (rather than durable) entities. Pods are created, assigned a unique
ID ([UID](/k8sDocs/docs/concepts/overview/working-with-objects/names/#uids)), and scheduled
to nodes where they remain until termination (according to restart policy) or
deletion.  
If a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;节点&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; dies, the Pods scheduled to that node
are [scheduled for deletion](#pod-garbage-collection) after a timeout period.

Pods do not, by themselves, self-heal. If a Pod is scheduled to a
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;node&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; that then fails,
or if the scheduling operation itself fails, the Pod is deleted; likewise, a Pod won&#39;t
survive an eviction due to a lack of resources or Node maintenance. Kubernetes uses a
higher-level abstraction, called a
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;controller&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt;, that handles the work of
managing the relatively disposable Pod instances.

A given Pod (as defined by a UID) is never &#34;rescheduled&#34; to a different node; instead,
that Pod can be replaced by a new, near-identical Pod, with even the same name i
desired, but with a different UID.

When something is said to have the same lifetime as a Pod, such as a
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;volume&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;,
that means that the thing exists as long as that specific Pod (with that exact UID)
exists. If that Pod is deleted for any reason, and even if an identical replacement
is created, the related thing (a volume, in this example) is also destroyed and
created anew.

&lt;figure&gt;
    &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/images/docs/pod.svg&#34; width=&#34;50%&#34;/&gt; &lt;figcaption&gt;
            &lt;h4&gt;Pod diagram&lt;/h4&gt;
        &lt;/figcaption&gt;
&lt;/figure&gt;


*A multi-container Pod that contains a file puller and a
web server that uses a persistent volume for shared storage between the containers.*

 --&gt;
&lt;h2 id=&#34;pod-的一生&#34;&gt;Pod 的一生&lt;/h2&gt;
&lt;p&gt;与单独使用应用容器一样, Pod 可以被认为是一个相对临时(而不是长期存在)的实体. Pod 在创建时会被分配
一个唯一的 ID(&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names/#uids&#34;&gt;UID&lt;/a&gt;),
然后被到一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;node&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; 上,直到被终止(依照重启策略)或者被删除.
如果一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;node&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; 挂了, 这个节点上的 Pod 会在超时后
&lt;a href=&#34;#pod-garbage-collection&#34;&gt;因删除被调度&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Pod 并不能独自实现自愈. 如果 Pod 被调度的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;node&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; 挂了,
或者是调度操作本身失败, Pod 就被删除了, Pod 也不会在因为资源不足或 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/00-nodes/&#39; target=&#39;_blank&#39;&gt;node&lt;span class=&#39;tooltip-text&#39;&gt;一个节点就是 k8s 中的一个工作机&lt;/span&gt;
&lt;/a&gt; 节点被驱逐中幸存.
k8s 通过一个叫 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; 的更高层级的抽象,来处理这些相对来说是一次的的 Pod 实例的管理工作.&lt;/p&gt;
&lt;p&gt;某个 Pod(拥有特定 UID) 是永远不会被重新调度到另一个节点上; 而是被一个基本相同, 甚至可以名称也相同,但 UID 不同的 Pod 所取代.&lt;/p&gt;
&lt;p&gt;当某些对象(如 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;volume&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;) 被描述为与 Pod 拥有一致的生命期,
表示这些对象会与指定的 (拥有那个 UID 的) &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 同时存在,
如果那个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 因为某些原为被删除, 即便同样的代替 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 已经被创建,
相关的对象(如 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;卷(Volume)&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt; 也会被随同 Pod 一起被销毁重建).&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
    &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/docs/pod.svg&#34;
         alt=&#34;Pod diagram&#34; width=&#34;50%&#34;/&gt; 
&lt;/figure&gt;

&lt;em&gt;有一个容器作为 web 服务，为共享数据卷的文件提供访问服务，另一个独立的容器作为 边车，负责从远程的源更新这些文件&lt;/em&gt;&lt;/p&gt;
&lt;!--  
## Pod phase

A Pod&#39;s `status` field is a
[PodStatus](/k8sDocs/reference/generated/kubernetes-api/v1.19/#podstatus-v1-core)
object, which has a `phase` field.

The phase of a Pod is a simple, high-level summary of where the Pod is in its
lifecycle. The phase is not intended to be a comprehensive rollup of observations
of container or Pod state, nor is it intended to be a comprehensive state machine.

The number and meanings of Pod phase values are tightly guarded.
Other than what is documented here, nothing should be assumed about Pods that
have a given `phase` value.

Here are the possible values for `phase`:

Value | Description
:-----|:-----------
`Pending` | The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to bescheduled as well as the time spent downloading container images over the network.
`Running` | The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.
`Succeeded` | All containers in the Pod have terminated in success, and will not be restarted.
`Failed` | All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system.
`Unknown` | For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.

If a node dies or is disconnected from the rest of the cluster, Kubernetes
applies a policy for setting the `phase` of all Pods on the lost node to Failed.
--&gt;
&lt;h2 id=&#34;pod-的人生阶段&#34;&gt;Pod 的人生阶段&lt;/h2&gt;
&lt;p&gt;在 Pod 的 &lt;code&gt;status&lt;/code&gt; 字段是一个 &lt;a href=&#34;https://kubernetes.io/k8sDocs/reference/generated/kubernetes-api/v1.19/#podstatus-v1-core&#34;&gt;PodStatus&lt;/a&gt;
对象, 上面有一个 &lt;code&gt;phase&lt;/code&gt; 字段.&lt;/p&gt;
&lt;p&gt;Pod 的人生阶段是对 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 生命周期的调度总结.
Pod 的人生阶段并不是对容器或 Pod 状态的容易理解的总结, 也不是一个容易理解的状态机.&lt;/p&gt;
&lt;p&gt;Pod 的人生阶段的数量与意义与其值都是很有限的. 除了以下对各阶段的说明, Pod 不会有其它的阶段.
以下为 &lt;code&gt;phase&lt;/code&gt; 字段的可能值:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;字段值&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Pending&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 已经被集群确立， 但是其中的一个或多个容器还没有完成配置并准备就绪。 包括 Pod 等调度的时间和从网上下载容器镜像的时间&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Running&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 已经在节点上，并且其中的所有容器已经完成创建，至少有一个容器正在运行，或在启动或重启的过程中&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Succeeded&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 中所有的容器都已经成功运行完成并终止，并且不会再被重启&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Failed&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Pod 中所有的容器被终止，且至少有一个容器是因为失败而被终止的。 容器失败的原因可能是因返回非零而退出或被系统终止&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;Unknown&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;因为某些原因导至无法获取 Pod 的状态。 这个阶段一般是因为与 Pod 所在的节点无法通信。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;如果一个节点挂了或者与集群失联， k8s 会执行一个策略，让节点上所有的 Pod 的 &lt;code&gt;phase&lt;/code&gt; 字段设置为 &lt;code&gt;Failed&lt;/code&gt;。&lt;/p&gt;
&lt;!--
## Container states

As well as the [phase](#pod-phase) of the Pod overall, Kubernetes tracks the state of
each container inside a Pod. You can use
[container lifecycle hooks](/k8sDocs/docs/concepts/containers/container-lifecycle-hooks/) to
trigger events to run at certain points in a container&#39;s lifecycle.

Once the &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-scheduler/&#39; target=&#39;_blank&#39;&gt;scheduler&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.&lt;/span&gt;
&lt;/a&gt;
assigns a Pod to a Node, the kubelet starts creating containers for that Pod
using a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;container runtime&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt;.
There are three possible container states: `Waiting`, `Running`, and `Terminated`.

To the check state of a Pod&#39;s containers, you can use
`kubectl describe pod &lt;name-of-pod&gt;`. The output shows the state for each container
within that Pod.

Each state has a specific meaning:

 --&gt;
&lt;h2 id=&#34;容器的状态&#34;&gt;容器的状态&lt;/h2&gt;
&lt;p&gt;与 Pod 存在几个&lt;a href=&#34;#pod-phase&#34;&gt;阶段&lt;/a&gt;一个样，k8s 也会跟踪 Pod 内的容器的状态。 用户可以通过
&lt;a href=&#34;../../../containers/container-lifecycle-hooks/&#34;&gt;容器的生命周期钩子&lt;/a&gt;
来以容器生命周期事件来触发一些需要工作的运行&lt;/p&gt;
&lt;p&gt;当一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-scheduler/&#39; target=&#39;_blank&#39;&gt;kube-scheduler&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.&lt;/span&gt;
&lt;/a&gt;
将一个 Pod 调度到一个节点时， kubelet 就会通过 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;容器运行环境&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt;
为该 Pod 创建对应的容器。 容器可能存在三种状态 &lt;code&gt;Waiting&lt;/code&gt;, &lt;code&gt;Running&lt;/code&gt;, &lt;code&gt;Terminated&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;用户可以通过命令 &lt;code&gt;kubectl describe pod &amp;lt;name-of-pod&amp;gt;&lt;/code&gt; 查看 Pod 中容器的状态。
命令输出结果会包含其中所有容器的状态。
接下来介绍每一种状的具体含义&lt;/p&gt;
&lt;h3 id=&#34;container-state-waiting&#34;&gt;&lt;code&gt;Waiting&lt;/code&gt;&lt;/h3&gt;
&lt;!--
If a container is not in either the `Running` or `Terminated` state, it `Waiting`.
A container in the `Waiting` state is still running the operations it requires in
order to complete start up: for example, pulling the container image from a container
image registry, or applying &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;存放如密码, OAuth, ssh 密钥等敏感信息&lt;/span&gt;
&lt;/a&gt;
data.
When you use `kubectl` to query a Pod with a container that is `Waiting`, you also see
a Reason field to summarize why the container is in that state.
 --&gt;
&lt;p&gt;当一个容器的状不是 &lt;code&gt;Running&lt;/code&gt; 或 &lt;code&gt;Terminated&lt;/code&gt; 就是 &lt;code&gt;Waiting&lt;/code&gt;。当一个容器状态为 &lt;code&gt;Waiting&lt;/code&gt;
表示容器正在进行启动需要的前置操作: 如， 从镜像仓库拉取容器所需要的镜像， 或配置
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;存放如密码, OAuth, ssh 密钥等敏感信息&lt;/span&gt;
&lt;/a&gt; 数据。&lt;/p&gt;
&lt;p&gt;当 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;Kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt; 查询到Pod中的状是 &lt;code&gt;Waiting&lt;/code&gt;， 同时也会看到
一个 &lt;code&gt;Reason&lt;/code&gt; 字段，其值是对容器保持在该状态原因的总结&lt;/p&gt;
&lt;h3 id=&#34;container-state-running&#34;&gt;&lt;code&gt;Running&lt;/code&gt;&lt;/h3&gt;
&lt;!--
The `Running` status indicates that a container is executing without issues. If there
was a `postStart` hook configured, it has already executed and executed. When you use
`kubectl` to query a Pod with a container that is `Running`, you also see information
about when the container entered the `Running` state.

 --&gt;
&lt;p&gt;&lt;code&gt;Running&lt;/code&gt; 状态表示容器正在欢快地运行，没啥毛病。 如果配置了钩子 &lt;code&gt;postStart&lt;/code&gt;， 这个钩子的处理器也
已经执行而且成功完成。 当使用 &lt;code&gt;kubectl&lt;/code&gt; 查询容器为 &lt;code&gt;Running&lt;/code&gt; 的 Pod 时，同时可以看到容器进入
&lt;code&gt;Running&lt;/code&gt; 状态的时长。&lt;/p&gt;
&lt;h3 id=&#34;container-state-terminated&#34;&gt;&lt;code&gt;Terminated&lt;/code&gt;&lt;/h3&gt;
&lt;!--
A container in the `Terminated` state has begin execution and has then either run to
completion or has failed for some reason. When you use `kubectl` to query a Pod with
a container that is `Terminated`, you see a reason, and exit code, and the start and
finish time for that container&#39;s period of execution.

If a container has a `preStop` hook configured, that runs before the container enters
the `Terminated` state.
 --&gt;
&lt;p&gt;当一个容器状态为 &lt;code&gt;Terminated&lt;/code&gt; 时，表示任务已经执行了，要么执行完成，要么因为某些原因失败了。
当使用 &lt;code&gt;kubectl&lt;/code&gt; 查看容器为 &lt;code&gt;Terminated&lt;/code&gt; 状态的 Pod 时， 可以看到原因和退出码， 还有
容器内任务执行的开始和结束时间。&lt;/p&gt;
&lt;p&gt;如果一个容器配置了钩子 &lt;code&gt;preStop&lt;/code&gt;， 那么钩子对应的处理器会在容器进行&lt;code&gt;Terminated&lt;/code&gt; 状态之前执行。&lt;/p&gt;
&lt;!--
## Container restart policy {#restart-policy}

The `spec` of a Pod has a `restartPolicy` field with possible values Always, OnFailure,
and Never. The default value is Always.

The `restartPolicy` applies to all containers in the Pod. `restartPolicy` only
refers to restarts of the containers by the kubelet on the same node. After containers
in a Pod exit, the kubelet restarts them with an exponential back-off delay (10s, 20s,
40s, …), that is capped at five minutes. Once a container has executed with no problems
for 10 minutes without any problems, the kubelet resets the restart backoff timer for
that container.
 --&gt;
&lt;h2 id=&#34;restart-policy&#34;&gt;容器的重启策略&lt;/h2&gt;
&lt;p&gt;在 Pod 的 &lt;code&gt;spec&lt;/code&gt; 子对象上有一个 &lt;code&gt;restartPolicy&lt;/code&gt; 字段，可能的值有 &lt;code&gt;Always&lt;/code&gt;, &lt;code&gt;OnFailure&lt;/code&gt;, &lt;code&gt;Never&lt;/code&gt;
默认为 &lt;code&gt;Always&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;restartPolicy&lt;/code&gt; 适用于 Pod 中的所有容器。 &lt;code&gt;restartPolicy&lt;/code&gt; 只能让一个节点上的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;Kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt;
来重启其上的容器。 当 Pod 中的容器退出后， &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;Kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt;
会以指数延迟(10s, 20s, 40s, …)补偿机制来重启容器，延迟时间最长为 5 分钟。 当一个容器正常运行 10 分钟
后，  &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;Kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt; 才会重置该容器的补偿时钟。&lt;/p&gt;
&lt;!--
## Pod conditions

A Pod has a PodStatus, which has an array of
[PodConditions](https://kubernetes.io/k8sDocs/reference/generated/kubernetes-api/v1.19/#podcondition-v1-core)
through which the Pod has or has not passed:

* `PodScheduled`: the Pod has been scheduled to a node.
* `ContainersReady`: all containers in the Pod are ready.
* `Initialized`: all [init containers](/k8sDocs/docs/concepts/workloads/pods/init-containers/)
  have started successfully.
* `Ready`: the Pod is able to serve requests and should be added to the load
  balancing pools of all matching Services.

Field name           | Description
:--------------------|:-----------
`type`               | Name of this Pod condition.
`status`             | Indicates whether that condition is applicable, with possible values &#34;`True`&#34;, &#34;`False`&#34;, or &#34;`Unknown`&#34;.
`lastProbeTime`      | Timestamp of when the Pod condition was last probed.
`lastTransitionTime` | Timestamp for when the Pod last transitioned from one status to another.
`reason`             | Machine-readable, UpperCamelCase text indicating the reason for the condition&#39;s last transition.
`message`            | Human-readable message indicating details about the last status transition.
 --&gt;
&lt;h2 id=&#34;pod-的就绪条件&#34;&gt;Pod 的就绪条件&lt;/h2&gt;
&lt;p&gt;在 Pod 上面有一个 &lt;code&gt;PodStatus&lt;/code&gt; 子对象，其中包含一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/generated/kubernetes-api/v1.19/#podcondition-v1-core&#34;&gt;PodConditions&lt;/a&gt;
的数组。表示这个 Pod 有没有通过这些条件， 具体如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PodScheduled&lt;/code&gt;: Pod 已经被调度到节点上.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ContainersReady&lt;/code&gt;: Pod 中所有的容器都已经就绪.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Initialized&lt;/code&gt;: 所有 &lt;a href=&#34;../init-containers/&#34;&gt;初始化容器&lt;/a&gt;
都启动成功.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Ready&lt;/code&gt;: Pod 已经能够处理请求，应该被加入对应 Service 的负载均衡池中。&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;字段名称&lt;/th&gt;
&lt;th style=&#34;text-align:left&#34;&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;type&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;这个 Pod 条件的名称&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;status&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;表示这个条件的达成情况， 可能的值有 &amp;ldquo;&lt;code&gt;True&lt;/code&gt;&amp;rdquo;, &amp;ldquo;&lt;code&gt;False&lt;/code&gt;&amp;rdquo;, 或 &amp;ldquo;&lt;code&gt;Unknown&lt;/code&gt;&amp;rdquo;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;lastProbeTime&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;该条件上次探测的时间戳&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;lastTransitionTime&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;该条件的值最近发生变更的时间戳&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;reason&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;机器可读, 大写驼峰的文本，说明最近一次值变化的原因&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;&lt;code&gt;message&lt;/code&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:left&#34;&gt;人类可读的消息，详细说明最近一次值变化的原因&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
### Pod readiness {#pod-readiness-gate}






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.14 [stable]&lt;/code&gt;
&lt;/div&gt;



Your application can inject extra feedback or signals into PodStatus:
_Pod readiness_. To use this, set `readinessGates` in the Pod&#39;s `spec` to
specify a list of additional conditions that the kubelet evaluates for Pod readiness.

Readiness gates are determined by the current state of `status.condition`
fields for the Pod. If Kubernetes cannot find such a condition in the
`status.conditions` field of a Pod, the status of the condition
is defaulted to &#34;`False`&#34;.

Here is an example:

```yaml
kind: Pod
...
spec:
  readinessGates:
    - conditionType: &#34;www.example.com/feature-1&#34;
status:
  conditions:
    - type: Ready                              # a built in PodCondition
      status: &#34;False&#34;
      lastProbeTime: null
      lastTransitionTime: 2018-01-01T00:00:00Z
    - type: &#34;www.example.com/feature-1&#34;        # an extra PodCondition
      status: &#34;False&#34;
      lastProbeTime: null
      lastTransitionTime: 2018-01-01T00:00:00Z
  containerStatuses:
    - containerID: docker://abcd...
      ready: true
...
```

The Pod conditions you add must have names that meet the Kubernetes [label key format](/k8sDocs/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set).
  --&gt;
&lt;h3 id=&#34;pod-readiness-gate&#34;&gt;Pod 就绪阀&lt;/h3&gt;
&lt;p&gt;用户可以向应用中注入额外的反馈或信号到 &lt;code&gt;PodStatus&lt;/code&gt;: Pod readiness. 要使用该特性，需要在 Pod 的 &lt;code&gt;spec&lt;/code&gt; 子对象上设置 &lt;code&gt;readinessGates&lt;/code&gt;，定义追加额外的就绪条件到 k8s 检测 Pod 就绪条件列表中。&lt;/p&gt;
&lt;p&gt;就绪阀由 Pod 当前 &lt;code&gt;status.condition&lt;/code&gt; 的状态决定。 如果 k8s 不能在 Pod 的 &lt;code&gt;status.condition&lt;/code&gt; 中找到该条件， 条件的默认值为 &lt;code&gt;False&lt;/code&gt;
以下为示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
...
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;readinessGates&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;conditionType&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;www.example.com/feature-1&amp;#34;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;status&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;conditions&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ready                              # 内置的 PodCondition&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;status&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;False&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;lastProbeTime&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;null&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;lastTransitionTime&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;2018-01-01T00:00:00Z&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;www.example.com/feature-1&amp;#34;&lt;/span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;# 外挂的 PodCondition&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;status&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;False&amp;#34;&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;lastProbeTime&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;null&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;lastTransitionTime&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;2018-01-01T00:00:00Z&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containerStatuses&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;containerID&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;docker://abcd...&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;ready&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;用户添加的条件在命名是需要符合 k8s 的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set&#34;&gt;标签命名格式&lt;/a&gt;&lt;/p&gt;
&lt;!--
### Status for Pod readiness {#pod-readiness-status}

The `kubectl patch` command does not support patching object status.
To set these `status.conditions` for the pod, applications and
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/extend-kubernetes/operator/&#39; target=&#39;_blank&#39;&gt;operators&lt;span class=&#39;tooltip-text&#39;&gt;A specialized controller used to manage a custom resource&lt;/span&gt;
&lt;/a&gt; should use
the `PATCH` action.
You can use a [Kubernetes client library](/k8sDocs/reference/using-api/client-libraries/) to
write code that sets custom Pod conditions for Pod readiness.

For a Pod that uses custom conditions, that Pod is evaluated to be ready **only**
when both the following statements apply:

* All containers in the Pod are ready.
* All conditions specified in `readinessGates` are `True`.

When a Pod&#39;s containers are Ready but at least one custom condition is missing or
`False`, the kubelet sets the Pod&#39;s [condition](#pod-condition) to `ContainersReady`.
 --&gt;
&lt;h3 id=&#34;pod-readiness-status&#34;&gt;Pod 就绪条件的状态&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl patch&lt;/code&gt; 命令不支持对对象状态的修改。 想要对 Pod 的 &lt;code&gt;status.conditions&lt;/code&gt;， 应用，或其它进行 &lt;code&gt;PATCH&lt;/code&gt; 的操作
可以通过 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/using-api/client-libraries/&#34;&gt;k8s 客户端库&lt;/a&gt;
写代码的方式来自定义 Pod 就绪条件。&lt;/p&gt;
&lt;p&gt;对于使用自定义就绪条件的 Pod， 只有在达成以下条件时才能进入就绪状态:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod 中所有的容器都已经就绪&lt;/li&gt;
&lt;li&gt;所有有容器配置的 &lt;code&gt;readinessGates&lt;/code&gt; 的值都为 &lt;code&gt;True&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当一个 Pod 中所有容器已经就绪，但至少有一个自定义条件不存在或值为 &lt;code&gt;False&lt;/code&gt;，
kubelet 会将 Pod 的&lt;a href=&#34;#pod-condition&#34;&gt;就绪条件&lt;/a&gt;值设置为 &lt;code&gt;ContainersReady&lt;/code&gt;&lt;/p&gt;
&lt;!--
## Container probes

A [Probe](/k8sDocs/reference/generated/kubernetes-api/v1.19/#probe-v1-core) is a diagnostic
performed periodically by the [kubelet](/k8sDocs/admin/kubelet/)
on a Container. To perform a diagnostic,
the kubelet calls a
[Handler](/k8sDocs/reference/generated/kubernetes-api/v1.19/#handler-v1-core) implemented by
the container. There are three types of handlers:

* [ExecAction](/k8sDocs/reference/generated/kubernetes-api/v1.19/#execaction-v1-core):
  Executes a specified command inside the container. The diagnostic
  is considered successful if the command exits with a status code of 0.

* [TCPSocketAction](/k8sDocs/reference/generated/kubernetes-api/v1.19/#tcpsocketaction-v1-core):
  Performs a TCP check against the Pod&#39;s IP address on
  a specified port. The diagnostic is considered successful if the port is open.

* [HTTPGetAction](/k8sDocs/reference/generated/kubernetes-api/v1.19/#httpgetaction-v1-core):
  Performs an HTTP `GET` request against the Pod&#39;s IP
  address on a specified port and path. The diagnostic is considered successful
  if the response has a status code greater than or equal to 200 and less than 400.

Each probe has one of three results:

* `Success`: The container passed the diagnostic.
* `Failure`: The container failed the diagnostic.
* `Unknown`: The diagnostic failed, so no action should be taken.

The kubelet can optionally perform and react to three kinds of probes on running
containers:

* `livenessProbe`: Indicates whether the container is running. If
   the liveness probe fails, the kubelet kills the container, and the container
   is subjected to its [restart policy](#restart-policy). If a Container does not
   provide a liveness probe, the default state is `Success`.

* `readinessProbe`: Indicates whether the container is ready to respond to requests.
   If the readiness probe fails, the endpoints controller removes the Pod&#39;s IP
   address from the endpoints of all Services that match the Pod. The default
   state of readiness before the initial delay is `Failure`. If a Container does
   not provide a readiness probe, the default state is `Success`.

* `startupProbe`: Indicates whether the application within the container is started.
   All other probes are disabled if a startup probe is provided, until it succeeds.
   If the startup probe fails, the kubelet kills the container, and the container
   is subjected to its [restart policy](#restart-policy). If a Container does not
   provide a startup probe, the default state is `Success`.

For more information about how to set up a liveness, readiness, or startup probe,
see [Configure Liveness, Readiness and Startup Probes](/k8sDocs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/).
 --&gt;
&lt;h2 id=&#34;容器探针&#34;&gt;容器探针&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#probe-v1-core&#34;&gt;探针&lt;/a&gt;
就是由 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/admin/kubelet/&#34;&gt;kubelet&lt;/a&gt; 定时对容器进行诊断操作，
诊断操作则是由 kubelet 调用一个由容器实现的
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#handler-v1-core&#34;&gt;处理器&lt;/a&gt;。
有以下三种类型处理器:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#execaction-v1-core&#34;&gt;ExecAction&lt;/a&gt;:
在容器内执行一个指定命令. 如果命令执行结束代码为 0 则表示诊断结果为成功&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#tcpsocketaction-v1-core&#34;&gt;TCPSocketAction&lt;/a&gt;:
向指定 IP 地址和端口发起 TCP 请求。 如果成功打开端口，表示诊断结果为成功&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#httpgetaction-v1-core&#34;&gt;HTTPGetAction&lt;/a&gt;:
向指定IP 地址，端口和路径发起 HTTP &lt;code&gt;GET&lt;/code&gt; 请求。如果响应码在 200 &amp;lt;= code &amp;lt; 400
表示诊断结果为成功&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上探针的结果的值可能为以下任意一个:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Success&lt;/code&gt;: 容器通过了诊断.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Failure&lt;/code&gt;: 容器没有通过诊断.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Unknown&lt;/code&gt;: 诊断过程失败，不执行任何操作.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;kubelet 可以选择是否对容器中以下探针的结果作出相应的操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;livenessProbe&lt;/code&gt;(存活探针): 指示容器是否正在运行.&lt;/p&gt;
&lt;p&gt;如果存活探针的诊断结果为未通过， 则 kubelet 会杀掉这个容器，而后容器操作则由其 &lt;a href=&#34;#restart-policy&#34;&gt;重启策略&lt;/a&gt;决定。
如果容器没有配置存活探针，则默认状态为成功。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;readinessProbe&lt;/code&gt;(就绪探针): 指示容器是否可以响应请求
如果就绪探针的诊断结果为未通过， 则 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-endpoint&#39; target=&#39;_blank&#39;&gt;Endpoints&lt;span class=&#39;tooltip-text&#39;&gt;Endpoint 跟踪匹配 Service 选择器的 Pod 的 IP 地址&lt;/span&gt;
&lt;/a&gt; 控制器
就会把该 Pod 从所有配置该 Pod 的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 的
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-endpoint&#39; target=&#39;_blank&#39;&gt;Endpoints&lt;span class=&#39;tooltip-text&#39;&gt;Endpoint 跟踪匹配 Service 选择器的 Pod 的 IP 地址&lt;/span&gt;
&lt;/a&gt; 中移出。
如果容器没有配置就绪探针，则默认状态为成功。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;startupProbe&lt;/code&gt;(启动探针): 指示容器内的应用是否启动
如果配置了启动探针，除非启动探针诊断结果为通过，否则所有其它探针都不会工作。 如果启动探针的诊断
结果为未通过，则 kubelet 会杀掉是这个容器， 而后容器操作则由其 &lt;a href=&#34;#restart-policy&#34;&gt;重启策略&lt;/a&gt;决定。
如果容器没有配置启动探针，则默认状态为成功。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;了角更多关于如何配置 存活探针，就绪探针，启动探针，见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/&#34;&gt;存活探针，就绪探针，启动探针配置&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### When should you use a liveness probe?






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.0 [stable]&lt;/code&gt;
&lt;/div&gt;



If the process in your container is able to crash on its own whenever it
encounters an issue or becomes unhealthy, you do not necessarily need a liveness
probe; the kubelet will automatically perform the correct action in accordance
with the Pod&#39;s `restartPolicy`.

If you&#39;d like your container to be killed and restarted if a probe fails, then
specify a liveness probe, and specify a `restartPolicy` of Always or OnFailure.
  --&gt;
&lt;h3 id=&#34;为啥需要用就绪readiness探针&#34;&gt;为啥需要用就绪(readiness)探针?&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.0 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;如果应用内的进程在出毛病或变得不健康是就能够自己挂掉，那么就是需要配置生存探针， kubelet 会
自动根据 Pod 的 &lt;code&gt;restartPolicy&lt;/code&gt; 正确处理这些问题。&lt;/p&gt;
&lt;p&gt;如果用户需要在探针诊断结果为未通过时杀掉容器并重启，就可以配置一个存活探针，并将 &lt;code&gt;restartPolicy&lt;/code&gt;
的值设置为 &lt;code&gt;Always&lt;/code&gt; 或 &lt;code&gt;OnFailure&lt;/code&gt;.&lt;/p&gt;
&lt;!--
### When should you use a readiness probe?






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.0 [stable]&lt;/code&gt;
&lt;/div&gt;



If you&#39;d like to start sending traffic to a Pod only when a probe succeeds,
specify a readiness probe. In this case, the readiness probe might be the same
as the liveness probe, but the existence of the readiness probe in the spec means
that the Pod will start without receiving any traffic and only start receiving
traffic after the probe starts succeeding.
If your container needs to work on loading large data, configuration files, or
migrations during startup, specify a readiness probe.

If you want your container to be able to take itself down for maintenance, you
can specify a readiness probe that checks an endpoint specific to readiness that
is different from the liveness probe.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If you just want to be able to drain requests when the Pod is deleted, you do not
necessarily need a readiness probe; on deletion, the Pod automatically puts itself
into an unready state regardless of whether the readiness probe exists.
The Pod remains in the unready state while it waits for the containers in the Pod
to stop.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;啥时候应该用就绪readiness探针&#34;&gt;啥时候应该用就绪(readiness)探针?&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.0 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;如果用户期望仅在探针诊断状态为通过时才向 Pod 调度流量，此时应该配置就绪探针。
在这种情况下，就绪探针可能与存活探针区别不大， 但就绪探针存在的意义在于 Pod 不会在就绪探针通过之前
接收到任何流量，只有在通过之后才会开始接收流量。
如果用户容器在启动时需要加载大量数据，配置文件，或迁移数据，这时就需要配置就绪探针。&lt;/p&gt;
&lt;p&gt;如果用户期望在需要维护容器可以自挂东南枝， 就可以设置一个与存活探针不同的探测接口作为就绪探针。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果用户期望仅在 Pod 被删除是不接收流量， 则不需要配置就绪探针。 在 Pod 被删除时，无论有没有就绪探针都自动将其状态
设置为未就绪状态。 Pod 在等待其中容器正常停止的过程中状态一直也都是未就绪。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### When should you use a startup probe?






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [alpha]&lt;/code&gt;
&lt;/div&gt;



Startup probes are useful for Pods that have containers that take a long time to
come into service. Rather than set a long liveness interval, you can configure
a separate configuration for probing the container as it starts up, allowing
a time longer than the liveness interval would allow.

If your container usually starts in more than
`initialDelaySeconds + failureThreshold × periodSeconds`, you should specify a
startup probe that checks the same endpoint as the liveness probe. The default for
`periodSeconds` is 30s. You should then set its `failureThreshold` high enough to
allow the container to start, without changing the default values of the liveness
probe. This helps to protect against deadlocks.
 --&gt;
&lt;h3 id=&#34;啥时候应该用启动探针&#34;&gt;啥时候应该用启动探针？&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;启动探针对于那些其中容器需要花费很长时间才能提供服务的 Pod 是相当有用的。并不需要配置一个长时间间隔的存活探针，
只需要配置一个独立的配置的探测容器的启动。而这样可以允许比存活探针时间间隔更长的时间来等待容器启动。&lt;/p&gt;
&lt;p&gt;如果用户容器通过启动时间大于 &lt;code&gt;initialDelaySeconds + failureThreshold × periodSeconds&lt;/code&gt;，
就应该配置一个与存活探针检查点相同的启动探针。 &lt;code&gt;periodSeconds&lt;/code&gt; 默认为 30秒。 所以需要设置一个
足够大的 &lt;code&gt;failureThreshold&lt;/code&gt; 值，以保证容器能够有足够的时间启动， 而不需要修改存活探针的默认配置。
这也能避免出现死锁。&lt;/p&gt;
&lt;!--
## Termination of Pods {#pod-termination}

Because Pods represent processes running on nodes in the cluster, it is important to
allow those processes to gracefully terminate when they are no longer needed (rather
than being abruptly stopped with a `KILL` signal and having no chance to clean up).

The design aim is for you to be able to request deletion and know when processes
terminate, but also be able to ensure that deletes eventually complete.
When you request deletion of a Pod, the cluster records and tracks the intended grace period
before the Pod is allowed to be forcefully killed. With that forceful shutdown tracking in
place, the &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubelet&#39; target=&#39;_blank&#39;&gt;kubelet&lt;span class=&#39;tooltip-text&#39;&gt;An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.&lt;/span&gt;
&lt;/a&gt; attempts graceful
shutdown.

Typically, the container runtime sends a a TERM signal is sent to the main process in each
container. Once the grace period has expired, the KILL signal is sent to any remainig
processes, and the Pod is then deleted from the
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API Server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt;. If the kubelet or the
container runtime&#39;s management service is restarted while waiting for processes to terminate, the
cluster retries from the start including the full original grace period.

An example flow:

1. You use the `kubectl` tool to manually delete a specific Pod, with the default grace period
   (30 seconds).
1. The Pod in the API server is updated with the time beyond which the Pod is considered &#34;dead&#34;
   along with the grace period.  
   If you use `kubectl describe` to check on the Pod you&#39;re deleting, that Pod shows up as
   &#34;Terminating&#34;.  
   On the node where the Pod is running: as soon as the kubelet sees that a Pod has been marked
   as terminating (a graceful shutdown duration has been set), the kubelet begins the local Pod
   shutdown process.
   1. If one of the Pod&#39;s containers has defined a `preStop`
      [hook](/k8sDocs/docs/concepts/containers/container-lifecycle-hooks/#hook-details), the kubelet
      runs that hook inside of the container. If the `preStop` hook is still running after the
      grace period expires, the kubelet requests a small, one-off grace period extension of 2
      seconds.
      &lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If the &lt;code&gt;preStop&lt;/code&gt; hook needs longer to complete than the default grace period allows,
you must modify &lt;code&gt;terminationGracePeriodSeconds&lt;/code&gt; to suit this.&lt;/div&gt;
&lt;/blockquote&gt;

   1. The kubelet triggers the container runtime to send a TERM signal to process 1 inside each
      container.
      &lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The containers in the Pod receive the TERM signal at different times and in an arbitrary
order. If the order of shutdowns matters, consider using a &lt;code&gt;preStop&lt;/code&gt; hook to synchronize.&lt;/div&gt;
&lt;/blockquote&gt;

1. At the same time as the kubelet is starting graceful shutdown, the control plane removes that
   shutting-down Pod from Endpoints (and, if enabled, EndpointSlice) objects where these represent
   a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; with a configured
   &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/overview/working-with-objects/labels/&#39; target=&#39;_blank&#39;&gt;selector&lt;span class=&#39;tooltip-text&#39;&gt;允许用户基于标签过滤资源列表&lt;/span&gt;
&lt;/a&gt;.
   &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#39; target=&#39;_blank&#39;&gt;ReplicaSets&lt;span class=&#39;tooltip-text&#39;&gt;ReplicaSet 保证在任意时间点上某个 Pod 有指定数量副本在运行&lt;/span&gt;
&lt;/a&gt; and other workload resources
   no longer treat the shutting-down Pod as a valid, in-service replica. Pods that shut down slowly
   cannot continue to serve traffic as load balancers (like the service proxy) remove the Pod from
   the list of endpoints as soon as the termination grace period _begins_.
1. When the grace period expires, the kubelet triggers forcible shutdown. The container runtime sends
   `SIGKILL` to any processes still running in any container in the Pod.
   The kubelet also cleans up a hidden `pause` container if that container runtime uses one.
1. The kubelet triggers forcible removal of Pod object from the API server, by setting grace period
   to 0 (immediate deletion).  
1. The API server deletes the Pod&#39;s API object, which is then no longer visible from any client.
 --&gt;
&lt;h2 id=&#34;pod-termination&#34;&gt;Pod 的终结过程&lt;/h2&gt;
&lt;p&gt;因为 Pod 代表运行在集群节点上的一系列进程， 而要让这些进程在不需要时能够死得瞑目(而不是通过 KILL 信号突然被停止，连收尾的机会都没得)。&lt;/p&gt;
&lt;p&gt;在设计上旨在用户能够在发起删除请求并能够知晓啥时候进程终止， 但最终还要保证删除操作最终需要完成。
当用户发起删除一个 Pod 的请求， 集群会在预期的时间内跟踪和记录，如果超过这个时间则会强制终止 Pod 的进程。
在强制终止之前， kubelet 都会尝试平滑关闭。&lt;/p&gt;
&lt;p&gt;通常情况下，&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;容器运行环境&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt; 会向每个容器的主进程
发送一个 &lt;code&gt;TERM&lt;/code&gt; 信号。如果超过预期时间，再和仍然存在的进程发送 &lt;code&gt;KILL&lt;/code&gt; 信号， 然后从
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 中删除该 Pod 对象。 如果在这个等待过程中
kubelet 或 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;容器运行环境&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt; 发生重启， 集群会尝试对该删除操作
重启开启计时。&lt;/p&gt;
&lt;p&gt;以下为一个示例：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用户使用 kubectl 命令手动删除一个 Pod，使用默认的预期时间(30s).&lt;/li&gt;
&lt;li&gt;从命令执行开始到预期时间内 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 中的
Pod 对象就会更新，并标记为已经挂了。 如果使用 &lt;code&gt;kubectl describe&lt;/code&gt; 查看正在删除的 Pod，
看到它的状态应该是 &lt;code&gt;Terminating&lt;/code&gt;。 在 Pod 所在的节点上： 当 kubelet 看到 Pod 被标记为终止时(添加一个平滑关闭标记)
kubelet 就开始并本地的 Pod 的进程。
&lt;ol&gt;
&lt;li&gt;如果 Pod 中有任意容器配置了&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/containers/container-lifecycle-hooks/#hook-details&#34;&gt;钩子&lt;/a&gt; &lt;code&gt;preStop&lt;/code&gt;,
kubelet 会在对应容器中执行这个钩子。 如果在预期时间到达时 &lt;code&gt;preStop&lt;/code&gt; 钩子仍在运行，则 kubelet 一次性多给 2 秒。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果 &lt;code&gt;preStop&lt;/code&gt; 需要比默认的预期时间更长的时间，则需要设置一个合适的 &lt;code&gt;terminationGracePeriodSeconds&lt;/code&gt; 值&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;kubelet 触发 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;容器运行环境&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt; 向 Pod 中的每个容器的 1 号进程
发送 TERM 信号
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Pod 中的容器可能会以不同的顺序和时间接收到 TERM 信号， 如果需要进行有序关闭，考虑使用 &lt;code&gt;preStop&lt;/code&gt; 钩子来实现同步锁&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;在 kubelet 开始平滑关闭 Pod 的进程的同时， 控制中心将正在删除的 Pod 从 对应配置选择的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;
所代表的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-endpoint&#39; target=&#39;_blank&#39;&gt;Endpoints&lt;span class=&#39;tooltip-text&#39;&gt;Endpoint 跟踪匹配 Service 选择器的 Pod 的 IP 地址&lt;/span&gt;
&lt;/a&gt; (如果开启也可能是 EndpointSlice)中移出。
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#39; target=&#39;_blank&#39;&gt;ReplicaSet&lt;span class=&#39;tooltip-text&#39;&gt;ReplicaSet 保证在任意时间点上某个 Pod 有指定数量副本在运行&lt;/span&gt;
&lt;/a&gt; 和其它的工作负载资源都会将该Pod认作是失效的，对于那些半天关不掉又不能提供服务的Pod
负载均衡(如 service proxy)会在删除预期时间开始时就从 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-endpoint&#39; target=&#39;_blank&#39;&gt;Endpoints&lt;span class=&#39;tooltip-text&#39;&gt;Endpoint 跟踪匹配 Service 选择器的 Pod 的 IP 地址&lt;/span&gt;
&lt;/a&gt; 列表中移出。&lt;/li&gt;
&lt;li&gt;当预期时间用完后，就会触发 kubelet 强制删除。 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/setup/production-environment/container-runtimes&#39; target=&#39;_blank&#39;&gt;容器运行环境&lt;span class=&#39;tooltip-text&#39;&gt;容器运行环境就是负责运行容器的软件&lt;/span&gt;
&lt;/a&gt;
会向所有剩余的进程发送 &lt;code&gt;SIGKILL&lt;/code&gt; 信号。 如果容器用到了隐藏的 &lt;code&gt;pause&lt;/code&gt; 容器 kubelet 也会一起清理&lt;/li&gt;
&lt;li&gt;kubectl 通过将预期时间设置为0(立马删除)触发从 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt;
中强制删除 Pod 对象。&lt;/li&gt;
&lt;li&gt;&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 会在 Pod 对象对所有客户端不可见时，将其删除&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
### Forced Pod termination {#pod-termination-forced}

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; Forced deletions can be potentially disruptiove for some workloads and their Pods.&lt;/div&gt;
&lt;/blockquote&gt;


By default, all deletes are graceful within 30 seconds. The `kubectl delete` command supports
the `--grace-period=&lt;seconds&gt;` option which allows you to override the default and specify your
own value.

Setting the grace period to `0` forcibly and immediately deletes the Pod from the API
server. If the pod was still running on a node, that forcible deletion triggers the kubelet to
begin immediate cleanup.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You must specify an additional flag &lt;code&gt;--force&lt;/code&gt; along with &lt;code&gt;--grace-period=0&lt;/code&gt; in order to perform force deletions.&lt;/div&gt;
&lt;/blockquote&gt;


When a force deletion is performed, the API server does not wait for confirmation
from the kubelet that the Pod has been terminated on the node it was running on. It
removes the Pod in the API immediately so a new Pod can be created with the same
name. On the node, Pods that are set to terminate immediately will still be given
a small grace period before being force killed.

If you need to force-delete Pods that are part of a StatefulSet, refer to the task
documentation for
[deleting Pods from a StatefulSet](/k8sDocs/tasks/run-application/force-delete-stateful-set-pod/).
 --&gt;
&lt;h3 id=&#34;pod-termination-forced&#34;&gt;Pod 的强制删除&lt;/h3&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 强制删除存在破坏一些工作负载或其 Pod的潜在风险。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;在默认情况下，所有的删除操作的预期时间都是 30 秒，&lt;code&gt;kubectl delete&lt;/code&gt; 命令支持通过
&lt;code&gt;--grace-period=&amp;lt;seconds&amp;gt;&lt;/code&gt; 选择来自定义预期时间。&lt;/p&gt;
&lt;p&gt;将预期时间设置为 0， 会强制立马从 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 中删除 Pod 对象。
如果 Pod 仍然运行在某个节点上， 这种强制删除会触发 kubelet 开始立即清理。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 需要同时使用 &lt;code&gt;--force&lt;/code&gt; 和 &lt;code&gt;--grace-period=0&lt;/code&gt; 在能实现强制删除。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;当一个强制删除被执行时， &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 不会等待
Pod 所在节点的的 kubelet 确认 Pod 已经被终止。 只是立马删除 Pod 对象，这时可以马上创建一个同名的新 Pod
而在节点上，被设置为立马终止的 Pod 在被强制杀死前也会给予一小会时间，以期可能平滑关闭。&lt;/p&gt;
&lt;p&gt;如果用户需要强制删除一个 StatefulSet 的 Pod，
请见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/force-delete-stateful-set-pod/&#34;&gt;删除一个属于StatefulSet的Pod&lt;/a&gt;.&lt;/p&gt;
&lt;!--  
### Garbage collection of failed Pods {#pod-garbage-collection}

For failed Pods, the API objects remain in the cluster&#39;s API until a human or
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;controller&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; process
explicitly removes them.

The control plane cleans up terminated Pods (with a phase of `Succeeded` or
`Failed`), when the number of Pods exceeds the configured threshold
(determined by `terminated-pod-gc-threshold` in the kube-controller-manager).
This avoids a resource leak as Pods are created and terminated over time.
--&gt;
&lt;h3 id=&#34;pod-garbage-collection&#34;&gt;对失效 Pod 的垃圾回收&lt;/h3&gt;
&lt;p&gt;对于失效的 Pod, 其对应会存在于集群 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; 中，
直至人工或 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;controller&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; 明确的删除它们&lt;/p&gt;
&lt;p&gt;控制中心清理终止的Pod (阶段的 &lt;code&gt;Succeeded&lt;/code&gt; 或 &lt;code&gt;Failed&lt;/code&gt;)， 如 Pod 的数量超过配置的阈值(由 kube-controller-manager 中的&lt;code&gt;terminated-pod-gc-threshold&lt;/code&gt;配置决定)
这会在长时间 Pod 创建和终止的过程中避免出现资源泄漏。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;实践
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/attach-handler-lifecycle-event/&#34;&gt;attaching handlers to Container lifecycle events&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这溃
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/&#34;&gt;configuring Liveness, Readiness and Startup Probes&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;了解&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/containers/container-lifecycle-hooks/&#34;&gt;container lifecycle hooks&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;更多关于 Pod / Container 状态的 API, 见 &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#podstatus-v1-core&#34;&gt;PodStatus&lt;/a&gt;,
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#containerstatus-v1-core&#34;&gt;ContainerStatus&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 初始化容器</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/init-containers/</link>
      <pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/init-containers/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- erictune
title: Init Containers
content_type: concept
weight: 40
---
--&gt;
&lt;!-- overview --&gt;
&lt;!--
This page provides an overview of init containers: specialized containers that run
before app containers in a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;.
Init containers can contain utilities or setup scripts not present in an app image.

You can specify init containers in the Pod specification alongside the `containers`
array (which describes app containers).
  --&gt;
&lt;p&gt;本文主要介绍初始化容器: 一种在 Pod 中在应用容器启动之前运行的专用容器。
初始化容器用于提供应用镜像没有的工具或初始化脚本。
初始化容器定义与应用容器定义是相邻关系&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Understanding init containers

A &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; can have multiple containers
running apps within it, but it can also have one or more init containers, which are run
before the app containers are started.

Init containers are exactly like regular containers, except:

* Init containers always run to completion.
* Each init container must complete successfully before the next one starts.

If a Pod&#39;s init container fails, Kubernetes repeatedly restarts the Pod until the init container
succeeds. However, if the Pod has a `restartPolicy` of Never, Kubernetes does not restart the Pod.

To specify an init container for a Pod, add the `initContainers` field into
the Pod specification, as an array of objects of type
[Container](/docs/reference/generated/kubernetes-api/v1.19/#container-v1-core),
alongside the app `containers` array.
The status of the init containers is returned in `.status.initContainerStatuses`
field as an array of the container statuses (similar to the `.status.containerStatuses`
field).
 --&gt;
&lt;h2 id=&#34;理解初始化容器是做什么的&#34;&gt;理解初始化容器是做什么的&lt;/h2&gt;
&lt;p&gt;一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 中可以饮上多个应用容器，同时还可以
有一个或多个初始化容器，这些初始化容器会在应用容器之前运行并结束。&lt;/p&gt;
&lt;p&gt;初始化容器与普通容器并无太多不同，除了以下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;初始化化容器运行在有限时间结束&lt;/li&gt;
&lt;li&gt;只有在上一初始化容器运行结束后，下一个才能开始运行。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果 Pod 的初始化容器挂了， k8s 会不停重启该 Pod 直至初始化容器成功运行完成。
但如果 Pod 的 &lt;code&gt;restartPolicy&lt;/code&gt; 值为 &lt;code&gt;Never&lt;/code&gt;， 则 k8s 不会重启该 Pod。&lt;/p&gt;
&lt;p&gt;要为一个 Pod 配置初始化容器， 需要在配置中添加 &lt;code&gt;initContainers&lt;/code&gt; 字段， 字段值为一个类型为
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#container-v1-core&#34;&gt;Container&lt;/a&gt;
的对象数组， 与 &lt;code&gt;containers&lt;/code&gt; 字段相邻。
初始化容器返回的状态存放于 &lt;code&gt;.status.initContainerStatuses&lt;/code&gt; 字段，是一个与容器状态字段(&lt;code&gt;.status.containerStatuses&lt;/code&gt;)类似的数组&lt;/p&gt;
&lt;!--
### Differences from regular containers

Init containers support all the fields and features of app containers,
including resource limits, volumes, and security settings. However, the
resource requests and limits for an init container are handled differently,
as documented in [Resources](#resources).

Also, init containers do not support `lifecycle`, `livenessProbe`, `readinessProbe`, or
`startupProbe` because they must run to completion before the Pod can be ready.

If you specify multiple init containers for a Pod, Kubelet runs each init
container sequentially. Each init container must succeed before the next can run.
When all of the init containers have run to completion, Kubelet initializes
the application containers for the Pod and runs them as usual.
 --&gt;
&lt;h3 id=&#34;初始化容器和普通容器有啥区别&#34;&gt;初始化容器和普通容器有啥区别&lt;/h3&gt;
&lt;p&gt;初始化容器动脚应用容器的所有字段和特性， 包括资源限制， 数据卷， 安全设置。
但是对初始化化容器对资源的限制处理方式有所区别，具体见 &lt;a href=&#34;#resources&#34;&gt;资源&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;还有，初始化容器不支持 &lt;code&gt;lifecycle&lt;/code&gt;, &lt;code&gt;livenessProbe&lt;/code&gt;, &lt;code&gt;readinessProbe&lt;/code&gt;, &lt;code&gt;startupProbe&lt;/code&gt;，
因为只有初始化容器执行完 Pod 才可能进入就绪状态。&lt;/p&gt;
&lt;p&gt;如果一个 Pod 中配置了多个初始化容器，则 kubelet 会顺序依次运行。
只有在上一初始化容器运行结束后，下一个才能开始运行。
当所有初始化容器运行完成后， kubelet 才会初始化 Pod 中的应用容器并以常规方式运行&lt;/p&gt;
&lt;!--
## Using init containers

Because init containers have separate images from app containers, they
have some advantages for start-up related code:

* Init containers can contain utilities or custom code for setup that are not present in an app
  image. For example, there is no need to make an image `FROM` another image just to use a tool like
  `sed`, `awk`, `python`, or `dig` during setup.
* The application image builder and deployer roles can work independently without
  the need to jointly build a single app image.
* Init containers can run with a different view of the filesystem than app containers in the
  same Pod. Consequently, they can be given access to
  &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secrets&lt;span class=&#39;tooltip-text&#39;&gt;存放如密码, OAuth, ssh 密钥等敏感信息&lt;/span&gt;
&lt;/a&gt; that app containers cannot access.
* Because init containers run to completion before any app containers start, init containers offer
  a mechanism to block or delay app container startup until a set of preconditions are met. Once
  preconditions are met, all of the app containers in a Pod can start in parallel.
* Init containers can securely run utilities or custom code that would otherwise make an app
  container image less secure. By keeping unnecessary tools separate you can limit the attack
  surface of your app container image.
 --&gt;
&lt;h2 id=&#34;怎么使用初始化容器&#34;&gt;怎么使用初始化容器&lt;/h2&gt;
&lt;p&gt;因为初始化容器与应用容器是使用不同的镜像，所以在执行初始化相关代码有些优势:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;初始化容器可以包含应用容器中没有的工作或自定义代码。
例如， 不需要为了用像 &lt;code&gt;sed&lt;/code&gt;, &lt;code&gt;awk&lt;/code&gt;, &lt;code&gt;python&lt;/code&gt;, &lt;code&gt;dig&lt;/code&gt; 来做初始化而要在应用镜像中加入
&lt;code&gt;FROM&lt;/code&gt; 其它的镜像。&lt;/li&gt;
&lt;li&gt;应用镜像的构建和部署角色可以独立工作，而不需要打在一个应用镜像里&lt;/li&gt;
&lt;li&gt;同一个 Pod 的初始化化容器可以与应用容器运行在不同文件系统视角下， 因而可以让初始化容器可以读取
应用容器不能读取的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secrets&lt;span class=&#39;tooltip-text&#39;&gt;存放如密码, OAuth, ssh 密钥等敏感信息&lt;/span&gt;
&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;因为只能在初始化容器运行完成后应用容器才能启动， 初始化容器就提供了一种机制，可以让容器在
达成一定前置条件之前应用容器会被阻塞或延迟。 当前置条件达成， Pod 中所有的应用容器可以并行启动。&lt;/li&gt;
&lt;li&gt;可以在初始化容器可以安全地运行放在应用容器中可以不那么安全的工具或自定义代码。
通过将不必要的工具从应用镜像中移出(到初始化容器中)可以减少应用容器的攻击面&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Examples
Here are some ideas for how to use init containers:

* Wait for a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; to
  be created, using a shell one-line command like:
  ```shell
  for i in {1..100}; do sleep 1; if dig myservice; then exit 0; fi; done; exit 1
  ```

* Register this Pod with a remote server from the downward API with a command like:
  ```shell
  curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d &#39;instance=$(&lt;POD_NAME&gt;)&amp;ip=$(&lt;POD_IP&gt;)&#39;
  ```

* Wait for some time before starting the app container with a command like
  ```shell
  sleep 60
  ```

* Clone a Git repository into a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;Volume&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;

* Place values into a configuration file and run a template tool to dynamically
  generate a configuration file for the main app container. For example,
  place the `POD_IP` value in a configuration and generate the main app
  configuration file using Jinja.
 --&gt;
&lt;h3 id=&#34;示例&#34;&gt;示例&lt;/h3&gt;
&lt;p&gt;以下为几个怎么用初始化容器的点子:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;等待一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 创建完成，
使用以下 shell 命令:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i in &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;1..100&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;; &lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt; sleep 1; &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; dig myservice; &lt;span style=&#34;color:#66d9ef&#34;&gt;then&lt;/span&gt; exit 0; &lt;span style=&#34;color:#66d9ef&#34;&gt;fi&lt;/span&gt;; &lt;span style=&#34;color:#66d9ef&#34;&gt;done&lt;/span&gt;; exit &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;通过以下命令将该 Pod 注册到一个远程服务的 WEB API
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;instance=$(&amp;lt;POD_NAME&amp;gt;)&amp;amp;ip=$(&amp;lt;POD_IP&amp;gt;)&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;等待一定时间后再启动 Pod中的应用容器
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;sleep &lt;span style=&#34;color:#ae81ff&#34;&gt;60&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;从 Git 仓库中克隆一个库到 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;Volume&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;将配置值放入配置文件，通过一个模板工具为应用容器动态生成配置文件， 例， 将 &lt;code&gt;POD_IP&lt;/code&gt; 放丰配置文件中
通过如 &lt;code&gt;Jinja&lt;/code&gt; 这样的工具生成主应用的配置文件&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
#### Init containers in use

This example defines a simple Pod that has two init containers.
The first waits for `myservice`, and the second waits for `mydb`. Once both
init containers complete, the Pod runs the app container from its `spec` section.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: [&#39;sh&#39;, &#39;-c&#39;, &#39;echo The app is running! &amp;&amp; sleep 3600&#39;]
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: [&#39;sh&#39;, &#39;-c&#39;, &#34;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done&#34;]
  - name: init-mydb
    image: busybox:1.28
    command: [&#39;sh&#39;, &#39;-c&#39;, &#34;until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done&#34;]
```

You can start this Pod by running:

```shell
kubectl apply -f myapp.yaml
```
```
pod/myapp-pod created
```

And check on its status with:
```shell
kubectl get -f myapp.yaml
```
```
NAME        READY     STATUS     RESTARTS   AGE
myapp-pod   0/1       Init:0/2   0          6m
```

or for more details:
```shell
kubectl describe -f myapp.yaml
```
```
Name:          myapp-pod
Namespace:     default
[...]
Labels:        app=myapp
Status:        Pending
[...]
Init Containers:
  init-myservice:
[...]
    State:         Running
[...]
  init-mydb:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Containers:
  myapp-container:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Events:
  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message
  ---------    --------    -----    ----                      -------------                           --------      ------        -------
  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201
  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image &#34;busybox&#34;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image &#34;busybox&#34;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container with docker id 5ced34a04634; Security:[seccomp=unconfined]
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container with docker id 5ced34a04634
```

To see logs for the init containers in this Pod, run:
```shell
kubectl logs myapp-pod -c init-myservice # Inspect the first init container
kubectl logs myapp-pod -c init-mydb      # Inspect the second init container
```

At this point, those init containers will be waiting to discover Services named
`mydb` and `myservice`.

Here&#39;s a configuration you can use to make those Services appear:

```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: myservice
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
---
apiVersion: v1
kind: Service
metadata:
  name: mydb
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9377
```

To create the `mydb` and `myservice` services:

```shell
kubectl apply -f services.yaml
```
```
service/myservice created
service/mydb created
```

You&#39;ll then see that those init containers complete, and that the `myapp-pod`
Pod moves into the Running state:

```shell
kubectl get -f myapp.yaml
```
```
NAME        READY     STATUS    RESTARTS   AGE
myapp-pod   1/1       Running   0          9m
```

This simple example should provide some inspiration for you to create your own
init containers. [What&#39;s next](#whats-next) contains a link to a more detailed example.
 --&gt;
&lt;h4 id=&#34;初始化容器实践&#34;&gt;初始化容器实践&lt;/h4&gt;
&lt;p&gt;本示例定义一个简单的Pod， 其中包含两个初始化容器。 第一个等待 &lt;code&gt;myservice&lt;/code&gt; 的创建，
第二个等待 &lt;code&gt;mydb&lt;/code&gt; 的创建。 当这两个初始化容器都运行完成，则运行 &lt;code&gt;spec&lt;/code&gt; 配置的应用容器。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myapp-pod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myapp&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myapp-container&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox:1.28&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sh&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;-c&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;echo The app is running! &amp;amp;&amp;amp; sleep 3600&amp;#39;&lt;/span&gt;]
  &lt;span style=&#34;color:#f92672&#34;&gt;initContainers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;init-myservice&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox:1.28&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sh&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;-c&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done&amp;#34;&lt;/span&gt;]
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;init-mydb&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox:1.28&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;sh&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;-c&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done&amp;#34;&lt;/span&gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;通过以下命令启动该 Pod&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl apply -f myapp.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;pod/myapp-pod created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过以下命令查看 Pod 状态&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get -f myapp.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;结果类似如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME        READY     STATUS     RESTARTS   AGE
myapp-pod   0/1       Init:0/2   0          6m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过以下命令可以查看更详情的信息&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl describe -f myapp.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;结果类似如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:          myapp-pod
Namespace:     default
[...]
Labels:        app=myapp
Status:        Pending
[...]
Init Containers:
  init-myservice:
[...]
    State:         Running
[...]
  init-mydb:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Containers:
  myapp-container:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Events:
  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message
  ---------    --------    -----    ----                      -------------                           --------      ------        -------
  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201
  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image &amp;quot;busybox&amp;quot;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image &amp;quot;busybox&amp;quot;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container with docker id 5ced34a04634; Security:[seccomp=unconfined]
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container with docker id 5ced34a04634
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过以下命令查看初始化容器的日志&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl logs myapp-pod -c init-myservice &lt;span style=&#34;color:#75715e&#34;&gt;# 查看第一个初始化容器&lt;/span&gt;
kubectl logs myapp-pod -c init-mydb      &lt;span style=&#34;color:#75715e&#34;&gt;# 查看第二个初始化容器&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;此时，这两个初始化容器都分别在等待 &lt;code&gt;myservice&lt;/code&gt; 和 &lt;code&gt;mydb&lt;/code&gt; 两个
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 被创建
以下为创建所需要 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt; 的定义文件&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;myservice&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9376&lt;/span&gt;
---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mydb&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;9377&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;通过以下命令创建 &lt;code&gt;myservice&lt;/code&gt; 和 &lt;code&gt;mydb&lt;/code&gt; 两个
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl apply -f services.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;service/myservice created
service/mydb created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这时候再使用以下命令就会发现初始化容器已经完成， 名叫 myapp-pod 的 Pod 进入运行状态&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;kubectl get -f myapp.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;NAME        READY     STATUS    RESTARTS   AGE
myapp-pod   1/1       Running   0          9m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个简单示例可以对用户创建自己的初始化容器有所启发。 更多关于初始化容器的示例见 &lt;a href=&#34;#whats-next&#34;&gt;相关资料&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Detailed behavior

During Pod startup, the kubelet delays running init containers until the networking
and storage are ready. Then the kubelet runs the Pod&#39;s init containers in the order
they appear in the Pod&#39;s spec.

Each init container must exit successfully before
the next container starts. If a container fails to start due to the runtime or
exits with failure, it is retried according to the Pod `restartPolicy`. However,
if the Pod `restartPolicy` is set to Always, the init containers use
`restartPolicy` OnFailure.

A Pod cannot be `Ready` until all init containers have succeeded. The ports on an
init container are not aggregated under a Service. A Pod that is initializing
is in the `Pending` state but should have a condition `Initialized` set to true.

If the Pod [restarts](#pod-restart-reasons), or is restarted, all init containers
must execute again.

Changes to the init container spec are limited to the container image field.
Altering an init container image field is equivalent to restarting the Pod.

Because init containers can be restarted, retried, or re-executed, init container
code should be idempotent. In particular, code that writes to files on `EmptyDirs`
should be prepared for the possibility that an output file already exists.

Init containers have all of the fields of an app container. However, Kubernetes
prohibits `readinessProbe` from being used because init containers cannot
define readiness distinct from completion. This is enforced during validation.

Use `activeDeadlineSeconds` on the Pod and `livenessProbe` on the container to
prevent init containers from failing forever. The active deadline includes init
containers.

The name of each app and init container in a Pod must be unique; a
validation error is thrown for any container sharing a name with another.
 --&gt;
&lt;h2 id=&#34;一些细节行为&#34;&gt;一些细节行为&lt;/h2&gt;
&lt;p&gt;在 Pod 的启动过程中， kubelet 会等待网络和存储就绪后才会运行初始化容器。
kubelet 会按照定义配置中的顺序运行初始化容器。&lt;/p&gt;
&lt;p&gt;后一个初始化容器已经在前一个运行且成功退出后才启动。 如果一个容器因为运行环境而挂掉或错误退出，
会根据 Pod 上配置 &lt;code&gt;restartPolicy&lt;/code&gt; 进行重试。 但如果 Pod &lt;code&gt;restartPolicy&lt;/code&gt; 值为 &lt;code&gt;Always&lt;/code&gt;，
初始化容器对应 &lt;code&gt;restartPolicy&lt;/code&gt; 的值为 &lt;code&gt;OnFailure&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;如果 Pod &lt;a href=&#34;#pod-restart-reasons&#34;&gt;重启&lt;/a&gt;，或已经重启，所有的初始化容器都会重新执行。&lt;/p&gt;
&lt;p&gt;初始化容器的定义配置中能修改的字段只有容器的 &lt;code&gt;image&lt;/code&gt; 字段，
如果修改初始化容器的 &lt;code&gt;image&lt;/code&gt; 字段，则表示要重启该 Pod。&lt;/p&gt;
&lt;p&gt;因为初始化容器可以重启，重试，或重新执行， 所以初始化容器的代码必须是幂等的。 特别是，如果代码
需要向 &lt;code&gt;EmptyDirs&lt;/code&gt; 写入文件，需要考虑到输出文件已经存在的情况。&lt;/p&gt;
&lt;p&gt;初始化容器包含应用容器拥有的所有字段。但 kubelet 禁止在初始化容器上使用 &lt;code&gt;readinessProbe&lt;/code&gt;，
因为定义的就绪探针与一个执行完就退出的任务是不适用的。 这会在验证是检查，如果出现则报错。&lt;/p&gt;
&lt;p&gt;在 Pod 上使用 &lt;code&gt;activeDeadlineSeconds&lt;/code&gt; 和容器上使用 &lt;code&gt;livenessProbe&lt;/code&gt; 可以防止初始化容器
一直失败的情况出现。 活跃死线包含初始化容器。&lt;/p&gt;
&lt;p&gt;在 Pod 中的应用容器和初始化容器的名称全局(Pod 作用域内)唯一。 如果有相同的名称在验证时会报错。&lt;/p&gt;
&lt;!--
### Resources

Given the ordering and execution for init containers, the following rules
for resource usage apply:

* The highest of any particular resource request or limit defined on all init
  containers is the *effective init request/limit*
* The Pod&#39;s *effective request/limit* for a resource is the higher of:
  * the sum of all app containers request/limit for a resource
  * the effective init request/limit for a resource
* Scheduling is done based on effective requests/limits, which means
  init containers can reserve resources for initialization that are not used
  during the life of the Pod.
* The QoS (quality of service) tier of the Pod&#39;s *effective QoS tier* is the
  QoS tier for init containers and app containers alike.

Quota and limits are applied based on the effective Pod request and
limit.

Pod level control groups (cgroups) are based on the effective Pod request and
limit, the same as the scheduler.
 --&gt;
&lt;h3 id=&#34;resources&#34;&gt;资源&lt;/h3&gt;
&lt;p&gt;为了让初始化容器能获取到执行所需要的资源，以下为资源申请的规则:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在所有初始化容器中的资源 下限/上限 的单项资源最高值为 &lt;em&gt;有效初始化下限/上限&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Pod 的每项资源的 &lt;em&gt;有效 下限/上限&lt;/em&gt; 是以下中较大的一个:
&lt;ul&gt;
&lt;li&gt;所有应用容器该资源的 下限/上限 的总和&lt;/li&gt;
&lt;li&gt;该资源的 有效初始化下限/上限&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;调度是基于 资源 有效 下限/上限来进行的。也就是初始化容器可以申请用于初始化的资源可能在 Pod
的余生中都用不到了。&lt;/li&gt;
&lt;li&gt;Pod 的 有效 Qos 层中的 QoS (服务质量)层就是初始化容器和应用容器通用的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;配额和限制都是基于有效 Pod 下限/上限执行。&lt;/p&gt;
&lt;p&gt;Pod 级别的控制组(cgroups)基于 有效 Pod 下限/上限， 与调度器一致&lt;/p&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Pod restart reasons {#pod-restart-reasons}

A Pod can restart, causing re-execution of init containers, for the following
reasons:

* A user updates the Pod specification, causing the init container image to change.
  Any changes to the init container image restarts the Pod. App container image
  changes only restart the app container.
* The Pod infrastructure container is restarted. This is uncommon and would
  have to be done by someone with root access to nodes.
* All containers in a Pod are terminated while `restartPolicy` is set to Always,
  forcing a restart, and the init container completion record has been lost due
  to garbage collection.
 --&gt;
&lt;h3 id=&#34;pod-restart-reasons&#34;&gt;引起 Pod 重启的原因&lt;/h3&gt;
&lt;p&gt;可能引起 一个 Pod 重启并导致初始化容器的重新执行的原因如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;某个用户更新的 Pod 定义配置， 导致初始化容器的镜像变更。 任意对初始化容器镜像的修改都会导致 Pod 重启。
应用容器镜像变更只能重启该应用容器&lt;/li&gt;
&lt;li&gt;Pod 基础设施容器被重启，这种情况不常见， 只能由拥有节点 root 权限的用户进行。&lt;/li&gt;
&lt;li&gt;Pod 中所有的容器都被终止，因为&lt;code&gt;restartPolicy&lt;/code&gt; 的值为 &lt;code&gt;Always&lt;/code&gt;，所以强制重启， 此时初始化容器
的完成记录因为垃圾清楚而丢失。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
* Read about [creating a Pod that has an init container](/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container)
* Learn how to [debug init containers](/docs/tasks/debug-application-cluster/debug-init-containers/)
 --&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container&#34;&gt;创建带有初始化容器的 Pod&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/debug-application-cluster/debug-init-containers/&#34;&gt;调试初始化容器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Pod 拓扑分布约束条件</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/pod-topology-spread-constraints/</link>
      <pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/pod-topology-spread-constraints/</guid>
      <description>
        
        
        &lt;!--
---
title: Pod Topology Spread Constraints
content_type: concept
weight: 40
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--  





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [beta]&lt;/code&gt;
&lt;/div&gt;



You can use _topology spread constraints_ to control how &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; are spread across your cluster among failure-domains such as regions, zones, nodes, and other user-defined topology domains. This can help to achieve high availability as well as efficient resource utilization.
--&gt;
&lt;p&gt;




&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [beta]&lt;/code&gt;
&lt;/div&gt;


用户可以通过 &lt;em&gt;拓扑分布约束条件&lt;/em&gt; 来控制 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
在包含集群中故障域 （如 地区，分区，节点和其它用户定义拓扑域）中是怎样分布的。
该功能可以帮助用户在实现高可用的同时充分利用资源。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Prerequisites

### Enable Feature Gate

The `EvenPodsSpread` [feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
must be enabled for the
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API Server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; **and**
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-scheduler/&#39; target=&#39;_blank&#39;&gt;scheduler&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.&lt;/span&gt;
&lt;/a&gt;.

### Node Labels

Topology spread constraints rely on node labels to identify the topology domain(s) that each Node is in. For example, a Node might have labels: `node=node1,zone=us-east-1a,region=us-east-1`

Suppose you have a 4-node cluster with the following labels:

```
NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &lt;none&gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &lt;none&gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &lt;none&gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &lt;none&gt;   2m43s   v1.16.0   node=node4,zone=zoneB
```

Then the cluster is logically viewed as below:

```
+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
```

Instead of manually applying labels, you can also reuse the [well-known labels](/docs/reference/kubernetes-api/labels-annotations-taints/) that are created and populated automatically on most clusters.
 --&gt;
&lt;h2 id=&#34;准备工作&#34;&gt;准备工作&lt;/h2&gt;
&lt;h3 id=&#34;打开功能开关&#34;&gt;打开功能开关&lt;/h3&gt;
&lt;p&gt;需要打开
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-apiserver/&#39; target=&#39;_blank&#39;&gt;API Server&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that serves the Kubernetes API.&lt;/span&gt;
&lt;/a&gt; &lt;strong&gt;和&lt;/strong&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kube-scheduler/&#39; target=&#39;_blank&#39;&gt;scheduler&lt;span class=&#39;tooltip-text&#39;&gt;Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.&lt;/span&gt;
&lt;/a&gt;
中叫 &lt;code&gt;EvenPodsSpread&lt;/code&gt; 的&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;为节点添加恰当的标签&#34;&gt;为节点添加恰当的标签&lt;/h3&gt;
&lt;p&gt;拓扑分布约束条件信赖于节点标签来区分其所在的拓扑域。 例如， 某节点标签可以为:
&lt;code&gt;node=node1,zone=us-east-1a,region=us-east-1&lt;/code&gt;
假设集群中有4个节点，标签如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &amp;lt;none&amp;gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &amp;lt;none&amp;gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &amp;lt;none&amp;gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &amp;lt;none&amp;gt;   2m43s   v1.16.0   node=node4,zone=zoneB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;那么该集群的逻辑视图如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;相较于手动添加标签，可以重用在大多数集群会自动创建和添加的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/kubernetes-api/labels-annotations-taints/&#34;&gt;常用标签&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Spread Constraints for Pods

### API

The field `pod.spec.topologySpreadConstraints` is introduced in 1.16 as below:

```
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  topologySpreadConstraints:
    - maxSkew: &lt;integer&gt;
      topologyKey: &lt;string&gt;
      whenUnsatisfiable: &lt;string&gt;
      labelSelector: &lt;object&gt;
```

You can define one or multiple `topologySpreadConstraint` to instruct the kube-scheduler how to place each incoming Pod in relation to the existing Pods across your cluster. The fields are:

- **maxSkew** describes the degree to which Pods may be unevenly distributed. It&#39;s the maximum permitted difference between the number of matching Pods in any two topology domains of a given topology type. It must be greater than zero.
- **topologyKey** is the key of node labels. If two Nodes are labelled with this key and have identical values for that label, the scheduler treats both Nodes as being in the same topology. The scheduler tries to place a balanced number of Pods into each topology domain.
- **whenUnsatisfiable** indicates how to deal with a Pod if it doesn&#39;t satisfy the spread constraint:
  - `DoNotSchedule` (default) tells the scheduler not to schedule it.
  - `ScheduleAnyway` tells the scheduler to still schedule it while prioritizing nodes that minimize the skew.
- **labelSelector** is used to find matching Pods. Pods that match this label selector are counted to determine the number of Pods in their corresponding topology domain. See [Label Selectors](/docs/concepts/overview/working-with-objects/labels/#label-selectors) for more details.

You can read more about this field by running `kubectl explain Pod.spec.topologySpreadConstraints`.
 --&gt;
&lt;h2 id=&#34;pod-的扩散约束&#34;&gt;Pod 的扩散约束&lt;/h2&gt;
&lt;h3 id=&#34;api&#34;&gt;API&lt;/h3&gt;
&lt;p&gt;在 &lt;code&gt;1.16&lt;/code&gt; 版本中加入了 &lt;code&gt;pod.spec.topologySpreadConstraints&lt;/code&gt; 字段，如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  topologySpreadConstraints:
    - maxSkew: &amp;lt;integer&amp;gt;
      topologyKey: &amp;lt;string&amp;gt;
      whenUnsatisfiable: &amp;lt;string&amp;gt;
      labelSelector: &amp;lt;object&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用户可以在 Pod 上定义一个或多个 &lt;code&gt;topologySpreadConstraint&lt;/code&gt;， 用于指导 &lt;code&gt;kube-scheduler&lt;/code&gt;
在集群中有与已经存在的 Pod 相关的新的 Pod 时应该怎么放置。有如下字段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;maxSkew&lt;/strong&gt; 该字段描述 Pod 分布不均匀的程度。 在指定拓扑类型的两个拓扑域中特定 Pod 数量相差数允许的最大值，这个值必须大于 0&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;topologyKey&lt;/strong&gt; 该字段使用节点的标签键， 如果有两个节点包含一个键，且该键值也相同，
调度器会将这两个节点认为在同一个拓扑。 调度器会尝试让两个拓扑域中的 Pod 数量平衡。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;whenUnsatisfiable&lt;/strong&gt; 该字段设置怎么处理不满足分布约束的Pod
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;DoNotSchedule&lt;/code&gt; (默认) 让调度器不要调度该 Pod&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ScheduleAnyway&lt;/code&gt; 让调度器仍然调度，但调度到不均匀度(skew)最低的节点&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;labelSelector&lt;/strong&gt; 用于找到匹配的 Pod。 匹配到的 Pod 会作为对应拓扑域的的一员(参与数量统计)
更多标签和选择器见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/labels/#label-selectors&#34;&gt;标签和选择器&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更新多关于该字段的信息请查看 &lt;code&gt;kubectl explain Pod.spec.topologySpreadConstraints&lt;/code&gt; 命令结果。&lt;/p&gt;
&lt;!--
### Example: One TopologySpreadConstraint

Suppose you have a 4-node cluster where 3 Pods labeled `foo:bar` are located in node1, node2 and node3 respectively (`P` represents Pod):

```
+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
|   P   |   P   |   P   |       |
+-------+-------+-------+-------+
```

If we want an incoming Pod to be evenly spread with existing Pods across zones, the spec can be given as:



 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintsone-constraintyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/one-constraint.yaml&#34; download=&#34;pods/topology-spread-constraints/one-constraint.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/one-constraint.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintsone-constraintyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/one-constraint.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



`topologyKey: zone` implies the even distribution will only be applied to the nodes which have label pair &#34;zone:&amp;lt;any value&amp;gt;&#34; present. `whenUnsatisfiable: DoNotSchedule` tells the scheduler to let it stay pending if the incoming Pod can’t satisfy the constraint.

If the scheduler placed this incoming Pod into &#34;zoneA&#34;, the Pods distribution would become [3, 1], hence the actual skew is 2 (3 - 1) - which violates `maxSkew: 1`. In this example, the incoming Pod can only be placed onto &#34;zoneB&#34;:

```
+---------------+---------------+      +---------------+---------------+
|     zoneA     |     zoneB     |      |     zoneA     |     zoneB     |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |  OR  | node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
|   P   |   P   |   P   |   P   |      |   P   |   P   |  P P  |       |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
```

You can tweak the Pod spec to meet various kinds of requirements:

- Change `maxSkew` to a bigger value like &#34;2&#34; so that the incoming Pod can be placed onto &#34;zoneA&#34; as well.
- Change `topologyKey` to &#34;node&#34; so as to distribute the Pods evenly across nodes instead of zones. In the above example, if `maxSkew` remains &#34;1&#34;, the incoming Pod can only be placed onto &#34;node4&#34;.
- Change `whenUnsatisfiable: DoNotSchedule` to `whenUnsatisfiable: ScheduleAnyway` to ensure the incoming Pod to be always schedulable (suppose other scheduling APIs are satisfied). However, it’s preferred to be placed onto the topology domain which has fewer matching Pods. (Be aware that this preferability is jointly normalized with other internal scheduling priorities like resource usage ratio, etc.)
 --&gt;
&lt;h3 id=&#34;示例-单个-topologyspreadconstraint&#34;&gt;示例: 单个 TopologySpreadConstraint&lt;/h3&gt;
&lt;p&gt;假设有一个4节点的集群中有三个标签包含标签为 &lt;code&gt;foo:bar&lt;/code&gt;的 Pod， 分别分布在 1，2，3 号节点上(一个 &lt;code&gt;P&lt;/code&gt; 代表一个 Pod)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
|   P   |   P   |   P   |       |
+-------+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果想要新加入的 Pod 与之前的三个节点均匀的分布在不同的区域内， 可以使用如下配置:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintsone-constraintyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/one-constraint.yaml&#34; download=&#34;pods/topology-spread-constraints/one-constraint.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/one-constraint.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintsone-constraintyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/one-constraint.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;配置中 &lt;code&gt;topologyKey: zone&lt;/code&gt; 表示均匀分布只针对包含标签 &lt;code&gt;zone:&amp;lt;any value&amp;gt;&lt;/code&gt;的节点
&lt;code&gt;whenUnsatisfiable: DoNotSchedul&lt;/code&gt; 表示针对不满足约束的的 Pod， 调度器应该让其挂起&lt;/p&gt;
&lt;p&gt;如果调度器将 Pod 分配的 “zoneA”中， 则 Pod 分布就变成 [3,1], 这时偏差(skew)就为 2(3 - 1)
这就与 &lt;code&gt;maxSkew: 1&lt;/code&gt; 相违背。所以在本例中，新加入的 Pod 就只能被分配到  “zoneB”:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+---------------+      +---------------+---------------+
|     zoneA     |     zoneB     |      |     zoneA     |     zoneB     |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |  OR  | node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
|   P   |   P   |   P   |   P   |      |   P   |   P   |  P P  |       |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用户也可以通过调整配置实现不同的需求&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将 &lt;code&gt;maxSkew&lt;/code&gt; 设置为大于 &lt;code&gt;2&lt;/code&gt;， 这样新加入 Pod 也可以分配到 “zoneA”中&lt;/li&gt;
&lt;li&gt;将 &lt;code&gt;topologyKey&lt;/code&gt; 设置为 &amp;ldquo;node&amp;rdquo;, 则 Pod 的均匀分布范围就从区域变为节点&lt;/li&gt;
&lt;li&gt;将 &lt;code&gt;whenUnsatisfiable&lt;/code&gt; 设置为 &lt;code&gt;ScheduleAnyway&lt;/code&gt; 来保证新加入的 Pod 都能被调度(假设满足其它的调度 API)
但是，会被优先调度到匹配 Pod 少的拓扑域中(也要注意这个优先还需要连同其它内部调度优先级如资源使用率等一起考量)。&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Example: Multiple TopologySpreadConstraints

This builds upon the previous example. Suppose you have a 4-node cluster where 3 Pods labeled `foo:bar` are located in node1, node2 and node3 respectively (`P` represents Pod):

```
+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
|   P   |   P   |   P   |       |
+-------+-------+-------+-------+
```

You can use 2 TopologySpreadConstraints to control the Pods spreading on both zone and node:



 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintstwo-constraintsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/two-constraints.yaml&#34; download=&#34;pods/topology-spread-constraints/two-constraints.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/two-constraints.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintstwo-constraintsyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/two-constraints.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;node&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



In this case, to match the first constraint, the incoming Pod can only be placed onto &#34;zoneB&#34;; while in terms of the second constraint, the incoming Pod can only be placed onto &#34;node4&#34;. Then the results of 2 constraints are ANDed, so the only viable option is to place on &#34;node4&#34;.

Multiple constraints can lead to conflicts. Suppose you have a 3-node cluster across 2 zones:

```
+---------------+-------+
|     zoneA     | zoneB |
+-------+-------+-------+
| node1 | node2 | node3 |
+-------+-------+-------+
|  P P  |   P   |  P P  |
+-------+-------+-------+
```

If you apply &#34;two-constraints.yaml&#34; to this cluster, you will notice &#34;mypod&#34; stays in `Pending` state. This is because: to satisfy the first constraint, &#34;mypod&#34; can only be put to &#34;zoneB&#34;; while in terms of the second constraint, &#34;mypod&#34; can only put to &#34;node2&#34;. Then a joint result of &#34;zoneB&#34; and &#34;node2&#34; returns nothing.

To overcome this situation, you can either increase the `maxSkew` or modify one of the constraints to use `whenUnsatisfiable: ScheduleAnyway`.
 --&gt;
&lt;h3 id=&#34;示例-多个-topologyspreadconstraint&#34;&gt;示例: 多个 TopologySpreadConstraint&lt;/h3&gt;
&lt;p&gt;与上一个示例相同， 假设有一个4节点的集群中有三个标签包含标签为 &lt;code&gt;foo:bar&lt;/code&gt;的 Pod，
分别分布在 1，2，3 号节点上 (一个 &lt;code&gt;P&lt;/code&gt; 代表一个 Pod)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
|   P   |   P   |   P   |       |
+-------+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这次用两个 TopologySpreadConstraints， 同时通过 区域 和节点为控制 Pod 的分布&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintstwo-constraintsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/two-constraints.yaml&#34; download=&#34;pods/topology-spread-constraints/two-constraints.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/two-constraints.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintstwo-constraintsyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/two-constraints.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;node&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;在这种情况下， 要符合第一个约束， 新加入的 Pod 就分被分配到 “zoneB”
再要符合第二个约束，新加入的 Pod 就分被分配到  “node4”
而这两个约束之间是逻辑与关系，也就最终可分配的就 “node4”。&lt;/p&gt;
&lt;p&gt;多个约束可能产生冲突。比如集群在两个区域中有三个节点:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+-------+
|     zoneA     | zoneB |
+-------+-------+-------+
| node1 | node2 | node3 |
+-------+-------+-------+
|  P P  |   P   |  P P  |
+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果在这个集群中执行 &lt;code&gt;two-constraints.yaml&lt;/code&gt;， 就会发现名称为 &lt;code&gt;mypod&lt;/code&gt; 的 Pod 状态一直是 &lt;code&gt;Pending&lt;/code&gt;。
这是因为，要符合第一个约束， 就只能分配到 “zoneB”， 同时要符合第二个约束， 就只能分配到 “node2”
而 “zoneB” 与 “node2” 的交集为空集。&lt;/p&gt;
&lt;p&gt;要解决这种情况， 可以通过增加 &lt;code&gt;maxSkew&lt;/code&gt; 的值，
或 修改其中一个约束的 &lt;code&gt;whenUnsatisfiable&lt;/code&gt;值为&lt;code&gt;ScheduleAnyway&lt;/code&gt;&lt;/p&gt;
&lt;!--  
### Conventions

There are some implicit conventions worth noting here:

- Only the Pods holding the same namespace as the incoming Pod can be matching candidates.

- Nodes without `topologySpreadConstraints[*].topologyKey` present will be bypassed. It implies that:

  1. the Pods located on those nodes do not impact `maxSkew` calculation - in the above example, suppose &#34;node1&#34; does not have label &#34;zone&#34;, then the 2 Pods will be disregarded, hence the incoming Pod will be scheduled into &#34;zoneA&#34;.
  2. the incoming Pod has no chances to be scheduled onto this kind of nodes - in the above example, suppose a &#34;node5&#34; carrying label `{zone-typo: zoneC}` joins the cluster, it will be bypassed due to the absence of label key &#34;zone&#34;.

- Be aware of what will happen if the incomingPod’s `topologySpreadConstraints[*].labelSelector` doesn’t match its own labels. In the above example, if we remove the incoming Pod’s labels, it can still be placed onto &#34;zoneB&#34; since the constraints are still satisfied. However, after the placement, the degree of imbalance of the cluster remains unchanged - it’s still zoneA having 2 Pods which hold label {foo:bar}, and zoneB having 1 Pod which holds label {foo:bar}. So if this is not what you expect, we recommend the workload’s `topologySpreadConstraints[*].labelSelector` to match its own labels.

- If the incoming Pod has `spec.nodeSelector` or `spec.affinity.nodeAffinity` defined, nodes not matching them will be bypassed.

    Suppose you have a 5-node cluster ranging from zoneA to zoneC:

    ```
    +---------------+---------------+-------+
    |     zoneA     |     zoneB     | zoneC |
    +-------+-------+-------+-------+-------+
    | node1 | node2 | node3 | node4 | node5 |
    +-------+-------+-------+-------+-------+
    |   P   |   P   |   P   |       |       |
    +-------+-------+-------+-------+-------+
    ```

    and you know that &#34;zoneC&#34; must be excluded. In this case, you can compose the yaml as below, so that &#34;mypod&#34; will be placed onto &#34;zoneB&#34; instead of &#34;zoneC&#34;. Similarly `spec.nodeSelector` is also respected.

    

 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintsone-constraint-with-nodeaffinityyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&#34; download=&#34;pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintsone-constraint-with-nodeaffinityyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;affinity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nodeAffinity&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelectorTerms&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NotIn&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;zoneC&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


--&gt;
&lt;h3 id=&#34;约定&#34;&gt;约定&lt;/h3&gt;
&lt;p&gt;以下为一些值得注意的隐性约定:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;只有在同一个命名空间中的 Pod 才能作为匹配候选者&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;没有 &lt;code&gt;topologySpreadConstraints[*].topologyKey&lt;/code&gt; 的节点会被当作旁路，隐含的意思为:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;这些只点上的 Pod 不会被用于计算 &lt;code&gt;maxSkew&lt;/code&gt;， 在上面的例子中，假设 &lt;code&gt;node1&lt;/code&gt; 上没有标签 &lt;code&gt;zone&lt;/code&gt;，
这时其上的两个 Pod 会被忽略， 这样新加入的 Pod 就会被分配到  “zoneA”&lt;/li&gt;
&lt;li&gt;新加入的 Pod 也不会有机会被分配到此类节点上， 在上面的例子中，
假设集群中加入了一个 “node5” 上面有个标签为 &lt;code&gt;zone-typo: zoneC&lt;/code&gt;
这个节点(区域)会因为没有标签键 &lt;code&gt;zone&lt;/code&gt; 而被忽略&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;还可能发生的一种情况是 新加入的 Pod 上的 &lt;code&gt;topologySpreadConstraints[*].labelSelector&lt;/code&gt;
与自身的标签不匹配。在上面的例子中， 如果删除新加入 Pod 上的标签，该 Pod 也会被分配到 “zoneB”。
因为约束条件是满足的。 但是在 Pod 分配之后，集群的均衡程度并没有改变， 也就是 &lt;code&gt;zoneA&lt;/code&gt; 中
有两个包含标签 &lt;code&gt;foo:bar&lt;/code&gt;的 Pod， &lt;code&gt;zoneB&lt;/code&gt; 中 有一个包含标签 &lt;code&gt;foo:bar&lt;/code&gt;的 Pod。 如果这不是预期的行为，
官方推荐工作负载的 &lt;code&gt;topologySpreadConstraints[*].labelSelector&lt;/code&gt; 需要匹配自身的标签。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果新加入的 Pod 还定义了 &lt;code&gt;spec.nodeSelector&lt;/code&gt; 或 &lt;code&gt;spec.affinity.nodeAffinity&lt;/code&gt;
不匹配的节点也会被忽略。&lt;/p&gt;
&lt;p&gt;假如一个包含5个节点的集群，有A，B，C三个分区:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+---------------+---------------+-------+
|     zoneA     |     zoneB     | zoneC |
+-------+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 | node5 |
+-------+-------+-------+-------+-------+
|   P   |   P   |   P   |       |       |
+-------+-------+-------+-------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果想让 zoneC 被排除， 这时可以使用以下配置，让 &lt;code&gt;mypod&lt;/code&gt; 被分配到 &lt;code&gt;zoneB&lt;/code&gt; 而不是 &lt;code&gt;zoneC&lt;/code&gt;
同样的 spec.nodeSelector 也要考量&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;podstopology-spread-constraintsone-constraint-with-nodeaffinityyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&#34; download=&#34;pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&#34;&gt;
                    &lt;code&gt;pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podstopology-spread-constraintsone-constraint-with-nodeaffinityyaml&#39;)&#34; title=&#34;Copy pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;mypod&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topologySpreadConstraints&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DoNotSchedule&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;labelSelector&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;foo&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;bar&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;affinity&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nodeAffinity&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelectorTerms&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;matchExpressions&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;zone&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NotIn&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;zoneC&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pause&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;k8s.gcr.io/pause:3.1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Cluster-level default constraints






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [alpha]&lt;/code&gt;
&lt;/div&gt;



It is possible to set default topology spread constraints for a cluster. Default
topology spread constraints are applied to a Pod if, and only if:

- It doesn&#39;t define any constraints in its `.spec.topologySpreadConstraints`.
- It belongs to a service, replication controller, replica set or stateful set.

Default constraints can be set as part of the `PodTopologySpread` plugin args
in a [scheduling profile](/docs/reference/scheduling/profiles).
The constraints are specified with the same [API above](#api), except that
`labelSelector` must be empty. The selectors are calculated from the services,
replication controllers, replica sets or stateful sets that the Pod belongs to.

An example configuration might look like follows:

```yaml
apiVersion: kubescheduler.config.k8s.io/v1alpha2
kind: KubeSchedulerConfiguration

profiles:
  pluginConfig:
    - name: PodTopologySpread
      args:
        defaultConstraints:
          - maxSkew: 1
            topologyKey: failure-domain.beta.kubernetes.io/zone
            whenUnsatisfiable: ScheduleAnyway
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The score produced by default scheduling constraints might conflict with the
score produced by the
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/scheduling/profiles/#scheduling-plugins&#34;&gt;&lt;code&gt;DefaultPodTopologySpread&lt;/code&gt; plugin&lt;/a&gt;.
It is recommended that you disable this plugin in the scheduling profile when
using default constraints for &lt;code&gt;PodTopologySpread&lt;/code&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;集群级的默认约束&#34;&gt;集群级的默认约束&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.18 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;可以为一个集群设置默认的拓扑分布约束条件，默认拓扑分布约束条件能且仅能适用于:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod 没有在 &lt;code&gt;.spec.topologySpreadConstraints&lt;/code&gt; 中定义任何约束条件&lt;/li&gt;
&lt;li&gt;Pod 属于
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-replication-controller&#39; target=&#39;_blank&#39;&gt;ReplicationController&lt;span class=&#39;tooltip-text&#39;&gt;一个 (废弃的) API 对象用于管理多副本应用&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#39; target=&#39;_blank&#39;&gt;ReplicaSet&lt;span class=&#39;tooltip-text&#39;&gt;ReplicaSet 保证在任意时间点上某个 Pod 有指定数量副本在运行&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/statefulset/&#39; target=&#39;_blank&#39;&gt;StatefulSet&lt;span class=&#39;tooltip-text&#39;&gt;管理一个 Pod 集合的部署与容量伸缩， 这些 Pod 所以使用的存储是持久的(Pod 被替代后，新的 Pod 继承老 Pod 的存储)， Pod 的标识也是持久化的(重建 Pod 后名字不会变)&lt;/span&gt;
&lt;/a&gt; 之一&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;默认的约束条件可以作为 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/scheduling/profiles&#34;&gt;scheduling profile&lt;/a&gt;
中 &lt;code&gt;PodTopologySpread&lt;/code&gt; 插件参数的一部分。 这些约束条件可以能过同 &lt;a href=&#34;#api&#34;&gt;API&lt;/a&gt; 一样设置，
除了 &lt;code&gt;labelSelector&lt;/code&gt; 必须为空。 选择器通过 Pod 所属的&lt;br&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/services-networking/service/&#39; target=&#39;_blank&#39;&gt;Service&lt;span class=&#39;tooltip-text&#39;&gt;以网络服务的方式让一个由一组 Pod 构成的应用可以对外提供服务的一种方式&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/.Site.baseURL/reference/glossary/?all=true#term-replication-controller&#39; target=&#39;_blank&#39;&gt;ReplicationController&lt;span class=&#39;tooltip-text&#39;&gt;一个 (废弃的) API 对象用于管理多副本应用&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#39; target=&#39;_blank&#39;&gt;ReplicaSet&lt;span class=&#39;tooltip-text&#39;&gt;ReplicaSet 保证在任意时间点上某个 Pod 有指定数量副本在运行&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/statefulset/&#39; target=&#39;_blank&#39;&gt;StatefulSet&lt;span class=&#39;tooltip-text&#39;&gt;管理一个 Pod 集合的部署与容量伸缩， 这些 Pod 所以使用的存储是持久的(Pod 被替代后，新的 Pod 继承老 Pod 的存储)， Pod 的标识也是持久化的(重建 Pod 后名字不会变)&lt;/span&gt;
&lt;/a&gt;
计算得出。&lt;/p&gt;
&lt;p&gt;以下为示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kubescheduler.config.k8s.io/v1alpha2&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;KubeSchedulerConfiguration&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;profiles&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;pluginConfig&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;PodTopologySpread&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;defaultConstraints&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;maxSkew&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;topologyKey&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;failure-domain.beta.kubernetes.io/zone&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;whenUnsatisfiable&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ScheduleAnyway&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 由默认调度约束计算的结果可能与&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/scheduling/profiles/#scheduling-plugins&#34;&gt;&lt;code&gt;DefaultPodTopologySpread&lt;/code&gt; plugin&lt;/a&gt;计算结果相冲突。
建议用户在使用&lt;code&gt;PodTopologySpread&lt;/code&gt;的默认约束时，关掉调度配置中的插件。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Comparison with PodAffinity/PodAntiAffiniaty

In Kubernetes, directives related to &#34;Affinity&#34; control how Pods are
scheduled - more packed or more scattered.

- For `PodAffinity`, you can try to pack any number of Pods into qualifying
  topology domain(s)
- For `PodAntiAffinity`, only one Pod can be scheduled into a
  single topology domain.

The &#34;EvenPodsSpread&#34; feature provides flexible options to distribute Pods evenly across different
topology domains - to achieve high availability or cost-saving. This can also help on rolling update
workloads and scaling out replicas smoothly. See [Motivation](https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation) for more details.
 --&gt;
&lt;h2 id=&#34;约束条件-vs-podaffinitypodantiaffiniaty&#34;&gt;约束条件 vs &lt;code&gt;PodAffinity/PodAntiAffiniaty&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;在 k8s 中， 与 &lt;code&gt;Affinity&lt;/code&gt; 相关用于控制 Pod 怎么调度的指令，或集中或分散&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于 &lt;code&gt;PodAffinity&lt;/code&gt;， 用户可以尝试向有资格的拓扑域中塞进任意数量的 Pod&lt;/li&gt;
&lt;li&gt;对于 &lt;code&gt;PodAntiAffinity&lt;/code&gt;， 一个 Pod 只能被调度到一个拓扑域中&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;EvenPodsSpread&lt;/code&gt; 特性提供了灵活的选项来让 Pod 均匀的分布到不同的拓扑域中，来达到高可用或减少开支的目的。
这也可以让滚动发布和动态扩容变得更平滑。更多信息见
&lt;a href=&#34;https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation&#34;&gt;Motivation&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Known Limitations

As of 1.18, at which this feature is Beta, there are some known limitations:

- Scaling down a Deployment may result in imbalanced Pods distribution.
- Pods matched on tainted nodes are respected. See [Issue 80921](https://github.com/kubernetes/kubernetes/issues/80921)
 --&gt;
&lt;h2 id=&#34;已知的限制&#34;&gt;已知的限制&lt;/h2&gt;
&lt;p&gt;到 &lt;code&gt;1.18&lt;/code&gt;，该特性还是 &lt;code&gt;Beta&lt;/code&gt; 状态， 还有以下已知的限制:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;收缩 Deployment 的容量可能导致 Pod 分布的不均匀。&lt;/li&gt;
&lt;li&gt;匹配到有 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/&#39; target=&#39;_blank&#39;&gt;Taint&lt;span class=&#39;tooltip-text&#39;&gt;A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups.&lt;/span&gt;
&lt;/a&gt; 节点也会被计入， 见&lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/80921&#34;&gt;Issue 80921&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Pod 预设信息</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/podpreset/</link>
      <pubDate>Tue, 28 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/podpreset/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- jessfraz
title: Pod Presets
content_type: concept
weight: 50
---
 --&gt;
&lt;!-- overview --&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.6 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;This page provides an overview of PodPresets, which are objects for injecting
certain information into pods at creation time. The information can include
secrets, volumes, volume mounts, and environment variables.&lt;/p&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.6 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;本文简单介绍 &lt;code&gt;PodPreset&lt;/code&gt;， 一个用于在特定时间向 Pod 中注入特定信息的对象。
可注入的信息包括
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/secret/&#39; target=&#39;_blank&#39;&gt;Secret&lt;span class=&#39;tooltip-text&#39;&gt;存放如密码, OAuth, ssh 密钥等敏感信息&lt;/span&gt;
&lt;/a&gt;,
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;卷(Volume)&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt;,
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/volumes/&#39; target=&#39;_blank&#39;&gt;卷(Volume)&lt;span class=&#39;tooltip-text&#39;&gt;一个可以被 Pod 中的容器访问的包含数据的目录&lt;/span&gt;
&lt;/a&gt; 挂载,
和环境变量&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Understanding Pod presets

A PodPreset is an API resource for injecting additional runtime requirements
into a Pod at creation time.
You use [label selectors](/docs/concepts/overview/working-with-objects/labels/#label-selectors)
to specify the Pods to which a given PodPreset applies.

Using a PodPreset allows pod template authors to not have to explicitly provide
all information for every pod. This way, authors of pod templates consuming a
specific service do not need to know all the details about that service.
--&gt;
&lt;h2 id=&#34;理解-pod-预设信息&#34;&gt;理解 Pod 预设信息&lt;/h2&gt;
&lt;p&gt;PodPreset 是在Pod 创建时向其中注入的运行环境需要的额外信息的 API 对象资源。
用户可以通过 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/labels/#label-selectors&#34;&gt;label selectors&lt;/a&gt;
来指定哪些 PodPreset 要用到该 Pod 上面。&lt;/p&gt;
&lt;p&gt;PodPreset 可以让 Pod 模板的创建者不必要为每个 Pod 提供都提供所有信息。
通过这种方式，Pod 模板的创建者在消费特定服务时，不需要知道该服务的所有细节&lt;/p&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Enable PodPreset in your cluster {#enable-pod-preset}

In order to use Pod presets in your cluster you must ensure the following:

1. You have enabled the API type `settings.k8s.io/v1alpha1/podpreset`. For
   example, this can be done by including `settings.k8s.io/v1alpha1=true` in
   the `--runtime-config` option for the API server. In minikube add this flag
   `--extra-config=apiserver.runtime-config=settings.k8s.io/v1alpha1=true` while
   starting the cluster.
1. You have enabled the admission controller named `PodPreset`. One way to doing this
   is to include `PodPreset` in the `--enable-admission-plugins` option value specified
   for the API server. For example, if you use Minikube, add this flag:

   ```shell
   --extra-config=apiserver.enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset
   ```

   while starting your cluster.
 --&gt;
&lt;h2 id=&#34;打开集群中的-podpreset&#34;&gt;打开集群中的 &lt;code&gt;PodPreset&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;为了能使用 Pod 预设信息，集群需要保证达到以下条件:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;需要启用 API 类型 &lt;code&gt;settings.k8s.io/v1alpha1/podpreset&lt;/code&gt;. 具体操作是:
在 api-server 中的 &lt;code&gt;--runtime-config&lt;/code&gt; 选项中添加 &lt;code&gt;settings.k8s.io/v1alpha1=true&lt;/code&gt;;
对于 minikube， 需要在集群启动时添加
&lt;code&gt;--extra-config=apiserver.runtime-config=settings.k8s.io/v1alpha1=true&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;需要启用一个叫 &lt;code&gt;PodPreset&lt;/code&gt; 的准入控制器。
一种方式是在 api-server &lt;code&gt;--enable-admission-plugins&lt;/code&gt; 选择值中添加 &lt;code&gt;PodPreset&lt;/code&gt;
对于 minikube, 则在集群启动时添加以下参数:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt; --extra-config&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;apiserver.enable-admission-plugins&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## How it works

Kubernetes provides an admission controller (`PodPreset`) which, when enabled,
applies Pod Presets to incoming pod creation requests.
When a pod creation request occurs, the system does the following:

1. Retrieve all `PodPresets` available for use.
1. Check if the label selectors of any `PodPreset` matches the labels on the
   pod being created.
1. Attempt to merge the various resources defined by the `PodPreset` into the
   Pod being created.
1. On error, throw an event documenting the merge error on the pod, and create
   the pod _without_ any injected resources from the `PodPreset`.
1. Annotate the resulting modified Pod spec to indicate that it has been
   modified by a `PodPreset`. The annotation is of the form
   `podpreset.admission.kubernetes.io/podpreset-&lt;pod-preset name&gt;: &#34;&lt;resource version&gt;&#34;`.

Each Pod can be matched by zero or more PodPresets; and each PodPreset can be
applied to zero or more Pods. When a PodPreset is applied to one or more
Pods, Kubernetes modifies the Pod Spec. For changes to `env`, `envFrom`, and
`volumeMounts`, Kubernetes modifies the container spec for all containers in
the Pod; for changes to `volumes`, Kubernetes modifies the Pod Spec.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;A Pod Preset is capable of modifying the following fields in a Pod spec when appropriate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;.spec.containers&lt;/code&gt; field&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;.spec.initContainers&lt;/code&gt; field&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;工作原理&#34;&gt;工作原理&lt;/h2&gt;
&lt;p&gt;k8s 提供了一个准入控制器(&lt;code&gt;PodPreset&lt;/code&gt;), 当这个控制器打开时，就会向进入的 Pod 创建请求执行。
当一个 Pod 的创建请求发生时， 系统会做以下操作:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;取得所有可用的 &lt;code&gt;PodPresets&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;检查 &lt;code&gt;PodPresets&lt;/code&gt; 标签选择器是否与新创建的 Pod 上的标签匹配&lt;/li&gt;
&lt;li&gt;尝试将 &lt;code&gt;PodPreset&lt;/code&gt; 中定义的各种资源合并到新创建的 Pod&lt;/li&gt;
&lt;li&gt;如果出错， 抛出一个一个事件描述合并出错到 Pod 上， 并在 &lt;em&gt;不&lt;/em&gt; 注意任意 &lt;code&gt;PodPreset&lt;/code&gt; 资源的情况下创建 Pod&lt;/li&gt;
&lt;li&gt;将由 &lt;code&gt;PodPreset&lt;/code&gt; 修改的结果加入到注解备查。注解格式为 &lt;code&gt;podpreset.admission.kubernetes.io/podpreset-&amp;lt;pod-preset name&amp;gt;: &amp;quot;&amp;lt;resource version&amp;gt;&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
### Disable Pod Preset for a specific pod

There may be instances where you wish for a Pod to not be altered by any Pod
preset mutations. In these cases, you can add an annotation in the Pod&#39;s `.spec`
of the form: `podpreset.admission.kubernetes.io/exclude: &#34;true&#34;`.
 --&gt;
&lt;h3 id=&#34;在指定-pod-禁用-podpreset&#34;&gt;在指定 Pod 禁用 &lt;code&gt;PodPreset&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;针对某些实例，用户可能不希望其被 &lt;code&gt;PodPreset&lt;/code&gt; 修改， 这种情况下， 用户可以在 Pod 定义上添加注解，
格式为 &lt;code&gt;podpreset.admission.kubernetes.io/exclude: &amp;quot;true&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
See [Injecting data into a Pod using PodPreset](/docs/tasks/inject-data-application/podpreset/)

For more information about the background, see the [design proposal for PodPreset](https://git.k8s.io/community/contributors/design-proposals/service-catalog/pod-preset.md).
 --&gt;
&lt;p&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/inject-data-application/podpreset/&#34;&gt;使用 PodPreset 向 Pod 注入数据&lt;/a&gt;
更多背景信息， 见&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/service-catalog/pod-preset.md&#34;&gt;design proposal for PodPreset&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Disruptions</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/disruptions/</link>
      <pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/disruptions/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- erictune
- foxish
- davidopp
title: Disruptions
content_type: concept
weight: 60
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
This guide is for application owners who want to build
highly available applications, and thus need to understand
what types of disruptions can happen to Pods.

It is also for cluster administrators who want to perform automated
cluster actions, like upgrading and autoscaling clusters.
 --&gt;
&lt;p&gt;本文主要给
那些需要构建高可用应用的应用所属者，需要理解 Pod 可能遇到哪些类型的故障
也适用于那些需要实现自动集群操作，如升级或集群自动扩容的集群管理员&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Voluntary and involuntary disruptions

Pods do not disappear until someone (a person or a controller) destroys them, or
there is an unavoidable hardware or system software error.

We call these unavoidable cases *involuntary disruptions* to
an application.  Examples are:

- a hardware failure of the physical machine backing the node
- cluster administrator deletes VM (instance) by mistake
- cloud provider or hypervisor failure makes VM disappear
- a kernel panic
- the node disappears from the cluster due to cluster network partition
- eviction of a pod due to the node being [out-of-resources](/docs/tasks/administer-cluster/out-of-resource/).

Except for the out-of-resources condition, all these conditions
should be familiar to most users; they are not specific
to Kubernetes.

We call other cases *voluntary disruptions*.  These include both
actions initiated by the application owner and those initiated by a Cluster
Administrator.  Typical application owner actions include:

- deleting the deployment or other controller that manages the pod
- updating a deployment&#39;s pod template causing a restart
- directly deleting a pod (e.g. by accident)

Cluster administrator actions include:

- [Draining a node](/docs/tasks/administer-cluster/safely-drain-node/) for repair or upgrade.
- Draining a node from a cluster to scale the cluster down (learn about
[Cluster Autoscaling](/docs/tasks/administer-cluster/cluster-management/#cluster-autoscaler)
).
- Removing a pod from a node to permit something else to fit on that node.

These actions might be taken directly by the cluster administrator, or by automation
run by the cluster administrator, or by your cluster hosting provider.

Ask your cluster administrator or consult your cloud provider or distribution documentation
to determine if any sources of voluntary disruptions are enabled for your cluster.
If none are enabled, you can skip creating Pod Disruption Budgets.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; Not all voluntary disruptions are constrained by Pod Disruption Budgets. For example,
deleting deployments or pods bypasses Pod Disruption Budgets.&lt;/div&gt;
&lt;/blockquote&gt;


 --&gt;
&lt;h2 id=&#34;计划内和计划外的故障&#34;&gt;计划内和计划外的故障&lt;/h2&gt;
&lt;p&gt;Pod 只会在有人(真人或一个控制器)销毁时或有不可用的硬件或系统软件错误时才会消失。&lt;/p&gt;
&lt;p&gt;我们把这类在应用上出现不可避免出现的情况称为 &lt;em&gt;计划外故障&lt;/em&gt;， 例如:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节点所以的物理机出现硬件故障&lt;/li&gt;
&lt;li&gt;集群管理误操作删除了虚拟机实例&lt;/li&gt;
&lt;li&gt;云提供商或虚拟化软件故障导致虚拟机丢失&lt;/li&gt;
&lt;li&gt;内核故障&lt;/li&gt;
&lt;li&gt;因为网络分区导致节点与集群失联&lt;/li&gt;
&lt;li&gt;因为节点&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/out-of-resource/&#34;&gt;资源爆了&lt;/a&gt;导致 Pod 被驱逐&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除了资源瀑掉的情况，其它的情况对大多数用户来说都是比较熟悉的， 并不是 k8s 独有的。&lt;/p&gt;
&lt;p&gt;其它的情况就被称为 &lt;em&gt;计划内故障&lt;/em&gt;， 包括由应用所有者发起的行为和集群管理员发起的行为。 常见的应用所有者行为有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;删除管理 Pod 的 Deployment 或其它控制器(controller)&lt;/li&gt;
&lt;li&gt;更新 Pod 定义模板引起重启&lt;/li&gt;
&lt;li&gt;直接删除 Pod (如，误删除)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;集群管理发起的行为有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;因修复或升级 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/safely-drain-node/&#34;&gt;节点清场&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;因收缩集群容量而 节点清场
(更多见&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/cluster-management/#cluster-autoscaler&#34;&gt;集群动态容量&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;因要放入更适合该节点的其它东西而删除 Pod&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;询问系统管理或咨询云提供商或查看文档来确定集群是否开启了计划内故障&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 不是所有的计划内故障都包含在 Pod 故障预算中。 例如， 删除 Deployment 或 Pod 绕过了 Pod 故障预算&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--  
## Dealing with disruptions

Here are some ways to mitigate involuntary disruptions:

- Ensure your pod [requests the resources](/docs/tasks/configure-pod-container/assign-memory-resource) it needs.
- Replicate your application if you need higher availability.  (Learn about running replicated
  [stateless](/docs/tasks/run-application/run-stateless-application-deployment/)
  and [stateful](/docs/tasks/run-application/run-replicated-stateful-application/) applications.)
- For even higher availability when running replicated applications,
  spread applications across racks (using
  [anti-affinity](/docs/user-guide/node-selection/#inter-pod-affinity-and-anti-affinity-beta-feature))
  or across zones (if using a
  [multi-zone cluster](/docs/setup/multiple-zones).)

The frequency of voluntary disruptions varies.  On a basic Kubernetes cluster, there are
no voluntary disruptions at all.  However, your cluster administrator or hosting provider
may run some additional services which cause voluntary disruptions. For example,
rolling out node software updates can cause voluntary disruptions. Also, some implementations
of cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes.
Your cluster administrator or hosting provider should have documented what level of voluntary
disruptions, if any, to expect.
--&gt;
&lt;h2 id=&#34;如何应对故障&#34;&gt;如何应对故障&lt;/h2&gt;
&lt;p&gt;可以通过以下方式缓解计划外故障:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;保证 Pod &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/assign-memory-resource&#34;&gt;申请所需要的资源&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;如果需要高可以，部署多个应用副本。(更多关于如何运行
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/run-stateless-application-deployment/&#34;&gt;无状态&lt;/a&gt;
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/run-replicated-stateful-application/&#34;&gt;有状态&lt;/a&gt;
应用
)&lt;/li&gt;
&lt;li&gt;如果多副本应用还需要更高的可用性， 让应用副本分布在不同的机架(
通过 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/user-guide/node-selection/#inter-pod-affinity-and-anti-affinity-beta-feature&#34;&gt;anti-affinity&lt;/a&gt;
) 或 不同的区域(
如果使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/setup/multiple-zones&#34;&gt;多区域集群&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Pod disruption budgets






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.5 [beta]&lt;/code&gt;
&lt;/div&gt;



Kubernetes offers features to help you run highly available applications even when you
introduce frequent voluntary disruptions.

As an application owner, you can create a PodDisruptionBudget (PDB) for each application.
A PDB limits the number of Pods of a replicated application that are down simultaneously from
voluntary disruptions. For example, a quorum-based application would
like to ensure that the number of replicas running is never brought below the
number needed for a quorum. A web front end might want to
ensure that the number of replicas serving load never falls below a certain
percentage of the total.

Cluster managers and hosting providers should use tools which
respect PodDisruptionBudgets by calling the [Eviction API](/docs/tasks/administer-cluster/safely-drain-node/#the-eviction-api)
instead of directly deleting pods or deployments.

For example, the `kubectl drain` subcommand lets you mark a node as going out of
service. When you run `kubectl drain`, the tool tries to evict all of the Pods on
the Node you&#39;re taking out of service. The eviction request that `kubectl` submits on
your behalf may be temporarily rejected, so the tool periodically retries all failed
requests until all Pods on the target node are terminated, or until a configurable timeout
is reached.

A PDB specifies the number of replicas that an application can tolerate having, relative to how
many it is intended to have.  For example, a Deployment which has a `.spec.replicas: 5` is
supposed to have 5 pods at any given time.  If its PDB allows for there to be 4 at a time,
then the Eviction API will allow voluntary disruption of one (but not two) pods at a time.

The group of pods that comprise the application is specified using a label selector, the same
as the one used by the application&#39;s controller (deployment, stateful-set, etc).

The &#34;intended&#34; number of pods is computed from the `.spec.replicas` of the workload resource
that is managing those pods. The control plane discovers the owning workload resource by
examining the `.metadata.ownerReferences` of the Pod.

PDBs cannot prevent [involuntary disruptions](#voluntary-and-involuntary-disruptions) from
occurring, but they do count against the budget.

Pods which are deleted or unavailable due to a rolling upgrade to an application do count
against the disruption budget, but workload resources (such as Deployment and StatefulSet)
are not limited by PDBs when doing rolling upgrades. Instead, the handling of failures
during application updates is configured in the spec for the specific workload resource.

When a pod is evicted using the eviction API, it is gracefully
[terminated](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination), honoring the
`terminationGracePeriodSeconds` setting in its [PodSpec](/docs/reference/generated/kubernetes-api/v1.19/#podspec-v1-core).)
 --&gt;
&lt;h2 id=&#34;pod-故障预算&#34;&gt;Pod 故障预算&lt;/h2&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.5 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;k8s 提供了即便在频繁有计划内故障的情况下依然可以运行高可用应用的特性。&lt;/p&gt;
&lt;p&gt;作为一个应用所有者，用户可以为每个应用创建一个 &lt;code&gt;PodDisruptionBudget&lt;/code&gt; (PDB)，
PDB 会限制在计划内故障时应用副本的 Pod 同时挂掉的数量。例如，一个基于选举的应用，就必须要保证
运行的副本数不能少于选举所需要的数量。 一个WEB前端可能需要保证提供服务的副本数量不能少于某个百分比&lt;/p&gt;
&lt;p&gt;集群管理器和主机提供都应该使用工具来调用遵循 &lt;code&gt;PodDisruptionBudget&lt;/code&gt; 的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/safely-drain-node/#the-eviction-api&#34;&gt;Eviction API&lt;/a&gt;
而不是直接删除 Pod 或 Deployment.&lt;/p&gt;
&lt;p&gt;例如，&lt;code&gt;kubectl drain&lt;/code&gt; 命令将节点标签为即将停止服务。 当运行 &lt;code&gt;kubectl drain&lt;/code&gt; 后，
工具将尝试将停止服务的节点上所有的 Pod 驱逐掉。 由 kubelet 发起的驱逐请求可能会临时被拒绝，
所以工具会对失败的请求周期性重试直到节点上所有的 Pod 被终止或最终超时(超时时间可配置)。&lt;/p&gt;
&lt;p&gt;PDB 指定一个应用可以接受的最少同时正常运行的副本数量。 例如，一个 Deployment 上定义为 &lt;code&gt;.spec.replicas: 5&lt;/code&gt;
也就是任意时间都要有5个正常运行的副本。 如果它的 PDB 允许同时至少要有 4 个副本， 则 驱逐 API
允许故同一时间内的计划内障数为一(不是二)&lt;/p&gt;
&lt;p&gt;而应用是通过特定标签选择器匹配到的 Pod 组成的， 与应用的控制器(deployment, stateful-set,等)一样&lt;/p&gt;
&lt;p&gt;Pod 的预期数量是通过管理这些 Pod 的工作负载资源上的 &lt;code&gt;.spec.replicas&lt;/code&gt; 计算等到的。
控制中心通过 Pod 上的 &lt;code&gt;.metadata.ownerReferences&lt;/code&gt; 来查看它的拥有者。&lt;/p&gt;
&lt;p&gt;PDB 不能阻止 &lt;a href=&#34;#voluntary-and-involuntary-disruptions&#34;&gt;计划内故障&lt;/a&gt;的发生，
但可以让它发生在预算控制内。&lt;/p&gt;
&lt;p&gt;由应用滚动更新造成的 Pod 删除或不可用是遵照故障预算的， 但工作负载(如 Deployment 和 StatefulSet)
的滚动更新则不受 PDB 限制。 而是由工作负载中的配置来处理更新失败的。&lt;/p&gt;
&lt;p&gt;当一个 Pod 因使用 驱逐 API而被驱逐， 会平滑地被 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination&#34;&gt;终止&lt;/a&gt;
或  &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#podspec-v1-core&#34;&gt;PodSpec&lt;/a&gt;.)
中配置的 &lt;code&gt;terminationGracePeriodSeconds&lt;/code&gt; 后被杀死。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## PodDisruptionBudget example {#pdb-example}

Consider a cluster with 3 nodes, `node-1` through `node-3`.
The cluster is running several applications.  One of them has 3 replicas initially called
`pod-a`, `pod-b`, and `pod-c`.  Another, unrelated pod without a PDB, called `pod-x`, is also shown.
Initially, the pods are laid out as follows:

|       node-1         |       node-2        |       node-3       |
|:--------------------:|:-------------------:|:------------------:|
| pod-a  *available*   | pod-b *available*   | pod-c *available*  |
| pod-x  *available*   |                     |                    |

All 3 pods are part of a deployment, and they collectively have a PDB which requires
there be at least 2 of the 3 pods to be available at all times.

For example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel.
The cluster administrator first tries to drain `node-1` using the `kubectl drain` command.
That tool tries to evict `pod-a` and `pod-x`.  This succeeds immediately.
Both pods go into the `terminating` state at the same time.
This puts the cluster in this state:

|   node-1 *draining*  |       node-2        |       node-3       |
|:--------------------:|:-------------------:|:------------------:|
| pod-a  *terminating* | pod-b *available*   | pod-c *available*  |
| pod-x  *terminating* |                     |                    |

The deployment notices that one of the pods is terminating, so it creates a replacement
called `pod-d`.  Since `node-1` is cordoned, it lands on another node.  Something has
also created `pod-y` as a replacement for `pod-x`.

(Note: for a StatefulSet, `pod-a`, which would be called something like `pod-0`, would need
to terminate completely before its replacement, which is also called `pod-0` but has a
different UID, could be created.  Otherwise, the example applies to a StatefulSet as well.)

Now the cluster is in this state:

|   node-1 *draining*  |       node-2        |       node-3       |
|:--------------------:|:-------------------:|:------------------:|
| pod-a  *terminating* | pod-b *available*   | pod-c *available*  |
| pod-x  *terminating* | pod-d *starting*    | pod-y              |

At some point, the pods terminate, and the cluster looks like this:

|    node-1 *drained*  |       node-2        |       node-3       |
|:--------------------:|:-------------------:|:------------------:|
|                      | pod-b *available*   | pod-c *available*  |
|                      | pod-d *starting*    | pod-y              |

At this point, if an impatient cluster administrator tries to drain `node-2` or
`node-3`, the drain command will block, because there are only 2 available
pods for the deployment, and its PDB requires at least 2.  After some time passes, `pod-d` becomes available.

The cluster state now looks like this:

|    node-1 *drained*  |       node-2        |       node-3       |
|:--------------------:|:-------------------:|:------------------:|
|                      | pod-b *available*   | pod-c *available*  |
|                      | pod-d *available*   | pod-y              |

Now, the cluster administrator tries to drain `node-2`.
The drain command will try to evict the two pods in some order, say
`pod-b` first and then `pod-d`.  It will succeed at evicting `pod-b`.
But, when it tries to evict `pod-d`, it will be refused because that would leave only
one pod available for the deployment.

The deployment creates a replacement for `pod-b` called `pod-e`.
Because there are not enough resources in the cluster to schedule
`pod-e` the drain will again block.  The cluster may end up in this
state:

|    node-1 *drained*  |       node-2        |       node-3       | *no node*          |
|:--------------------:|:-------------------:|:------------------:|:------------------:|
|                      | pod-b *terminating* | pod-c *available*  | pod-e *pending*    |
|                      | pod-d *available*   | pod-y              |                    |

At this point, the cluster administrator needs to
add a node back to the cluster to proceed with the upgrade.

You can see how Kubernetes varies the rate at which disruptions
can happen, according to:

- how many replicas an application needs
- how long it takes to gracefully shutdown an instance
- how long it takes a new instance to start up
- the type of controller
- the cluster&#39;s resource capacity
 --&gt;
&lt;h2 id=&#34;poddisruptionbudget-使用示例&#34;&gt;PodDisruptionBudget 使用示例&lt;/h2&gt;
&lt;p&gt;假设有一个三个节点的集群，节点名称依次为 &lt;code&gt;node-1&lt;/code&gt; 到 &lt;code&gt;node-3&lt;/code&gt;, 集群中运行了多个应用。
其中一个应用包含三个副本， 初始叫 &lt;code&gt;pod-a&lt;/code&gt;, &lt;code&gt;pod-a&lt;/code&gt;, &lt;code&gt;pod-c&lt;/code&gt;. 还有一个不相关的 Pod 不包含 PDB
叫 pod-x,初始分布如下:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-1&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-2&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-a  &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-b &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-c &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-x  &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;三个Pod 都属于一个 Deployment, 且共同拥有一个PDB，要求3个中至少有2个 Pod 始终可用。&lt;/p&gt;
&lt;p&gt;例如， 假设集群管理员想要更新内核版本以修复一个 bug, 所以需要重启。
集群管理员第一步通过 &lt;code&gt;kubectl drain&lt;/code&gt; 命令 尝试对 node-1 清场。
这时会尝试驱逐 &lt;code&gt;pod-a&lt;/code&gt; 和 &lt;code&gt;pod-x&lt;/code&gt;。 这步操作应该立即能成功。 两个 Pod 都会同时进入 &lt;code&gt;terminating&lt;/code&gt; 状态。
集群状态变更为:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-1 &lt;em&gt;draining&lt;/em&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-2&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-a  &lt;em&gt;terminating&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-b &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-c &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-x  &lt;em&gt;terminating&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Deployment 发现它有一个 Pod 正在被终止， 所以它会创建一个替代 Pod 叫 &lt;code&gt;pod-d&lt;/code&gt;,
因为 node-1 不可用， 所以会被调度到其它节点上。 其它某个控制器或工具也会创建一个 &lt;code&gt;pod-y&lt;/code&gt; 替代 &lt;code&gt;pod-x&lt;/code&gt;&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 对于一个 StatefulSet， pod-a 对应的名称应该是 pod-0, 且需要被替换的 Pod 完全终止后，才会创建替代的 Pod 仍叫 pod-0, 但 UID 不一样。
如此这样，这个示例也对 StatefulSet 适用。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;集群状态再次变更为:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-1 &lt;em&gt;draining&lt;/em&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-2&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-a  &lt;em&gt;terminating&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-b &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-c &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-x  &lt;em&gt;terminating&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-d &lt;em&gt;starting&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-y&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;一段时间后， Pod 被终止，集群状态变更为:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-1 &lt;em&gt;drained&lt;/em&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-2&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-b &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-c &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-d &lt;em&gt;starting&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-y&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这时如果遇到一个急躁的集群管理尝试去对 &lt;code&gt;node-2&lt;/code&gt; 或 &lt;code&gt;node-3&lt;/code&gt; 进行清场， 则清场命令会被阻塞，
因为 Deployment 只有两个可用的 Pod， 而 PDB 要求至少要两个。 一段时间之后， &lt;code&gt;pod-d&lt;/code&gt; 状态变更为可用&lt;/p&gt;
&lt;p&gt;集群状态变更为:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-1 &lt;em&gt;drained&lt;/em&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-2&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-b &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-c &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-d &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-y&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;此时，假设集群管理员清场的是 node-2. 清场命令会以某个顺序驱逐这两个 Pod， 假设先是 pod-b, 然后是 pod-d.
对 pod-b 的驱逐会成功， 但是对 pod-d 的驱逐会被拒绝，因为如果 pod-d 被终止， Deployment 就只剩下一个可用 Pod。&lt;/p&gt;
&lt;p&gt;Deployment 会创建一个叫 pod-e 来替代 pod-b. 因为集群中没有足够的资源来让 pod-e 被调度， 清场命令会再次被阻塞。
最终集群的状态为成为这样:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-1 &lt;em&gt;drained&lt;/em&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-2&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;node-3&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;em&gt;no node&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-b &lt;em&gt;terminating&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-c &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-e &lt;em&gt;pending&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-d &lt;em&gt;available&lt;/em&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;pod-y&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;至些， 集群管理员需要把完成升级的节点加入到集群中。&lt;/p&gt;
&lt;p&gt;k8s 可以接受的各种故障可能发生的频次，由如下因素决定:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用必须要多少个副本&lt;/li&gt;
&lt;li&gt;一个实例平滑关闭所需要的时间&lt;/li&gt;
&lt;li&gt;一个新实例启动需要多少时间&lt;/li&gt;
&lt;li&gt;控制器的类型&lt;/li&gt;
&lt;li&gt;集群资源的容量&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Separating Cluster Owner and Application Owner Roles

Often, it is useful to think of the Cluster Manager
and Application Owner as separate roles with limited knowledge
of each other.   This separation of responsibilities
may make sense in these scenarios:

- when there are many application teams sharing a Kubernetes cluster, and
  there is natural specialization of roles
- when third-party tools or services are used to automate cluster management

Pod Disruption Budgets support this separation of roles by providing an
interface between the roles.

If you do not have such a separation of responsibilities in your organization,
you may not need to use Pod Disruption Budgets.
 --&gt;
&lt;h2 id=&#34;集群管理员和应用管理员的角色划分&#34;&gt;集群管理员和应用管理员的角色划分&lt;/h2&gt;
&lt;p&gt;通常，将集群管理员和应用管理当作不同的角色，是很有用的。
对责任和区分在以下场景很有意义：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当集群中有多个应用团队共同使用该集群， 只预置的角色。&lt;/li&gt;
&lt;li&gt;当第三方工作或服务对集群进行自动化管理&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pod 故障预算通过角色之间的接口来实现对角色区分的支持&lt;/p&gt;
&lt;p&gt;如果用户组织中没有对责任区分的需求，则可能不震要使用 Pod 故障预算&lt;/p&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## How to perform Disruptive Actions on your Cluster

If you are a Cluster Administrator, and you need to perform a disruptive action on all
the nodes in your cluster, such as a node or system software upgrade, here are some options:

- Accept downtime during the upgrade.
- Failover to another complete replica cluster.
   -  No downtime, but may be costly both for the duplicated nodes
     and for human effort to orchestrate the switchover.
- Write disruption tolerant applications and use PDBs.
   - No downtime.
   - Minimal resource duplication.
   - Allows more automation of cluster administration.
   - Writing disruption-tolerant applications is tricky, but the work to tolerate voluntary
     disruptions largely overlaps with work to support autoscaling and tolerating
     involuntary disruptions.
 --&gt;
&lt;h2 id=&#34;怎么在集群中执行干扰操作&#34;&gt;怎么在集群中执行干扰操作&lt;/h2&gt;
&lt;p&gt;如果用户为集群管理员，需要对集群中的所有节点执行干扰操作, 使用对节点或系统软件升级， 以下为一些选项：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以接收升级期间停止服务&lt;/li&gt;
&lt;li&gt;故障转移到另一个完整的副本集群
&lt;ul&gt;
&lt;li&gt;没有宕机时间， 但可能需要双倍的节点和运行人员来实现精细的切换&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;编写可忍受干扰的应用并使用 PDB。
&lt;ul&gt;
&lt;li&gt;没有宕机时间&lt;/li&gt;
&lt;li&gt;最少资源重复&lt;/li&gt;
&lt;li&gt;允许更多集群自动化管理&lt;/li&gt;
&lt;li&gt;编写可忍受干扰的应用是相当难的。但实现对计划内故障的忍受，则能够很大程度上覆盖了支持自动伸缩容量和忍受计划外故障的工作内容&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;!--
* Follow steps to protect your application by [configuring a Pod Disruption Budget](/docs/tasks/run-application/configure-pdb/).

* Learn more about [draining nodes](/docs/tasks/administer-cluster/safely-drain-node/)

* Learn about [updating a deployment](/docs/concepts/workloads/controllers/deployment/#updating-a-deployment)
  including steps to maintain its availability during the rollout.
 --&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/configure-pdb/&#34;&gt;配置 PodDisruptionBudget&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/administer-cluster/safely-drain-node/&#34;&gt;节点清场&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;了解 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/deployment/#updating-a-deployment&#34;&gt;更新 Deployment&lt;/a&gt;
包括在回滚时用哪些步骤保持其可用性&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 临时容器</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/ephemeral-containers/</link>
      <pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/ephemeral-containers/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- verb
- yujuhong
title: Ephemeral Containers
content_type: concept
weight: 80
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [alpha]&lt;/code&gt;
&lt;/div&gt;



This page provides an overview of ephemeral containers: a special type of container
that runs temporarily in an existing &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; to
accomplish user-initiated actions such as troubleshooting. You use ephemeral
containers to inspect services rather than to build applications.

&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; Ephemeral containers are in early alpha state and are not suitable for production
clusters. In accordance with the &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/using-api/deprecation-policy/&#34;&gt;Kubernetes Deprecation Policy&lt;/a&gt;, this alpha feature could change
significantly in the future or be removed entirely.&lt;/div&gt;
&lt;/blockquote&gt;


 --&gt;
&lt;p&gt;




&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.16 [alpha]&lt;/code&gt;
&lt;/div&gt;


本文主要简述临时容器: 一个特殊类型的容器，用来实现如应用调试而临时运行在一个已经存在的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
中的容器。用户应该仅使用它来调试服务而不是用来构建应用&lt;/p&gt;
&lt;blockquote class=&#34;warning&#34;&gt;
  &lt;div&gt;&lt;strong&gt;警告：&lt;/strong&gt; 临时容器目前还是 alpha 状态， 不适合用于生产一并。 并且根据 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/using-api/deprecation-policy/&#34;&gt;k8s 废弃策略&lt;/a&gt;, 这些处于 alpha 版本特性在未来可能被
动大刀子或直接被干掉&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!-- body --&gt;
&lt;!--  
## Understanding ephemeral containers

&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; are the fundamental building
block of Kubernetes applications. Since Pods are intended to be disposable and
replaceable, you cannot add a container to a Pod once it has been created.
Instead, you usually delete and replace Pods in a controlled fashion using
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#39; target=&#39;_blank&#39;&gt;deployments&lt;span class=&#39;tooltip-text&#39;&gt;管理集群中的一个多副本应用&lt;/span&gt;
&lt;/a&gt;.

Sometimes it&#39;s necessary to inspect the state of an existing Pod, however, for
example to troubleshoot a hard-to-reproduce bug. In these cases you can run
an ephemeral container in an existing Pod to inspect its state and run
arbitrary commands.
--&gt;
&lt;h2 id=&#34;临时容器是啥东西&#34;&gt;临时容器是啥东西&lt;/h2&gt;
&lt;p&gt;&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 是 k8s 应用构建的基石. 但在 Pod 的设计理念上它就是一个
一次性的可替换的组件，所以就不能在 Pod 创建后再往里面塞容器了。 如果要做变更通常就是通过
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#39; target=&#39;_blank&#39;&gt;Deployment&lt;span class=&#39;tooltip-text&#39;&gt;管理集群中的一个多副本应用&lt;/span&gt;
&lt;/a&gt; 这种删除替换的套路。&lt;/p&gt;
&lt;p&gt;但是有时候又需要查看已经存在 Pod 内的某些状太，比如找一个很难重现的虫。 在这种情况下就可以在已经
存在的 Pod 里面塞一个临时容器然后在里面使劲折腾了。&lt;/p&gt;
&lt;!--
### What is an ephemeral container?

Ephemeral containers differ from other containers in that they lack guarantees
for resources or execution, and they will never be automatically restarted, so
they are not appropriate for building applications.  Ephemeral containers are
described using the same `ContainerSpec` as regular containers, but many fields
are incompatible and disallowed for ephemeral containers.

- Ephemeral containers may not have ports, so fields such as `ports`,
  `livenessProbe`, `readinessProbe` are disallowed.
- Pod resource allocations are immutable, so setting `resources` is disallowed.
- For a complete list of allowed fields, see the [EphemeralContainer reference
  documentation](/docs/reference/generated/kubernetes-api/v1.19/#ephemeralcontainer-v1-core).

Ephemeral containers are created using a special `ephemeralcontainers` handler
in the API rather than by adding them directly to `pod.spec`, so it&#39;s not
possible to add an ephemeral container using `kubectl edit`.

Like regular containers, you may not change or remove an ephemeral container
after you have added it to a Pod.
 --&gt;
&lt;h3 id=&#34;临时容器是什么样一个东西&#34;&gt;临时容器是什么样一个东西&lt;/h3&gt;
&lt;p&gt;临时容器与其它容器有点不同，它缺乏对资源与执行的保证，它永远不会被重启，所以它也不适合用来构建应用。
临时容器与其它容器一样也是通过 &lt;code&gt;ContainerSpec&lt;/code&gt; 来定义， 但有许多字段在临时容器上是不可用的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;临时容器不能有端口，所以与端口相关的 &lt;code&gt;ports&lt;/code&gt;, &lt;code&gt;livenessProbe&lt;/code&gt;, &lt;code&gt;readinessProbe&lt;/code&gt; 都是不能用的。&lt;/li&gt;
&lt;li&gt;Pod 分配的资源是不可变的，所以也不能设置 &lt;code&gt;resources&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;所以可用字段列表见 &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#ephemeralcontainer-v1-core&#34;&gt;EphemeralContainer 参考文档&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;临时容器是用一个在 API 叫 &lt;code&gt;ephemeralcontainers&lt;/code&gt; 的特殊处理器创建的， 并不能通过直接添加 &lt;code&gt;pod.spec&lt;/code&gt; 来创建。
所以也不能通过 &lt;code&gt;kubectl edit&lt;/code&gt; 来向 Pod 中添加一个临时容器。&lt;/p&gt;
&lt;p&gt;与普通容器一样，临时容器被加到 Pod 中之后就能再修改或删除。&lt;/p&gt;
&lt;!--
## Uses for ephemeral containers

Ephemeral containers are useful for interactive troubleshooting when `kubectl
exec` is insufficient because a container has crashed or a container image
doesn&#39;t include debugging utilities.

In particular, [distroless images](https://github.com/GoogleContainerTools/distroless)
enable you to deploy minimal container images that reduce attack surface
and exposure to bugs and vulnerabilities. Since distroless images do not include a
shell or any debugging utilities, it&#39;s difficult to troubleshoot distroless
images using `kubectl exec` alone.

When using ephemeral containers, it&#39;s helpful to enable [process namespace sharing](/docs/tasks/configure-pod-container/share-process-namespace/) so
you can view processes in other containers.

See [Debugging with Ephemeral Debug Container](/docs/tasks/debug-application-cluster/debug-running-pod/#debugging-with-ephemeral-debug-container)
for examples of troubleshooting using ephemeral containers.
 --&gt;
&lt;h2 id=&#34;临时容器是怎么用的&#34;&gt;临时容器是怎么用的&lt;/h2&gt;
&lt;p&gt;临时容器在遇到需要交互式地找虫， 而 &lt;code&gt;kubectl exec&lt;/code&gt; 又因为容器已经挂了或都容器镜像中没有调试工具时很有用的。&lt;/p&gt;
&lt;p&gt;特别是 &lt;a href=&#34;https://github.com/GoogleContainerTools/distroless&#34;&gt;distroless images&lt;/a&gt;
允许用户能够部署最小化的容器镜像而达到减少攻击面，减少缺陷和漏洞的暴露。 但 &lt;code&gt;distroless&lt;/code&gt; 镜像中是没
shell 或任何其它的调试工具的，所以对 &lt;code&gt;distroless&lt;/code&gt; 镜像要只通过 &lt;code&gt;kubectl exec&lt;/code&gt; 来调试就变得无比困难了。&lt;/p&gt;
&lt;p&gt;在使用临时容器时，建议打开 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/share-process-namespace/&#34;&gt;进程命名空间共享&lt;/a&gt;
这样就可以看到其它容器中的进程了。&lt;/p&gt;
&lt;p&gt;更多使用临时容器调试的例子见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/debug-application-cluster/debug-running-pod/#debugging-with-ephemeral-debug-container&#34;&gt;使用临时容器找虫&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Ephemeral containers API

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The examples in this section require the &lt;code&gt;EphemeralContainers&lt;/code&gt; &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;feature
gate&lt;/a&gt; to be
enabled, and Kubernetes client and server version v1.16 or later.&lt;/div&gt;
&lt;/blockquote&gt;


The examples in this section demonstrate how ephemeral containers appear in
the API. You would normally use `kubectl alpha debug` or another `kubectl`
[plugin](/docs/tasks/extend-kubectl/kubectl-plugins/) to automate these steps
rather than invoking the API directly.

Ephemeral containers are created using the `ephemeralcontainers` subresource
of Pod, which can be demonstrated using `kubectl --raw`. First describe
the ephemeral container to add as an `EphemeralContainers` list:

```json
{
    &#34;apiVersion&#34;: &#34;v1&#34;,
    &#34;kind&#34;: &#34;EphemeralContainers&#34;,
    &#34;metadata&#34;: {
            &#34;name&#34;: &#34;example-pod&#34;
    },
    &#34;ephemeralContainers&#34;: [{
        &#34;command&#34;: [
            &#34;sh&#34;
        ],
        &#34;image&#34;: &#34;busybox&#34;,
        &#34;imagePullPolicy&#34;: &#34;IfNotPresent&#34;,
        &#34;name&#34;: &#34;debugger&#34;,
        &#34;stdin&#34;: true,
        &#34;tty&#34;: true,
        &#34;terminationMessagePolicy&#34;: &#34;File&#34;
    }]
}
```

To update the ephemeral containers of the already running `example-pod`:

```shell
kubectl replace --raw /api/v1/namespaces/default/pods/example-pod/ephemeralcontainers  -f ec.json
```

This will return the new list of ephemeral containers:

```json
{
   &#34;kind&#34;:&#34;EphemeralContainers&#34;,
   &#34;apiVersion&#34;:&#34;v1&#34;,
   &#34;metadata&#34;:{
      &#34;name&#34;:&#34;example-pod&#34;,
      &#34;namespace&#34;:&#34;default&#34;,
      &#34;selfLink&#34;:&#34;/api/v1/namespaces/default/pods/example-pod/ephemeralcontainers&#34;,
      &#34;uid&#34;:&#34;a14a6d9b-62f2-4119-9d8e-e2ed6bc3a47c&#34;,
      &#34;resourceVersion&#34;:&#34;15886&#34;,
      &#34;creationTimestamp&#34;:&#34;2019-08-29T06:41:42Z&#34;
   },
   &#34;ephemeralContainers&#34;:[
      {
         &#34;name&#34;:&#34;debugger&#34;,
         &#34;image&#34;:&#34;busybox&#34;,
         &#34;command&#34;:[
            &#34;sh&#34;
         ],
         &#34;resources&#34;:{

         },
         &#34;terminationMessagePolicy&#34;:&#34;File&#34;,
         &#34;imagePullPolicy&#34;:&#34;IfNotPresent&#34;,
         &#34;stdin&#34;:true,
         &#34;tty&#34;:true
      }
   ]
}
```

You can view the state of the newly created ephemeral container using `kubectl describe`:

```shell
kubectl describe pod example-pod
```

```
...
Ephemeral Containers:
  debugger:
    Container ID:  docker://cf81908f149e7e9213d3c3644eda55c72efaff67652a2685c1146f0ce151e80f
    Image:         busybox
    Image ID:      docker-pullable://busybox@sha256:9f1003c480699be56815db0f8146ad2e22efea85129b5b5983d0e0fb52d9ab70
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      sh
    State:          Running
      Started:      Thu, 29 Aug 2019 06:42:21 +0000
    Ready:          False
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:         &lt;none&gt;
...
```

You can interact with the new ephemeral container in the same way as other
containers using `kubectl attach`, `kubectl exec`, and `kubectl logs`, for
example:

```shell
kubectl attach -it example-pod -c debugger
```
 --&gt;
&lt;h2 id=&#34;临时容器-api&#34;&gt;临时容器 API&lt;/h2&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 以下示例需要打开 &lt;code&gt;EphemeralContainers&lt;/code&gt; &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt;
k8s 客户端和服务端版本大于等于 &lt;code&gt;v1.16&lt;/code&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;本节示例通过演示彼把临时容器塞进 API 中的， 用户通常可以使用 &lt;code&gt;kubectl alpha debug&lt;/code&gt;
或另一个 &lt;code&gt;kubectl&lt;/code&gt; &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/extend-kubectl/kubectl-plugins/&#34;&gt;插件&lt;/a&gt; 来自动实现这些步骤而不需要直接调用 API&lt;/p&gt;
&lt;p&gt;临时容器是使用 Pod 的子资源 &lt;code&gt;ephemeralcontainers&lt;/code&gt; 来创建的。 使用 &lt;code&gt;kubectl --raw&lt;/code&gt;
第一步将临时容器作为 &lt;code&gt;EphemeralContainers&lt;/code&gt; 列表添加:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;{
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;apiVersion&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;v1&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;kind&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;EphemeralContainers&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;metadata&amp;#34;&lt;/span&gt;: {
            &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;example-pod&amp;#34;&lt;/span&gt;
    },
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;ephemeralContainers&amp;#34;&lt;/span&gt;: [{
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;command&amp;#34;&lt;/span&gt;: [
            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sh&amp;#34;&lt;/span&gt;
        ],
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;image&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;busybox&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;imagePullPolicy&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;IfNotPresent&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;debugger&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;stdin&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;,
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;tty&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;,
        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;terminationMessagePolicy&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;File&amp;#34;&lt;/span&gt;
    }]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;而要更新已经在运行的 &lt;code&gt;example-pod&lt;/code&gt; 中的临时容器：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl replace --raw /api/v1/namespaces/default/pods/example-pod/ephemeralcontainers  -f ec.json
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;返回一个新的临时容器列表：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;{
   &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;kind&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;EphemeralContainers&amp;#34;&lt;/span&gt;,
   &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;apiVersion&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;v1&amp;#34;&lt;/span&gt;,
   &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;metadata&amp;#34;&lt;/span&gt;:{
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;example-pod&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;namespace&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;selfLink&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/api/v1/namespaces/default/pods/example-pod/ephemeralcontainers&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;uid&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;a14a6d9b-62f2-4119-9d8e-e2ed6bc3a47c&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;resourceVersion&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;15886&amp;#34;&lt;/span&gt;,
      &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;creationTimestamp&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2019-08-29T06:41:42Z&amp;#34;&lt;/span&gt;
   },
   &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;ephemeralContainers&amp;#34;&lt;/span&gt;:[
      {
         &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;debugger&amp;#34;&lt;/span&gt;,
         &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;image&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;busybox&amp;#34;&lt;/span&gt;,
         &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;command&amp;#34;&lt;/span&gt;:[
            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sh&amp;#34;&lt;/span&gt;
         ],
         &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;resources&amp;#34;&lt;/span&gt;:{

         },
         &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;terminationMessagePolicy&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;File&amp;#34;&lt;/span&gt;,
         &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;imagePullPolicy&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;IfNotPresent&amp;#34;&lt;/span&gt;,
         &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;stdin&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;,
         &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;tty&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
      }
   ]
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以通过 &lt;code&gt;kubectl describe&lt;/code&gt; 查看新创建临时容器的状态:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe pod example-pod
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;返回类似如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;...
Ephemeral Containers:
  debugger:
    Container ID:  docker://cf81908f149e7e9213d3c3644eda55c72efaff67652a2685c1146f0ce151e80f
    Image:         busybox
    Image ID:      docker-pullable://busybox@sha256:9f1003c480699be56815db0f8146ad2e22efea85129b5b5983d0e0fb52d9ab70
    Port:          &amp;lt;none&amp;gt;
    Host Port:     &amp;lt;none&amp;gt;
    Command:
      sh
    State:          Running
      Started:      Thu, 29 Aug 2019 06:42:21 +0000
    Ready:          False
    Restart Count:  0
    Environment:    &amp;lt;none&amp;gt;
    Mounts:         &amp;lt;none&amp;gt;
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用户也可以与普通容器一样用 &lt;code&gt;kubectl attach&lt;/code&gt;, &lt;code&gt;kubectl exec&lt;/code&gt;, &lt;code&gt;kubectl logs&lt;/code&gt; 与临时容器交互。
例如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl attach -it example-pod -c debugger
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
      </description>
    </item>
    
  </channel>
</rss>
