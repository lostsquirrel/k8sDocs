<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes – 控制器</title>
    <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/</link>
    <description>Recent content in 控制器 on Kubernetes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 02 Jul 2020 00:00:00 +0000</lastBuildDate>
    
	  <atom:link href="https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Deployment</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/deployment/</link>
      <pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/deployment/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- janetkuo
title: Deployments
feature:
  title: Automated rollouts and rollbacks
  description: &gt;
    Kubernetes progressively rolls out changes to your application or its configuration, while monitoring application health to ensure it doesn&#39;t kill all your instances at the same time. If something goes wrong, Kubernetes will rollback the change for you. Take advantage of a growing ecosystem of deployment solutions.

content_type: concept
weight: 10
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
A _Deployment_ provides declarative updates for &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#39; target=&#39;_blank&#39;&gt;ReplicaSets&lt;span class=&#39;tooltip-text&#39;&gt;ReplicaSet 保证在任意时间点上某个 Pod 有指定数量副本在运行&lt;/span&gt;
&lt;/a&gt;.

You describe a _desired state_ in a Deployment, and the Deployment &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Do not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use case is not covered below.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;p&gt;&lt;em&gt;Deployment&lt;/em&gt; 为 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/replicaset/&#39; target=&#39;_blank&#39;&gt;ReplicaSets&lt;span class=&#39;tooltip-text&#39;&gt;ReplicaSet 保证在任意时间点上某个 Pod 有指定数量副本在运行&lt;/span&gt;
&lt;/a&gt; 管理的 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pods&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
提供了声明式的更新。&lt;/p&gt;
&lt;p&gt;用户在 Deployment 中描述了一个 &lt;em&gt;期望状态&lt;/em&gt;， 然后这个 Deployment &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt;
以控制器的方式变更实际状态为期望状态。 用户可以定义 Deployment 来创建一个新的 &lt;code&gt;ReplicaSet&lt;/code&gt;，
或者删除一个已经存在的 Deployment， 再用一个新的 Deployment 来接管它的所以资源。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 不要管属于 &lt;code&gt;Deployment&lt;/code&gt; 的 &lt;code&gt;ReplicaSet&lt;/code&gt;。 如果以下提到的应用场景都没办法满足你的需求请考虑到 k8s 主仓库提一个问题单&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!-- body --&gt;
&lt;!--
## Use Case

The following are typical use cases for Deployments:

* [Create a Deployment to rollout a ReplicaSet](#creating-a-deployment). The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.
* [Declare the new state of the Pods](#updating-a-deployment) by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.
* [Rollback to an earlier Deployment revision](#rolling-back-a-deployment) if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.
* [Scale up the Deployment to facilitate more load](#scaling-a-deployment).
* [Pause the Deployment](#pausing-and-resuming-a-deployment) to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.
* [Use the status of the Deployment](#deployment-status) as an indicator that a rollout has stuck.
* [Clean up older ReplicaSets](#clean-up-policy) that you don&#39;t need anymore.
 --&gt;
&lt;h2 id=&#34;应用场景&#34;&gt;应用场景&lt;/h2&gt;
&lt;p&gt;以下为 &lt;code&gt;Deployment&lt;/code&gt; 常见应用场景:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#creating-a-deployment&#34;&gt;使用 Deployment 来管理 ReplicaSet&lt;/a&gt;. ReplicaSet 会在后台创建 Pod， 查看它发布的这些发布是否成功&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#updating-a-deployment&#34;&gt;声明 Pod 的新状态&lt;/a&gt; 通过更新 Deployment 的 Pod 模板定义(PodTemplateSpec). 创建一个新的 ReplicaSet 并以一个受控的频率将 Pod 从旧的 ReplicaSet 移到新的 ReplicaSet。每创建一个新的 ReplicaSet 都会更新一次 Deployment 的版本号&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rolling-back-a-deployment&#34;&gt;Deployment 回滚到之前一个版本&lt;/a&gt;， 如果 Deployment 当前部署的应用状态不稳定，每次回滚也会更新 Deployment 版本&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scaling-a-deployment&#34;&gt;让 Deployment 扩容以承载更多的负载&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pausing-and-resuming-a-deployment&#34;&gt;暂停 Deployment&lt;/a&gt; 后一次修改 PodTemplateSpec 的多个地方，然后恢复这个 Deployment 让它开始一个新的发布&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deployment-status&#34;&gt;通过 Deployment 的状态&lt;/a&gt; 来标示一个发布应用的状态&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clean-up-policy&#34;&gt;清除不需要的 ReplicaSet&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Creating a Deployment

The following is an example of a Deployment. It creates a ReplicaSet to bring up three `nginx` Pods:



 













&lt;table class=&#34;includecode&#34; id=&#34;controllersnginx-deploymentyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/nginx-deployment.yaml&#34; download=&#34;controllers/nginx-deployment.yaml&#34;&gt;
                    &lt;code&gt;controllers/nginx-deployment.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersnginx-deploymentyaml&#39;)&#34; title=&#34;Copy controllers/nginx-deployment.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx-deployment&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx:1.14.2&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



In this example:

* A Deployment named `nginx-deployment` is created, indicated by the `.metadata.name` field.
* The Deployment creates three replicated Pods, indicated by the `.spec.replicas` field.
* The `.spec.selector` field defines how the Deployment finds which Pods to manage.
  In this case, you simply select a label that is defined in the Pod template (`app: nginx`).
  However, more sophisticated selection rules are possible,
  as long as the Pod template itself satisfies the rule.

  &lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The &lt;code&gt;.spec.selector.matchLabels&lt;/code&gt; field is a map of {key,value} pairs.
A single {key,value} in the &lt;code&gt;matchLabels&lt;/code&gt; map is equivalent to an element of &lt;code&gt;matchExpressions&lt;/code&gt;,
whose key field is &amp;ldquo;key&amp;rdquo; the operator is &amp;ldquo;In&amp;rdquo;, and the values array contains only &amp;ldquo;value&amp;rdquo;.
All of the requirements, from both &lt;code&gt;matchLabels&lt;/code&gt; and &lt;code&gt;matchExpressions&lt;/code&gt;, must be satisfied in order to match.&lt;/div&gt;
&lt;/blockquote&gt;


* The `template` field contains the following sub-fields:
  * The Pods are labeled `app: nginx`using the `.metadata.labels` field.
  * The Pod template&#39;s specification, or `.template.spec` field, indicates that
  the Pods run one container, `nginx`, which runs the `nginx`
  [Docker Hub](https://hub.docker.com/) image at version 1.14.2.
  * Create one container and name it `nginx` using the `.spec.template.spec.containers[0].name` field.

Before you begin, make sure your Kubernetes cluster is up and running.
Follow the steps given below to create the above Deployment:


1. Create the Deployment by running the following command:

   ```shell
   kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml
   ```

  &lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You can specify the &lt;code&gt;--record&lt;/code&gt; flag to write the command executed in the resource annotation &lt;code&gt;kubernetes.io/change-cause&lt;/code&gt;.
The recorded change is useful for future introspection. For example, to see the commands executed in each Deployment revision.&lt;/div&gt;
&lt;/blockquote&gt;



2. Run `kubectl get deployments` to check if the Deployment was created.

   If the Deployment is still being created, the output is similar to the following:
   ```shell
   NAME               READY   UP-TO-DATE   AVAILABLE   AGE
   nginx-deployment   0/3     0            0           1s
   ```
   When you inspect the Deployments in your cluster, the following fields are displayed:
   * `NAME` lists the names of the Deployments in the namespace.
   * `READY` displays how many replicas of the application are available to your users. It follows the pattern ready/desired.
   * `UP-TO-DATE` displays the number of replicas that have been updated to achieve the desired state.
   * `AVAILABLE` displays how many replicas of the application are available to your users.
   * `AGE` displays the amount of time that the application has been running.

   Notice how the number of desired replicas is 3 according to `.spec.replicas` field.

3. To see the Deployment rollout status, run `kubectl rollout status deployment.v1.apps/nginx-deployment`.

   The output is similar to:
   ```shell
   Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
   deployment.apps/nginx-deployment successfully rolled out
   ```

4. Run the `kubectl get deployments` again a few seconds later.
   The output is similar to this:
   ```shell
   NAME               READY   UP-TO-DATE   AVAILABLE   AGE
   nginx-deployment   3/3     3            3           18s
   ```
   Notice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available.

5. To see the ReplicaSet (`rs`) created by the Deployment, run `kubectl get rs`. The output is similar to this:
   ```shell
   NAME                          DESIRED   CURRENT   READY   AGE
   nginx-deployment-75675f5897   3         3         3       18s
   ```
   ReplicaSet output shows the following fields:

   * `NAME` lists the names of the ReplicaSets in the namespace.
   * `DESIRED` displays the desired number of _replicas_ of the application, which you define when you create the Deployment. This is the _desired state_.
   * `CURRENT` displays how many replicas are currently running.
   * `READY` displays how many replicas of the application are available to your users.
   * `AGE` displays the amount of time that the application has been running.

   Notice that the name of the ReplicaSet is always formatted as `[DEPLOYMENT-NAME]-[RANDOM-STRING]`.
   The random string is randomly generated and uses the `pod-template-hash` as a seed.

6. To see the labels automatically generated for each Pod, run `kubectl get pods --show-labels`.
   The output is similar to:
   ```shell
   NAME                                READY     STATUS    RESTARTS   AGE       LABELS
   nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
   nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
   nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
   ```
   The created ReplicaSet ensures that there are three `nginx` Pods.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;You must specify an appropriate selector and Pod template labels in a Deployment
(in this case, &lt;code&gt;app: nginx&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Do not overlap labels or selectors with other controllers (including other Deployments and StatefulSets).
Kubernetes doesn&amp;rsquo;t stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;创建一个-deployment&#34;&gt;创建一个 Deployment&lt;/h2&gt;
&lt;p&gt;以下为一个 Deployment 的示例。创建一个 ReplicaSet 来启动三个 &lt;code&gt;nginx&lt;/code&gt; 的 Pod：&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;controllersnginx-deploymentyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/nginx-deployment.yaml&#34; download=&#34;controllers/nginx-deployment.yaml&#34;&gt;
                    &lt;code&gt;controllers/nginx-deployment.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersnginx-deploymentyaml&#39;)&#34; title=&#34;Copy controllers/nginx-deployment.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Deployment&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx-deployment&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx:1.14.2&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;在这个示例中:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;创建了一个叫 &lt;code&gt;nginx-deployment&lt;/code&gt; Deployment， 这个名字定义在 &lt;code&gt;.metadata.name&lt;/code&gt; 字段&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这个 Deployment 会创建三个 Pod 为副本， 由 &lt;code&gt;.spec.replicas&lt;/code&gt; 字段定义&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;.spec.selector&lt;/code&gt; 字段定义 Deployment 怎么去找需要它管理的 Pod. 在本例中，只是定义在 Pod 模板中的
标签(&lt;code&gt;app: nginx&lt;/code&gt;), 更复杂的选择规则也是支持的，只要 Pod 模板满足这些规则
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;code&gt;.spec.selector.matchLabels&lt;/code&gt; 是一个键值对 ({key,value}) 字典。
&lt;code&gt;matchLabels&lt;/code&gt; 中的一个键值对相当于 &lt;code&gt;matchExpressions&lt;/code&gt; 中的一个元素
这个元素的键就是值对的键，操作符为 &lt;code&gt;In&lt;/code&gt;， 值为包含该值的数组。
需要同时满足 &lt;code&gt;matchLabels&lt;/code&gt; 和 &lt;code&gt;matchExpressions&lt;/code&gt; 中的条件，才是符合该选择器的目标&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;template&lt;/code&gt; 字段又包含以下子字段:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;.metadata.labels&lt;/code&gt; 字段中的 &lt;code&gt;app: nginx&lt;/code&gt; 是 Pod 的标签&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.template.spec&lt;/code&gt; 字段为 Pod 的模板定义， 表示这个 Pod 运行一个容器， 应用程序为 &lt;code&gt;nginx&lt;/code&gt;&lt;br&gt;
运行的是 &lt;a href=&#34;https://hub.docker.com/&#34;&gt;Docker Hub&lt;/a&gt; 上面的 &lt;code&gt;1.14.2&lt;/code&gt; 版本的 nginx 镜像&lt;/li&gt;
&lt;li&gt;&lt;code&gt;.spec.template.spec.containers[0].name&lt;/code&gt; 字段表示，创建的第一个也只有一个容器，它的名字为 &lt;code&gt;nginx&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在开始之前，需要保证 k8s 集群已经启动并运行。&lt;/p&gt;
&lt;p&gt;以下为创建这个 Deployment 的具体步骤:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通过以下命令创建该 Deployment：&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f nginx-deployment.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 用户可以通过在命令中添加 &lt;code&gt;--record&lt;/code&gt; 来把执行的命令写入到资源的 &lt;code&gt;kubernetes.io/change-cause&lt;/code&gt; 注解 中。
这个变更记录对于将来的回溯是很有用的。比如，查看 Deployment 每一个版本执行了什么命令&lt;/div&gt;
&lt;/blockquote&gt;

&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;运行 &lt;code&gt;kubectl get deployments&lt;/code&gt; 命令，查看 Deployment 是否已经创建&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果 Deployment 还在创建中，则输出结果如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   0/3     0            0           1s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中每个字段的说明如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;NAME&lt;/code&gt; 显示 Deployment 的名称&lt;/li&gt;
&lt;li&gt;&lt;code&gt;READY&lt;/code&gt; 展示应用的副本可用的数量， 格式为 就绪数/期望数&lt;/li&gt;
&lt;li&gt;&lt;code&gt;UP-TO-DATE&lt;/code&gt; 展示已经被更新达到预期状态的副本的数量&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AVAILABLE&lt;/code&gt; 展示已经能为用户提供服务的副本数量&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AGE&lt;/code&gt; 应用运行的时长&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以看到，根据 &lt;code&gt;.spec.replicas&lt;/code&gt; 字段， 期望的副本数量是 &lt;code&gt;3&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;要查看 Deployment 发布状态，可以通过命令 &lt;code&gt;kubectl rollout status deployment.v1.apps/nginx-deployment&lt;/code&gt;
输出类似如下:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
deployment.apps/nginx-deployment successfully rolled out
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;等几秒再执行命令 &lt;code&gt;kubectl get deployments&lt;/code&gt;
输出类似如下:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           18s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这时可以看到 Deployment 已经创建全部的三个副本， 所有副本的达成更新(是通过最新的 Pod 模板创建的)并且可用的&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;通过命令 &lt;code&gt;kubectl get rs&lt;/code&gt; 查看由 Deployment 创建的 ReplicaSet (&lt;code&gt;rs&lt;/code&gt;)
输出类似如下:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-75675f5897   3         3         3       18s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;字段说明如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;NAME&lt;/code&gt; ReplicaSet 的名称&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DESIRED&lt;/code&gt; 应用的 &lt;em&gt;期望&lt;/em&gt; 副本数，创建 Deployment 时指定。 这是期望状态&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CURRENT&lt;/code&gt; 展示当前正在运行的副本数&lt;/li&gt;
&lt;li&gt;&lt;code&gt;READY&lt;/code&gt; 展示当前可以提供服务的副本数&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AGE&lt;/code&gt; 展示应用的运行时长&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以看到 ReplicaSet 的名称格式总是为 &lt;code&gt;[DEPLOYMENT-NAME]-[RANDOM-STRING]&lt;/code&gt;
为个随机字符串是以 &lt;code&gt;pod-template-hash&lt;/code&gt; 为种子生成的&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;要查看每个 Pod 自动创建的标签，可能执行命令 &lt;code&gt;kubectl get pods --show-labels&lt;/code&gt;:
输出类似如下:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;NAME                                READY     STATUS    RESTARTS   AGE       LABELS
nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;ReplicaSet 会确保有三个 &lt;code&gt;nginx&lt;/code&gt; Pod&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 用户应该在 Deployment 中定义恰当的选择器和 Pod 模板标签(本例为 &lt;code&gt;app: nginx&lt;/code&gt;)
一定不要与其它的控制器(包括其它的 Deployment 和 StatefulSet)使用一样的标签或选择器&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Pod-template-hash label

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; Do not change this label.&lt;/div&gt;
&lt;/blockquote&gt;


The `pod-template-hash` label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.

This label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the `PodTemplate` of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels,
and in any existing Pods that the ReplicaSet might have.
 --&gt;
&lt;h3 id=&#34;标签-pod-template-hash&#34;&gt;标签 &lt;code&gt;pod-template-hash&lt;/code&gt;&lt;/h3&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; 这个标签一定不要改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code&gt;pod-template-hash&lt;/code&gt; 是由 Deployment 的控制器添加到每个由 Deployment 创建或收编的 ReplicaSet 的。
这个标签确保 Deployment 所属的 ReplicaSet 的标签不会重叠。 这个标签的值是对 ReplicaSet 的 &lt;code&gt;PodTemplate&lt;/code&gt; 进行哈希的结果，
并将这个标签添加到 ReplicaSet 的选择器和 Pod 模板的标签中，然后也会添加到已经由 ReplicaSet 管理或将来收编的所以 Pod 中。&lt;/p&gt;
&lt;!--
## Updating a Deployment

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A Deployment&amp;rsquo;s rollout is triggered if and only if the Deployment&amp;rsquo;s Pod template (that is, &lt;code&gt;.spec.template&lt;/code&gt;)
is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.&lt;/div&gt;
&lt;/blockquote&gt;


Follow the steps given below to update your Deployment:

1. Let&#39;s update the nginx Pods to use the `nginx:1.16.1` image instead of the `nginx:1.14.2` image.

    ```shell
    kubectl --record deployment.apps/nginx-deployment set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1
    ```
    or simply use the following command:

    ```shell
    kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 --record
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment image updated
    ```

    Alternatively, you can `edit` the Deployment and change `.spec.template.spec.containers[0].image` from `nginx:1.14.2` to `nginx:1.16.1`:

    ```shell
    kubectl edit deployment.v1.apps/nginx-deployment
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment edited
    ```

2. To see the rollout status, run:

    ```shell
    kubectl rollout status deployment.v1.apps/nginx-deployment
    ```

    The output is similar to this:
    ```
    Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
    ```
    or
    ```
    deployment.apps/nginx-deployment successfully rolled out
    ```

Get more details on your updated Deployment:

* After the rollout succeeds, you can view the Deployment by running `kubectl get deployments`.
    The output is similar to this:
    ```
    NAME               READY   UP-TO-DATE   AVAILABLE   AGE
    nginx-deployment   3/3     3            3           36s
    ```

* Run `kubectl get rs` to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it
up to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.

    ```shell
    kubectl get rs
    ```

    The output is similar to this:
    ```
    NAME                          DESIRED   CURRENT   READY   AGE
    nginx-deployment-1564180365   3         3         3       6s
    nginx-deployment-2035384211   0         0         0       36s
    ```

* Running `get pods` should now show only the new Pods:

    ```shell
    kubectl get pods
    ```

    The output is similar to this:
    ```
    NAME                                READY     STATUS    RESTARTS   AGE
    nginx-deployment-1564180365-khku8   1/1       Running   0          14s
    nginx-deployment-1564180365-nacti   1/1       Running   0          14s
    nginx-deployment-1564180365-z9gth   1/1       Running   0          14s
    ```

    Next time you want to update these Pods, you only need to update the Deployment&#39;s Pod template again.

    Deployment ensures that only a certain number of Pods are down while they are being updated. By default,
    it ensures that at least 75% of the desired number of Pods are up (25% max unavailable).

    Deployment also ensures that only a certain number of Pods are created above the desired number of Pods.
    By default, it ensures that at most 125% of the desired number of Pods are up (25% max surge).

    For example, if you look at the above Deployment closely, you will see that it first created a new Pod,
    then deleted some old Pods, and created new ones. It does not kill old Pods until a sufficient number of
    new Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed.
    It makes sure that at least 2 Pods are available and that at max 4 Pods in total are available.

* Get details of your Deployment:
  ```shell
  kubectl describe deployments
  ```
  The output is similar to this:
  ```
  Name:                   nginx-deployment
  Namespace:              default
  CreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000
  Labels:                 app=nginx
  Annotations:            deployment.kubernetes.io/revision=2
  Selector:               app=nginx
  Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
  StrategyType:           RollingUpdate
  MinReadySeconds:        0
  RollingUpdateStrategy:  25% max unavailable, 25% max surge
  Pod Template:
    Labels:  app=nginx
     Containers:
      nginx:
        Image:        nginx:1.16.1
        Port:         80/TCP
        Environment:  &lt;none&gt;
        Mounts:       &lt;none&gt;
      Volumes:        &lt;none&gt;
    Conditions:
      Type           Status  Reason
      ----           ------  ------
      Available      True    MinimumReplicasAvailable
      Progressing    True    NewReplicaSetAvailable
    OldReplicaSets:  &lt;none&gt;
    NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)
    Events:
      Type    Reason             Age   From                   Message
      ----    ------             ----  ----                   -------
      Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3
      Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1
      Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2
      Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2
      Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1
      Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3
      Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0
    ```
    Here you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211)
    and scaled it up to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet
    (nginx-deployment-1564180365) and scaled it up to 1 and then scaled down the old ReplicaSet to 2, so that at
    least 2 Pods were available and at most 4 Pods were created at all times. It then continued scaling up and down
    the new and the old ReplicaSet, with the same rolling update strategy. Finally, you&#39;ll have 3 available replicas
    in the new ReplicaSet, and the old ReplicaSet is scaled down to 0.
 --&gt;
&lt;h2 id=&#34;更新-deployment&#34;&gt;更新 Deployment&lt;/h2&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Deployment 有且仅有在对 Deployment Pod 模板(也就是 &lt;code&gt;.spec.template&lt;/code&gt;)发生变更后触发发布，
例如，如果模板标签或容器被更新。 其它如扩充 Deployment 副本数不会触发发布。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;以下步骤更新 Deployment：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 Pod 使用的 &lt;code&gt;nginx&lt;/code&gt; 镜像版本从 &lt;code&gt;nginx:1.14.2&lt;/code&gt; 升到 &lt;code&gt;nginx:1.16.1&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl --record deployment.apps/nginx-deployment set image deployment.v1.apps/nginx-deployment nginx&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx:1.16.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;或者简单使用如下命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl set image deployment/nginx-deployment nginx&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx:1.16.1 --record
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment image updated
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;另外还有一个方式，就是修改 Deployment 中 &lt;code&gt;.spec.template.spec.containers[0].image&lt;/code&gt; 的值将其从原来的 &lt;code&gt;nginx:1.14.2&lt;/code&gt; 改成 &lt;code&gt;nginx:1.16.1&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl edit deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment edited
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;查看发布状态，执行如下命令:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout status deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;或&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment successfully rolled out
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看更新后 Deployment 的更多细节:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;当发布执行完成后，查看 Deployment 执行命令  &lt;code&gt;kubectl get deployments&lt;/code&gt;：
输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           36s
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;执行 &lt;code&gt;kubectl get rs&lt;/code&gt; 命令可以看到 Deployment 是通过创建一个新的 ReplicaSet 并将其副本数扩充到 3
同时将旧的 ReplicaSet 的副本数收缩至 0&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       6s
nginx-deployment-2035384211   0         0         0       36s
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这时再执行命令 &lt;code&gt;get pods&lt;/code&gt;， 应该就只会看到新创建的 Pod:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1564180365-khku8   1/1       Running   0          14s
nginx-deployment-1564180365-nacti   1/1       Running   0          14s
nginx-deployment-1564180365-z9gth   1/1       Running   0          14s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当用户下次想更新这些 Pod 时， 只需要再次更新 Deployment 的 Pod 模板即可。&lt;/p&gt;
&lt;p&gt;Deployment 会确保在更新过程中挂掉的 Pod 数不会大于指定的值。 默认情况下，会保证至少有期望数量 75% 数量的 Pod 是正常运行的(最多只有 25% 是不可用的)&lt;/p&gt;
&lt;p&gt;Deployment 还会保证多于期望数量的 Pod 数不会大于指定的值。 默认情况下同时正常运行的 Pod 数不会大于期望数量的 125%(上限为 25%)&lt;/p&gt;
&lt;p&gt;如果仔细观察上面的例子。 就会发现更新过程中，首先会创建一个新的 Pod， 然后删除一些旧的，再创建新的。
在新创建并就绪的 Pod 的数量没有达到特定数量之前不会干掉旧的 Pod， 同时在没干掉特定数量旧的 Pod 之前不会创建新的 Pod。
这个过程中会保证在少的时候至少有 2 个 Pod 可用，在多的时候至多有 4 个 Pod 可用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;继续来看 Deployment 的更多细节：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe deployments
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=2
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
   Containers:
    nginx:
      Image:        nginx:1.16.1
      Port:         80/TCP
      Environment:  &amp;lt;none&amp;gt;
      Mounts:       &amp;lt;none&amp;gt;
    Volumes:        &amp;lt;none&amp;gt;
  Conditions:
    Type           Status  Reason
    ----           ------  ------
    Available      True    MinimumReplicasAvailable
    Progressing    True    NewReplicaSetAvailable
  OldReplicaSets:  &amp;lt;none&amp;gt;
  NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)
  Events:
    Type    Reason             Age   From                   Message
    ----    ------             ----  ----                   -------
    Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3
    Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1
    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2
    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2
    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1
    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3
    Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里可以看到， 在第一次创建 Deployment 的时候， 创建了一个 ReplicaSet (nginx-deployment-2035384211) 并直接将副本数扩充到 3.
当 更新 Deployment 的时候， 又创建了一个新的 ReplicaSet (nginx-deployment-1564180365) 并将副本数扩充到 1 ， 紧接着将旧的 ReplicaSet
的副本数收缩至 2， 因此至始终少有 2 Pod 可用， 至多有 4 个 Pod 可用。 接下来继续使用相同的滚动更新策略扩充新的，收缩旧的 ReplicaSet。到最后就变成新的
ReplicaSet 有 3 个副本可用， 旧的  ReplicaSet 副本数收缩为 0 。&lt;/p&gt;
&lt;!--
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rollover-aka-multiple-updates-in-flight&#34;&gt;Rollover (aka multiple updates in-flight)&lt;/h3&gt;
&lt;p&gt;Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up
the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels
match &lt;code&gt;.spec.selector&lt;/code&gt; but whose template does not match &lt;code&gt;.spec.template&lt;/code&gt; are scaled down. Eventually, the new
ReplicaSet is scaled to &lt;code&gt;.spec.replicas&lt;/code&gt; and all old ReplicaSets is scaled to 0.&lt;/p&gt;
&lt;p&gt;If you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet
as per the update and start scaling that up, and rolls over the ReplicaSet that it was scaling up previously
&amp;ndash; it will add it to its list of old ReplicaSets and start scaling it down.&lt;/p&gt;
&lt;p&gt;For example, suppose you create a Deployment to create 5 replicas of &lt;code&gt;nginx:1.14.2&lt;/code&gt;,
but then update the Deployment to create 5 replicas of &lt;code&gt;nginx:1.16.1&lt;/code&gt;, when only 3
replicas of &lt;code&gt;nginx:1.14.2&lt;/code&gt; had been created. In that case, the Deployment immediately starts
killing the 3 &lt;code&gt;nginx:1.14.2&lt;/code&gt; Pods that it had created, and starts creating
&lt;code&gt;nginx:1.16.1&lt;/code&gt; Pods. It does not wait for the 5 replicas of &lt;code&gt;nginx:1.14.2&lt;/code&gt; to be created
before changing course.
&amp;ndash;&amp;gt;&lt;/p&gt;
&lt;h3 id=&#34;rollover-aka-multiple-updates-in-flight-没想到比较恰当的&#34;&gt;Rollover (aka multiple updates in-flight) 没想到比较恰当的&lt;/h3&gt;
&lt;p&gt;每次 Deployment 控制器发现新创建了一个 Deployment， 就会创建一个新的 ReplicaSet 来创建期望数量的 Pod.
如 Deployment 被更新后， 对于之前存在的 ReplicaSet， 它控制的 Pod 的标签能够匹配 &lt;code&gt;.spec.selector&lt;/code&gt; 但是模板不能匹配 &lt;code&gt;.spec.template&lt;/code&gt;，副本数量就会被收缩。
最后， 新创建的 ReplicaSet 的副本数扩充到 &lt;code&gt;.spec.replicas&lt;/code&gt; 配置的数量。 所以旧的 ReplicaSet 副本收缩为 0。&lt;/p&gt;
&lt;p&gt;如果在前一个发布还在进行时又更新的 Deployment， Deployment 会为被次更新创建一个对应的 ReplicaSet，就开始扩容。
前一个正在扩容的 ReplicaSet 就会被跳过。 它会添加到旧 ReplicaSet 的列表，然后开始收缩容量。&lt;/p&gt;
&lt;p&gt;例如， 假设创建一个包含 5 个 &lt;code&gt;nginx:1.14.2&lt;/code&gt;  副本的 Deployment 这时将 Deployment 改为包含 &lt;code&gt;nginx:1.16.1&lt;/code&gt; 副本， 此时只创建了 3 个 &lt;code&gt;nginx:1.14.2&lt;/code&gt; 副本。
在这种情况下， Deployment 会马上开始干掉 已经被创建的 3 个 &lt;code&gt;nginx:1.14.2&lt;/code&gt; Pod， 并开始创建 &lt;code&gt;nginx:1.16.1&lt;/code&gt; Pod。 并不会等 5 个 &lt;code&gt;nginx:1.14.2&lt;/code&gt; 创建完成后才应用变更引发的任务&lt;/p&gt;
&lt;!--
### Label selector updates

It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front.
In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped
all of the implications.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; In API version &lt;code&gt;apps/v1&lt;/code&gt;, a Deployment&amp;rsquo;s label selector is immutable after it gets created.&lt;/div&gt;
&lt;/blockquote&gt;


* Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too,
otherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does
not select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and
creating a new ReplicaSet.
* Selector updates changes the existing value in a selector key -- result in the same behavior as additions.
* Selector removals removes an existing key from the Deployment selector -- do not require any changes in the
Pod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the
removed label still exists in any existing Pods and ReplicaSets.
 --&gt;
&lt;h3 id=&#34;标签选择器的更新&#34;&gt;标签选择器的更新&lt;/h3&gt;
&lt;p&gt;通常不建议在标签选择器上做修改，而是预先就想好标签选择器。 在任何你想要修改标签器的情况下，一定要小心小心再小心， 确保已经理清楚所有的头绪。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 对于版本为 &lt;code&gt;apps/v1&lt;/code&gt; API， Deployment 在创建后不可变更。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;标签选择器添加条目必须要 Deployment 的 Pod 模板的标签也要添加该条目， 否则会返回一个校验错误。 也不能让这个变更与原来的不重叠，不重叠的意思是新的选择器
不能匹配到旧的选择创建的 ReplicaSet 主 Pod， 导致旧的 ReplicaSet 脱钩 然后创建新的ReplicaSet&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;标签选择器更新的是其中的已经存在的一个键的值， 与添加条目的行为一至。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;标签选择器更新的删除一个存在的键， 不会对 Pod 模板做任何修改。 存在的 ReplicaSet 不会脱钩， 也不会创建新的 ReplicaSet， 但要注意删除的标签键，依然存在于这些
ReplicaSet 和 Pod 中&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Rolling Back a Deployment

Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping.
By default, all of the Deployment&#39;s rollout history is kept in the system so that you can rollback anytime you want
(you can change that by modifying revision history limit).

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A Deployment&amp;rsquo;s revision is created when a Deployment&amp;rsquo;s rollout is triggered. This means that the
new revision is created if and only if the Deployment&amp;rsquo;s Pod template (&lt;code&gt;.spec.template&lt;/code&gt;) is changed,
for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment,
do not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling.
This means that when you roll back to an earlier revision, only the Deployment&amp;rsquo;s Pod template part is
rolled back.&lt;/div&gt;
&lt;/blockquote&gt;


* Suppose that you made a typo while updating the Deployment, by putting the image name as `nginx:1.161` instead of `nginx:1.16.1`:

    ```shell
    kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.161 --record=true
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment image updated
    ```

* The rollout gets stuck. You can verify it by checking the rollout status:

    ```shell
    kubectl rollout status deployment.v1.apps/nginx-deployment
    ```

    The output is similar to this:
    ```
    Waiting for rollout to finish: 1 out of 3 new replicas have been updated...
    ```

* Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts,
[read more here](#deployment-status).

* You see that the number of old replicas (`nginx-deployment-1564180365` and `nginx-deployment-2035384211`) is 2, and new replicas (nginx-deployment-3066724191) is 1.

    ```shell
    kubectl get rs
    ```

    The output is similar to this:
    ```
    NAME                          DESIRED   CURRENT   READY   AGE
    nginx-deployment-1564180365   3         3         3       25s
    nginx-deployment-2035384211   0         0         0       36s
    nginx-deployment-3066724191   1         1         0       6s
    ```

* Looking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop.

    ```shell
    kubectl get pods
    ```

    The output is similar to this:
    ```
    NAME                                READY     STATUS             RESTARTS   AGE
    nginx-deployment-1564180365-70iae   1/1       Running            0          25s
    nginx-deployment-1564180365-jbqqo   1/1       Running            0          25s
    nginx-deployment-1564180365-hysrc   1/1       Running            0          25s
    nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s
    ```

    &lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; The Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on the rollingUpdate parameters (&lt;code&gt;maxUnavailable&lt;/code&gt; specifically) that you have specified. Kubernetes by default sets the value to 25%.&lt;/div&gt;
&lt;/blockquote&gt;


* Get the description of the Deployment:
    ```shell
    kubectl describe deployment
    ```

    The output is similar to this:
    ```
    Name:           nginx-deployment
    Namespace:      default
    CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700
    Labels:         app=nginx
    Selector:       app=nginx
    Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable
    StrategyType:       RollingUpdate
    MinReadySeconds:    0
    RollingUpdateStrategy:  25% max unavailable, 25% max surge
    Pod Template:
      Labels:  app=nginx
      Containers:
       nginx:
        Image:        nginx:1.161
        Port:         80/TCP
        Host Port:    0/TCP
        Environment:  &lt;none&gt;
        Mounts:       &lt;none&gt;
      Volumes:        &lt;none&gt;
    Conditions:
      Type           Status  Reason
      ----           ------  ------
      Available      True    MinimumReplicasAvailable
      Progressing    True    ReplicaSetUpdated
    OldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)
    NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)
    Events:
      FirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message
      --------- --------    -----   ----                    -------------   --------    ------              -------
      1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3
      22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1
      22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2
      22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2
      21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1
      21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3
      13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0
      13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1
    ```

  To fix this, you need to rollback to a previous revision of Deployment that is stable.
 --&gt;
&lt;h2 id=&#34;deployment-回滚示例&#34;&gt;Deployment 回滚示例&lt;/h2&gt;
&lt;p&gt;有时候会需要对 Deployment 进行回滚，例如当一个 Deployment 进入像无限崩溃等不稳定情况时。
默认情况下系统会保存 Deployment 所有的发布历史以便用户通过回滚到之前的任意版本(用户可以修改限制保留历史数)&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Deployment 版本在 Deployment 发布(rollout)时触发。 也就是说一个新的版本在且仅在 Deployment 的 Pod 模板 (&lt;code&gt;.spec.template&lt;/code&gt;) 发生变更，
如更新了模板中的标签或容器的镜像。 其它的如扩充 Deployment 副本数是不会创建版本的，这样在手动或自动伸缩容量时不会引起版本的增加。
也就是说当用户回滚到一个之前的版本时，只有 Deployment 的 Pod 模板部署回到了这个版本的状态。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;假设在将 Deployment 的镜像名改为 &lt;code&gt;nginx:1.16.1&lt;/code&gt; 时，手抖写成了 &lt;code&gt;nginx:1.161&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl set image deployment.v1.apps/nginx-deployment nginx&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx:1.161 --record&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;true
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment image updated
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这时这个发布就卡住了， 可以通过以下命令查看发布状态&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout status deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Waiting for rollout to finish: 1 out of 3 new replicas have been updated...
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Ctrl-C&lt;/code&gt; 退出发布状态监控， 要查看卡住的 Deployment, 见&lt;a href=&#34;#deployment-status&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这时候可以看到旧的副本数 (&lt;code&gt;nginx-deployment-1564180365&lt;/code&gt; 和 &lt;code&gt;nginx-deployment-2035384211&lt;/code&gt;) 是 2, 新的副本数 (&lt;code&gt;nginx-deployment-3066724191&lt;/code&gt;) 是 1.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       25s
nginx-deployment-2035384211   0         0         0       36s
nginx-deployment-3066724191   1         1         0       6s
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;再来看创建的 Pod， 这时候看到由新的  ReplicaSet 因为拉取镜像出问题卡在那了&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                                READY     STATUS             RESTARTS   AGE
nginx-deployment-1564180365-70iae   1/1       Running            0          25s
nginx-deployment-1564180365-jbqqo   1/1       Running            0          25s
nginx-deployment-1564180365-hysrc   1/1       Running            0          25s
nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Deployment 控制器会自动停止错误的发布， 也会停止扩充新 ReplicaSet 的副本数。 这个行为信赖于配置的更新参数 (也就是&lt;code&gt;maxUnavailable&lt;/code&gt;)，
k8s 中默认设置为&lt;/div&gt;
&lt;/blockquote&gt;
25%。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看 Deployment 的描述信息&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:           nginx-deployment
Namespace:      default
CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700
Labels:         app=nginx
Selector:       app=nginx
Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable
StrategyType:       RollingUpdate
MinReadySeconds:    0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.161
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &amp;lt;none&amp;gt;
    Mounts:       &amp;lt;none&amp;gt;
  Volumes:        &amp;lt;none&amp;gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)
NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)
Events:
  FirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message
  --------- --------    -----   ----                    -------------   --------    ------              -------
  1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;而要修复这个问题，就需要将 Deployment 回滚到上一个稳定版本&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Checking Rollout History of a Deployment

Follow the steps given below to check the rollout history:

1. First, check the revisions of this Deployment:
    ```shell
    kubectl rollout history deployment.v1.apps/nginx-deployment
    ```
    The output is similar to this:
    ```
    deployments &#34;nginx-deployment&#34;
    REVISION    CHANGE-CAUSE
    1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml --record=true
    2           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 --record=true
    3           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.161 --record=true
    ```

    `CHANGE-CAUSE` is copied from the Deployment annotation `kubernetes.io/change-cause` to its revisions upon creation. You can specify the`CHANGE-CAUSE` message by:

    * Annotating the Deployment with `kubectl annotate deployment.v1.apps/nginx-deployment kubernetes.io/change-cause=&#34;image updated to 1.16.1&#34;`
    * Append the `--record` flag to save the `kubectl` command that is making changes to the resource.
    * Manually editing the manifest of the resource.

2. To see the details of each revision, run:
    ```shell
    kubectl rollout history deployment.v1.apps/nginx-deployment --revision=2
    ```

    The output is similar to this:
    ```
    deployments &#34;nginx-deployment&#34; revision 2
      Labels:       app=nginx
              pod-template-hash=1159050644
      Annotations:  kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 --record=true
      Containers:
       nginx:
        Image:      nginx:1.16.1
        Port:       80/TCP
         QoS Tier:
            cpu:      BestEffort
            memory:   BestEffort
        Environment Variables:      &lt;none&gt;
      No volumes.
    ```
 --&gt;
&lt;h3 id=&#34;查看-deployment-的历史版本&#34;&gt;查看 Deployment 的历史版本&lt;/h3&gt;
&lt;p&gt;以下步骤可以查看布历史:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;第一步， 查看 Deployment 历史版本列表:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout history deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployments &amp;quot;nginx-deployment&amp;quot;
REVISION    CHANGE-CAUSE
1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml --record=true
2           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 --record=true
3           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.161 --record=true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;CHANGE-CAUSE&lt;/code&gt; 是从 Deployment 中 &lt;code&gt;kubernetes.io/change-cause&lt;/code&gt; 注解中记录的信息。 可能通过以下方式指定 &lt;code&gt;CHANGE-CAUSE&lt;/code&gt; 中的消息&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 Deployment 上打注解 &lt;code&gt;kubectl annotate deployment.v1.apps/nginx-deployment kubernetes.io/change-cause=&amp;quot;image updated to 1.16.1&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;在修改该资源的 &lt;code&gt;kubectl&lt;/code&gt; 命令中添加 &lt;code&gt;--record&lt;/code&gt; 标志&lt;/li&gt;
&lt;li&gt;人工修改资源配置文件&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;要查看每个版本的详细信息，执行如下命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout history deployment.v1.apps/nginx-deployment --revision&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployments &amp;quot;nginx-deployment&amp;quot; revision 2
  Labels:       app=nginx
          pod-template-hash=1159050644
  Annotations:  kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 --record=true
  Containers:
   nginx:
    Image:      nginx:1.16.1
    Port:       80/TCP
     QoS Tier:
        cpu:      BestEffort
        memory:   BestEffort
    Environment Variables:      &amp;lt;none&amp;gt;
  No volumes.
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
### Rolling Back to a Previous Revision
Follow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2.

1. Now you&#39;ve decided to undo the current rollout and rollback to the previous revision:
    ```shell
    kubectl rollout undo deployment.v1.apps/nginx-deployment
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment rolled back
    ```
    Alternatively, you can rollback to a specific revision by specifying it with `--to-revision`:

    ```shell
    kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision=2
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment rolled back
    ```

    For more details about rollout related commands, read [`kubectl rollout`](/docs/reference/generated/kubectl/kubectl-commands#rollout).

    The Deployment is now rolled back to a previous stable revision. As you can see, a `DeploymentRollback` event
    for rolling back to revision 2 is generated from Deployment controller.

2. Check if the rollback was successful and the Deployment is running as expected, run:
    ```shell
    kubectl get deployment nginx-deployment
    ```

    The output is similar to this:
    ```
    NAME               READY   UP-TO-DATE   AVAILABLE   AGE
    nginx-deployment   3/3     3            3           30m
    ```
3. Get the description of the Deployment:
    ```shell
    kubectl describe deployment nginx-deployment
    ```
    The output is similar to this:
    ```
    Name:                   nginx-deployment
    Namespace:              default
    CreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500
    Labels:                 app=nginx
    Annotations:            deployment.kubernetes.io/revision=4
                            kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 --record=true
    Selector:               app=nginx
    Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
    StrategyType:           RollingUpdate
    MinReadySeconds:        0
    RollingUpdateStrategy:  25% max unavailable, 25% max surge
    Pod Template:
      Labels:  app=nginx
      Containers:
       nginx:
        Image:        nginx:1.16.1
        Port:         80/TCP
        Host Port:    0/TCP
        Environment:  &lt;none&gt;
        Mounts:       &lt;none&gt;
      Volumes:        &lt;none&gt;
    Conditions:
      Type           Status  Reason
      ----           ------  ------
      Available      True    MinimumReplicasAvailable
      Progressing    True    NewReplicaSetAvailable
    OldReplicaSets:  &lt;none&gt;
    NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)
    Events:
      Type    Reason              Age   From                   Message
      ----    ------              ----  ----                   -------
      Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3
      Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1
      Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2
      Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2
      Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1
      Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3
      Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0
      Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1
      Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment &#34;nginx-deployment&#34; to revision 2
      Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0
    ```

 --&gt;
&lt;h3 id=&#34;回滚到之前一个版本&#34;&gt;回滚到之前一个版本&lt;/h3&gt;
&lt;p&gt;以下步骤给出把 Deployment 版本从当前版本回滚到之前的版本， 这里是编号为2的版本&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;将当前版本回滚到之前一个版本:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout undo deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment rolled back
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;另外还有一个方式， 可以通过 &lt;code&gt;--to-revision&lt;/code&gt; 参数指定回滚到指定版本:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment rolled back
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;更多发布相关的命令，见&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#rollout&#34;&gt;&lt;code&gt;kubectl rollout&lt;/code&gt;&lt;/a&gt;.
这时 Deployment 就已经回滚到之前的稳定版本。如你所见， 一个用来将 Deployment 回滚到版本 2 的 &lt;code&gt;DeploymentRollback&lt;/code&gt; 事件由 Deployment 控制器创建&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;检查回滚是否成功， Deployment 是否达到预期， 执行发中下命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get deployment nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           30m
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查看 Deployment 描述信息:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe deployment nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=4
                        kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 --record=true
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.16.1
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &amp;lt;none&amp;gt;
    Mounts:       &amp;lt;none&amp;gt;
  Volumes:        &amp;lt;none&amp;gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &amp;lt;none&amp;gt;
NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)
Events:
  Type    Reason              Age   From                   Message
  ----    ------              ----  ----                   -------
  Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1
  Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment &amp;quot;nginx-deployment&amp;quot; to revision 2
  Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
## Scaling a Deployment

You can scale a Deployment by using the following command:

```shell
kubectl scale deployment.v1.apps/nginx-deployment --replicas=10
```
The output is similar to this:
```
deployment.apps/nginx-deployment scaled
```

Assuming [horizontal Pod autoscaling](/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/) is enabled
in your cluster, you can setup an autoscaler for your Deployment and choose the minimum and maximum number of
Pods you want to run based on the CPU utilization of your existing Pods.

```shell
kubectl autoscale deployment.v1.apps/nginx-deployment --min=10 --max=15 --cpu-percent=80
```
The output is similar to this:
```
deployment.apps/nginx-deployment scaled
```
 --&gt;
&lt;h2 id=&#34;deployment-副本数管理&#34;&gt;Deployment 副本数管理&lt;/h2&gt;
&lt;p&gt;可以通过以下命令修改 Deployment 副本数:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl scale deployment.v1.apps/nginx-deployment --replicas&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment scaled
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;假设集群启用了 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/horizontal-pod-autoscale-walkthrough/&#34;&gt;Pod 水平扩展&lt;/a&gt;， 用户可以为 Deployment 设置
一个自动扩展器，可以根据 Pod 的 CPU 使用率选择 Pod 数量的下限和上限。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl autoscale deployment.v1.apps/nginx-deployment --min&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt; --max&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt; --cpu-percent&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment scaled
&lt;/code&gt;&lt;/pre&gt;&lt;!--
### Proportional scaling

RollingUpdate Deployments support running multiple versions of an application at the same time. When you
or an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress
or paused), the Deployment controller balances the additional replicas in the existing active
ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called *proportional scaling*.

For example, you are running a Deployment with 10 replicas, [maxSurge](#max-surge)=3, and [maxUnavailable](#max-unavailable)=2.

* Ensure that the 10 replicas in your Deployment are running.
  ```shell
  kubectl get deploy
  ```
  The output is similar to this:

  ```
  NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
  nginx-deployment     10        10        10           10          50s
  ```

* You update to a new image which happens to be unresolvable from inside the cluster.
    ```shell
    kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:sometag
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment image updated
    ```

* The image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it&#39;s blocked due to the
`maxUnavailable` requirement that you mentioned above. Check out the rollout status:
    ```shell
    kubectl get rs
    ```
      The output is similar to this:
    ```
    NAME                          DESIRED   CURRENT   READY     AGE
    nginx-deployment-1989198191   5         5         0         9s
    nginx-deployment-618515232    8         8         8         1m
    ```

* Then a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas
to 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren&#39;t using
proportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you
spread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the
most replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the
ReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.

In our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the
new ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assuming
the new replicas become healthy. To confirm this, run:

```shell
kubectl get deploy
```

The output is similar to this:
```
NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     15        18        7            8           7m
```
The rollout status confirms how the replicas were added to each ReplicaSet.
```shell
kubectl get rs
```

The output is similar to this:
```
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   7         7         0         7m
nginx-deployment-618515232    11        11        11        7m
```
 --&gt;
&lt;h3 id=&#34;同比例扩容&#34;&gt;同比例扩容&lt;/h3&gt;
&lt;p&gt;更新策略为 RollingUpdate 的 Deployment 支持一个应用同时运行多个版本。 当用户或自动容量管量器在发布过程中(正在进行或暂停)进行容量伸缩，
Deployment 控制器会新增的副本按已经存在的活跃 ReplicaSet (包含 Pod 的 ReplicaSet)中副本数的比例分散到各个ReplicaSet中与以降低风险。
这就被称为 &lt;em&gt;同比例扩容&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;例如， 用户运行一个有 10 个副本的 Deployment， 它的 &lt;a href=&#34;#max-surge&#34;&gt;maxSurge&lt;/a&gt;=3, &lt;a href=&#34;#max-unavailable&#34;&gt;maxUnavailable&lt;/a&gt;=2.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;确保 Deployment 的 10 个副本正常运行&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get deploy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     10        10        10           10          50s
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;更新了一个集群中不可用的镜像&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl set image deployment.v1.apps/nginx-deployment nginx&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx:sometag
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment image updated
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;更新镜像创建， 开始创建 ReplicaSet &lt;code&gt;nginx-deployment-1989198191&lt;/code&gt;， 但因为上面提到的 &lt;code&gt;maxUnavailable&lt;/code&gt;， 所以会阻塞，通过以下命令查看发布状态：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   5         5         0         9s
nginx-deployment-618515232    8         8         8         1m
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这时候 Deployment 又来了一个扩容请求。 自动伸缩管理器将 Deployment 的副本数加到 15.
Deployment 控制器需要决定这增加的5个副本应该加到哪里。 根据 proportional scaling ， 会将新增的副本分散到所有的 ReplicaSet。
副本占比多的 ReplicaSet 新增的副本也多，副本占比少的新增副本就少。 余数再分给副本最多的 ReplicaSet。 副本数为 0 ReplicaSet 的则不会扩容。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在上面的例子中， 3 个副本被添加到旧的 ReplicaSet 中， 两个副本被添加到新的 ReplicaSet。 发布的继续推进最终会将所有的副本都移到新的 ReplicaSet
保证所以新的副本都是正常的。 执行以下命令验证:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get deploy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     15        18        7            8           7m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;发布状态也确认了副本是怎么加到每个 ReplicaSet 上的&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   7         7         0         7m
nginx-deployment-618515232    11        11        11        7m
&lt;/code&gt;&lt;/pre&gt;&lt;!--
## Pausing and Resuming a Deployment

You can pause a Deployment before triggering one or more updates and then resume it. This allows you to
apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.

* For example, with a Deployment that was just created:
  Get the Deployment details:
  ```shell
  kubectl get deploy
  ```
  The output is similar to this:
  ```
  NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
  nginx     3         3         3            3           1m
  ```
  Get the rollout status:
  ```shell
  kubectl get rs
  ```
  The output is similar to this:
  ```
  NAME               DESIRED   CURRENT   READY     AGE
  nginx-2142116321   3         3         3         1m
  ```

* Pause by running the following command:
    ```shell
    kubectl rollout pause deployment.v1.apps/nginx-deployment
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment paused
    ```

* Then update the image of the Deployment:
    ```shell
    kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment image updated
    ```

* Notice that no new rollout started:
    ```shell
    kubectl rollout history deployment.v1.apps/nginx-deployment
    ```

    The output is similar to this:
    ```
    deployments &#34;nginx&#34;
    REVISION  CHANGE-CAUSE
    1   &lt;none&gt;
    ```
* Get the rollout status to ensure that the Deployment is updates successfully:
    ```shell
    kubectl get rs
    ```

    The output is similar to this:
    ```
    NAME               DESIRED   CURRENT   READY     AGE
    nginx-2142116321   3         3         3         2m
    ```

* You can make as many updates as you wish, for example, update the resources that will be used:
    ```shell
    kubectl set resources deployment.v1.apps/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment resource requirements updated
    ```

    The initial state of the Deployment prior to pausing it will continue its function, but new updates to
    the Deployment will not have any effect as long as the Deployment is paused.

* Eventually, resume the Deployment and observe a new ReplicaSet coming up with all the new updates:
    ```shell
    kubectl rollout resume deployment.v1.apps/nginx-deployment
    ```

    The output is similar to this:
    ```
    deployment.apps/nginx-deployment resumed
    ```
* Watch the status of the rollout until it&#39;s done.
    ```shell
    kubectl get rs -w
    ```

    The output is similar to this:
    ```
    NAME               DESIRED   CURRENT   READY     AGE
    nginx-2142116321   2         2         2         2m
    nginx-3926361531   2         2         0         6s
    nginx-3926361531   2         2         1         18s
    nginx-2142116321   1         2         2         2m
    nginx-2142116321   1         2         2         2m
    nginx-3926361531   3         2         1         18s
    nginx-3926361531   3         2         1         18s
    nginx-2142116321   1         1         1         2m
    nginx-3926361531   3         3         1         18s
    nginx-3926361531   3         3         2         19s
    nginx-2142116321   0         1         1         2m
    nginx-2142116321   0         1         1         2m
    nginx-2142116321   0         0         0         2m
    nginx-3926361531   3         3         3         20s
    ```
* Get the status of the latest rollout:
    ```shell
    kubectl get rs
    ```

    The output is similar to this:
    ```
    NAME               DESIRED   CURRENT   READY     AGE
    nginx-2142116321   0         0         0         2m
    nginx-3926361531   3         3         3         28s
    ```
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You cannot rollback a paused Deployment until you resume it.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;deployment-的暂停与恢复&#34;&gt;Deployment 的暂停与恢复&lt;/h2&gt;
&lt;p&gt;用户可以一个或多个更新触发之前暂停 Deployment 然后再恢复它。 通过这种方式用户可以在暂停后进行多次修改然后恢复，这样在多次
修改的过程中就不会触发不必要的发布。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;例如以下为一个刚创建的 Deployment:
查看 Deployment 信息:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get deploy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     3         3         3            3           1m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;查看发布状态:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         1m
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;执行以下命令暂停 Deployment:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout pause deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment paused
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这时先修改 Deployment 的镜像:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl set image deployment.v1.apps/nginx-deployment nginx&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx:1.16.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment image updated
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可以看到现在没有触发新的发布:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout history deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployments &amp;quot;nginx&amp;quot;
REVISION  CHANGE-CAUSE
1   &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;查看发布状态，确保 Deployment 的更新的成功的：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         2m
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这时候用户就可以进行任意多次修改，例如，以下为修改资源占用:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl set resources deployment.v1.apps/nginx-deployment -c&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx --limits&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;cpu&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;200m,memory&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;512Mi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment resource requirements updated
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Deployment 会继续以暂停之前的状态提供功能， 但在恢复的在的所有修改都不会生效&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;最后，恢复 Deployment 观察新建的 ReplicaSet 会包含暂停过程中的所以的更新&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout resume deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment resumed
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;Watch the status of the rollout until it&amp;rsquo;s done.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;观察发布状态直至任务完成&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs -w
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   2         2         2         2m
nginx-3926361531   2         2         0         6s
nginx-3926361531   2         2         1         18s
nginx-2142116321   1         2         2         2m
nginx-2142116321   1         2         2         2m
nginx-3926361531   3         2         1         18s
nginx-3926361531   3         2         1         18s
nginx-2142116321   1         1         1         2m
nginx-3926361531   3         3         1         18s
nginx-3926361531   3         3         2         19s
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         20s
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;查看最终的发布状态:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         28s
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Deployment 在暂停后直至恢复，中间不能回滚&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Deployment status

A Deployment enters various states during its lifecycle. It can be [progressing](#progressing-deployment) while
rolling out a new ReplicaSet, it can be [complete](#complete-deployment), or it can [fail to progress](#failed-deployment).
 --&gt;
&lt;h2 id=&#34;deployment-status&#34;&gt;Deployment 的状态&lt;/h2&gt;
&lt;p&gt;一个 Deployment 在整个生命周期中会进入许多种状态。更新到一个新的 ReplicaSet 时， 状态可以是 &lt;a href=&#34;#progressing-deployment&#34;&gt;进行中&lt;/a&gt;
也可能是 &lt;a href=&#34;#complete-deployment&#34;&gt;完成&lt;/a&gt;，还可以是 &lt;a href=&#34;#failed-deployment&#34;&gt;失败&lt;/a&gt;&lt;/p&gt;
&lt;!--
### Progressing Deployment

Kubernetes marks a Deployment as _progressing_ when one of the following tasks is performed:

* The Deployment creates a new ReplicaSet.
* The Deployment is scaling up its newest ReplicaSet.
* The Deployment is scaling down its older ReplicaSet(s).
* New Pods become ready or available (ready for at least [MinReadySeconds](#min-ready-seconds)).
 --&gt;
&lt;h3 id=&#34;progressing-deployment&#34;&gt;Deployment 进行中&lt;/h3&gt;
&lt;p&gt;当 Deployment 在进行以下任一任务时，标记为 &lt;em&gt;进行中&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deployment 创建一个新的 ReplicaSet&lt;/li&gt;
&lt;li&gt;Deployment 在对最新的 ReplicaSet 扩容&lt;/li&gt;
&lt;li&gt;Deployment 在对旧的 ReplicaSet 缩减容量&lt;/li&gt;
&lt;li&gt;新创建的 Pod 正在进入就绪或可用状态(就绪最小时间&lt;a href=&#34;#min-ready-seconds&#34;&gt;MinReadySeconds&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;complete-deployment&#34;&gt;Complete Deployment&lt;/h3&gt;
&lt;p&gt;Kubernetes marks a Deployment as &lt;em&gt;complete&lt;/em&gt; when it has the following characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All of the replicas associated with the Deployment have been updated to the latest version you&amp;rsquo;ve specified, meaning any
updates you&amp;rsquo;ve requested have been completed.&lt;/li&gt;
&lt;li&gt;All of the replicas associated with the Deployment are available.&lt;/li&gt;
&lt;li&gt;No old replicas for the Deployment are running.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can check if a Deployment has completed by using &lt;code&gt;kubectl rollout status&lt;/code&gt;. If the rollout completed
successfully, &lt;code&gt;kubectl rollout status&lt;/code&gt; returns a zero exit code.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout status deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The output is similar to this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Waiting for rollout to finish: 2 of 3 updated replicas are available...
deployment.apps/nginx-deployment successfully rolled out
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and the exit status from &lt;code&gt;kubectl rollout&lt;/code&gt; is 0 (success):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;echo $?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;0
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;deployment-完成&#34;&gt;Deployment 完成&lt;/h3&gt;
&lt;p&gt;当 Deployment 拥有以下特征时被 k8s 标记为 完成&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有与 Deployment 关联的副本都已经使用了最新的配置，也就是说所以的更新请求都已经完成。&lt;/li&gt;
&lt;li&gt;所有与 Deployment 关联的副本都已经可用&lt;/li&gt;
&lt;li&gt;Deployment 没有旧的副本在运行&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用户可以通过命令 &lt;code&gt;kubectl rollout status&lt;/code&gt; 检测 Deployment 是否完成。 如果发布已经完成，
&lt;code&gt;kubectl rollout status&lt;/code&gt; 命令的退出码为 0&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout status deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Waiting for rollout to finish: 2 of 3 updated replicas are available...
deployment.apps/nginx-deployment successfully rolled out
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl rollout&lt;/code&gt; 的退出码为 0 (成功):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;echo $?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;0
&lt;/code&gt;&lt;/pre&gt;&lt;!--
### Failed Deployment

Your Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur
due to some of the following factors:

* Insufficient quota
* Readiness probe failures
* Image pull errors
* Insufficient permissions
* Limit ranges
* Application runtime misconfiguration

One way you can detect this condition is to specify a deadline parameter in your Deployment spec:
([`.spec.progressDeadlineSeconds`](#progress-deadline-seconds)). `.spec.progressDeadlineSeconds` denotes the
number of seconds the Deployment controller waits before indicating (in the Deployment status) that the
Deployment progress has stalled.

The following `kubectl` command sets the spec with `progressDeadlineSeconds` to make the controller report
lack of progress for a Deployment after 10 minutes:

```shell
kubectl patch deployment.v1.apps/nginx-deployment -p &#39;{&#34;spec&#34;:{&#34;progressDeadlineSeconds&#34;:600}}&#39;
```
The output is similar to this:
```
deployment.apps/nginx-deployment patched
```
Once the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following
attributes to the Deployment&#39;s `.status.conditions`:

* Type=Progressing
* Status=False
* Reason=ProgressDeadlineExceeded

See the [Kubernetes API conventions](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties) for more information on status conditions.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Kubernetes takes no action on a stalled Deployment other than to report a status condition with
&lt;code&gt;Reason=ProgressDeadlineExceeded&lt;/code&gt;. Higher level orchestrators can take advantage of it and act accordingly, for
example, rollback the Deployment to its previous version.&lt;/div&gt;
&lt;/blockquote&gt;


&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If you pause a Deployment, Kubernetes does not check progress against your specified deadline.
You can safely pause a Deployment in the middle of a rollout and resume without triggering
the condition for exceeding the deadline.&lt;/div&gt;
&lt;/blockquote&gt;


You may experience transient errors with your Deployments, either due to a low timeout that you have set or
due to any other kind of error that can be treated as transient. For example, let&#39;s suppose you have
insufficient quota. If you describe the Deployment you will notice the following section:

```shell
kubectl describe deployment nginx-deployment
```
The output is similar to this:
```
&lt;...&gt;
Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     True    ReplicaSetUpdated
  ReplicaFailure  True    FailedCreate
&lt;...&gt;
```

If you run `kubectl get deployment nginx-deployment -o yaml`, the Deployment status is similar to this:

```
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: Replica set &#34;nginx-deployment-4262182780&#34; is progressing.
    reason: ReplicaSetUpdated
    status: &#34;True&#34;
    type: Progressing
  - lastTransitionTime: 2016-10-04T12:25:42Z
    lastUpdateTime: 2016-10-04T12:25:42Z
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: &#34;True&#34;
    type: Available
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: &#39;Error creating: pods &#34;nginx-deployment-4262182780-&#34; is forbidden: exceeded quota:
      object-counts, requested: pods=1, used: pods=3, limited: pods=2&#39;
    reason: FailedCreate
    status: &#34;True&#34;
    type: ReplicaFailure
  observedGeneration: 3
  replicas: 2
  unavailableReplicas: 2
```

Eventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the
reason for the Progressing condition:

```
Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     False   ProgressDeadlineExceeded
  ReplicaFailure  True    FailedCreate
```

You can address an issue of insufficient quota by scaling down your Deployment, by scaling down other
controllers you may be running, or by increasing quota in your namespace. If you satisfy the quota
conditions and the Deployment controller then completes the Deployment rollout, you&#39;ll see the
Deployment&#39;s status update with a successful condition (`Status=True` and `Reason=NewReplicaSetAvailable`).

```
Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
```

`Type=Available` with `Status=True` means that your Deployment has minimum availability. Minimum availability is dictated
by the parameters specified in the deployment strategy. `Type=Progressing` with `Status=True` means that your Deployment
is either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum
required new replicas are available (see the Reason of the condition for the particulars - in our case
`Reason=NewReplicaSetAvailable` means that the Deployment is complete).

You can check if a Deployment has failed to progress by using `kubectl rollout status`. `kubectl rollout status`
returns a non-zero exit code if the Deployment has exceeded the progression deadline.

```shell
kubectl rollout status deployment.v1.apps/nginx-deployment
```
The output is similar to this:
```
Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
error: deployment &#34;nginx&#34; exceeded its progress deadline
```
and the exit status from `kubectl rollout` is 1 (indicating an error):
```shell
echo $?
```
```
1
```
 --&gt;
&lt;h3 id=&#34;deployment-失败&#34;&gt;Deployment 失败&lt;/h3&gt;
&lt;p&gt;Deployment 可能在尝试部署最新的 ReplicaSet 的时候被卡住然后再也完不成。造成这种情况的因素可能有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;配额不足&lt;/li&gt;
&lt;li&gt;就绪探针检测结果为失败&lt;/li&gt;
&lt;li&gt;镜像拉取出错&lt;/li&gt;
&lt;li&gt;权限不足&lt;/li&gt;
&lt;li&gt;应用运行环境配置错误&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;检测到这种情况的一种方法为在 Deployment 的配置中加入一个死线参数: (&lt;a href=&#34;#progress-deadline-seconds&#34;&gt;&lt;code&gt;.spec.progressDeadlineSeconds&lt;/code&gt;&lt;/a&gt;).
&lt;code&gt;.spec.progressDeadlineSeconds&lt;/code&gt; 表示 Deployment 控制器在(Deployment 状态上)显示 Deployment 停止之前等待的时间(单位秒)
以下 &lt;code&gt;kubectl&lt;/code&gt; 添加/修改 &lt;code&gt;progressDeadlineSeconds&lt;/code&gt;， 让控制器在 10 分钟还在进行中的发布标记为失败&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl patch deployment.v1.apps/nginx-deployment -p &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;progressDeadlineSeconds&amp;#34;:600}}&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;deployment.apps/nginx-deployment patched
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当超过设定时限后， Deployment 控制器会向 Deployment 添加一个拥有如下属性的 DeploymentCondition 到 &lt;code&gt;.status.conditions&lt;/code&gt; 。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Type=Progressing&lt;/li&gt;
&lt;li&gt;Status=False&lt;/li&gt;
&lt;li&gt;Reason=ProgressDeadlineExceeded&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更新关于状态条件的信息见 &lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties&#34;&gt;Kubernetes API conventions&lt;/a&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 对于停止的 Deployment k8s 除了添加一个 &lt;code&gt;Reason=ProgressDeadlineExceeded&lt;/code&gt; 条件之前不会做任何其它操作。
层级更高的编排者会根据这个条件进行相应的操作，例如，将 Deployment 回滚到前一个稳定版本&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果 Deployment 被暂停，则 k8s 不会受该时限影响。 用户可以发布乾中安全的暂停并恢复 Deployment 而不需要担心触发这个
时限条件。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;用户可能会遇到 Deployment 短时的错误，可能是因为用户设置一个较低的超时时间或其它任意可以被认作瞬时错误的。
例如， 假设配额不足。 这时时候获取 Deployment 描述信息：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe deployment nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;...&amp;gt;
Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     True    ReplicaSetUpdated
  ReplicaFailure  True    FailedCreate
&amp;lt;...&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;执行命令 &lt;code&gt;kubectl get deployment nginx-deployment -o yaml&lt;/code&gt;, Deployment 状态类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: Replica set &amp;quot;nginx-deployment-4262182780&amp;quot; is progressing.
    reason: ReplicaSetUpdated
    status: &amp;quot;True&amp;quot;
    type: Progressing
  - lastTransitionTime: 2016-10-04T12:25:42Z
    lastUpdateTime: 2016-10-04T12:25:42Z
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: &amp;quot;True&amp;quot;
    type: Available
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: &#39;Error creating: pods &amp;quot;nginx-deployment-4262182780-&amp;quot; is forbidden: exceeded quota:
      object-counts, requested: pods=1, used: pods=3, limited: pods=2&#39;
    reason: FailedCreate
    status: &amp;quot;True&amp;quot;
    type: ReplicaFailure
  observedGeneration: 3
  replicas: 2
  unavailableReplicas: 2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;最终， 当超出 Deployment 的处理时限时， k8s 更新状态和添加处理条件的原因&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     False   ProgressDeadlineExceeded
  ReplicaFailure  True    FailedCreate
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在遇到配额不足的情况，用户可以通过缩减 Deployment 副本数来解决， 也可以缩减用户其它控制器的副本数，还可以增加用户命名空间的配额。
如果达到配额条件， Deployment 这时候就会过成 Deployment 的发布， 这时候就会看到 Deployment 状态被更新为一个成功条件
(&lt;code&gt;Status=True&lt;/code&gt; 和 &lt;code&gt;Reason=NewReplicaSetAvailable&lt;/code&gt;)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;当 &lt;code&gt;Type=Available&lt;/code&gt; 的 &lt;code&gt;Status=True&lt;/code&gt; 时意味着 Deployment 达到了最小可用状。 最小可用状态受 deployment 策略中的参数控制。
&lt;code&gt;Type=Progressing&lt;/code&gt; 的 &lt;code&gt;Status=True&lt;/code&gt; 意味着 Deployment 的发布还在进行或已经处理成功完成，最小需求的副本数已经可用(见具体条件的原因 - 就本例来说
&lt;code&gt;Reason=NewReplicaSetAvailable&lt;/code&gt; 表示 Deployment 已经完成)&lt;/p&gt;
&lt;p&gt;用户可以通过执行命令 &lt;code&gt;kubectl rollout status&lt;/code&gt; 来检测 Deployment 是处理失败。如果 Deployment 已经超出处理时限
&lt;code&gt;kubectl rollout status&lt;/code&gt; 返回码非 0&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl rollout status deployment.v1.apps/nginx-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
error: deployment &amp;quot;nginx&amp;quot; exceeded its progress deadline
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl rollout&lt;/code&gt; 返回码为 1 (表示出错):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;echo $?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;1
&lt;/code&gt;&lt;/pre&gt;&lt;!--
### Operating on a failed deployment

All actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back
to a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template.
 --&gt;
&lt;h3 id=&#34;在失败状态的-deployment-上可以执行的操作&#34;&gt;在失败状态的 Deployment 上可以执行的操作&lt;/h3&gt;
&lt;p&gt;所有在完成的 Deployment 上可以执行的操作都可以在失败的 Deployment 执行. 用户可以扩充/缩减容量, 回滚到之前的一个版本, 甚至如果用户需要
多次修改 Deployment 的 Pod 模板可以将其暂停.&lt;/p&gt;
&lt;!--
## Clean up Policy

You can set `.spec.revisionHistoryLimit` field in a Deployment to specify how many old ReplicaSets for
this Deployment you want to retain. The rest will be garbage-collected in the background. By default,
it is 10.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Explicitly setting this field to 0, will result in cleaning up all the history of your Deployment
thus that Deployment will not be able to roll back.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;清理策略&#34;&gt;清理策略&lt;/h2&gt;
&lt;p&gt;用户可以通过设置 Deployment 中的 &lt;code&gt;.spec.revisionHistoryLimit&lt;/code&gt; 字段, 指定保留多少份旧的 ReplicaSets 的记录.
超出的部分会被后台垃圾回收. 默认是 10&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果将该字段设置为 0, 则会导致所以的历史都会被删除, Deployment 不能被回滚.&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Canary Deployment

If you want to roll out releases to a subset of users or servers using the Deployment, you
can create multiple Deployments, one for each release, following the canary pattern described in
[managing resources](/docs/concepts/cluster-administration/manage-deployment/#canary-deployments).
 --&gt;
&lt;h2 id=&#34;金丝雀-deployment&#34;&gt;金丝雀 Deployment&lt;/h2&gt;
&lt;p&gt;如查用户想让一部分用户或服务使用这个 Deployment 发布的版本, 可以创建多个 Deployment, 每一个发布一个版本,
依据&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/cluster-administration/manage-deployment/#canary-deployments&#34;&gt;资源管理&lt;/a&gt;中描述的金丝雀规则。&lt;/p&gt;
&lt;!--
## Writing a Deployment Spec

As with all other Kubernetes configs, a Deployment needs `.apiVersion`, `.kind`, and `.metadata` fields.
For general information about working with config files, see
[deploying applications](/docs/tasks/run-application/run-stateless-application-deployment/),
configuring containers, and [using kubectl to manage resources](/docs/concepts/overview/working-with-objects/object-management/) documents.
The name of a Deployment object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

A Deployment also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).
 --&gt;
&lt;h2 id=&#34;编写-deployment-spec&#34;&gt;编写 Deployment Spec&lt;/h2&gt;
&lt;p&gt;As with all other Kubernetes configs, a Deployment needs &lt;code&gt;.apiVersion&lt;/code&gt;, &lt;code&gt;.kind&lt;/code&gt;, and &lt;code&gt;.metadata&lt;/code&gt; fields.
For general information about working with config files, see
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/run-application/run-stateless-application-deployment/&#34;&gt;deploying applications&lt;/a&gt;,
configuring containers, and &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/object-management/&#34;&gt;using kubectl to manage resources&lt;/a&gt; documents.
The name of a Deployment object must be a valid
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS subdomain name&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A Deployment also needs a &lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;&lt;code&gt;.spec&lt;/code&gt; section&lt;/a&gt;.
与所以其它的 k8s 配置一样，Deployment 必须要有  &lt;code&gt;.apiVersion&lt;/code&gt;, &lt;code&gt;.kind&lt;/code&gt;, &lt;code&gt;.metadata&lt;/code&gt; 字段。
配置文件的通用信息见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/run-stateless-application-deployment/&#34;&gt;部署应用&lt;/a&gt;,
配置容器， &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/object-management/&#34;&gt;使用 kubect 管理资源&lt;/a&gt;.
Deployment 对象的名称必须是一个有效的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Deployment 还需要有一个 &lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;&lt;code&gt;.spec&lt;/code&gt; 定义区&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### Pod Template

The `.spec.template` and `.spec.selector` are the only required field of the `.spec`.

The `.spec.template` is a [Pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;, except it is nested and does not have an `apiVersion` or `kind`.

In addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate
labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See [selector](#selector)).

Only a [`.spec.template.spec.restartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Always` is
allowed, which is the default if not specified.
 --&gt;
&lt;h3 id=&#34;pod-模板&#34;&gt;Pod 模板&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec&lt;/code&gt; 的必要字段仅有 &lt;code&gt;.spec.template&lt;/code&gt; 和 &lt;code&gt;.spec.selector&lt;/code&gt;两个。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;.spec.template&lt;/code&gt; 字段就是一个 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/#pod-templates&#34;&gt;Pod 模板&lt;/a&gt;对象。定义规范与 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;
完全一样， 除了因为它嵌套的 Deployment 中所以没 &lt;code&gt;apiVersion&lt;/code&gt; 或 &lt;code&gt;kind&lt;/code&gt; 字段。&lt;/p&gt;
&lt;p&gt;相对裸 Pod 所以需要的字段， Deployment 中的 Pod 模板需要指定适当的标签和重启策略。 对于标签定义，需要注意不要与其它的控制器重叠了。 见 &lt;a href=&#34;#selector&#34;&gt;selector&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy&#34;&gt;&lt;code&gt;.spec.template.spec.restartPolicy&lt;/code&gt;&lt;/a&gt; 的值只能是 &lt;code&gt;Always&lt;/code&gt;， 这也是默认值。&lt;/p&gt;
&lt;!--
### Replicas

`.spec.replicas` is an optional field that specifies the number of desired Pods. It defaults to 1.
 --&gt;
&lt;h3 id=&#34;副本数&#34;&gt;副本数&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.replicas&lt;/code&gt; 是一个可选字段，表示期望的 Pod 的数量。 默认为 1&lt;/p&gt;
&lt;!--
### Selector

`.spec.selector` is a required field that specifies a [label selector](/docs/concepts/overview/working-with-objects/labels/)
for the Pods targeted by this Deployment.

`.spec.selector` must match `.spec.template.metadata.labels`, or it will be rejected by the API.

In API version `apps/v1`, `.spec.selector` and `.metadata.labels` do not default to `.spec.template.metadata.labels` if not set. So they must be set explicitly. Also note that `.spec.selector` is immutable after creation of the Deployment in `apps/v1`.

A Deployment may terminate Pods whose labels match the selector if their template is different
from `.spec.template` or if the total number of such Pods exceeds `.spec.replicas`. It brings up new
Pods with `.spec.template` if the number of Pods is less than the desired number.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; You should not create other Pods whose labels match this selector, either directly, by creating
another Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you
do so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this.&lt;/div&gt;
&lt;/blockquote&gt;


If you have multiple controllers that have overlapping selectors, the controllers will fight with each
other and won&#39;t behave correctly.
 --&gt;
&lt;h3 id=&#34;选择器&#34;&gt;选择器&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.selector&lt;/code&gt; 是一个必要字段，用于定义 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/labels/&#34;&gt;标签选择器&lt;/a&gt;, 标签选择器用于选择 Deployment 下属的 Pod&lt;/p&gt;
&lt;p&gt;&lt;code&gt;.spec.selector&lt;/code&gt; 必须与 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt;配置, 否则 API 会拒绝.&lt;/p&gt;
&lt;p&gt;API 版本 &lt;code&gt;apps/v1&lt;/code&gt; 中， &lt;code&gt;.spec.selector&lt;/code&gt; 和 &lt;code&gt;.metadata.labels&lt;/code&gt; 如果没有设置不会默认设置为 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt;。 所以这两个字段必须要显示的设置。&lt;/p&gt;
&lt;p&gt;Deployment 终结标签匹配选择器，但模板与  &lt;code&gt;.spec.template&lt;/code&gt; 不同的 Pod。
如果标签选择器匹配的 Pod 数量大于 &lt;code&gt;.spec.replicas&lt;/code&gt; 的数量，则会终结多于的 Pod。
如果标签选择器匹配的 Pod 数量小于 &lt;code&gt;.spec.replicas&lt;/code&gt; 的数量， 会使用&lt;code&gt;.spec.template&lt;/code&gt;创建新的 Pod， 使总数与期望数相同。&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 用户需要注意不能创建标签匹配该选择的的 Pod， 不管理是直接创建还是通过 Deployment 创建，或者通过其它的如 ReplicaSet 或 ReplicationController
控制器来创建。 如果这样做了，第一个 Deployment 这些 Pod 也是它创建的。k8s 不会阻止用户这样做&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;如果有多个控制器使用了一样的选择器，这些选择器就会打架，然后不能正常工作。&lt;/p&gt;
&lt;!--
### Strategy

`.spec.strategy` specifies the strategy used to replace old Pods by new ones.
`.spec.strategy.type` can be &#34;Recreate&#34; or &#34;RollingUpdate&#34;. &#34;RollingUpdate&#34; is
the default value.
 --&gt;
&lt;h3 id=&#34;策略&#34;&gt;策略&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.strategy&lt;/code&gt; 指定新 Pod 代替旧 Pod 使用的策略
&lt;code&gt;.spec.strategy.type&lt;/code&gt; 的值可以是 &lt;code&gt;Recreate&lt;/code&gt; 或 &lt;code&gt;RollingUpdate&lt;/code&gt;。 默认为 &lt;code&gt;RollingUpdate&lt;/code&gt;&lt;/p&gt;
&lt;!--
#### Recreate Deployment

All existing Pods are killed before new ones are created when `.spec.strategy.type==Recreate`.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; This will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods
of the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new
revision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the
replacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an
&amp;ldquo;at most&amp;rdquo; guarantee for your Pods, you should consider using a
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/statefulset/&#34;&gt;StatefulSet&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h4 id=&#34;deployment-重建策略&#34;&gt;Deployment 重建策略&lt;/h4&gt;
&lt;p&gt;当设置 &lt;code&gt;.spec.strategy.type==Recreate&lt;/code&gt; 时，会先将所以旧的 Pod 删除再创建新的&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 这个策略只能保证终止旧 Pod 的操作在创建新 Pod 的操作之前。 如果用户更新的一个 Deployment， 所以旧的 Pod 就会马上被标记为终止产状。
成功删除则要等到有任意一个新版本的 Pod 创建成功之后。 如果在这之前，用户手动删除了一个 Pod， 这时它的生命周期还受 ReplicaSet 控制，
所以会立马又创建一个旧的(尽管旧的 Pod 依然处于终止中的状态)。 如果用户需要 “最大的” 保证， 则建议考虑使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/statefulset/&#34;&gt;StatefulSet&lt;/a&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
#### Rolling Update Deployment

The Deployment updates Pods in a rolling update
fashion when `.spec.strategy.type==RollingUpdate`. You can specify `maxUnavailable` and `maxSurge` to control
the rolling update process.
 --&gt;
&lt;h4 id=&#34;deployment-滚动更新策略&#34;&gt;Deployment 滚动更新策略&lt;/h4&gt;
&lt;p&gt;当 &lt;code&gt;.spec.strategy.type==RollingUpdate&lt;/code&gt; 时 Deployment 以滚动方式更新 Pod。
用户可以通过 &lt;code&gt;maxUnavailable&lt;/code&gt; 和 &lt;code&gt;maxSurge&lt;/code&gt; 来控制更新进程&lt;/p&gt;
&lt;!--
##### Max Unavailable

`.spec.strategy.rollingUpdate.maxUnavailable` is an optional field that specifies the maximum number
of Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5)
or a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by
rounding down. The value cannot be 0 if `.spec.strategy.rollingUpdate.maxSurge` is 0. The default value is 25%.

For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired
Pods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled
down further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available
at all times during the update is at least 70% of the desired Pods.
 --&gt;
&lt;h5 id=&#34;最大不可用数&#34;&gt;最大不可用数&lt;/h5&gt;
&lt;p&gt;&lt;code&gt;.spec.strategy.rollingUpdate.maxUnavailable&lt;/code&gt; 是一个可选字段， 用于设置在更新过程最大不可用 Pod 的数量。 字段值可以是一个数字(如， 5)
也可以是期望副本数的百分比(如，10%)。 数字为百分比向下取整。 如果 &lt;code&gt;.spec.strategy.rollingUpdate.maxSurge&lt;/code&gt; 的值是 0 则该字段值不能是 0.
默认值为 &lt;code&gt;25%&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;例如， 当这个值设置为 30%, 旧的 ReplicaSet 在滚动更新开始时就能缩减容量是期望数量的 70%， 旧的 ReplicaSet 会在 新的 ReplicaSet 扩容后
相应的收缩自己的容量，以确保在整个更新过程中可用的 Pod 数不低于期望数量的 70%。&lt;/p&gt;
&lt;!--
##### Max Surge

`.spec.strategy.rollingUpdate.maxSurge` is an optional field that specifies the maximum number of Pods
that can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a
percentage of desired Pods (for example, 10%). The value cannot be 0 if `MaxUnavailable` is 0. The absolute number
is calculated from the percentage by rounding up. The default value is 25%.

For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the
rolling update starts, such that the total number of old and new Pods does not exceed 130% of desired
Pods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the
total number of Pods running at any time during the update is at most 130% of desired Pods.
 --&gt;
&lt;h5 id=&#34;最大可超预期数&#34;&gt;最大可超预期数&lt;/h5&gt;
&lt;p&gt;&lt;code&gt;.spec.strategy.rollingUpdate.maxSurge&lt;/code&gt; 是一个可选字段， 用于设置在更新过程最大可超过期望数的数量。 这个值可以是一个数量(如， 5)
也可以是期望副本数的百分比(如，10%)。如果 &lt;code&gt;MaxUnavailable&lt;/code&gt; 值为 0， 则该字段值不能是 0. 数字为百分比向上取整。默认值为 25%。&lt;/p&gt;
&lt;p&gt;例如， 当这个值设置为 30%, 新的 ReplicaSet 会在滚动更新开始后马上扩容。 只要新旧 Pod 总数不超过期望数量的 130%。
当旧的 Pod 被干掉， 新的 ReplicaSet 就能继续扩容， 只要始终确保总的 Pod 不超过期望数量的 130%。&lt;/p&gt;
&lt;!--
### Progress Deadline Seconds

`.spec.progressDeadlineSeconds` is an optional field that specifies the number of seconds you want
to wait for your Deployment to progress before the system reports back that the Deployment has
[failed progressing](#failed-deployment) - surfaced as a condition with `Type=Progressing`, `Status=False`.
and `Reason=ProgressDeadlineExceeded` in the status of the resource. The Deployment controller will keep
retrying the Deployment. This defaults to 600. In the future, once automatic rollback will be implemented, the Deployment
controller will roll back a Deployment as soon as it observes such a condition.

If specified, this field needs to be greater than `.spec.minReadySeconds`.
 --&gt;
&lt;h3 id=&#34;更新处理时限&#34;&gt;更新处理时限&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.progressDeadlineSeconds&lt;/code&gt; 是一个可选字段， 用于设置 Deployment 进行更新处理的时限(单位秒)， 如果超过这个时限
这次更新还没有完成则系统就会报告 Deployment &lt;a href=&#34;#failed-deployment&#34;&gt;处理失败&lt;/a&gt;，报告为向状态资源添加一个包含以下字段的条件
&lt;code&gt;Type=Progressing&lt;/code&gt;, &lt;code&gt;Status=False&lt;/code&gt;, &lt;code&gt;Reason=ProgressDeadlineExceeded&lt;/code&gt;. Deployment 控制器会持续重试这个 Deployment.
这个字段默认值为 600. 在将来，当自动回滚实现以后，当发现这个超时条件就会自动回滚这个 Deployment。&lt;/p&gt;
&lt;p&gt;如果为设置该字段，该字段的值需要大于 &lt;code&gt;.spec.minReadySeconds&lt;/code&gt;&lt;/p&gt;
&lt;!--
### Min Ready Seconds

`.spec.minReadySeconds` is an optional field that specifies the minimum number of seconds for which a newly
created Pod should be ready without any of its containers crashing, for it to be considered available.
This defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when
a Pod is considered ready, see [Container Probes](/docs/concepts/workloads/pods/pod-lifecycle/#container-probes).
 --&gt;
&lt;h3 id=&#34;最小就绪时间&#34;&gt;最小就绪时间&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.minReadySeconds&lt;/code&gt; 是一个可选字段， 用于设置一个新创建的 Pod 在没有任何容器崩掉的情况下达成就绪到被认为可用的最小时限(单位秒)
默认是 0 (Pod 在就绪后就认为可用)。 了解更多关于 Pod 被认为就绪的信息，见&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/#container-probes&#34;&gt;容器探针&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### Revision History Limit

A Deployment&#39;s revision history is stored in the ReplicaSets it controls.

`.spec.revisionHistoryLimit` is an optional field that specifies the number of old ReplicaSets to retain
to allow rollback. These old ReplicaSets consume resources in `etcd` and crowd the output of `kubectl get rs`. The configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the frequency and stability of new Deployments.

More specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up.
In this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up.
 --&gt;
&lt;h3 id=&#34;历史版本记录限制&#34;&gt;历史版本记录限制&lt;/h3&gt;
&lt;p&gt;Deployment 的版本历史存在受它控制的 ReplicaSet 中。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;.spec.revisionHistoryLimit&lt;/code&gt; 是一个可选字段。 用户设置允许回滚而保留的旧的 ReplicaSet 的数量。
这个旧的 ReplicaSet 会被存储在 &lt;code&gt;etcd&lt;/code&gt; 中， 可以通过命令 &lt;code&gt;kubectl get rs&lt;/code&gt; 查看。 Deployment 每个版本的配置被保存在它的 ReplicaSet 中，
因此，当一个旧的 ReplicaSet 被删除，就会的会回滚到该版本的能力。 默认会保留 10 个旧的 ReplicaSet， 但是合理的值基于新发 Deployment 的频次和稳定性。&lt;/p&gt;
&lt;p&gt;更准确的说， 如果把该字段值设置为0， 意味着所以副本数为 0 的旧 ReplicaSet 都会被删除， 在这种情况下， 新的 Deployment 就没办法回滚了，因为历史版本都清空了。&lt;/p&gt;
&lt;!--
### Paused

`.spec.paused` is an optional boolean field for pausing and resuming a Deployment. The only difference between
a paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused
Deployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default when
it is created.
 --&gt;
&lt;h3 id=&#34;暂停&#34;&gt;暂停&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.paused&lt;/code&gt; 是一个可选布尔字段，用于暂停和恢复 Deployment。 Deployment 有没有暂停的区别是对 PodTemplateSpec 变更， 暂停的 Deployment
在被恢复之前不会触发更新。 Deployment 创建时默认没有暂停。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: ReplicaSet</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/replicaset/</link>
      <pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/replicaset/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- Kashomon
- bprashanth
- madhusudancs
title: ReplicaSet
content_type: concept
weight: 20
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
A ReplicaSet&#39;s purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often
used to guarantee the availability of a specified number of identical Pods.
 --&gt;
&lt;p&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 的作用是在任意时间内维持一个稳定的 Pod 副本集。因此它经常被用来保证特定 Pod 在指定的数量以提供可用性。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## How a ReplicaSet works

A ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can acquire, a number
of replicas indicating how many Pods it should be maintaining, and a pod template specifying the data of new Pods
it should create to meet the number of replicas criteria. A ReplicaSet then fulfills its purpose by creating
and deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Pod
template.

A ReplicaSet is linked to its Pods via the Pods&#39; [metadata.ownerReferences](/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents)
field, which specifies what resource the current object is owned by. All Pods acquired by a ReplicaSet have their owning
ReplicaSet&#39;s identifying information within their ownerReferences field. It&#39;s through this link that the ReplicaSet
knows of the state of the Pods it is maintaining and plans accordingly.

A ReplicaSet identifies new Pods to acquire by using its selector. If there is a Pod that has no OwnerReference or the
OwnerReference is not a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; and it matches a ReplicaSet&#39;s selector, it will be immediately acquired by said
ReplicaSet.
 --&gt;
&lt;h2 id=&#34;how-a-replicaset-works&#34;&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 是怎么工作的&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 通过以下字段定义，
&lt;code&gt;selector&lt;/code&gt; 标签选择器，决定集合中的 Pod，
&lt;code&gt;replicas&lt;/code&gt; 副本数量， 需要维持多少个副本，
&lt;code&gt;template&lt;/code&gt; Pod 的定义模板，
&lt;code&gt;ReplicaSet&lt;/code&gt; 会创建 Pod 或删除 Pod 让 Pod 的数量达到预期的数量。
&lt;code&gt;ReplicaSet&lt;/code&gt; 在需要创建新的 Pod 时会依照 Pod 的定义模来创建 Pod。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 通过 Pod 上的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents&#34;&gt;metadata.ownerReferences&lt;/a&gt;
字段实现它们的所以关系。 所以属于该 &lt;code&gt;ReplicaSet&lt;/code&gt; 的每一个 Pod 上面都有自己的 ownerReferences 字段来存储它们自己的 &lt;code&gt;ReplicaSet&lt;/code&gt; 信息&lt;/p&gt;
&lt;p&gt;通过这个链接信息 &lt;code&gt;ReplicaSet&lt;/code&gt; 可以知道对应的 Pod 的状态，并根据状态进行相应的维护与计划&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 通过选择器识别新的 Pod。 如果一个 Pod 上没有 &lt;code&gt;OwnerReference&lt;/code&gt; 或 &lt;code&gt;OwnerReference&lt;/code&gt;
不是一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; 然后它又匹配该 &lt;code&gt;ReplicaSet&lt;/code&gt; 的选择器，
那么它马上就会被该 &lt;code&gt;ReplicaSet&lt;/code&gt; 捕获。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## When to use a ReplicaSet

A ReplicaSet ensures that a specified number of pod replicas are running at any given
time. However, a Deployment is a higher-level concept that manages ReplicaSets and
provides declarative updates to Pods along with a lot of other useful features.
Therefore, we recommend using Deployments instead of directly using ReplicaSets, unless
you require custom update orchestration or don&#39;t require updates at all.

This actually means that you may never need to manipulate ReplicaSet objects:
use a Deployment instead, and define your application in the spec section.
 --&gt;
&lt;h2 id=&#34;什么时候应该用-replicaset&#34;&gt;什么时候应该用 &lt;code&gt;ReplicaSet&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 保证在任意时间点上某个 Pod 的拥有指定的副本数。
但是 &lt;code&gt;Deployment&lt;/code&gt; 是一个更高层级的抽象概念，实现对 &lt;code&gt;ReplicaSet&lt;/code&gt; 的管理并提供了对 Pod 的声明式更新及其它一系列有用的特性。
所以，我们建议使用 &lt;code&gt;Deployment&lt;/code&gt; 而不是直接使用 &lt;code&gt;ReplicaSet&lt;/code&gt;
除非用户需要自定义的更新编排甚至干脆不需要更新。&lt;/p&gt;
&lt;p&gt;也就是说一般用户永远都不需要操作 &lt;code&gt;ReplicaSet&lt;/code&gt;： 使用 &lt;code&gt;Deployment&lt;/code&gt; 更佳，只需要在 &lt;code&gt;spec&lt;/code&gt; 区域定义应用就行&lt;/p&gt;
&lt;!--
## Example



 













&lt;table class=&#34;includecode&#34; id=&#34;controllersfrontendyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/frontend.yaml&#34; download=&#34;controllers/frontend.yaml&#34;&gt;
                    &lt;code&gt;controllers/frontend.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersfrontendyaml&#39;)&#34; title=&#34;Copy controllers/frontend.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicaSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;guestbook&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#75715e&#34;&gt;# modify replicas according to your case&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;php-redis&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/gb-frontend:v3-google_samples&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



Saving this manifest into `frontend.yaml` and submitting it to a Kubernetes cluster will
create the defined ReplicaSet and the Pods that it manages.

```shell
kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
```

You can then get the current ReplicaSets deployed:

```shell
kubectl get rs
```

And see the frontend one you created:

```shell
NAME       DESIRED   CURRENT   READY   AGE
frontend   3         3         3       6s
```

You can also check on the state of the ReplicaSet:

```shell
kubectl describe rs/frontend
```

And you will see output similar to:

```shell
Name:         frontend
Namespace:    default
Selector:     tier=frontend
Labels:       app=guestbook
              tier=frontend
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {&#34;apiVersion&#34;:&#34;apps/v1&#34;,&#34;kind&#34;:&#34;ReplicaSet&#34;,&#34;metadata&#34;:{&#34;annotations&#34;:{},&#34;labels&#34;:{&#34;app&#34;:&#34;guestbook&#34;,&#34;tier&#34;:&#34;frontend&#34;},&#34;name&#34;:&#34;frontend&#34;,...
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  tier=frontend
  Containers:
   php-redis:
    Image:        gcr.io/google_samples/gb-frontend:v3
    Port:         &lt;none&gt;
    Host Port:    &lt;none&gt;
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  117s  replicaset-controller  Created pod: frontend-wtsmm
  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-b2zdv
  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-vcmts
```

And lastly you can check for the Pods brought up:

```shell
kubectl get pods
```

You should see Pod information similar to:

```shell
NAME             READY   STATUS    RESTARTS   AGE
frontend-b2zdv   1/1     Running   0          6m36s
frontend-vcmts   1/1     Running   0          6m36s
frontend-wtsmm   1/1     Running   0          6m36s
```

You can also verify that the owner reference of these pods is set to the frontend ReplicaSet.
To do this, get the yaml of one of the Pods running:

```shell
kubectl get pods frontend-b2zdv -o yaml
```

The output will look similar to this, with the frontend ReplicaSet&#39;s info set in the metadata&#39;s ownerReferences field:

```shell
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: &#34;2020-02-12T07:06:16Z&#34;
  generateName: frontend-
  labels:
    tier: frontend
  name: frontend-b2zdv
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: frontend
    uid: f391f6db-bb9b-4c09-ae74-6a1f77f3d5cf
...
```
 --&gt;
&lt;h2 id=&#34;示例&#34;&gt;示例&lt;/h2&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;controllersfrontendyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/frontend.yaml&#34; download=&#34;controllers/frontend.yaml&#34;&gt;
                    &lt;code&gt;controllers/frontend.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersfrontendyaml&#39;)&#34; title=&#34;Copy controllers/frontend.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicaSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;guestbook&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#75715e&#34;&gt;# modify replicas according to your case&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;php-redis&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/gb-frontend:v3-google_samples&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;将以上配置保存到 &lt;code&gt;frontend.yaml&lt;/code&gt;， 然后发布到 k8s 集群，就会在集群中创建一个 ReplicaSet 和对应 Pod。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f frontend.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以通过以下命令查看当前部署的 &lt;code&gt;ReplicaSet&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get rs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果中有类似如下如果:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;NAME       DESIRED   CURRENT   READY   AGE
frontend   &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;         &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;         &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;       6s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;也可以通过以下命令查看 &lt;code&gt;ReplicaSet&lt;/code&gt; 状态:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe rs/frontend
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;Name:         frontend
Namespace:    default
Selector:     tier&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;frontend
Labels:       app&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;guestbook
              tier&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;frontend
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;apiVersion&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;apps/v1&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;kind&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ReplicaSet&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;metadata&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;annotations&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#f92672&#34;&gt;{}&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;labels&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;app&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;guestbook&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;tier&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;frontend&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;frontend&amp;#34;&lt;/span&gt;,...
Replicas:     &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; current / &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; desired
Pods Status:  &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; Running / &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; Waiting / &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; Succeeded / &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; Failed
Pod Template:
  Labels:  tier&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;frontend
  Containers:
   php-redis:
    Image:        gcr.io/google_samples/gb-frontend:v3
    Port:         &amp;lt;none&amp;gt;
    Host Port:    &amp;lt;none&amp;gt;
    Environment:  &amp;lt;none&amp;gt;
    Mounts:       &amp;lt;none&amp;gt;
  Volumes:        &amp;lt;none&amp;gt;
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  117s  replicaset-controller  Created pod: frontend-wtsmm
  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-b2zdv
  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-vcmts
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;最后再用以下命令查看创建的 Pod:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;NAME             READY   STATUS    RESTARTS   AGE
frontend-b2zdv   1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          6m36s
frontend-vcmts   1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          6m36s
frontend-wtsmm   1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          6m36s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;通过以正下命令输出的 yaml 可以验证这些 Pod 所属 &lt;code&gt;ReplicaSet&lt;/code&gt;：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods frontend-b2zdv -o yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下，可以看到 &lt;code&gt;metadata.ownerReferences&lt;/code&gt; 字段中就是这个叫 &lt;code&gt;frontend&lt;/code&gt; 的 &lt;code&gt;ReplicaSet&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2020-02-12T07:06:16Z&amp;#34;&lt;/span&gt;
  generateName: frontend-
  labels:
    tier: frontend
  name: frontend-b2zdv
  namespace: default
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: frontend
    uid: f391f6db-bb9b-4c09-ae74-6a1f77f3d5cf
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Non-Template Pod acquisitions

While you can create bare Pods with no problems, it is strongly recommended to make sure that the bare Pods do not have
labels which match the selector of one of your ReplicaSets. The reason for this is because a ReplicaSet is not limited
to owning Pods specified by its template-- it can acquire other Pods in the manner specified in the previous sections.

Take the previous frontend ReplicaSet example, and the Pods specified in the  following manifest:



 













&lt;table class=&#34;includecode&#34; id=&#34;podspod-rsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/pod-rs.yaml&#34; download=&#34;pods/pod-rs.yaml&#34;&gt;
                    &lt;code&gt;pods/pod-rs.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podspod-rsyaml&#39;)&#34; title=&#34;Copy pods/pod-rs.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hello1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/hello-app:2.0-google-samples&lt;/span&gt;

---

&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod2&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hello2&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/hello-app:1.0-google-samples&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



As those Pods do not have a Controller (or any object) as their owner reference and match the selector of the frontend
ReplicaSet, they will immediately be acquired by it.

Suppose you create the Pods after the frontend ReplicaSet has been deployed and has set up its initial Pod replicas to
fulfill its replica count requirement:

```shell
kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
```

The new Pods will be acquired by the ReplicaSet, and then immediately terminated as the ReplicaSet would be over
its desired count.

Fetching the Pods:

```shell
kubectl get pods
```

The output shows that the new Pods are either already terminated, or in the process of being terminated:

```shell
NAME             READY   STATUS        RESTARTS   AGE
frontend-b2zdv   1/1     Running       0          10m
frontend-vcmts   1/1     Running       0          10m
frontend-wtsmm   1/1     Running       0          10m
pod1             0/1     Terminating   0          1s
pod2             0/1     Terminating   0          1s
```

If you create the Pods first:

```shell
kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
```

And then create the ReplicaSet however:

```shell
kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
```

You shall see that the ReplicaSet has acquired the Pods and has only created new ones according to its spec until the
number of its new Pods and the original matches its desired count. As fetching the Pods:

```shell
kubectl get pods
```

Will reveal in its output:
```shell
NAME             READY   STATUS    RESTARTS   AGE
frontend-hmmj2   1/1     Running   0          9s
pod1             1/1     Running   0          36s
pod2             1/1     Running   0          36s
```

In this manner, a ReplicaSet can own a non-homogenous set of Pods
 --&gt;
&lt;h2 id=&#34;不是自己模板定义的-pod-的捕获&#34;&gt;不是自己模板定义的 Pod 的捕获&lt;/h2&gt;
&lt;p&gt;用户可以自由的创建裸 Pod， 但是推荐在创建裸 Pod 时不要在上面打上已经存在的 &lt;code&gt;ReplicaSet&lt;/code&gt; 选择器对应的标签。
不这么做的原因是 &lt;code&gt;ReplicaSet&lt;/code&gt; 并不仅限于自身模板创建的 Pod， 也可以捕获其它拥有对应标签的 Pod(上一节介绍过)&lt;/p&gt;
&lt;p&gt;继续上一个例子中的 &lt;code&gt;frontend&lt;/code&gt; &lt;code&gt;ReplicaSet&lt;/code&gt;， 再加下如下定义的 Pod&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;podspod-rsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/pods/pod-rs.yaml&#34; download=&#34;pods/pod-rs.yaml&#34;&gt;
                    &lt;code&gt;pods/pod-rs.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;podspod-rsyaml&#39;)&#34; title=&#34;Copy pods/pod-rs.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hello1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/hello-app:2.0-google-samples&lt;/span&gt;

---

&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pod2&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hello2&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/hello-app:1.0-google-samples&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;这些 Pod 没并有在所属者引用上有控制器(或其它对象)， 但是它上面的标签又被 &lt;code&gt;frontend&lt;/code&gt; &lt;code&gt;ReplicaSet&lt;/code&gt;
的选择器所匹配，它们马上就会被这个 &lt;code&gt;ReplicaSet&lt;/code&gt; 捕获。&lt;/p&gt;
&lt;p&gt;假设 Pod 的创建时间是在 &lt;code&gt;frontend&lt;/code&gt; &lt;code&gt;ReplicaSet&lt;/code&gt; 部署之后，并且其创建的 Pod 已经达到了所期望的数量：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f pod-rs.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;新创建的 Pod 会被 &lt;code&gt;ReplicaSet&lt;/code&gt; 捕获，然后发现 &lt;code&gt;ReplicaSet&lt;/code&gt; 副本数已经超过预期数量，所以就会终止这些多余的 Pod&lt;/p&gt;
&lt;p&gt;查看 Pod 状态:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出类似如下，会发现新创建的 Pod 正在被干掉或已经被干掉:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;NAME             READY   STATUS        RESTARTS   AGE
frontend-b2zdv   1/1     Running       &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          10m
frontend-vcmts   1/1     Running       &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          10m
frontend-wtsmm   1/1     Running       &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          10m
pod1             0/1     Terminating   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          1s
pod2             0/1     Terminating   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          1s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果创建的顺序调换一下，先创建这些 Pod:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f pod-rs.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;然后再创建 &lt;code&gt;ReplicaSet&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f frontend.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这次就会发现 &lt;code&gt;ReplicaSet&lt;/code&gt; 会捕获已经创建的两个 Pod，然后根据预期的状态中需要3个，所以就再创建一个新的，就完工了。
再次查看 Pod 状态&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl get pods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果就会变得不一样，具体如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;NAME             READY   STATUS    RESTARTS   AGE
frontend-hmmj2   1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          9s
pod1             1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          36s
pod2             1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          36s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;也就是， &lt;code&gt;ReplicaSet&lt;/code&gt; 下属的 Pod 可以是不是通过本身模板创建的 Pod&lt;/p&gt;
&lt;!--  
## Writing a ReplicaSet manifest

As with all other Kubernetes API objects, a ReplicaSet needs the `apiVersion`, `kind`, and `metadata` fields.
For ReplicaSets, the kind is always just ReplicaSet.
In Kubernetes 1.9 the API version `apps/v1` on the ReplicaSet kind is the current version and is enabled by default. The API version `apps/v1beta2` is deprecated.
Refer to the first lines of the `frontend.yaml` example for guidance.

The name of a ReplicaSet object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

A ReplicaSet also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).
--&gt;
&lt;h2 id=&#34;编写-replicaset-定义清单&#34;&gt;编写 &lt;code&gt;ReplicaSet&lt;/code&gt; 定义清单&lt;/h2&gt;
&lt;p&gt;与其它所以其它 k8s API 对象一样， &lt;code&gt;ReplicaSet&lt;/code&gt; 需要有 &lt;code&gt;apiVersion&lt;/code&gt;, &lt;code&gt;kind&lt;/code&gt;, &lt;code&gt;metadata&lt;/code&gt; 字段.
对于 &lt;code&gt;ReplicaSet&lt;/code&gt;， &lt;code&gt;kind&lt;/code&gt; 字段就一直是 &lt;code&gt;ReplicaSet&lt;/code&gt;。
在 k8s v1.9+ &lt;code&gt;apiVersion&lt;/code&gt; 字段的值为 &lt;code&gt;apps/v1&lt;/code&gt;，默认开启。 API 版本 &lt;code&gt;apps/v1beta2&lt;/code&gt; 被废弃。
具体可以看之前示例中 &lt;code&gt;frontend.yaml&lt;/code&gt; 文件内容。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 名称必须是一个有效的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.
&lt;code&gt;ReplicaSet&lt;/code&gt; 还必须要有一个
&lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;&lt;code&gt;.spec&lt;/code&gt; 字段&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### Pod Template

The `.spec.template` is a [pod template](/docs/concepts/workloads/Pods/pod-overview/#pod-templates) which is also
required to have labels in place. In our `frontend.yaml` example we had one label: `tier: frontend`.
Be careful not to overlap with the selectors of other controllers, lest they try to adopt this Pod.

For the template&#39;s [restart policy](/docs/concepts/workloads/Pods/pod-lifecycle/#restart-policy) field,
`.spec.template.spec.restartPolicy`, the only allowed value is `Always`, which is the default.
 --&gt;
&lt;h3 id=&#34;pod-模板&#34;&gt;Pod 模板&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.template&lt;/code&gt; 字段就是 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/Pods/pod-overview/#pod-templates&#34;&gt;pod template&lt;/a&gt; 而且其中还必须要定义标签.
在示例中的 &lt;code&gt;frontend.yaml&lt;/code&gt; 打的标签是: &lt;code&gt;tier: frontend&lt;/code&gt;.
注意不能与其它控制器的选择器相重叠，以免它们会争抢这个 Pod&lt;/p&gt;
&lt;p&gt;模板中的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/Pods/pod-lifecycle/#restart-policy&#34;&gt;重启策略&lt;/a&gt; 字段,
&lt;code&gt;.spec.template.spec.restartPolicy&lt;/code&gt;, 允许的值只能是 &lt;code&gt;Always&lt;/code&gt;, 默认也是  &lt;code&gt;Always&lt;/code&gt;.&lt;/p&gt;
&lt;!--
### Pod Selector

The `.spec.selector` field is a [label selector](/docs/concepts/overview/working-with-objects/labels/). As discussed
[earlier](#how-a-replicaset-works) these are the labels used to identify potential Pods to acquire. In our
`frontend.yaml` example, the selector was:
```shell
matchLabels:
	tier: frontend
```

In the ReplicaSet, `.spec.template.metadata.labels` must match `spec.selector`, or it will
be rejected by the API.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; For 2 ReplicaSets specifying the same &lt;code&gt;.spec.selector&lt;/code&gt; but different &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt; and &lt;code&gt;.spec.template.spec&lt;/code&gt; fields, each ReplicaSet ignores the Pods created by the other ReplicaSet.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;pod-选择器&#34;&gt;Pod 选择器&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.selector&lt;/code&gt; 字段是一个 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/labels/&#34;&gt;标签选择器&lt;/a&gt;
&lt;a href=&#34;#how-a-replicaset-works&#34;&gt;之前&lt;/a&gt;讨论过，这些标签用于识别潜在的捕获对象。 在之前示例中 &lt;code&gt;frontend.yaml&lt;/code&gt;
定义的选择器如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;tier&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在 &lt;code&gt;ReplicaSet&lt;/code&gt; 中， &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt; 必须要与 &lt;code&gt;spec.selector&lt;/code&gt; 匹配，否则会被 API 拒绝。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果两个 &lt;code&gt;ReplicaSet&lt;/code&gt; 拥有相同的 &lt;code&gt;.spec.selector&lt;/code&gt;， 但 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt; 和 &lt;code&gt;.spec.template.spec&lt;/code&gt; 字段不同
则相互之间会忽略对方创建的 Pod&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p&gt;HOW？&lt;/p&gt;
&lt;!--
### Replicas

You can specify how many Pods should run concurrently by setting `.spec.replicas`. The ReplicaSet will create/delete
its Pods to match this number.

If you do not specify `.spec.replicas`, then it defaults to 1.
 --&gt;
&lt;h3 id=&#34;副本数&#34;&gt;副本数&lt;/h3&gt;
&lt;p&gt;用户可以通过 &lt;code&gt;.spec.replicas&lt;/code&gt; 设置需要同时运行 Pod 的数量。 &lt;code&gt;ReplicaSet&lt;/code&gt; 会创建/删除 它管理的 Pod 以达成该数量。
如果在配置中没有 &lt;code&gt;.spec.replicas&lt;/code&gt;， 则默认为 1.&lt;/p&gt;
&lt;!--
## Working with ReplicaSets

### Deleting a ReplicaSet and its Pods

To delete a ReplicaSet and all of its Pods, use [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete). The [Garbage collector](/docs/concepts/workloads/controllers/garbage-collection/) automatically deletes all of the dependent Pods by default.

When using the REST API or the `client-go` library, you must set `propagationPolicy` to `Background` or `Foreground` in
the -d option.
For example:
```shell
kubectl proxy --port=8080
curl -X DELETE  &#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend&#39; \
&gt; -d &#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Foreground&#34;}&#39; \
&gt; -H &#34;Content-Type: application/json&#34;
```
 --&gt;
&lt;h2 id=&#34;replicaset-的使用&#34;&gt;ReplicaSet 的使用&lt;/h2&gt;
&lt;h3 id=&#34;删除一个-replicaset-及其-pod&#34;&gt;删除一个 &lt;code&gt;ReplicaSet&lt;/code&gt; 及其 Pod&lt;/h3&gt;
&lt;p&gt;要删除一个 &lt;code&gt;ReplicaSet&lt;/code&gt; 及其所属的全部 Pod， 可以使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubectl/kubectl-commands#delete&#34;&gt;&lt;code&gt;kubectl delete&lt;/code&gt;&lt;/a&gt;
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/garbage-collection/&#34;&gt;垃圾回收&lt;/a&gt; 默认会自动删除它所属的全部 Pod。&lt;/p&gt;
&lt;p&gt;当使用  &lt;code&gt;REST API&lt;/code&gt; 或 &lt;code&gt;client-go&lt;/code&gt; 库， 必须将 &lt;code&gt;propagationPolicy&lt;/code&gt; 设置为 &lt;code&gt;Background&lt;/code&gt; 或 &lt;code&gt;Foreground&lt;/code&gt;
例如:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl proxy --port&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
curl -X DELETE  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;&amp;gt; -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#34;kind&amp;#34;:&amp;#34;DeleteOptions&amp;#34;,&amp;#34;apiVersion&amp;#34;:&amp;#34;v1&amp;#34;,&amp;#34;propagationPolicy&amp;#34;:&amp;#34;Foreground&amp;#34;}&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;&amp;gt; -H &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Content-Type: application/json&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Deleting just a ReplicaSet

You can delete a ReplicaSet without affecting any of its Pods using [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete) with the `--cascade=false` option.
When using the REST API or the `client-go` library, you must set `propagationPolicy` to `Orphan`.
For example:
```shell
kubectl proxy --port=8080
curl -X DELETE  &#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend&#39; \
&gt; -d &#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Orphan&#34;}&#39; \
&gt; -H &#34;Content-Type: application/json&#34;
```

Once the original is deleted, you can create a new ReplicaSet to replace it.  As long
as the old and new `.spec.selector` are the same, then the new one will adopt the old Pods.
However, it will not make any effort to make existing Pods match a new, different pod template.
To update Pods to a new spec in a controlled way, use a
[Deployment](/docs/concepts/workloads/controllers/deployment/#creating-a-deployment), as ReplicaSets do not support a rolling update directly.
 --&gt;
&lt;h3 id=&#34;仅删除-replicaset&#34;&gt;仅删除 &lt;code&gt;ReplicaSet&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;用户可以通过 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/generated/kubectl/kubectl-commands#delete&#34;&gt;&lt;code&gt;kubectl delete&lt;/code&gt;&lt;/a&gt;
加 &lt;code&gt;--cascade=false&lt;/code&gt; 选项，实现仅删除 &lt;code&gt;ReplicaSet&lt;/code&gt; 对象，而不删除其所属的 Pod。
当使用  &lt;code&gt;REST API&lt;/code&gt; 或 &lt;code&gt;client-go&lt;/code&gt; 库， 必须将 &lt;code&gt;propagationPolicy&lt;/code&gt; 设置为 &lt;code&gt;Orphan&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;例如:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl proxy --port&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
curl -X DELETE  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;&amp;gt; -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#34;kind&amp;#34;:&amp;#34;DeleteOptions&amp;#34;,&amp;#34;apiVersion&amp;#34;:&amp;#34;v1&amp;#34;,&amp;#34;propagationPolicy&amp;#34;:&amp;#34;Orphan&amp;#34;}&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;&amp;gt; -H &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Content-Type: application/json&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;当原来的 &lt;code&gt;ReplicaSet&lt;/code&gt; 就可以创建一个新的来替代它。 新创建的 ReplicaSet 的 &lt;code&gt;.spec.selector&lt;/code&gt; 需要与原来的一样， 这样它就能接管这些 Pod。
但是它不会让旧的 Pod 使用新的模板。
而想要以控制器方式更新 Pod 的模板，需要使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/deployment/#creating-a-deployment&#34;&gt;Deployment&lt;/a&gt;
因为 &lt;code&gt;ReplicaSet&lt;/code&gt; 不支持直接的滚动更新。&lt;/p&gt;
&lt;!--
### Isolating Pods from a ReplicaSet

You can remove Pods from a ReplicaSet by changing their labels. This technique may be used to remove Pods
from service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (
assuming that the number of replicas is not also changed).
 --&gt;
&lt;h3 id=&#34;将-pod-从-replicaset-剥离出来&#34;&gt;将 Pod 从 &lt;code&gt;ReplicaSet&lt;/code&gt; 剥离出来&lt;/h3&gt;
&lt;p&gt;用户可以通过修改标签的方式将 Pod 从 &lt;code&gt;ReplicaSet&lt;/code&gt; 中移出， 也可以通过这种方式将 Pod 从其它的服务中移出后用作调度，数据恢复等。
用这种方式移出的 Pod 会自动被替代(如果副本数没有同步修改)&lt;/p&gt;
&lt;!--
### Scaling a ReplicaSet

A ReplicaSet can be easily scaled up or down by simply updating the `.spec.replicas` field. The ReplicaSet controller
ensures that a desired number of Pods with a matching label selector are available and operational.
 --&gt;
&lt;h3 id=&#34;replicaset-容量伸缩&#34;&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 容量伸缩&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 可以通过修改 &lt;code&gt;.spec.replicas&lt;/code&gt; 字段来实现容量的扩大或缩小。 &lt;code&gt;ReplicaSet&lt;/code&gt; 控制器会保证 所属的 Pod 的数量与预期的相同&lt;/p&gt;
&lt;!--
### ReplicaSet as a Horizontal Pod Autoscaler Target

A ReplicaSet can also be a target for
[Horizontal Pod Autoscalers (HPA)](/docs/tasks/run-application/horizontal-pod-autoscale/). That is,
a ReplicaSet can be auto-scaled by an HPA. Here is an example HPA targeting
the ReplicaSet we created in the previous example.



 













&lt;table class=&#34;includecode&#34; id=&#34;controllershpa-rsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/hpa-rs.yaml&#34; download=&#34;controllers/hpa-rs.yaml&#34;&gt;
                    &lt;code&gt;controllers/hpa-rs.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllershpa-rsyaml&#39;)&#34; title=&#34;Copy controllers/hpa-rs.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;autoscaling/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;HorizontalPodAutoscaler&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend-scaler&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;scaleTargetRef&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicaSet&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;minReplicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;maxReplicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;targetCPUUtilizationPercentage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



Saving this manifest into `hpa-rs.yaml` and submitting it to a Kubernetes cluster should
create the defined HPA that autoscales the target ReplicaSet depending on the CPU usage
of the replicated Pods.

```shell
kubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml
```

Alternatively, you can use the `kubectl autoscale` command to accomplish the same
(and it&#39;s easier!)

```shell
kubectl autoscale rs frontend --max=10 --min=3 --cpu-percent=50
```
 --&gt;
&lt;h3 id=&#34;replicaset-作为-pod-水平自动扩展目标&#34;&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 作为 Pod 水平自动扩展目标&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 也可作为 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/horizontal-pod-autoscale/&#34;&gt;Horizontal Pod Autoscalers (HPA)&lt;/a&gt;
的目标。 也就是 &lt;code&gt;ReplicaSet&lt;/code&gt; 可以通过一个 HPA 来自动伸缩。 以下为一个 &lt;code&gt;ReplicaSet&lt;/code&gt; 为目标的 HPA 的示例:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;controllershpa-rsyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/hpa-rs.yaml&#34; download=&#34;controllers/hpa-rs.yaml&#34;&gt;
                    &lt;code&gt;controllers/hpa-rs.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllershpa-rsyaml&#39;)&#34; title=&#34;Copy controllers/hpa-rs.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;autoscaling/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;HorizontalPodAutoscaler&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend-scaler&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;scaleTargetRef&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicaSet&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;frontend&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;minReplicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;maxReplicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;targetCPUUtilizationPercentage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;将配置文件内容存入 &lt;code&gt;hpa-rs.yaml&lt;/code&gt; 并提交到 k8s 集群， 会创建一个 HPA，它会根据所属 Pod CPU使用自动伸缩目标 &lt;code&gt;ReplicaSet&lt;/code&gt; 的容量&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f hpa-rs.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;相应的，也可能通过 &lt;code&gt;kubectl autoscale&lt;/code&gt; 达到相同的效果(更容易)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl autoscale rs frontend --max&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt; --min&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; --cpu-percent&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Alternatives to ReplicaSet

### Deployment (recommended)

[`Deployment`](/docs/concepts/workloads/controllers/deployment/) is an object which can own ReplicaSets and update
them and their Pods via declarative, server-side rolling updates.
While ReplicaSets can be used independently, today they&#39;re  mainly used by Deployments as a mechanism to orchestrate Pod
creation, deletion and updates. When you use Deployments you don’t have to worry about managing the ReplicaSets that
they create. Deployments own and manage their ReplicaSets.
As such, it is recommended to use Deployments when you want ReplicaSets.
 --&gt;
&lt;h2 id=&#34;replicaset-替代方案&#34;&gt;&lt;code&gt;ReplicaSet&lt;/code&gt; 替代方案&lt;/h2&gt;
&lt;h3 id=&#34;deployment-推荐&#34;&gt;Deployment (推荐)&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/deployment/&#34;&gt;&lt;code&gt;Deployment&lt;/code&gt;&lt;/a&gt; 对象可以包含 &lt;code&gt;ReplicaSet&lt;/code&gt; 并且可以通过声明方式更新自身及所属的 Pod，
服务端滚动更新。 虽然 ReplicaSet 可以独立使用，但是现在主要是使用 Deployment 作为组织 Pod 创建，删除，更新的方式。
当用户使用 &lt;code&gt;Deployment&lt;/code&gt; 不需要关心它所创建的 &lt;code&gt;ReplicaSet&lt;/code&gt;， &lt;code&gt;Deployment&lt;/code&gt; 会管理所属的 &lt;code&gt;ReplicaSet&lt;/code&gt;
因此用户在想要使用 &lt;code&gt;ReplicaSet&lt;/code&gt; 时推荐使用 &lt;code&gt;Deployment&lt;/code&gt; 代替。&lt;/p&gt;
&lt;!--
### Bare Pods

Unlike the case where a user directly created Pods, a ReplicaSet replaces Pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicaSet even if your application requires only a single Pod. Think of it similarly to a process supervisor, only it supervises multiple Pods across multiple nodes instead of individual processes on a single node. A ReplicaSet delegates local container restarts to some agent on the node (for example, Kubelet or Docker).
 --&gt;
&lt;h3 id=&#34;让-pod-裸奔&#34;&gt;让 Pod 裸奔&lt;/h3&gt;
&lt;p&gt;与用户直接创建 Pod 不同， &lt;code&gt;ReplicaSet&lt;/code&gt; 在 Pod 因为某些原因被删除或终止时会创建替代的 Pod， 这些原因可能是 节点挂了，节点因维护如升级内格而引起的计划内故障。
对于这些原因，我们建议用户使用 &lt;code&gt;ReplicaSet&lt;/code&gt; 即便这个应用只需要一个 Pod。 可以把它认为是一个进程监控， 只是它可以监控多个节点上的多个 Pod，而不是一个节点上的一个进程。
&lt;code&gt;ReplicaSet&lt;/code&gt; 会委托节点上的代理(如 kubelet 或 docker)来实现对本地容器的重启。&lt;/p&gt;
&lt;!--
### Job

Use a [`Job`](/docs/concepts/jobs/run-to-completion-finite-workloads/) instead of a ReplicaSet for Pods that are expected to terminate on their own
(that is, batch jobs).
 --&gt;
&lt;h3 id=&#34;job&#34;&gt;Job&lt;/h3&gt;
&lt;p&gt;在运行那会在有限时间内自己运行结束后自动终止的 Pod(批处理任务)，请使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/jobs/run-to-completion-finite-workloads/&#34;&gt;&lt;code&gt;Job&lt;/code&gt;&lt;/a&gt;,就不要用 &lt;code&gt;ReplicaSet&lt;/code&gt;了&lt;/p&gt;
&lt;!--
### DaemonSet

Use a [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) instead of a ReplicaSet for Pods that provide a
machine-level function, such as machine monitoring or machine logging.  These Pods have a lifetime that is tied
to a machine lifetime: the Pod needs to be running on the machine before other Pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.
 --&gt;
&lt;h3 id=&#34;daemonset&#34;&gt;DaemonSet&lt;/h3&gt;
&lt;p&gt;当 Pod 需要使用到机器级别的功能，如机器监控或机器日志时，使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/daemonset/&#34;&gt;&lt;code&gt;DaemonSet&lt;/code&gt;&lt;/a&gt;
因为这些 Pod 的生存期会与对应机器的生存期绑定在一起: 这些 Pod 需要在机器上其它 Pod 运行之前就在节点上运行， 只有在机器准备重启或关机时才能
安全的终止。&lt;/p&gt;
&lt;!--
### ReplicationController

ReplicaSets are the successors to [_ReplicationControllers_](/docs/concepts/workloads/controllers/replicationcontroller/).
The two serve the same purpose, and behave similarly, except that a ReplicationController does not support set-based
selector requirements as described in the [labels user guide](/docs/concepts/overview/working-with-objects/labels/#label-selectors).
As such, ReplicaSets are preferred over ReplicationControllers
 --&gt;
&lt;h3 id=&#34;replicationcontroller&#34;&gt;ReplicationController&lt;/h3&gt;
&lt;p&gt;ReplicaSet 是  &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/replicationcontroller/&#34;&gt;&lt;em&gt;ReplicationControllers&lt;/em&gt;&lt;/a&gt; 的继任者。
它们两个可以达成相同的目的，而且行为方式也相似， 除了 &lt;code&gt;ReplicationController&lt;/code&gt; 不支持像
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/labels/#label-selectors&#34;&gt;标签使用&lt;/a&gt; 中介绍的基于集合的选择器。
因此相对于 ReplicationControllers 推荐使用 &lt;code&gt;ReplicaSet&lt;/code&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: StatefulSet</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/statefulset/</link>
      <pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/statefulset/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- enisoc
- erictune
- foxish
- janetkuo
- kow3ns
- smarterclayton
title: StatefulSets
content_type: concept
weight: 30
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
StatefulSet is the workload API object used to manage stateful applications.

&lt;p&gt;管理一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 集合的部署与容量伸缩，&lt;em&gt;并提供了顺序的一致性和唯一性&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;与 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#39; target=&#39;_blank&#39;&gt;Deployment&lt;span class=&#39;tooltip-text&#39;&gt;管理集群中的一个多副本应用&lt;/span&gt;
&lt;/a&gt; 类似， StatefulSet 也是基于相同的容器定义来管理 Pod 的。
与 Deployment 不同的是 StatefulSet 维护了其所管理的每个 Pod 的唯一身份。 这些 Pod 是使用现一份定义创建的，但它们之间是不可互换的:
每个 Pod 在所有的重新调度过程中都会被维护使其拥有唯一的持久化标识。&lt;/p&gt;
&lt;p&gt;如果用户需要在工作负载中使用存储卷来提供持久化，可以使用 StatefulSet 作为解决方式的一部分。 尽管 StatefulSet 中每个独立的 Pod 是容易挂掉的，
但是拥有持久化身份标识的 Pod 能够很容易地将已经存在的数据卷与新创建用于替换挂掉的 Pod 重新绑定&lt;/p&gt;
 --&gt;
&lt;p&gt;StatefulSet 是一种用于运行有状态应用的工作负载 API 对象
&lt;p&gt;管理一个 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 集合的部署与容量伸缩，&lt;em&gt;并提供了顺序的一致性和唯一性&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;与 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/deployment/&#39; target=&#39;_blank&#39;&gt;Deployment&lt;span class=&#39;tooltip-text&#39;&gt;管理集群中的一个多副本应用&lt;/span&gt;
&lt;/a&gt; 类似， StatefulSet 也是基于相同的容器定义来管理 Pod 的。
与 Deployment 不同的是 StatefulSet 维护了其所管理的每个 Pod 的唯一身份。 这些 Pod 是使用现一份定义创建的，但它们之间是不可互换的:
每个 Pod 在所有的重新调度过程中都会被维护使其拥有唯一的持久化标识。&lt;/p&gt;
&lt;p&gt;如果用户需要在工作负载中使用存储卷来提供持久化，可以使用 StatefulSet 作为解决方式的一部分。 尽管 StatefulSet 中每个独立的 Pod 是容易挂掉的，
但是拥有持久化身份标识的 Pod 能够很容易地将已经存在的数据卷与新创建用于替换挂掉的 Pod 重新绑定&lt;/p&gt;&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Using StatefulSets


StatefulSets are valuable for applications that require one or more of the
following.

* Stable, unique network identifiers.
* Stable, persistent storage.
* Ordered, graceful deployment and scaling.
* Ordered, automated rolling updates.

In the above, stable is synonymous with persistence across Pod (re)scheduling.
If an application doesn&#39;t require any stable identifiers or ordered deployment,
deletion, or scaling, you should deploy your application using a workload object
that provides a set of stateless replicas.
[Deployment](/docs/concepts/workloads/controllers/deployment/) or
[ReplicaSet](/docs/concepts/workloads/controllers/replicaset/) may be better suited to your stateless needs.
 --&gt;
&lt;h2 id=&#34;什么时候用-statefulset&#34;&gt;什么时候用 StatefulSet&lt;/h2&gt;
&lt;p&gt;StatefulSet 适用于那些满足以下一个或多个条件的应用.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;需要拥有稳定且唯一的网络标识的&lt;/li&gt;
&lt;li&gt;需要拥有稳定的持久化存储的&lt;/li&gt;
&lt;li&gt;需要按顺序进行部署或进行容量伸缩的&lt;/li&gt;
&lt;li&gt;需要按顺序进行自动滚动更新的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在上面的条件中，稳定是指在 Pod 调度(或重新调度)后依然不变(功能上).
如果一个应用不需要任何稳定的标识或顺序的部署，删除，伸缩容量，用户应该使用那些无状态的工作负载对象。
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/deployment/&#34;&gt;Deployment&lt;/a&gt; 或
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/replicaset/&#34;&gt;ReplicaSet&lt;/a&gt; 可能就更适合用于无状态的需求。&lt;/p&gt;
&lt;!--
## Limitations

* The storage for a given Pod must either be provisioned by a [PersistentVolume Provisioner](https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md) based on the requested `storage class`, or pre-provisioned by an admin.
* Deleting and/or scaling a StatefulSet down will *not* delete the volumes associated with the StatefulSet. This is done to ensure data safety, which is generally more valuable than an automatic purge of all related StatefulSet resources.
* StatefulSets currently require a [Headless Service](/docs/concepts/services-networking/service/#headless-services) to be responsible for the network identity of the Pods. You are responsible for creating this Service.
* StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is possible to scale the StatefulSet down to 0 prior to deletion.
* When using [Rolling Updates](#rolling-updates) with the default
  [Pod Management Policy](#pod-management-policies) (`OrderedReady`),
  it&#39;s possible to get into a broken state that requires
  [manual intervention to repair](#forced-rollback).
--&gt;
&lt;h2 id=&#34;limitations&#34;&gt;限制&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;给予某个指定 Pod 的存储必须要由 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md&#34;&gt;PersistentVolume Provisioner&lt;/a&gt; 基于 &lt;code&gt;storage class&lt;/code&gt; 的请求或由管理员预先创建好。&lt;/li&gt;
&lt;li&gt;删除或收缩 StatefulSet 的副本数 &lt;em&gt;不会&lt;/em&gt; 删除与这个 StatefulSet 相关联的卷。 这么做是为了保证数据这险，相对与自动删除所有 StatefulSet 相关资源，这样保留下来通常更有意义。&lt;/li&gt;
&lt;li&gt;StatefulSet 目前还需要一个 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/service/#headless-services&#34;&gt;Headless Service&lt;/a&gt;  来作为这些 Pod 的网络标识。
需要用户负责创建。&lt;/li&gt;
&lt;li&gt;当删除一个 StatefulSet 时，并不能对 Pod 的终结提供任何保障。 而要达成有序和平滑的终止，将 StatefulSet 的副本设置为 0 比直接删除更有保障。&lt;/li&gt;
&lt;li&gt;在使用基于 &lt;a href=&#34;#pod-management-policies&#34;&gt;Pod 管理策略&lt;/a&gt; (&lt;code&gt;OrderedReady&lt;/code&gt;) 的 &lt;a href=&#34;#rolling-updates&#34;&gt;Rolling Updates&lt;/a&gt;时，
可能会出错且需要 &lt;a href=&#34;#forced-rollback&#34;&gt;人工介入进行修复&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Components

The example below demonstrates the components of a StatefulSet.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: &#34;nginx&#34;
  replicas: 3 # by default is 1
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ &#34;ReadWriteOnce&#34; ]
      storageClassName: &#34;my-storage-class&#34;
      resources:
        requests:
          storage: 1Gi
```

In the above example:

* A Headless Service, named `nginx`, is used to control the network domain.
* The StatefulSet, named `web`, has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.
* The `volumeClaimTemplates` will provide stable storage using [PersistentVolumes](/docs/concepts/storage/persistent-volumes/) provisioned by a PersistentVolume Provisioner.

The name of a StatefulSet object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
 --&gt;
&lt;h2 id=&#34;组件&#34;&gt;组件&lt;/h2&gt;
&lt;p&gt;以下示例演示了 StatefulSet 的组件。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;web&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;clusterIP&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;None&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
---
&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;StatefulSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;web&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx # 与要匹配 .spec.template.metadata.labels&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;serviceName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;nginx&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# 默认为 1&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx # 与要匹配 .spec.selector.matchLabels&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;terminationGracePeriodSeconds&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/nginx-slim:0.8&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;web&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/usr/share/nginx/html&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;volumeClaimTemplates&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;www&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;accessModes&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ReadWriteOnce&amp;#34;&lt;/span&gt; ]
      &lt;span style=&#34;color:#f92672&#34;&gt;storageClassName&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;my-storage-class&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# 需要替换成实际的&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;storage&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1Gi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在上面的例子中:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个 Headless Service， 名称为 &lt;code&gt;nginx&lt;/code&gt;， 用来控制网络域&lt;/li&gt;
&lt;li&gt;StatefulSet， 名称为 &lt;code&gt;web&lt;/code&gt;， 在定义中指定有 3 个副本的 nginx 的容器启动在不同的 Pod 中&lt;/li&gt;
&lt;li&gt;&lt;code&gt;volumeClaimTemplates&lt;/code&gt; 通过 PersistentVolume 提供者提供的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/&#34;&gt;PersistentVolumes&lt;/a&gt;
提供稳定存储&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;StatefulSet 的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Pod Selector

You must set the `.spec.selector` field of a StatefulSet to match the labels of its `.spec.template.metadata.labels`. Prior to Kubernetes 1.8, the `.spec.selector` field was defaulted when omitted. In 1.8 and later versions, failing to specify a matching Pod Selector will result in a validation error during StatefulSet creation.
 --&gt;
&lt;h2 id=&#34;pod-选择器&#34;&gt;Pod 选择器&lt;/h2&gt;
&lt;p&gt;用户必须让 StatefulSet 的 &lt;code&gt;.spec.selector&lt;/code&gt; 字段与其 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt; 配置的标签相匹配。
在 k8s &lt;code&gt;1.8&lt;/code&gt; 之前  &lt;code&gt;.spec.selector&lt;/code&gt; 不设置会被添加默认。 &lt;code&gt;1.8&lt;/code&gt; 及之后的版本，如果标签与选择器不匹配，StatefulSet 在创建时就会验证不过。&lt;/p&gt;
&lt;!--
## Pod Identity

StatefulSet Pods have a unique identity that is comprised of an ordinal, a
stable network identity, and stable storage. The identity sticks to the Pod,
regardless of which node it&#39;s (re)scheduled on.
 --&gt;
&lt;h2 id=&#34;pod-标识&#34;&gt;Pod 标识&lt;/h2&gt;
&lt;p&gt;StatefulSet 的 Pod 都有一个顺序的唯一的标识， 一个稳定的网络标识，和稳定的存储。
标识与 Pod 绑定，不受节点之间的调度影响&lt;/p&gt;
&lt;!--
### Ordinal Index

For a StatefulSet with N replicas, each Pod in the StatefulSet will be
assigned an integer ordinal, from 0 up through N-1, that is unique over the Set.
 --&gt;
&lt;h3 id=&#34;有序索引&#34;&gt;有序索引&lt;/h3&gt;
&lt;p&gt;对于一个有 N 个副本的 StatefulSet， StatefulSet 中的每个 Pod 都会被分配一个从 0 到 N - 1 的整数序号，这些序号在这个集合中唯一&lt;/p&gt;
&lt;!--
### Stable Network ID

Each Pod in a StatefulSet derives its hostname from the name of the StatefulSet
and the ordinal of the Pod. The pattern for the constructed hostname
is `$(statefulset name)-$(ordinal)`. The example above will create three Pods
named `web-0,web-1,web-2`.
A StatefulSet can use a [Headless Service](/docs/concepts/services-networking/service/#headless-services)
to control the domain of its Pods. The domain managed by this Service takes the form:
`$(service name).$(namespace).svc.cluster.local`, where &#34;cluster.local&#34; is the
cluster domain.
As each Pod is created, it gets a matching DNS subdomain, taking the form:
`$(podname).$(governing service domain)`, where the governing service is defined
by the `serviceName` field on the StatefulSet.

Depending on how DNS is configured in your cluster, you may not be able to look up the DNS
name for a newly-run Pod immediately. This behavior can occur when other clients in the
cluster have already sent queries for the hostname of the Pod before it was created.
Negative caching (normal in DNS) means that the results of previous failed lookups are
remembered and reused, even after the Pod is running, for at least a few seconds.

If you need to discover Pods promptly after they are created, you have a few options:

- Query the Kubernetes API directly (for example, using a watch) rather than relying on DNS lookups.
- Decrease the time of caching in your Kubernetes DNS provider (tpyically this means editing the config map for CoreDNS, which currently caches for 30 seconds).


As mentioned in the [limitations](#limitations) section, you are responsible for
creating the [Headless Service](/docs/concepts/services-networking/service/#headless-services)
responsible for the network identity of the pods.

Here are some examples of choices for Cluster Domain, Service name,
StatefulSet name, and how that affects the DNS names for the StatefulSet&#39;s Pods.

Cluster Domain | Service (ns/name) | StatefulSet (ns/name)  | StatefulSet Domain  | Pod DNS | Pod Hostname |
-------------- | ----------------- | ----------------- | -------------- | ------- | ------------ |
 cluster.local | default/nginx     | default/web       | nginx.default.svc.cluster.local | web-{0..N-1}.nginx.default.svc.cluster.local | web-{0..N-1} |
 cluster.local | foo/nginx         | foo/web           | nginx.foo.svc.cluster.local     | web-{0..N-1}.nginx.foo.svc.cluster.local     | web-{0..N-1} |
 kube.local    | foo/nginx         | foo/web           | nginx.foo.svc.kube.local        | web-{0..N-1}.nginx.foo.svc.kube.local        | web-{0..N-1} |

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; Cluster Domain will be set to &lt;code&gt;cluster.local&lt;/code&gt; unless
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/services-networking/dns-pod-service/&#34;&gt;otherwise configured&lt;/a&gt;.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;稳定的网络-id&#34;&gt;稳定的网络 ID&lt;/h3&gt;
&lt;p&gt;StatefulSet 中 Pod 的主机名由 StatefulSet 名称和 Pod 的序号组成。 格式为 &lt;code&gt;$(statefulset name)-$(ordinal)&lt;/code&gt;
如果 Pod 数量为 3 则名称依次为 &lt;code&gt;web-0,web-1,web-2&lt;/code&gt;.
StatefulSet 可以使用一个 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/service/#headless-services&#34;&gt;Headless Service&lt;/a&gt;
来控制其 Pod 的网络域。 Service 的域名格式为 &lt;code&gt;$(service name).$(namespace).svc.cluster.local&lt;/code&gt;
&lt;code&gt;cluster.local&lt;/code&gt; 为集群域。
当每个 Pod 被创建后，都会获得一个相应的 DNS 子域名，格式为 &lt;code&gt;$(podname).$(governing service domain)&lt;/code&gt;
其中 &lt;code&gt;governing service&lt;/code&gt; 就是 StatefulSet 定义中 &lt;code&gt;serviceName&lt;/code&gt; 字段&lt;/p&gt;
&lt;p&gt;基于集群中的 DNS 配置方式， 用户可能不能在 Pod 创建后，马上就能查询到对应的 DNS 名称。 当这个 Pod 在创建之前，
有其它的客户端对该主机名查询就可能出现这种情况。 因为 DNS 服务中会缓存之前失败的的查询记录，即便在 Pod 运行之后
至少几秒种内依然可能会有这种情况出现。&lt;/p&gt;
&lt;p&gt;如果想要在 Pod 创建后就能查询有，有以下几个选择:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;直接查询 k8s API(例如通过 watch), 而不是信赖 DNS 查询&lt;/li&gt;
&lt;li&gt;减少 k8s DNS 提供者(通常是修改 CoreDNS 的配置， 当前配置的缓存时间是 30s)的缓存时间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 &lt;a href=&#34;#limitations&#34;&gt;限制&lt;/a&gt; 一节种提到，需要用户在负责创建用于 Pod 网络标识的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/service/#headless-services&#34;&gt;Headless Service&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;以下为怎么设置 集群域，Service 命名， StatefulSet 命名，以及这命名对 StatefulSet 的 Pod 的 DNS 记录的影响:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Cluster Domain&lt;/th&gt;
&lt;th&gt;Service (ns/name)&lt;/th&gt;
&lt;th&gt;StatefulSet (ns/name)&lt;/th&gt;
&lt;th&gt;StatefulSet Domain&lt;/th&gt;
&lt;th&gt;Pod DNS&lt;/th&gt;
&lt;th&gt;Pod Hostname&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;cluster.local&lt;/td&gt;
&lt;td&gt;default/nginx&lt;/td&gt;
&lt;td&gt;default/web&lt;/td&gt;
&lt;td&gt;nginx.default.svc.cluster.local&lt;/td&gt;
&lt;td&gt;web-{0..N-1}.nginx.default.svc.cluster.local&lt;/td&gt;
&lt;td&gt;web-{0..N-1}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cluster.local&lt;/td&gt;
&lt;td&gt;foo/nginx&lt;/td&gt;
&lt;td&gt;foo/web&lt;/td&gt;
&lt;td&gt;nginx.foo.svc.cluster.local&lt;/td&gt;
&lt;td&gt;web-{0..N-1}.nginx.foo.svc.cluster.local&lt;/td&gt;
&lt;td&gt;web-{0..N-1}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube.local&lt;/td&gt;
&lt;td&gt;foo/nginx&lt;/td&gt;
&lt;td&gt;foo/web&lt;/td&gt;
&lt;td&gt;nginx.foo.svc.kube.local&lt;/td&gt;
&lt;td&gt;web-{0..N-1}.nginx.foo.svc.kube.local&lt;/td&gt;
&lt;td&gt;web-{0..N-1}&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果没有 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/dns-pod-service/&#34;&gt;其它的配置&lt;/a&gt; 集群域会是 &lt;code&gt;cluster.local&lt;/code&gt;&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
### Stable Storage

Kubernetes creates one [PersistentVolume](/docs/concepts/storage/persistent-volumes/) for each
VolumeClaimTemplate. In the nginx example above, each Pod will receive a single PersistentVolume
with a StorageClass of `my-storage-class` and 1 Gib of provisioned storage. If no StorageClass
is specified, then the default StorageClass will be used. When a Pod is (re)scheduled
onto a node, its `volumeMounts` mount the PersistentVolumes associated with its
PersistentVolume Claims. Note that, the PersistentVolumes associated with the
Pods&#39; PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted.
This must be done manually.
 --&gt;
&lt;h3 id=&#34;稳定的存储&#34;&gt;稳定的存储&lt;/h3&gt;
&lt;p&gt;k8s 会依照 VolumeClaimTemplate 为每个 Pod 创建一个 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/storage/persistent-volumes/&#34;&gt;PersistentVolume&lt;/a&gt;
在上面的示例中， 每个 Pod 都会收到一个 StorageClass 为 &lt;code&gt;my-storage-class&lt;/code&gt; 容量为 1G 的 PersistentVolume。
如果没有配置 StorageClass， 则会使用默认的 StorageClass。 当一个 Pod 被(重新)调度到一个节点时，它的 &lt;code&gt;volumeMounts&lt;/code&gt;
挂载 PersistentVolumeClaim 中对应的 PersistentVolume。 要注意与 PersistentVolumeClaim 关联的 PersistentVolume
在 Pod 或 StatefulSet 删除时不会被删除。想要删除必须要手动删除才行。&lt;/p&gt;
&lt;!--
### Pod Name Label

When the StatefulSet &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; creates a Pod,
it adds a label, `statefulset.kubernetes.io/pod-name`, that is set to the name of
the Pod. This label allows you to attach a Service to a specific Pod in
the StatefulSet.
 --&gt;
&lt;h3 id=&#34;pod-名称的标签&#34;&gt;Pod 名称的标签&lt;/h3&gt;
&lt;p&gt;当 StatefulSet &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; 创建一个 Pod 进会向它塞一个标签
标签的键为 &lt;code&gt;statefulset.kubernetes.io/pod-name&lt;/code&gt; 值为 Pod 的名称。这个标签可以让用户配置
Service 指向 StatefulSet 的特定 Pod。&lt;/p&gt;
&lt;!--
## Deployment and Scaling Guarantees

* For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.
* When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.
* Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.
* Before a Pod is terminated, all of its successors must be completely shutdown.

The StatefulSet should not specify a `pod.Spec.TerminationGracePeriodSeconds` of 0. This practice is unsafe and strongly discouraged. For further explanation, please refer to [force deleting StatefulSet Pods](/docs/tasks/run-application/force-delete-stateful-set-pod/).

When the nginx example above is created, three Pods will be deployed in the order
web-0, web-1, web-2. web-1 will not be deployed before web-0 is
[Running and Ready](/docs/concepts/workloads/pods/pod-lifecycle/), and web-2 will not be deployed until
web-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before
web-2 is launched, web-2 will not be launched until web-0 is successfully relaunched and
becomes Running and Ready.

If a user were to scale the deployed example by patching the StatefulSet such that
`replicas=1`, web-2 would be terminated first. web-1 would not be terminated until web-2
is fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated and
is completely shutdown, but prior to web-1&#39;s termination, web-1 would not be terminated
until web-0 is Running and Ready.
 --&gt;
&lt;h2 id=&#34;deployment-and-scaling-guarantees&#34;&gt;部署和容量伸缩保证&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;对于一个有 N 个副本的 StatefulSet， 当 Pod 被部署时，是以 {0..N-1} 的顺序依次创建的&lt;/li&gt;
&lt;li&gt;当 Pod 被删除除时，则是以 {N-1..0} 与创建相反的顺序终止&lt;/li&gt;
&lt;li&gt;在一个扩容操作之前，它的前辈 Pod 必须是运行和就绪的&lt;/li&gt;
&lt;li&gt;在一个 Pod 终止之前，它的后辈必须全部完全关闭&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;StatefulSet 不应该配置 &lt;code&gt;pod.Spec.TerminationGracePeriodSeconds&lt;/code&gt; 为 0. 这个操作是不安全的，强烈建议不要这样搞。
更多解决请见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/force-delete-stateful-set-pod/&#34;&gt;强制删除 StatefulSet 的 Pod&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在上面的例子中， 三个 Pod 会以 web-0, web-1, web-2 的顺序创建， web-1 不会在 web-0 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/&#34;&gt;运行并就绪&lt;/a&gt;
之前部署。 如果 web-0 在 web-1 运行并就绪后挂了，则 web-2 在 web-0 重新成功启动然后运行并就绪之前不会启动。&lt;/p&gt;
&lt;p&gt;在上面的例子中，三个副本成功运行后，用户设置 &lt;code&gt;replicas=1&lt;/code&gt;。 web-2 会先终止。 web-1 会在 web-2 关闭并删除后
才会开始终止。 如果 web-0 刚好在 web-2 完全关闭，但 web-1 还没终止之前挂了， 则 web-1 会在 web-0 重新运行并就绪后才会终止。&lt;/p&gt;
&lt;!--
### Pod Management Policies
In Kubernetes 1.7 and later, StatefulSet allows you to relax its ordering guarantees while
preserving its uniqueness and identity guarantees via its `.spec.podManagementPolicy` field.

#### OrderedReady Pod Management

`OrderedReady` pod management is the default for StatefulSets. It implements the behavior
described [above](#deployment-and-scaling-guarantees).

#### Parallel Pod Management

`Parallel` pod management tells the StatefulSet controller to launch or
terminate all Pods in parallel, and to not wait for Pods to become Running
and Ready or completely terminated prior to launching or terminating another
Pod. This option only affects the behavior for scaling operations. Updates are not
affected.
 --&gt;
&lt;h3 id=&#34;pod-management-policies&#34;&gt;Pod 管理策略&lt;/h3&gt;
&lt;p&gt;在 k8s 1.7+, StatefulSet 允许使用宽松的序号保证，通过 &lt;code&gt;.spec.podManagementPolicy&lt;/code&gt; 字段来保证唯一性和标识保证。&lt;/p&gt;
&lt;h4 id=&#34;orderedready-pod-管理&#34;&gt;OrderedReady Pod 管理&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;OrderedReady&lt;/code&gt; Pod 管理是 StatefulSet 的默认方式。它实现的行为有 &lt;a href=&#34;#deployment-and-scaling-guarantees&#34;&gt;上面&lt;/a&gt;讲过.&lt;/p&gt;
&lt;h4 id=&#34;parallel-pod-管理&#34;&gt;Parallel Pod 管理&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;Parallel&lt;/code&gt; Pod 管理，这种策略让 StatefulSet 控制器在启动或终止 Pod 并行进行， 不需要在启动的时候等待前辈运行并就绪，
也不需要在终止的时候等待后台完成终止并删除。 这个选项只影响容量伸缩行为。更新行为不受影响&lt;/p&gt;
&lt;!--
## Update Strategies

In Kubernetes 1.7 and later, StatefulSet&#39;s `.spec.updateStrategy` field allows you to configure
and disable automated rolling updates for containers, labels, resource request/limits, and
annotations for the Pods in a StatefulSet.
 --&gt;
&lt;h2 id=&#34;更新策略&#34;&gt;更新策略&lt;/h2&gt;
&lt;p&gt;在 k8s 1.7+, StatefulSet &lt;code&gt;.spec.updateStrategy&lt;/code&gt; 允许用户配置和禁用在对容器，标签，资源需求/限制，和标签的更新触发的自动滚动更新&lt;/p&gt;
&lt;!--
### On Delete

The `OnDelete` update strategy implements the legacy (1.6 and prior) behavior. When a StatefulSet&#39;s
`.spec.updateStrategy.type` is set to `OnDelete`, the StatefulSet controller will not automatically
update the Pods in a StatefulSet. Users must manually delete Pods to cause the controller to
create new Pods that reflect modifications made to a StatefulSet&#39;s `.spec.template`.
 --&gt;
&lt;h3 id=&#34;ondelete&#34;&gt;&lt;code&gt;OnDelete&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;OnDelete&lt;/code&gt; 更新策略实现的是经典(&lt;code&gt;v1.6-&lt;/code&gt;)行为. 当 StatefulSet 的 &lt;code&gt;.spec.updateStrategy.type&lt;/code&gt; 设置为 &lt;code&gt;OnDelete&lt;/code&gt;，
StatefulSet 就不会自动的更新 StatefulSet 中的 Pod。只有用户手动删除 Pod 后 控制器才会以 StatefulSet 中 &lt;code&gt;.spec.template&lt;/code&gt;
创建应用修改后的新 Pod。&lt;/p&gt;
&lt;!--
### Rolling Updates

The `RollingUpdate` update strategy implements automated, rolling update for the Pods in a
StatefulSet. It is the default strategy when `.spec.updateStrategy` is left unspecified. When a StatefulSet&#39;s `.spec.updateStrategy.type` is set to `RollingUpdate`, the
StatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed
in the same order as Pod termination (from the largest ordinal to the smallest), updating
each Pod one at a time. It will wait until an updated Pod is Running and Ready prior to
updating its predecessor.
 --&gt;
&lt;h3 id=&#34;rolling-updates&#34;&gt;&lt;code&gt;RollingUpdate&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;RollingUpdate&lt;/code&gt; 更新策略实现了自动，滚动更新 StatefulSet 中的 Pod。如果 &lt;code&gt;.spec.updateStrategy&lt;/code&gt; 没有指定这，默认使用该策略。
当 StatefulSet 的 &lt;code&gt;.spec.updateStrategy.type&lt;/code&gt; 设置为 &lt;code&gt;RollingUpdate&lt;/code&gt;， StatefulSet 控制器会删除并重建 StatefulSet 中的每一个 Pod。
处理顺序与终止顺序相同(序号从大到小)，每次更新一个 Pod。 会等待上一个更新的 Pod 运行并就绪后，才会进行下一个。&lt;/p&gt;
&lt;!--
#### Partitions

The `RollingUpdate` update strategy can be partitioned, by specifying a
`.spec.updateStrategy.rollingUpdate.partition`. If a partition is specified, all Pods with an
ordinal that is greater than or equal to the partition will be updated when the StatefulSet&#39;s
`.spec.template` is updated. All Pods with an ordinal that is less than the partition will not
be updated, and, even if they are deleted, they will be recreated at the previous version. If a
StatefulSet&#39;s `.spec.updateStrategy.rollingUpdate.partition` is greater than its `.spec.replicas`,
updates to its `.spec.template` will not be propagated to its Pods.
In most cases you will not need to use a partition, but they are useful if you want to stage an
update, roll out a canary, or perform a phased roll out.
 --&gt;
&lt;h4 id=&#34;分区&#34;&gt;分区&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;RollingUpdate&lt;/code&gt; 更新策略可以分区， 通过 &lt;code&gt;.spec.updateStrategy.rollingUpdate.partition&lt;/code&gt;的方式。
如果分区指定， 所有序号大于等于分区的 Pod 会在 StatefulSet &lt;code&gt;.spec.template&lt;/code&gt; 更新时更新。
所有序号小于分区的 Pod 都不更新， 即便被删除，也会以之前的版本重建。 如果一个 StatefulSet 的 &lt;code&gt;.spec.updateStrategy.rollingUpdate.partition&lt;/code&gt;
的值比它的 &lt;code&gt;.spec.replicas&lt;/code&gt; 还大，则所有对  &lt;code&gt;.spec.template&lt;/code&gt; 的修改都不会应用到 Pod 上。
大多数情况下，用户不需要使用到分区。但当用户想要分步更新，或金丝雀发布，或分阶段发布(phased roll out)时都相当有用&lt;/p&gt;
&lt;!--
#### Forced Rollback

When using [Rolling Updates](#rolling-updates) with the default
[Pod Management Policy](#pod-management-policies) (`OrderedReady`),
it&#39;s possible to get into a broken state that requires manual intervention to repair.

If you update the Pod template to a configuration that never becomes Running and
Ready (for example, due to a bad binary or application-level configuration error),
StatefulSet will stop the rollout and wait.

In this state, it&#39;s not enough to revert the Pod template to a good configuration.
Due to a [known issue](https://github.com/kubernetes/kubernetes/issues/67250),
StatefulSet will continue to wait for the broken Pod to become Ready
(which never happens) before it will attempt to revert it back to the working
configuration.

After reverting the template, you must also delete any Pods that StatefulSet had
already attempted to run with the bad configuration.
StatefulSet will then begin to recreate the Pods using the reverted template.
 --&gt;
&lt;h4 id=&#34;forced-rollback&#34;&gt;强制回退&lt;/h4&gt;
&lt;p&gt;当使用 &lt;a href=&#34;#rolling-updates&#34;&gt;滚动更新&lt;/a&gt; 配合 &lt;a href=&#34;#pod-management-policies&#34;&gt;Pod 管理策略&lt;/a&gt; (&lt;code&gt;OrderedReady&lt;/code&gt;) 可能会造成故障，需要人工介入修复。&lt;/p&gt;
&lt;p&gt;如果用户更新的 Pod 模板永远都不会进入运行和就绪状态(例如，因为二进制文件损坏或应用级配置错误)。 StatefulSet 就会停止发布并等待。&lt;/p&gt;
&lt;p&gt;在这种状态下。就不能回退 Pod 模板到正常的配置。 因为一个 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/67250&#34;&gt;已知的问题单&lt;/a&gt;
StatefulSet 在尝试回滚到正常的配置之前会一直等着那个故障的 Pod 变成就绪状态(但永远不会)&lt;/p&gt;
&lt;p&gt;在回退模板后，用户还需要删除所以 StatefulSet 使用错误的配置已经启动的 Pod 。StatefulSet 这时候才会以回退后的模板重新创建 Pod。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tutorials/stateful-application/basic-stateful-set/&#34;&gt;部署一个有状态的应用&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tutorials/stateful-application/cassandra/&#34;&gt;用StatefulSet 部署  Cassandra &lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/run-replicated-stateful-application/&#34;&gt;部署一个多副本的 有状态的应用&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: DaemonSet</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/daemonset/</link>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/daemonset/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- enisoc
- erictune
- foxish
- janetkuo
- kow3ns
title: DaemonSet
content_type: concept
weight: 40
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--
A _DaemonSet_ ensures that all (or some) Nodes run a copy of a Pod.  As nodes are added to the
cluster, Pods are added to them.  As nodes are removed from the cluster, those Pods are garbage
collected.  Deleting a DaemonSet will clean up the Pods it created.

Some typical uses of a DaemonSet are:

- running a cluster storage daemon on every node
- running a logs collection daemon on every node
- running a node monitoring daemon on every node

In a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.
A more complex setup might use multiple DaemonSets for a single type of daemon, but with
different flags and/or different memory and cpu requests for different hardware types.
 --&gt;
&lt;p&gt;&lt;em&gt;DaemonSet&lt;/em&gt; 确保所以(或部分)节点上都会运行一个副本的 Pod。 当有节点加入到集群时，这个 Pod 也会自动加到该节点上。
当节点从集群移除时， 这些 Pod 也会被垃圾清理掉。 删除一个 DaemonSet 也会清除其所创建的 Pod。&lt;/p&gt;
&lt;p&gt;DaemonSet 常见使用场景:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在每个节点上运行集群存储守护进程&lt;/li&gt;
&lt;li&gt;在每个节点上运行日志收集守护进程&lt;/li&gt;
&lt;li&gt;在每个节点上运行监控守护进程&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个简单的用法是对于每个类型的守护进程使用一个 DaemonSet 运行在每一个节点。
复杂点的配置就是对一个类型的守护进程使用多个 DaemonSet， 针对不同的硬件类型使用 不同的标志， 内存， CPU 需求的 DaemonSet&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Writing a DaemonSet Spec

### Create a DaemonSet

You can describe a DaemonSet in a YAML file. For example, the `daemonset.yaml` file below describes a DaemonSet that runs the fluentd-elasticsearch Docker image:



 













&lt;table class=&#34;includecode&#34; id=&#34;controllersdaemonsetyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/daemonset.yaml&#34; download=&#34;controllers/daemonset.yaml&#34;&gt;
                    &lt;code&gt;controllers/daemonset.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersdaemonsetyaml&#39;)&#34; title=&#34;Copy controllers/daemonset.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DaemonSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-elasticsearch&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube-system&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;k8s-app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-logging&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-elasticsearch&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-elasticsearch&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:
      &lt;span style=&#34;color:#75715e&#34;&gt;# this toleration is to have the daemonset runnable on master nodes&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# remove it if your masters can&amp;#39;t run pods&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# 该耐受表示在主控节点上运行 DaemonSet,如果用户的主控节点不能运行 Pod，请移除&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;node-role.kubernetes.io/master&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NoSchedule&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-elasticsearch&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# quay.io/fluentd_elasticsearch/fluentd:v2.5.2&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/fluentd:v2.5.2-fluentd_elasticsearch&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;limits&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;memory&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;200Mi&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;cpu&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;100m&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;memory&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;200Mi&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;varlog&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/log&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;varlibdockercontainers&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/lib/docker/containers&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;terminationGracePeriodSeconds&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;varlog&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/log&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;varlibdockercontainers&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/lib/docker/containers&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



Create a DaemonSet based on the YAML file:

```
kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml
```
 --&gt;
&lt;h2 id=&#34;编写-daemonset-配制&#34;&gt;编写 DaemonSet 配制&lt;/h2&gt;
&lt;h3 id=&#34;创建-daemonset&#34;&gt;创建 DaemonSet&lt;/h3&gt;
&lt;p&gt;用户可以通过一个 YAML 文件在描述一个 DaemonSet。 例如， 下面的 &lt;code&gt;daemonset.yaml&lt;/code&gt; 中就描述了一个运行 &lt;code&gt;fluentd-elasticsearch&lt;/code&gt; Docker 镜像的 DaemonSet:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;controllersdaemonsetyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/daemonset.yaml&#34; download=&#34;controllers/daemonset.yaml&#34;&gt;
                    &lt;code&gt;controllers/daemonset.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersdaemonsetyaml&#39;)&#34; title=&#34;Copy controllers/daemonset.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;DaemonSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-elasticsearch&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;kube-system&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;k8s-app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-logging&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-elasticsearch&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-elasticsearch&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;tolerations&lt;/span&gt;:
      &lt;span style=&#34;color:#75715e&#34;&gt;# this toleration is to have the daemonset runnable on master nodes&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# remove it if your masters can&amp;#39;t run pods&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# 该耐受表示在主控节点上运行 DaemonSet,如果用户的主控节点不能运行 Pod，请移除&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;node-role.kubernetes.io/master&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;effect&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;NoSchedule&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;fluentd-elasticsearch&lt;/span&gt;
      &lt;span style=&#34;color:#75715e&#34;&gt;# quay.io/fluentd_elasticsearch/fluentd:v2.5.2&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;registry.cn-hangzhou.aliyuncs.com/lisong/fluentd:v2.5.2-fluentd_elasticsearch&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;resources&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;limits&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;memory&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;200Mi&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;requests&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;cpu&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;100m&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;memory&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;200Mi&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;volumeMounts&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;varlog&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/log&lt;/span&gt;
        - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;varlibdockercontainers&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;mountPath&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/lib/docker/containers&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;readOnly&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;terminationGracePeriodSeconds&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;volumes&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;varlog&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/log&lt;/span&gt;
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;varlibdockercontainers&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;hostPath&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/var/lib/docker/containers&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;基于 YAML 文件创建一个　DaemonSet&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl apply -f daemonset.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;!--
### Required Fields

As with all other Kubernetes config, a DaemonSet needs `apiVersion`, `kind`, and `metadata` fields.  For
general information about working with config files, see
[running stateless applications](/docs/tasks/run-application/run-stateless-application-deployment/),
[configuring containers](/docs/tasks/), and [object management using kubectl](/docs/concepts/overview/working-with-objects/object-management/) documents.

The name of a DaemonSet object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

A DaemonSet also needs a [`.spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status) section.
 --&gt;
&lt;h3 id=&#34;必要字段&#34;&gt;必要字段&lt;/h3&gt;
&lt;p&gt;与其它所有其它的 k8s 配置一样， DaemonSet 必须有 &lt;code&gt;apiVersion&lt;/code&gt;, &lt;code&gt;kind&lt;/code&gt;, &lt;code&gt;metadata&lt;/code&gt; 字段，
关于配置文件的通用信息见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/run-stateless-application-deployment/&#34;&gt;运行无状态应用&lt;/a&gt;,
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/&#34;&gt;配置容器&lt;/a&gt;, &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/object-management/&#34;&gt;使用 kubectl 管理对象&lt;/a&gt;
DaemonSet 的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;DaemonSet 也是必须要有一个 &lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;&lt;code&gt;.spec&lt;/code&gt;&lt;/a&gt; 配置区.&lt;/p&gt;
&lt;!--
### Pod Template

The `.spec.template` is one of the required fields in `.spec`.

The `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;, except it is nested and does not have an `apiVersion` or `kind`.

In addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriate
labels (see [pod selector](#pod-selector)).

A Pod Template in a DaemonSet must have a [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)
 equal to `Always`, or be unspecified, which defaults to `Always`.
 --&gt;
&lt;h3 id=&#34;pod-模板&#34;&gt;Pod 模板&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.template&lt;/code&gt; 是 &lt;code&gt;.spec&lt;/code&gt; 中的一个必要字段.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;.spec.template&lt;/code&gt; 是一个 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/#pod-templates&#34;&gt;pod 模板&lt;/a&gt;.
除了因为嵌套没有 &lt;code&gt;apiVersion&lt;/code&gt; 或 &lt;code&gt;kind&lt;/code&gt;字段外，与 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;的定义完全相同,&lt;/p&gt;
&lt;p&gt;相较与裸 Pod 增加的字段还有 DaemonSet 需要设置恰当的标签 (见 &lt;a href=&#34;#pod-selector&#34;&gt;Pod 选择器&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;DaemonSet 中的 Pod 模板必须要有 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy&#34;&gt;&lt;code&gt;RestartPolicy&lt;/code&gt;&lt;/a&gt;
且值为 &lt;code&gt;Always&lt;/code&gt; 或 留空，然后默认为 &lt;code&gt;Always&lt;/code&gt;&lt;/p&gt;
&lt;!--
### Pod Selector

The `.spec.selector` field is a pod selector.  It works the same as the `.spec.selector` of
a [Job](/docs/concepts/workloads/controllers/job/).

As of Kubernetes 1.8, you must specify a pod selector that matches the labels of the
`.spec.template`. The pod selector will no longer be defaulted when left empty. Selector
defaulting was not compatible with `kubectl apply`. Also, once a DaemonSet is created,
its `.spec.selector` can not be mutated. Mutating the pod selector can lead to the
unintentional orphaning of Pods, and it was found to be confusing to users.

The `.spec.selector` is an object consisting of two fields:

* `matchLabels` - works the same as the `.spec.selector` of a [ReplicationController](/docs/concepts/workloads/controllers/replicationcontroller/).
* `matchExpressions` - allows to build more sophisticated selectors by specifying key,
  list of values and an operator that relates the key and values.

When the two are specified the result is ANDed.

If the `.spec.selector` is specified, it must match the `.spec.template.metadata.labels`. Config with these not matching will be rejected by the API.

Also you should not normally create any Pods whose labels match this selector, either directly, via
another DaemonSet, or via another workload resource such as ReplicaSet.  Otherwise, the DaemonSet
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; will think that those Pods were created by it.
Kubernetes will not stop you from doing this. One case where you might want to do this is manually
create a Pod with a different value on a node for testing.
 --&gt;
&lt;h3 id=&#34;pod-selector&#34;&gt;Pod 选择器&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.selector&lt;/code&gt; 字段是一个 Pod 选择器.  与 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/job/&#34;&gt;Job&lt;/a&gt; 的 &lt;code&gt;.spec.selector&lt;/code&gt; 是一样的。&lt;/p&gt;
&lt;p&gt;从 k8s 1.8 开始，用户必须要设置一个与 &lt;code&gt;.spec.template&lt;/code&gt; 中标签相匹配的标签选择器。不再是留空会添加默认值。
选择器默认添加的行为与 &lt;code&gt;kubectl apply&lt;/code&gt; 不兼容。 并且，当一个 DaemonSet 创建后，&lt;code&gt;.spec.selector&lt;/code&gt; 字段将不可变。
对标签选择器的修改可能无意间导致产生孤儿 Pod， 这样会让用户费解。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;.spec.selector&lt;/code&gt; 对象由以下两个字段组成:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;matchLabels&lt;/code&gt; - 与 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/replicationcontroller/&#34;&gt;ReplicationController&lt;/a&gt; 中的 &lt;code&gt;.spec.selector&lt;/code&gt; 作用一样。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;matchExpressions&lt;/code&gt; - 能通过键与对应的值列表，操作符组成更复杂的选择器
如果以上两个字段都有设置，它们之间是逻辑与关系&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果 &lt;code&gt;.spec.selector&lt;/code&gt; 设置了值(对应 1.8 之前的版本), 则必须与 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt;匹配. 如果配置不匹配会被 api-server 拒绝.&lt;/p&gt;
&lt;p&gt;在此之外用户不应该再直接或间接(如其它的 DaemonSet 或如 ReplicaSet 之类的工作负载)创建与该选择器匹配的 Pod，
否则 DaemonSet &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; 会认为这些 Pod 也是它创建的。
k8s 不会阻止用户这么干。 能这么干的和种场景是手动创建与这个不一样配置的 Pod 用于测试&lt;/p&gt;
&lt;!--
### Running Pods on select Nodes

If you specify a `.spec.template.spec.nodeSelector`, then the DaemonSet controller will
create Pods on nodes which match that [node
selector](/docs/concepts/scheduling-eviction/assign-pod-node/). Likewise if you specify a `.spec.template.spec.affinity`,
then DaemonSet controller will create Pods on nodes which match that [node affinity](/docs/concepts/scheduling-eviction/assign-pod-node/).
If you do not specify either, then the DaemonSet controller will create Pods on all nodes.
 --&gt;
&lt;h3 id=&#34;只在选定的节点上运行-pod&#34;&gt;只在选定的节点上运行 Pod&lt;/h3&gt;
&lt;p&gt;如果指定了 &lt;code&gt;.spec.template.spec.nodeSelector&lt;/code&gt;， 则 DaemonSet 控制器只会在那些匹配 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/assign-pod-node/&#34;&gt;节点选择器&lt;/a&gt;
节点上创建 Pod。 类似地，如果指定了 &lt;code&gt;.spec.template.spec.affinity&lt;/code&gt; 控制器只会在那些匹配 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/assign-pod-node/&#34;&gt;node affinity&lt;/a&gt;
节点上创建 Pod。如果一个都没指定，则 DaemonSet 控制器会在所有的节点上创建 Pod。&lt;/p&gt;
&lt;!--
## How Daemon Pods are scheduled

### Scheduled by default scheduler






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [stable]&lt;/code&gt;
&lt;/div&gt;



A DaemonSet ensures that all eligible nodes run a copy of a Pod. Normally, the
node that a Pod runs on is selected by the Kubernetes scheduler. However,
DaemonSet pods are created and scheduled by the DaemonSet controller instead.
That introduces the following issues:

 * Inconsistent Pod behavior: Normal Pods waiting to be scheduled are created
   and in `Pending` state, but DaemonSet pods are not created in `Pending`
   state. This is confusing to the user.
 * [Pod preemption](/docs/concepts/configuration/pod-priority-preemption/)
   is handled by default scheduler. When preemption is enabled, the DaemonSet controller
   will make scheduling decisions without considering pod priority and preemption.

`ScheduleDaemonSetPods` allows you to schedule DaemonSets using the default
scheduler instead of the DaemonSet controller, by adding the `NodeAffinity` term
to the DaemonSet pods, instead of the `.spec.nodeName` term. The default
scheduler is then used to bind the pod to the target host. If node affinity of
the DaemonSet pod already exists, it is replaced (the original node affinity was taken into account before selecting the target host). The DaemonSet controller only
performs these operations when creating or modifying DaemonSet pods, and no
changes are made to the `spec.template` of the DaemonSet.

```yaml
nodeAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
    nodeSelectorTerms:
    - matchFields:
      - key: metadata.name
        operator: In
        values:
        - target-host-name
```

In addition, `node.kubernetes.io/unschedulable:NoSchedule` toleration is added
automatically to DaemonSet Pods. The default scheduler ignores
`unschedulable` Nodes when scheduling DaemonSet Pods.
 --&gt;
&lt;h2 id=&#34;daemonset-的-pod-是怎么调度的&#34;&gt;DaemonSet 的 Pod 是怎么调度的&lt;/h2&gt;
&lt;h3 id=&#34;由默认调度器调度&#34;&gt;由默认调度器调度&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.19 [stable]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;DaemonSet 会确保所有合适的节点者会运行一个 Pod 的副本。 通常 Pod 应该运行在哪个节点上是由 k8s 调度器来选的。
但是 DaemonSet 的 Pod 也可以由 DaemonSet 控制器来创建和调度。
由此也引出了一些问题:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod 的行为不一致: 普通的 Pod 在创建和等待调度时的状态是 &lt;code&gt;Pending&lt;/code&gt;，但 DaemonSet 的 Pod 创建时不是 &lt;code&gt;Pending&lt;/code&gt; 状态，这对用户来说比较费解。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/configuration/pod-priority-preemption/&#34;&gt;Pod preemption&lt;/a&gt; 是由默认调度器处理的。 当 优先权被开启时， DaemonSet
控制器在进行调度决策时不会考虑 Pod 的优先级(priority)和优先权(preemption)(这俩有啥区别？)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;ScheduleDaemonSetPods&lt;/code&gt; 允许用户可以让默认调度器来调度 DaemonSet 的 Pod，而不是 DaemonSet 的控制来调度，
只需要向 DaemonSet 的 Pod 模板添加 &lt;code&gt;NodeAffinity&lt;/code&gt; 项， 而不是添加 &lt;code&gt;.spec.nodeName&lt;/code&gt; 项就可以。
这时默认调度就与 Pod 的目标主机绑定了。 如果 DaemonSet Pod 已经存在了节点亲和性，会被替换(原本的节点亲和性在选择目标主机之间也会被考量)
DaemonSet 控制器只会在创建或修改 DaemonSet Pod 时执行这些操作， 不会对 DaemonSet 的 &lt;code&gt;spec.template&lt;/code&gt; 作任何修改。&lt;/p&gt;
&lt;p&gt;{{ &lt;todo-optimize&gt; }}&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;nodeAffinity&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;requiredDuringSchedulingIgnoredDuringExecution&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;nodeSelectorTerms&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;matchFields&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;metadata.name&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;operator&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;In&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;values&lt;/span&gt;:
        - &lt;span style=&#34;color:#ae81ff&#34;&gt;target-host-name&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In addition, &lt;code&gt;node.kubernetes.io/unschedulable:NoSchedule&lt;/code&gt; toleration is added
automatically to DaemonSet Pods. The default scheduler ignores
&lt;code&gt;unschedulable&lt;/code&gt; Nodes when scheduling DaemonSet Pods.
另外 &lt;code&gt;node.kubernetes.io/unschedulable:NoSchedule&lt;/code&gt; 耐受会默认添加到 DaemonSet 的 Pod 上。
默认调度器在调度 DaemonSet 的 Pod时会忽略 &lt;code&gt;unschedulable&lt;/code&gt; 节点&lt;/p&gt;
&lt;!--
### Taints and Tolerations

Although Daemon Pods respect
[taints and tolerations](/docs/concepts/scheduling-eviction/taint-and-toleration/),
the following tolerations are added to DaemonSet Pods automatically according to
the related features.

| Toleration Key                           | Effect     | Version | Description |
| ---------------------------------------- | ---------- | ------- | ----------- |
| `node.kubernetes.io/not-ready`           | NoExecute  | 1.13+   | DaemonSet pods will not be evicted when there are node problems such as a network partition. |
| `node.kubernetes.io/unreachable`         | NoExecute  | 1.13+   | DaemonSet pods will not be evicted when there are node problems such as a network partition. |
| `node.kubernetes.io/disk-pressure`       | NoSchedule | 1.8+    | |
| `node.kubernetes.io/memory-pressure`     | NoSchedule | 1.8+    | |
| `node.kubernetes.io/unschedulable`       | NoSchedule | 1.12+   | DaemonSet pods tolerate unschedulable attributes by default scheduler. |
| `node.kubernetes.io/network-unavailable` | NoSchedule | 1.12+   | DaemonSet pods, who uses host network, tolerate network-unavailable attributes by default scheduler. |
 --&gt;
&lt;h3 id=&#34;毒点taint与耐受toleration&#34;&gt;毒点(Taint)与耐受(Toleration)&lt;/h3&gt;
&lt;p&gt;尽管 DaemonSet 的 Pod 是遵守 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/scheduling-eviction/taint-and-toleration/&#34;&gt;毒点与耐受&lt;/a&gt;的,
但以下耐受会根据相关的特性自动添加到 DaemonSet 的 Pod 上。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Toleration Key&lt;/th&gt;
&lt;th&gt;Effect&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node.kubernetes.io/not-ready&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;NoExecute&lt;/td&gt;
&lt;td&gt;1.13+&lt;/td&gt;
&lt;td&gt;DaemonSet 的 Pod 在遇到如网络分区的节点问题时不会衩上踢出去&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node.kubernetes.io/unreachable&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;NoExecute&lt;/td&gt;
&lt;td&gt;1.13+&lt;/td&gt;
&lt;td&gt;DaemonSet 的 Pod 在遇到如网络分区的节点问题时不会衩上踢出去&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node.kubernetes.io/disk-pressure&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;NoSchedule&lt;/td&gt;
&lt;td&gt;1.8+&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node.kubernetes.io/memory-pressure&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;NoSchedule&lt;/td&gt;
&lt;td&gt;1.8+&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node.kubernetes.io/unschedulable&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;NoSchedule&lt;/td&gt;
&lt;td&gt;1.12+&lt;/td&gt;
&lt;td&gt;DaemonSet 的 Pod 由默认调度器调度时耐受 &lt;code&gt;unschedulable&lt;/code&gt; 属性&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;node.kubernetes.io/network-unavailable&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;NoSchedule&lt;/td&gt;
&lt;td&gt;1.12+&lt;/td&gt;
&lt;td&gt;DaemonSet 使用主机网络的 Pod 由默认调度器调度时耐受 &lt;code&gt;network-unavailable&lt;/code&gt; 属性&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
## Communicating with Daemon Pods

Some possible patterns for communicating with Pods in a DaemonSet are:

- **Push**: Pods in the DaemonSet are configured to send updates to another service, such
  as a stats database.  They do not have clients.
- **NodeIP and Known Port**: Pods in the DaemonSet can use a `hostPort`, so that the pods are reachable via the node IPs.  Clients know the list of node IPs somehow, and know the port by convention.
- **DNS**: Create a [headless service](/docs/concepts/services-networking/service/#headless-services) with the same pod selector,
  and then discover DaemonSets using the `endpoints` resource or retrieve multiple A records from
  DNS.
- **Service**: Create a service with the same Pod selector, and use the service to reach a
  daemon on a random node. (No way to reach specific node.)
 --&gt;
&lt;h2 id=&#34;与-daemonset-的-pod-通信&#34;&gt;与 DaemonSet 的 Pod 通信&lt;/h2&gt;
&lt;p&gt;与 DaemonSet 的 Pod 通信 可行模式有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Push&lt;/strong&gt;: DaemonSet 中的 Pod 配置为向另一个服务发送更新， 如 一个状态数据。 它们没有客户端。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NodeIP and Known Port&lt;/strong&gt;: DaemonSet 中的 Pod 可以使用 &lt;code&gt;hostPort&lt;/code&gt;， 这样就可以通过节点IP来访问这些 Pod。 客户端可以通过某些方便的方式获得节点的IP 和端口。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DNS&lt;/strong&gt;: 创建一个有相同 Pod 选择器的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/services-networking/service/#headless-services&#34;&gt;headless Service&lt;/a&gt;, 这样就可以通过 &lt;code&gt;endpoints&lt;/code&gt; 找到 DaemonSet 或 通过 DNS 找到一个 A 记录列表&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Service&lt;/strong&gt;: 创建个拥有相同 Pod 选择器的 Service , 通过 Service 随机访问一个节点上的 Pod(但没办法直接访问指定节点)&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Updating a DaemonSet

If node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete
Pods from newly not-matching nodes.

You can modify the Pods that a DaemonSet creates.  However, Pods do not allow all
fields to be updated.  Also, the DaemonSet controller will use the original template the next
time a node (even with the same name) is created.

You can delete a DaemonSet.  If you specify `--cascade=false` with `kubectl`, then the Pods
will be left on the nodes.  If you subsequently create a new DaemonSet with the same selector,
the new DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replaces
them according to its `updateStrategy`.

You can [perform a rolling update](/docs/tasks/manage-daemon/update-daemon-set/) on a DaemonSet.
 --&gt;
&lt;h2 id=&#34;更新-daemonset&#34;&gt;更新 DaemonSet&lt;/h2&gt;
&lt;p&gt;如果节点的标签发生变更，则 DaemonSet 及时时在新匹配的节点上添加 Pod，将不匹配的节点上的 Pod 删掉。&lt;/p&gt;
&lt;p&gt;用户可以修改由 DaemonSet 创建的 Pod， 但不是所以的 Pod 字段都是可以更新的。 同时 DaemonSet 会使用原版的模板来创建新加入的节点(即使节点名是一样的)&lt;/p&gt;
&lt;p&gt;用户可以删除一个 DaemonSet。 如果在删除时 &lt;code&gt;kubectl&lt;/code&gt; 添加了参数 &lt;code&gt;--cascade=false&lt;/code&gt;， 则对应的 Pod 会被保留在节点上。
如果用户接着又以相同的选择器创建了新的 DaemonSet， 这个新的 DaemonSet 会接管这些 Pod。 如果有 Pod 需要被替换则会根据 &lt;code&gt;updateStrategy&lt;/code&gt; 进行替换&lt;/p&gt;
&lt;p&gt;用户也可以在 DaemonSet 上&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/manage-daemon/update-daemon-set/&#34;&gt;执行滚动更新&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Alternatives to DaemonSet

### Init scripts

It is certainly possible to run daemon processes by directly starting them on a node (e.g. using
`init`, `upstartd`, or `systemd`).  This is perfectly fine.  However, there are several advantages to
running such processes via a DaemonSet:

- Ability to monitor and manage logs for daemons in the same way as applications.
- Same config language and tools (e.g. Pod templates, `kubectl`) for daemons and applications.
- Running daemons in containers with resource limits increases isolation between daemons from app
  containers.  However, this can also be accomplished by running the daemons in a container but not in a Pod
  (e.g. start directly via Docker).
 --&gt;
&lt;h2 id=&#34;daemonset-的替代方案&#34;&gt;DaemonSet 的替代方案&lt;/h2&gt;
&lt;h3 id=&#34;初始化脚本&#34;&gt;初始化脚本&lt;/h3&gt;
&lt;p&gt;有时可以直接在节点上(通过如 &lt;code&gt;init&lt;/code&gt;, &lt;code&gt;upstartd&lt;/code&gt;, &lt;code&gt;systemd&lt;/code&gt;)的方式直接运行守护进程。 这样也是很好的方案。
但是使用 DaemonSet 运行这些进程有如下优势:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以与应用相同的方式来实现这些进程的监控和日志管理&lt;/li&gt;
&lt;li&gt;守护进程与应用使用同样的配置语言与工具(如， Pod 模板 &lt;code&gt;kubectl&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;在容器中运行带资源限制的空城计进程可以提与应用容器之间的隔离级别。 当然，这也可以通过在容器中运行守护进程而不是 Pod(如直接通过 Docker 启动)来运行守护进程&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
### Bare Pods

It is possible to create Pods directly which specify a particular node to run on.  However,
a DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of
node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you should
use a DaemonSet rather than creating individual Pods.
 --&gt;
&lt;h3 id=&#34;裸-pod&#34;&gt;裸 Pod&lt;/h3&gt;
&lt;p&gt;可以直接在指定节点上直接创建 Pod。 但这些 Pod 可能因为某些如节点挂掉或节点维护(升级内核)引起的故障原因被删除或终止。
因此应该使用 DaemonSet，而不是单独使用 Pod。&lt;/p&gt;
&lt;!--
### Static Pods

It is possible to create Pods by writing a file to a certain directory watched by Kubelet.  These
are called [static pods](/docs/tasks/configure-pod-container/static-pod/).
Unlike DaemonSet, static Pods cannot be managed with kubectl
or other Kubernetes API clients.  Static Pods do not depend on the apiserver, making them useful
in cluster bootstrapping cases.  Also, static Pods may be deprecated in the future.
--&gt;
&lt;h3 id=&#34;静态-pod&#34;&gt;静态 Pod&lt;/h3&gt;
&lt;p&gt;可以直接在 kubelet 监控的目录中创建 Pod 配置文件. 这些被称为 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/configure-pod-container/static-pod/&#34;&gt;静态 Pod&lt;/a&gt;.
与 DaemonSet 不现, 静态 Pod 不能通过 kubectl 或其它 k8s API 客户端管理. 静态 Pod 也不信赖于 api-server, 让它在
集群初始化的时候很有用. 并且,静态 Pod 可以在未来的版本中被废弃.&lt;/p&gt;
&lt;!--
### Deployments

DaemonSets are similar to [Deployments](/docs/concepts/workloads/controllers/deployment/) in that
they both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers,
storage servers).

Use a Deployment for stateless services, like frontends, where scaling up and down the
number of replicas and rolling out updates are more important than controlling exactly which host
the Pod runs on.  Use a DaemonSet when it is important that a copy of a Pod always run on
all or certain hosts, and when it needs to start before other Pods.
 --&gt;
&lt;h3 id=&#34;deployment&#34;&gt;Deployment&lt;/h3&gt;
&lt;p&gt;DaemonSet 与 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/deployment/&#34;&gt;Deployment&lt;/a&gt; 类似, 它们都可以创建 Pod
并且这些 Pod 运行的都是不应该被终止的进程(如, web 服务器,存储服务器)&lt;/p&gt;
&lt;p&gt;Deployment 用于无状态的服务, 如前端,可以通过副本数来扩容或缩减容量, 发布更新比控制 Pod 在哪个主机上运行更重要.
在 Pod 需要始终在所有或特定主机上运行进更重要些时,并且需要它们在其它 Pod 之前启动时,应该使用 DaemonSet&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Job</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/job/</link>
      <pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/job/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- erictune
- soltysh
title: Jobs
content_type: concept
feature:
  title: Batch execution
  description: &gt;
    In addition to services, Kubernetes can manage your batch and CI workloads, replacing containers that fail, if desired.
weight: 50
---
 --&gt;
&lt;!-- overview --&gt;
&lt;p&gt;一个 Job 会创建一个或多个 Pod 并保证这些 Pod 中指定数量的最终执行完成并终止。 当一个 Pod 成功完成时， Job 跟踪这些成功完成的状态。
当成功完成的数量达到指定的数量时， 这个任务(Job)就完成了。 删除一个 Job 会清除它所创建的 Pod。&lt;/p&gt;
&lt;p&gt;使用 Pod 的一个简单场景为创建一个 Job 对象以保证一个 Pod 可靠地完成。 Job 对象会在第一个 Pod 挂掉或被删除(如因为节点硬件故障或节点重启)后创建一个新的 Pod。&lt;/p&gt;
&lt;p&gt;用户也可以使用 Job 并行地运行多个 Pod。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--  
## Running an example Job

Here is an example Job config.  It computes π to 2000 places and prints it out.
It takes around 10s to complete.



 













&lt;table class=&#34;includecode&#34; id=&#34;controllersjobyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/job.yaml&#34; download=&#34;controllers/job.yaml&#34;&gt;
                    &lt;code&gt;controllers/job.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersjobyaml&#39;)&#34; title=&#34;Copy controllers/job.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;batch/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Job&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pi&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pi&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;perl&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;perl&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-Mbignum=bpi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-wle&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;print bpi(2000)&amp;#34;&lt;/span&gt;]
      &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;backoffLimit&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



You can run the example with this command:

```shell
kubectl apply -f https://kubernetes.io/examples/controllers/job.yaml
```
```
job.batch/pi created
```

Check on the status of the Job with `kubectl`:

```shell
kubectl describe jobs/pi
```
```
Name:           pi
Namespace:      default
Selector:       controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
Labels:         controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
                job-name=pi
Annotations:    kubectl.kubernetes.io/last-applied-configuration:
                  {&#34;apiVersion&#34;:&#34;batch/v1&#34;,&#34;kind&#34;:&#34;Job&#34;,&#34;metadata&#34;:{&#34;annotations&#34;:{},&#34;name&#34;:&#34;pi&#34;,&#34;namespace&#34;:&#34;default&#34;},&#34;spec&#34;:{&#34;backoffLimit&#34;:4,&#34;template&#34;:...
Parallelism:    1
Completions:    1
Start Time:     Mon, 02 Dec 2019 15:20:11 +0200
Completed At:   Mon, 02 Dec 2019 15:21:16 +0200
Duration:       65s
Pods Statuses:  0 Running / 1 Succeeded / 0 Failed
Pod Template:
  Labels:  controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
           job-name=pi
  Containers:
   pi:
    Image:      perl
    Port:       &lt;none&gt;
    Host Port:  &lt;none&gt;
    Command:
      perl
      -Mbignum=bpi
      -wle
      print bpi(2000)
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  14m   job-controller  Created pod: pi-5rwd7
```

To view completed Pods of a Job, use `kubectl get pods`.

To list all the Pods that belong to a Job in a machine readable form, you can use a command like this:

```shell
pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath=&#39;{.items[*].metadata.name}&#39;)
echo $pods
```
```
pi-5rwd7
```

Here, the selector is the same as the selector for the Job.  The `--output=jsonpath` option specifies an expression
that just gets the name from each Pod in the returned list.

View the standard output of one of the pods:

```shell
kubectl logs $pods
```
The output is similar to this:
```shell
3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
```
--&gt;
&lt;h2 id=&#34;运行一个示例-job&#34;&gt;运行一个示例 Job&lt;/h2&gt;
&lt;p&gt;以下为一个示例 Job 的配置。 它会计算圆周率(π)后 2000 位并打印出来. 大概用时为 10 秒。&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;controllersjobyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/job.yaml&#34; download=&#34;controllers/job.yaml&#34;&gt;
                    &lt;code&gt;controllers/job.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersjobyaml&#39;)&#34; title=&#34;Copy controllers/job.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;batch/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Job&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pi&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pi&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;perl&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;perl&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-Mbignum=bpi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-wle&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;print bpi(2000)&amp;#34;&lt;/span&gt;]
      &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;backoffLimit&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;通过以下命令运行该示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f job.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;job.batch/pi created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过 &lt;code&gt;kubectl&lt;/code&gt; 命令查看 Job 状态:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe jobs/pi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;Name:           pi
Namespace:      default
Selector:       controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
Labels:         controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
                job-name=pi
Annotations:    kubectl.kubernetes.io/last-applied-configuration:
                  {&amp;quot;apiVersion&amp;quot;:&amp;quot;batch/v1&amp;quot;,&amp;quot;kind&amp;quot;:&amp;quot;Job&amp;quot;,&amp;quot;metadata&amp;quot;:{&amp;quot;annotations&amp;quot;:{},&amp;quot;name&amp;quot;:&amp;quot;pi&amp;quot;,&amp;quot;namespace&amp;quot;:&amp;quot;default&amp;quot;},&amp;quot;spec&amp;quot;:{&amp;quot;backoffLimit&amp;quot;:4,&amp;quot;template&amp;quot;:...
Parallelism:    1
Completions:    1
Start Time:     Mon, 02 Dec 2019 15:20:11 +0200
Completed At:   Mon, 02 Dec 2019 15:21:16 +0200
Duration:       65s
Pods Statuses:  0 Running / 1 Succeeded / 0 Failed
Pod Template:
  Labels:  controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
           job-name=pi
  Containers:
   pi:
    Image:      perl
    Port:       &amp;lt;none&amp;gt;
    Host Port:  &amp;lt;none&amp;gt;
    Command:
      perl
      -Mbignum=bpi
      -wle
      print bpi(2000)
    Environment:  &amp;lt;none&amp;gt;
    Mounts:       &amp;lt;none&amp;gt;
  Volumes:        &amp;lt;none&amp;gt;
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  14m   job-controller  Created pod: pi-5rwd7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用过命令 &lt;code&gt;kubectl get pods&lt;/code&gt; 查看 Job 中执行完成的 Pod。
以机器可读的模式列举属于 Job 的所有 Pod， 执行以下命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;pods&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;kubectl get pods --selector&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;job-name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;pi --output&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;jsonpath&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{.items[*].metadata.name}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;
echo $pods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;pi-5rwd7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;命令里面的选择器与 Job 的选择器是一样的。 &lt;code&gt;--output=jsonpath&lt;/code&gt; 选项中的表达式用于指定返回列表中只有 Pod 的名称&lt;/p&gt;
&lt;p&gt;查看 Pod 的标准输出:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl logs $pods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出内容类似如下:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
## Writing a Job spec

As with all other Kubernetes config, a Job needs `apiVersion`, `kind`, and `metadata` fields.
Its name must be a valid [DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

A Job also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).
 --&gt;
&lt;h2 id=&#34;编写-job-配置&#34;&gt;编写 Job 配置&lt;/h2&gt;
&lt;p&gt;与其它所有 k8s 配置一样， Job 的必要字段有 &lt;code&gt;apiVersion&lt;/code&gt;, &lt;code&gt;kind&lt;/code&gt;, &lt;code&gt;metadata&lt;/code&gt;。
它的名字必须是一个有效的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Job 还是需要有一个 &lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;&lt;code&gt;.spec&lt;/code&gt;&lt;/a&gt; 配置区域&lt;/p&gt;
&lt;!--
### Pod Template

The `.spec.template` is the only required field of the `.spec`.


The `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;, except it is nested and does not have an `apiVersion` or `kind`.

In addition to required fields for a Pod, a pod template in a Job must specify appropriate
labels (see [pod selector](#pod-selector)) and an appropriate restart policy.

Only a [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Never` or `OnFailure` is allowed.
 --&gt;
&lt;h3 id=&#34;pod-template&#34;&gt;Pod Template&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.template&lt;/code&gt; 是 &lt;code&gt;.spec&lt;/code&gt; 唯一必要字段。
&lt;code&gt;.spec.template&lt;/code&gt; 是一个 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/#pod-templates&#34;&gt;Pod 模板&lt;/a&gt;
除了因为嵌套没有 &lt;code&gt;apiVersion&lt;/code&gt; 或 &lt;code&gt;kind&lt;/code&gt; 字段外， 与 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt; 的配置格式完全一样。&lt;/p&gt;
&lt;p&gt;相对于 Pod 新增的字段是 Job 中的 Pod 模板需要有恰当的标签 (见 &lt;a href=&#34;#pod-selector&#34;&gt;Pod 选择器&lt;/a&gt;) 和重启策略。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy&#34;&gt;&lt;code&gt;RestartPolicy&lt;/code&gt;&lt;/a&gt; 的值只能是 &lt;code&gt;Never&lt;/code&gt; 或 &lt;code&gt;OnFailure&lt;/code&gt;&lt;/p&gt;
&lt;!--  
### Pod selector

The `.spec.selector` field is optional.  In almost all cases you should not specify it.
See section [specifying your own pod selector](#specifying-your-own-pod-selector).
--&gt;
&lt;h3 id=&#34;pod-selector&#34;&gt;Pod 选择器&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.selector&lt;/code&gt; 字段为可选，几乎所有的情景中用户都不应该指定。见 &lt;a href=&#34;#specifying-your-own-pod-selector&#34;&gt;设置自己的 Pod 选择器&lt;/a&gt;&lt;/p&gt;
&lt;!--
### Parallel execution for Jobs

There are three main types of task suitable to run as a Job:

1. Non-parallel Jobs
   - normally, only one Pod is started, unless the Pod fails.
   - the Job is complete as soon as its Pod terminates successfully.
1. Parallel Jobs with a *fixed completion count*:
   - specify a non-zero positive value for `.spec.completions`.
   - the Job represents the overall task, and is complete when there is one successful Pod for each value in the range 1 to `.spec.completions`.
   - **not implemented yet:** Each Pod is passed a different index in the range 1 to `.spec.completions`.
1. Parallel Jobs with a *work queue*:
   - do not specify `.spec.completions`, default to `.spec.parallelism`.
   - the Pods must coordinate amongst themselves or an external service to determine what each should work on. For example, a Pod might fetch a batch of up to N items from the work queue.
   - each Pod is independently capable of determining whether or not all its peers are done, and thus that the entire Job is done.
   - when _any_ Pod from the Job terminates with success, no new Pods are created.
   - once at least one Pod has terminated with success and all Pods are terminated, then the Job is completed with success.
   - once any Pod has exited with success, no other Pod should still be doing any work for this task or writing any output.  They should all be in the process of exiting.

For a _non-parallel_ Job, you can leave both `.spec.completions` and `.spec.parallelism` unset.  When both are
unset, both are defaulted to 1.

For a _fixed completion count_ Job, you should set `.spec.completions` to the number of completions needed.
You can set `.spec.parallelism`, or leave it unset and it will default to 1.

For a _work queue_ Job, you must leave `.spec.completions` unset, and set `.spec.parallelism` to
a non-negative integer.

For more information about how to make use of the different types of job, see the [job patterns](#job-patterns) section.
 --&gt;
&lt;h3 id=&#34;parallel-jobs&#34;&gt;并行执行的&lt;/h3&gt;
&lt;p&gt;以下为三种适合用 Job 跑的任务:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;非并行 Job&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;通常除非挂掉否则只启动一个 Pod&lt;/li&gt;
&lt;li&gt;当 Pod 以成功状态终结则 Job 也同时完成&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;带 &lt;em&gt;固定完成数&lt;/em&gt; 的并行 Job&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;.spec.completions&lt;/code&gt; 值是一个非零正数&lt;/li&gt;
&lt;li&gt;这个 Job 代表任务的总包，每个 Pod 执行成功则在完成则相当于领到 1 到 &lt;code&gt;.spec.completions&lt;/code&gt; 之间的一个号&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;还没有实现的功能:&lt;/strong&gt; 每个 Pod 完成可以传递邮不是 1 到 &lt;code&gt;.spec.completions&lt;/code&gt; 之间的索引&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;带 &lt;em&gt;工作队列&lt;/em&gt; 的并行 Job&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;不要设置 &lt;code&gt;.spec.completions&lt;/code&gt;， 默认使用 &lt;code&gt;.spec.parallelism&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;这些 Pod 需要相互协作或由外部服务来决定每个应该做什么。 比如一个 Pod 可以从工作队列中获取一批 N 个条目。&lt;/li&gt;
&lt;li&gt;每个 Pod 都能独立决定它的协作者是否干完，如果都干完了整个 Job 就完成。&lt;/li&gt;
&lt;li&gt;当 Job 中的 &lt;em&gt;任意&lt;/em&gt; Pod 成功完成并终止，就不会再创建新的 Pod。&lt;/li&gt;
&lt;li&gt;当至少有一个 Pod 成功完成并终止且所以 Pod 被终止，则 Job 成功完成&lt;/li&gt;
&lt;li&gt;当任意 Pod 成功完成并退出， 其它的 Pod 就不能再干任何工作或写任何输出。它们都应该在退出的过程中。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于 &lt;em&gt;非并行&lt;/em&gt; Job， 可以不设置 &lt;code&gt;.spec.completions&lt;/code&gt; 和 &lt;code&gt;.spec.parallelism&lt;/code&gt;， 它们默认都是 1&lt;/p&gt;
&lt;p&gt;对于 &lt;em&gt;固定完成数&lt;/em&gt; 的并行 Job， 用户应该通过设置 &lt;code&gt;.spec.completions&lt;/code&gt; 来指定需要完成的数量。
可以设置 &lt;code&gt;.spec.parallelism&lt;/code&gt;，也可以不设置，让它使用默认的 1&lt;/p&gt;
&lt;p&gt;对于 &lt;em&gt;工作队列&lt;/em&gt; 的并行 Job， 用户必须不要设置 &lt;code&gt;.spec.completions&lt;/code&gt;， 并将  &lt;code&gt;.spec.parallelism&lt;/code&gt; 设置为非负数(0怎么说？)&lt;/p&gt;
&lt;p&gt;For more information about how to make use of the different types of job, see the &lt;a href=&#34;#job-patterns&#34;&gt;job patterns&lt;/a&gt; section.&lt;/p&gt;
&lt;p&gt;更多关于怎么用不同类型的 Job 的信息见 &lt;a href=&#34;#job-patterns&#34;&gt;Job 模式&lt;/a&gt; 小节&lt;/p&gt;
&lt;!--
#### Controlling parallelism

The requested parallelism (`.spec.parallelism`) can be set to any non-negative value.
If it is unspecified, it defaults to 1.
If it is specified as 0, then the Job is effectively paused until it is increased.

Actual parallelism (number of pods running at any instant) may be more or less than requested
parallelism, for a variety of reasons:

- For _fixed completion count_ Jobs, the actual number of pods running in parallel will not exceed the number of
  remaining completions.   Higher values of `.spec.parallelism` are effectively ignored.
- For _work queue_ Jobs, no new Pods are started after any Pod has succeeded -- remaining Pods are allowed to complete, however.
- If the Job &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; has not had time to react.
- If the Job controller failed to create Pods for any reason (lack of `ResourceQuota`, lack of permission, etc.),
  then there may be fewer pods than requested.
- The Job controller may throttle new Pod creation due to excessive previous pod failures in the same Job.
- When a Pod is gracefully shut down, it takes time to stop.
 --&gt;
&lt;h4 id=&#34;并行控制&#34;&gt;并行控制&lt;/h4&gt;
&lt;p&gt;申请并行数量 (&lt;code&gt;.spec.parallelism&lt;/code&gt;) 可以被设置为任意非零的数。
如果没有设置则默认为 1.
如果设置为 0， 则这个 Job 实际上是被暂停，想要恢复需要改大这个值。&lt;/p&gt;
&lt;p&gt;实际运行数量(任意时刻运行的 Pod 数量)可能会比请求的数量或多或少差一些， 可能的原因有:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于 &lt;em&gt;固定完成数&lt;/em&gt; Job， 实际并行运行的 Pod 数量不会超出未完成的数量。 设置更大的 &lt;code&gt;.spec.parallelism&lt;/code&gt; 值实际是被忽略的。&lt;/li&gt;
&lt;li&gt;对于 &lt;em&gt;工作队列&lt;/em&gt; Job， 在任意 Pod 成功后就不会再创建新的 Pod &amp;ndash; 但是，剩下的 Pod 还是允许继续完成&lt;/li&gt;
&lt;li&gt;如果 Job &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; 没有反应过来。&lt;/li&gt;
&lt;li&gt;如果 Job 控制器因为某些原因(&lt;code&gt;ResourceQuota&lt;/code&gt; 资源配额不足，权限不足，等)，可以会比请求数量要少。&lt;/li&gt;
&lt;li&gt;Job 控制器可能会因为同一个 Job 之前的 Pod 的故障情况来限制新 Pod 的创建。&lt;/li&gt;
&lt;li&gt;当一个 Pod 是平滑关闭的时候，需要时间来停止。&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Handling Pod and container failures

A container in a Pod may fail for a number of reasons, such as because the process in it exited with
a non-zero exit code, or the container was killed for exceeding a memory limit, etc.  If this
happens, and the `.spec.template.spec.restartPolicy = &#34;OnFailure&#34;`, then the Pod stays
on the node, but the container is re-run.  Therefore, your program needs to handle the case when it is
restarted locally, or else specify `.spec.template.spec.restartPolicy = &#34;Never&#34;`.
See [pod lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/#example-states) for more information on `restartPolicy`.

An entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the node
(node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the
`.spec.template.spec.restartPolicy = &#34;Never&#34;`.  When a Pod fails, then the Job controller
starts a new Pod.  This means that your application needs to handle the case when it is restarted in a new
pod.  In particular, it needs to handle temporary files, locks, incomplete output and the like
caused by previous runs.

Note that even if you specify `.spec.parallelism = 1` and `.spec.completions = 1` and
`.spec.template.spec.restartPolicy = &#34;Never&#34;`, the same program may
sometimes be started twice.

If you do specify `.spec.parallelism` and `.spec.completions` both greater than 1, then there may be
multiple pods running at once.  Therefore, your pods must also be tolerant of concurrency.
 --&gt;
&lt;h2 id=&#34;如何应对-pod-和容器的故障&#34;&gt;如何应对 Pod 和容器的故障&lt;/h2&gt;
&lt;p&gt;Pod 中的容器可能因为几种原因失败， 例如因为容器内的进程以非 0 的返回码退出， 或容器因为超出内存限制而退出，等。
如果发生了这些情况，并且 &lt;code&gt;.spec.template.spec.restartPolicy = &amp;quot;OnFailure&amp;quot;&lt;/code&gt;， 这个 Pod 会留在原有的节点上，但容器会重启。
因此，用户的应用程序需要通过应对这种在本地的重新启动，或都设置 &lt;code&gt;.spec.template.spec.restartPolicy = &amp;quot;Never&amp;quot;&lt;/code&gt;
更多关于重启策略(&lt;code&gt;restartPolicy&lt;/code&gt;)的信息见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/#example-states&#34;&gt;Pod 生命周期&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;整个 Pod 也可能整个挂掉，也可能有几种原因， 例如当 Pod 被从节点上踢出(节点在更新，重启，删除，等)， 或者 Pod 中的容器挂了而
&lt;code&gt;.spec.template.spec.restartPolicy = &amp;quot;Never&amp;quot;&lt;/code&gt;。 当一个 Pod 挂掉，Job 控制器会重启一个新的 Pod。 也就是说用户的应用能够
处理这种在新 Pod 重启的情况。特别是要能够处理临时文件，锁，未完成的输出和与前一次运行相似的原因(这意思不太明白?)&lt;/p&gt;
&lt;p&gt;还要注意即便设置了  &lt;code&gt;.spec.parallelism = 1&lt;/code&gt; 和 &lt;code&gt;.spec.completions = 1&lt;/code&gt; 且
&lt;code&gt;.spec.template.spec.restartPolicy = &amp;quot;Never&amp;quot;&lt;/code&gt;， 同一个程序也有时候可能会启动两次。&lt;/p&gt;
&lt;p&gt;如果用户设置 &lt;code&gt;.spec.parallelism&lt;/code&gt; 和 &lt;code&gt;.spec.completions&lt;/code&gt; 的值都大于 1，那么就可能同时启动并运行多个 Pod，
因此要保证这些 Pod 是能够处理并发的。&lt;/p&gt;
&lt;!--
### Pod backoff failure policy

There are situations where you want to fail a Job after some amount of retries
due to a logical error in configuration etc.
To do so, set `.spec.backoffLimit` to specify the number of retries before
considering a Job as failed. The back-off limit is set by default to 6. Failed
Pods associated with the Job are recreated by the Job controller with an
exponential back-off delay (10s, 20s, 40s ...) capped at six minutes. The
back-off count is reset when a Job&#39;s Pod is deleted or successful without any
other Pods for the Job failing around that time.

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; If your job has &lt;code&gt;restartPolicy = &amp;quot;OnFailure&amp;quot;&lt;/code&gt;, keep in mind that your container running the Job
will be terminated once the job backoff limit has been reached. This can make debugging the Job&amp;rsquo;s executable more difficult. We suggest setting
&lt;code&gt;restartPolicy = &amp;quot;Never&amp;quot;&lt;/code&gt; when debugging the Job or using a logging system to ensure output
from failed Jobs is not lost inadvertently.&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h3 id=&#34;pod-失效补尝策略&#34;&gt;Pod 失效补尝策略&lt;/h3&gt;
&lt;p&gt;在有些情况下，用户期望因为配置中的一些逻辑错误在进行一定次数的重试后，让一个 Job 失败。
要达到这个目的，将 &lt;code&gt;.spec.backoffLimit&lt;/code&gt; 设置为认为失败前重试的次数。 这个补尝数默认为 6.
Job 相关的 Pod 在挂掉以后， Job 控制器会以指数延迟(10s, 20s, 40s &amp;hellip;)最长至六分钟来尝试重建 Pod.
补尝计数在 Job 的 Pod 被删除或该 Job 的其它 Pod 都没有出毛病时被重置&lt;/p&gt;
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果 Job 配置有 &lt;code&gt;restartPolicy = &amp;quot;OnFailure&amp;quot;&lt;/code&gt;， 就要注意到这个 Job 中运行的容器会在达到补尝上限后终止。
这会使得调度 Job 内部的程序更麻烦。 我们建议在调度 Job 使用配置 &lt;code&gt;restartPolicy = &amp;quot;Never&amp;quot;&lt;/code&gt; 或者使用日志系统
以保证失败的 Job 的日志不会无意间丢失。&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Job termination and cleanup

When a Job completes, no more Pods are created, but the Pods are not deleted either.  Keeping them around
allows you to still view the logs of completed pods to check for errors, warnings, or other diagnostic output.
The job object also remains after it is completed so that you can view its status.  It is up to the user to delete
old jobs after noting their status.  Delete the job with `kubectl` (e.g. `kubectl delete jobs/pi` or `kubectl delete -f ./job.yaml`). When you delete the job using `kubectl`, all the pods it created are deleted too.

By default, a Job will run uninterrupted unless a Pod fails (`restartPolicy=Never`) or a Container exits in error (`restartPolicy=OnFailure`), at which point the Job defers to the
`.spec.backoffLimit` described above. Once `.spec.backoffLimit` has been reached the Job will be marked as failed and any running Pods will be terminated.

Another way to terminate a Job is by setting an active deadline.
Do this by setting the `.spec.activeDeadlineSeconds` field of the Job to a number of seconds.
The `activeDeadlineSeconds` applies to the duration of the job, no matter how many Pods are created.
Once a Job reaches `activeDeadlineSeconds`, all of its running Pods are terminated and the Job status will become `type: Failed` with `reason: DeadlineExceeded`.

Note that a Job&#39;s `.spec.activeDeadlineSeconds` takes precedence over its `.spec.backoffLimit`. Therefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once it reaches the time limit specified by `activeDeadlineSeconds`, even if the `backoffLimit` is not yet reached.

Example:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi-with-timeout
spec:
  backoffLimit: 5
  activeDeadlineSeconds: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: [&#34;perl&#34;,  &#34;-Mbignum=bpi&#34;, &#34;-wle&#34;, &#34;print bpi(2000)&#34;]
      restartPolicy: Never
```

Note that both the Job spec and the [Pod template spec](/docs/concepts/workloads/pods/init-containers/#detailed-behavior) within the Job have an `activeDeadlineSeconds` field. Ensure that you set this field at the proper level.

Keep in mind that the `restartPolicy` applies to the Pod, and not to the Job itself: there is no automatic Job restart once the Job status is `type: Failed`.
That is, the Job termination mechanisms activated with `.spec.activeDeadlineSeconds` and `.spec.backoffLimit` result in a permanent Job failure that requires manual intervention to resolve.
 --&gt;
&lt;h2 id=&#34;job-终止与清理&#34;&gt;Job 终止与清理&lt;/h2&gt;
&lt;p&gt;当一个 Job 完成后，就不会再创建新的 Pod， 但现有的 Pod 也不会删除。 保留这个 Pod 的目的是为了让用户能够
在 Job 过成后还能够查看 Pod 的日志，以方便检查错误，警告或其它诊断输出。 Job 对象本身也会在完成后继续保底以方便用户能够查看它的状态。
由用户决定在查看状态后是否删除这些 Job。 可以通过 &lt;code&gt;kubectl&lt;/code&gt; (如. &lt;code&gt;kubectl delete jobs/pi&lt;/code&gt; 或 &lt;code&gt;kubectl delete -f ./job.yaml&lt;/code&gt;) 删除 Job。
当用户使用 &lt;code&gt;kubectl&lt;/code&gt; 删除 Job 后， 它所创建的所有的 Pod 也一起被删除了。&lt;/p&gt;
&lt;p&gt;默认情况下， 一个 Job 是连续运行的， 但当一个 Pod 挂了(&lt;code&gt;restartPolicy=Never&lt;/code&gt;) 或一个容器存在错误(&lt;code&gt;restartPolicy=OnFailure&lt;/code&gt;)
在这些情况下 Job 就会进行上面介经的补尝 &lt;code&gt;.spec.backoffLimit&lt;/code&gt; 。 当 &lt;code&gt;.spec.backoffLimit&lt;/code&gt; 上限达到后， Job 就会被标记为失败，其它仍在运行的 Pod 也会被终止。&lt;/p&gt;
&lt;p&gt;另一个终止 Job 的办法是活跃死线(active deadline), 通过 Job 的 &lt;code&gt;.spec.activeDeadlineSeconds&lt;/code&gt; 设置秒数。
&lt;code&gt;activeDeadlineSeconds&lt;/code&gt; 表示 Job 的持续时间， 无论创建了多少个 Pod。当 Job 达到 &lt;code&gt;activeDeadlineSeconds&lt;/code&gt; 时，
它所以正在运行的 Pod都会被终止，Job 的状态会进入 &lt;code&gt;type: Failed&lt;/code&gt;， &lt;code&gt;reason: DeadlineExceeded&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;注意 Job 的 &lt;code&gt;.spec.activeDeadlineSeconds&lt;/code&gt; 优先级比它的 &lt;code&gt;.spec.backoffLimit&lt;/code&gt; 高。 因此，如果 Job 在 Pod 失败后尝试一两次后
如果达到 &lt;code&gt;activeDeadlineSeconds&lt;/code&gt; 设置的时间就会再部署额外的 Pod 了， 即便 &lt;code&gt;backoffLimit&lt;/code&gt; 还没有达到。&lt;/p&gt;
&lt;p&gt;示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;batch/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Job&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pi-with-timeout&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;backoffLimit&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;activeDeadlineSeconds&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pi&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;perl&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;perl&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-Mbignum=bpi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-wle&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;print bpi(2000)&amp;#34;&lt;/span&gt;]
      &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意 Job 本身的配置和 Job 内的 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/init-containers/#detailed-behavior&#34;&gt;Pod 模板配置&lt;/a&gt;
都有 &lt;code&gt;activeDeadlineSeconds&lt;/code&gt; 字段， 要注意设置的是哪个。&lt;/p&gt;
&lt;p&gt;还要注意重启策略(&lt;code&gt;restartPolicy&lt;/code&gt;) 是应用在 Pod 上的，而不是在 Job 上面: 当 Job 状态是 &lt;code&gt;type: Failed&lt;/code&gt; 时是没有怎么重启的。
也就是当 Job 被 &lt;code&gt;.spec.activeDeadlineSeconds&lt;/code&gt; 和 &lt;code&gt;.spec.backoffLimit&lt;/code&gt; 机制触发了之后， Job 就会进行永久的失败状态，需要人工介入才能解决。&lt;/p&gt;
&lt;!--
## Clean up finished jobs automatically

Finished Jobs are usually no longer needed in the system. Keeping them around in
the system will put pressure on the API server. If the Jobs are managed directly
by a higher level controller, such as
[CronJobs](/docs/concepts/workloads/controllers/cron-jobs/), the Jobs can be
cleaned up by CronJobs based on the specified capacity-based cleanup policy.
 --&gt;
&lt;h2 id=&#34;自动清理已经完成的-job&#34;&gt;自动清理已经完成的 Job&lt;/h2&gt;
&lt;p&gt;完成的 Job 通常不需要再留在系统中。 把它们留在系统中会增加 api-server 的压力。 如果 Job 是由
更高级的控制器，如 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/cron-jobs/&#34;&gt;CronJobs&lt;/a&gt;,
Job 可以被 CronJob 基于指定容量的清理策略来清除。&lt;/p&gt;
&lt;!--
### TTL mechanism for finished Jobs






&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.12 [alpha]&lt;/code&gt;
&lt;/div&gt;



Another way to clean up finished Jobs (either `Complete` or `Failed`)
automatically is to use a TTL mechanism provided by a
[TTL controller](/docs/concepts/workloads/controllers/ttlafterfinished/) for
finished resources, by specifying the `.spec.ttlSecondsAfterFinished` field of
the Job.

When the TTL controller cleans up the Job, it will delete the Job cascadingly,
i.e. delete its dependent objects, such as Pods, together with the Job. Note
that when the Job is deleted, its lifecycle guarantees, such as finalizers, will
be honored.

For example:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi-with-ttl
spec:
  ttlSecondsAfterFinished: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: [&#34;perl&#34;,  &#34;-Mbignum=bpi&#34;, &#34;-wle&#34;, &#34;print bpi(2000)&#34;]
      restartPolicy: Never
```

The Job `pi-with-ttl` will be eligible to be automatically deleted, `100`
seconds after it finishes.

If the field is set to `0`, the Job will be eligible to be automatically deleted
immediately after it finishes. If the field is unset, this Job won&#39;t be cleaned
up by the TTL controller after it finishes.

Note that this TTL mechanism is alpha, with feature gate `TTLAfterFinished`. For
more information, see the documentation for
[TTL controller](/docs/concepts/workloads/controllers/ttlafterfinished/) for
finished resources.
 --&gt;
&lt;h3 id=&#34;对于已完成的-job-的-ttl-机制&#34;&gt;对于已完成的 Job 的 TTL 机制&lt;/h3&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.12 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;另一种自动清理完成的 Job (无论是 完成(&lt;code&gt;Complete&lt;/code&gt;) 或失败(&lt;code&gt;Failed&lt;/code&gt;) 的)方式是使用 TTL 机制，
它由 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/ttlafterfinished/&#34;&gt;TTL 控制器&lt;/a&gt;提供用于
清理完成的资源，通过在 Job 上配置 &lt;code&gt;.spec.ttlSecondsAfterFinished&lt;/code&gt; 字段实现。&lt;/p&gt;
&lt;p&gt;当 TTL 控制器清理 Job 时，它会级联地删除 Job, 例如， 它依赖的对象，如 Pod 会与 Job 一起删除
注意当 Job 被删除后，它的生命周期保证对象如解构器也会被触发。(猜的)
示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;batch/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Job&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pi-with-ttl&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;ttlSecondsAfterFinished&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;pi&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;perl&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;command&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;perl&amp;#34;&lt;/span&gt;,  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-Mbignum=bpi&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-wle&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;print bpi(2000)&amp;#34;&lt;/span&gt;]
      &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Never&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个叫 &lt;code&gt;pi-with-ttl&lt;/code&gt; Job 当它结束 &lt;code&gt;100&lt;/code&gt; 秒后被自动删除。&lt;/p&gt;
&lt;p&gt;这个这个字段的值设置为 &lt;code&gt;0&lt;/code&gt;， 则这个 Job 会在执行完成之后马上就被自动删除了。 如果没有设置这个字段
则这个 Job 在结束后不会被 TTL 控制器清除。&lt;/p&gt;
&lt;p&gt;要注意 这个 TTL 机器还在 &lt;code&gt;alpha&lt;/code&gt; 状态， 需要使用 &lt;code&gt;TTLAfterFinished&lt;/code&gt; 功能阀启用。
更多相关信息见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/ttlafterfinished/&#34;&gt;TTL 控制器&lt;/a&gt;&lt;/p&gt;
&lt;!--
## Job patterns

The Job object can be used to support reliable parallel execution of Pods.  The Job object is not
designed to support closely-communicating parallel processes, as commonly found in scientific
computing.  It does support parallel processing of a set of independent but related *work items*.
These might be emails to be sent, frames to be rendered, files to be transcoded, ranges of keys in a
NoSQL database to scan, and so on.

In a complex system, there may be multiple different sets of work items.  Here we are just
considering one set of work items that the user wants to manage together &amp;mdash; a *batch job*.

There are several different patterns for parallel computation, each with strengths and weaknesses.
The tradeoffs are:

- One Job object for each work item, vs. a single Job object for all work items.  The latter is
  better for large numbers of work items.  The former creates some overhead for the user and for the
  system to manage large numbers of Job objects.
- Number of pods created equals number of work items, vs. each Pod can process multiple work items.
  The former typically requires less modification to existing code and containers.  The latter
  is better for large numbers of work items, for similar reasons to the previous bullet.
- Several approaches use a work queue.  This requires running a queue service,
  and modifications to the existing program or container to make it use the work queue.
  Other approaches are easier to adapt to an existing containerised application.


The tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs.
The pattern names are also links to examples and more detailed description.

|                            Pattern                                   | Single Job object | Fewer pods than work items? | Use app unmodified? |  Works in Kube 1.1? |
| -------------------------------------------------------------------- |:-----------------:|:---------------------------:|:-------------------:|:-------------------:|
| [Job Template Expansion](/docs/tasks/job/parallel-processing-expansion/)            |                   |                             |          ✓          |          ✓          |
| [Queue with Pod Per Work Item](/docs/tasks/job/coarse-parallel-processing-work-queue/)   |         ✓         |                             |      sometimes      |          ✓          |
| [Queue with Variable Pod Count](/docs/tasks/job/fine-parallel-processing-work-queue/)  |         ✓         |             ✓               |                     |          ✓          |
| Single Job with Static Work Assignment                               |         ✓         |                             |          ✓          |                     |

When you specify completions with `.spec.completions`, each Pod created by the Job controller
has an identical [`spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).  This means that
all pods for a task will have the same command line and the same
image, the same volumes, and (almost) the same environment variables.  These patterns
are different ways to arrange for pods to work on different things.

This table shows the required settings for `.spec.parallelism` and `.spec.completions` for each of the patterns.
Here, `W` is the number of work items.

|                             Pattern                                  | `.spec.completions` |  `.spec.parallelism` |
| -------------------------------------------------------------------- |:-------------------:|:--------------------:|
| [Job Template Expansion](/docs/tasks/job/parallel-processing-expansion/)           |          1          |     should be 1      |
| [Queue with Pod Per Work Item](/docs/tasks/job/coarse-parallel-processing-work-queue/)   |          W          |        any           |
| [Queue with Variable Pod Count](/docs/tasks/job/fine-parallel-processing-work-queue/)  |          1          |        any           |
| Single Job with Static Work Assignment                               |          W          |        any           |
 --&gt;
&lt;h2 id=&#34;job-的形式&#34;&gt;Job 的形式&lt;/h2&gt;
&lt;p&gt;Job 对象可用于支持可靠的并行 Pod。 Job 对象在设计上不是用来支持紧密通信的并行处理，这种场景常见于科学计算。
Job 支持的是并行处理一组相互独立但有关系的 &lt;em&gt;工作内容&lt;/em&gt;。 它们可能是发送电子邮件，需要渲染的帧，文件转码，NoSQL 数据库中需要被扫描的键，等等。&lt;/p&gt;
&lt;p&gt;在一个复杂的系统中，可能会有多个不同的工作项集合。 这里我们只考虑一个用户想要一起管理的工作项集群 — 一个 &lt;em&gt;批量任务&lt;/em&gt;，
对于并行计算有几种不同的模式，每一种有其优势和劣势。需要做出以下权衡:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;一个 Job 对象对一个工作项， vs. 一个 Job 对象对所有的工作项。 后一种更适合于工作项比较多的情况。
前一种对于用户来说有更多额外的创建工作和对系统来说要管理大量的 Job 对象。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;创建 Pod 的数量与工作项的数量相同， vs. 每个 Pod 可以处理多个工作项。
前者通常需要对现有代码和容器做出少量更改。后台适用于数量很大的工作项，与上一个条目的原因类似&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;以不同的方式来使用队列。 需要运行一个队列服务，还需要修改现有代码或容器使其能够使用工作队列。
前面介绍的方式更容易用在已经存在的容器化应用上。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上因素权衡总结如下， 包含上面提到的 4 种情况，分 2 列。
以下模式的名称有对象实例的连接地址，其中有更多详细说明。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Pattern&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Single Job object&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Fewer pods than work items?&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Use app unmodified?&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Works in Kube 1.1?&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/job/parallel-processing-expansion/&#34;&gt;Job Template Expansion&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/job/coarse-parallel-processing-work-queue/&#34;&gt;有队列，每个工作项一个 Pod&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;有时候&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/job/fine-parallel-processing-work-queue/&#34;&gt;Queue with Variable Pod Count&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;带静态任务分配的单个 Job&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;✓&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;当用户设置完成数(&lt;code&gt;.spec.completions&lt;/code&gt;)时， Job 控制器创建的每一个 Pod 都拥有完全相同的 &lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;&lt;code&gt;spec&lt;/code&gt;&lt;/a&gt;。 也就是说这个任务所有的 Pod 都拥有相同的命令，相同的镜像，
相同的数据卷， (几乎)相同的环境变量。 这些模式是以不同的方式组织 Pod 以应对不同的工作内容。&lt;/p&gt;
&lt;p&gt;以下表格展示每种模式所需要设置的 &lt;code&gt;.spec.parallelism&lt;/code&gt; 和 &lt;code&gt;.spec.completions&lt;/code&gt; 字段的值。
其中， &lt;code&gt;W&lt;/code&gt; 是工作项的数量&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Pattern&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;code&gt;.spec.completions&lt;/code&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;code&gt;.spec.parallelism&lt;/code&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/job/parallel-processing-expansion/&#34;&gt;Job Template Expansion&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;should be 1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/job/coarse-parallel-processing-work-queue/&#34;&gt;有队列，每个工作项一个 Pod&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;W&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;any&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/tasks/job/fine-parallel-processing-work-queue/&#34;&gt;Queue with Variable Pod Count&lt;/a&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;any&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;带静态任务分配的单个 Job&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;W&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;any&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--
## Advanced usage

### Specifying your own Pod selector

Normally, when you create a Job object, you do not specify `.spec.selector`.
The system defaulting logic adds this field when the Job is created.
It picks a selector value that will not overlap with any other jobs.

However, in some cases, you might need to override this automatically set selector.
To do this, you can specify the `.spec.selector` of the Job.

Be very careful when doing this.  If you specify a label selector which is not
unique to the pods of that Job, and which matches unrelated Pods, then pods of the unrelated
job may be deleted, or this Job may count other Pods as completing it, or one or both
Jobs may refuse to create Pods or run to completion.  If a non-unique selector is
chosen, then other controllers (e.g. ReplicationController) and their Pods may behave
in unpredictable ways too.  Kubernetes will not stop you from making a mistake when
specifying `.spec.selector`.

Here is an example of a case when you might want to use this feature.

Say Job `old` is already running.  You want existing Pods
to keep running, but you want the rest of the Pods it creates
to use a different pod template and for the Job to have a new name.
You cannot update the Job because these fields are not updatable.
Therefore, you delete Job `old` but _leave its pods
running_, using `kubectl delete jobs/old --cascade=false`.
Before deleting it, you make a note of what selector it uses:

```
kubectl get job old -o yaml
```
```
kind: Job
metadata:
  name: old
  ...
spec:
  selector:
    matchLabels:
      controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
```

Then you create a new Job with name `new` and you explicitly specify the same selector.
Since the existing Pods have label `controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002`,
they are controlled by Job `new` as well.

You need to specify `manualSelector: true` in the new Job since you are not using
the selector that the system normally generates for you automatically.

```
kind: Job
metadata:
  name: new
  ...
spec:
  manualSelector: true
  selector:
    matchLabels:
      controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
```

The new Job itself will have a different uid from `a8f3d00d-c6d2-11e5-9f87-42010af00002`.  Setting
`manualSelector: true` tells the system to that you know what you are doing and to allow this
mismatch.
 --&gt;
&lt;h2 id=&#34;高级用法&#34;&gt;高级用法&lt;/h2&gt;
&lt;h3 id=&#34;specifying-your-own-pod-selector&#34;&gt;设置自定义 Pod 选择器&lt;/h3&gt;
&lt;p&gt;通常，当用户创始 Job 对象时，不需要配置 &lt;code&gt;.spec.selector&lt;/code&gt;. 系统默认逻辑会在 Job 创建时添加该字段。
它会选择一个与其它 Job 对象不重叠的值设置在上面。&lt;/p&gt;
&lt;p&gt;但是，有些时候，可能需要用户手动指定 &lt;code&gt;.spec.selector&lt;/code&gt; 来覆盖掉这种默认设置的选择器。&lt;/p&gt;
&lt;p&gt;这么做的时候要千万小心。 如果用户设置的选择器不止匹配到当前 Job 的 Pod，这些不相关的 Pod 可能就会被删除,
或者这个 Job 就会批别个 Job 完成的 Pod 认为是自己的，甚至其中一个或两个 Job 都不能创建 Pod 或 成功运行完成。
如果使用了一个非唯一的选择器，其它的控制器(如 ReplicationController) 和它们的 Pod 也可以出现不可预知的行为。
在设置 &lt;code&gt;.spec.selector&lt;/code&gt; 时， k8s 不会阻止你去犯错误。&lt;/p&gt;
&lt;p&gt;以下是一个用到该性场景的示例。&lt;/p&gt;
&lt;p&gt;假设有一个已经在运行的 Job 名字叫 &lt;code&gt;old&lt;/code&gt;。 用户想要让现有的 Pod 继续运行， 但余下的 Pod 需要以新的 Job 名称和新的 Pod 模板创建。
但是 Job 不能更新，因为这些字段不能更新。因此用户需要删除叫 &lt;code&gt;old&lt;/code&gt; 的 Job, 但是要 &lt;em&gt;保留它在运行的 Pod&lt;/em&gt;.
使用 &lt;code&gt;kubectl delete jobs/old --cascade=false&lt;/code&gt; 命令删除 Job前，需要先看看它使用的选择器:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get job old -o yaml
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;kind: Job
metadata:
  name: old
  ...
spec:
  selector:
    matchLabels:
      controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这时候创建一个叫 &lt;code&gt;new&lt;/code&gt; 的新 Job 并显示的设置相同的选择器。
因为已经存在的 Pod 都有标签 &lt;code&gt;controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002&lt;/code&gt;，
所以它们会被 &lt;code&gt;new&lt;/code&gt; Job 管理&lt;/p&gt;
&lt;p&gt;要使用自定义的选择器而不是系统自动创建的，需要在新 Job 上配置 &lt;code&gt;manualSelector: true&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kind: Job
metadata:
  name: new
  ...
spec:
  manualSelector: true
  selector:
    matchLabels:
      controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这个新的 Job 会拥有一个与 &lt;code&gt;a8f3d00d-c6d2-11e5-9f87-42010af00002&lt;/code&gt; 不同的 UID。
设置 &lt;code&gt;manualSelector: true&lt;/code&gt; 就是告诉系统你知道自己在做什么，这个不匹配是允许的。&lt;/p&gt;
&lt;!--
## Alternatives

### Bare Pods

When the node that a Pod is running on reboots or fails, the pod is terminated
and will not be restarted.  However, a Job will create new Pods to replace terminated ones.
For this reason, we recommend that you use a Job rather than a bare Pod, even if your application
requires only a single Pod.

### Replication Controller

Jobs are complementary to [Replication Controllers](/docs/concepts/workloads/controllers/replicationcontroller/).
A Replication Controller manages Pods which are not expected to terminate (e.g. web servers), and a Job
manages Pods that are expected to terminate (e.g. batch tasks).

As discussed in [Pod Lifecycle](/docs/concepts/workloads/pods/pod-lifecycle/), `Job` is *only* appropriate
for pods with `RestartPolicy` equal to `OnFailure` or `Never`.
(Note: If `RestartPolicy` is not set, the default value is `Always`.)

### Single Job starts controller Pod

Another pattern is for a single Job to create a Pod which then creates other Pods, acting as a sort
of custom controller for those Pods.  This allows the most flexibility, but may be somewhat
complicated to get started with and offers less integration with Kubernetes.

One example of this pattern would be a Job which starts a Pod which runs a script that in turn
starts a Spark master controller (see [spark example](https://github.com/kubernetes/examples/tree/master/staging/spark/README.md)), runs a spark
driver, and then cleans up.

An advantage of this approach is that the overall process gets the completion guarantee of a Job
object, but maintains complete control over what Pods are created and how work is assigned to them.
 --&gt;
&lt;h2 id=&#34;替代方案&#34;&gt;替代方案&lt;/h2&gt;
&lt;h3 id=&#34;裸-pod&#34;&gt;裸 Pod&lt;/h3&gt;
&lt;p&gt;当 Pod 所在的节点重启或挂掉时， Pod 就会终止并不会被重启。 但 Job 会创建新的 Job 来代替被终止掉的。
因为这个原因， 我们推荐用户使用 Job 而不是裸 Pod， 即使该应用只需要一个 Pod。&lt;/p&gt;
&lt;h3 id=&#34;replicationcontroller&#34;&gt;&lt;code&gt;ReplicationController&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Job 与 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/replicationcontroller/&#34;&gt;ReplicationController&lt;/a&gt;是互补的.
ReplicationController 管理那些不期望被终止的 Pod (如 web 服务)，
Job 管理那些预期要终止的 Pod (如 批量任务)&lt;/p&gt;
&lt;p&gt;就如在 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/&#34;&gt;Pod 生命周期&lt;/a&gt;中讨论的，
&lt;code&gt;Job&lt;/code&gt; 是唯一适合将 Pod 重启策略(&lt;code&gt;RestartPolicy&lt;/code&gt;) 设置为 在失败时(&lt;code&gt;OnFailure&lt;/code&gt;) 或 永不(&lt;code&gt;Never&lt;/code&gt;) (重启)的。
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 如果 &lt;code&gt;RestartPolicy&lt;/code&gt; 没有设置，则默认值为 总是(&lt;code&gt;Always&lt;/code&gt;)重启&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;h3 id=&#34;单个-job-启动作为控制器的-pod&#34;&gt;单个 Job 启动作为控制器的 Pod&lt;/h3&gt;
&lt;p&gt;另一种模式为单个 Job 创建一个 Pod， 这个 Pod 再创建其它的 Pod，这个 Pod 对于其它的 Pod 来说就表现为一个自定义控制器。
这种方式允许最大的灵活性， 但可能入门有点复杂并且与k8s 集成比较少。&lt;/p&gt;
&lt;p&gt;使用这个模式的一个示例为一个 Job 创建了一个 Pod， 这个 Pod 内运行了一个脚本启动一个 Spark 主控制器
(见 &lt;a href=&#34;https://github.com/kubernetes/examples/tree/master/staging/spark/README.md&#34;&gt;spark 示例&lt;/a&gt;)
运行一个 spark driver， 然后清理现场。&lt;/p&gt;
&lt;p&gt;这种方式是的一个好处是由一个 Job 对象来保证所以进程的完成， 只需要维护完成控制不震怒近 哪些 Pod 要创建，怎么给它们分配工作&lt;/p&gt;
&lt;h2 id=&#34;cron-jobs&#34;&gt;&lt;code&gt;CronJob&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;用户可以使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/cron-jobs/&#34;&gt;&lt;code&gt;CronJob&lt;/code&gt;&lt;/a&gt; 来创建一个在指定 时间/日期 运行的 Job,
与 Unix 的 &lt;code&gt;cron&lt;/code&gt; 类似。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 垃圾回收</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/garbage-collection/</link>
      <pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/garbage-collection/</guid>
      <description>
        
        
        &lt;!--  
---
title: Garbage Collection
content_type: concept
weight: 60
---
--&gt;
&lt;!-- overview --&gt;
&lt;p&gt;在 k8s 中垃圾回收的作用就是删除那些曾经有所有者，现在没有的对象。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## Owners and dependents

Some Kubernetes objects are owners of other objects. For example, a ReplicaSet
is the owner of a set of Pods. The owned objects are called *dependents* of the
owner object. Every dependent object has a `metadata.ownerReferences` field that
points to the owning object.

Sometimes, Kubernetes sets the value of `ownerReference` automatically. For
example, when you create a ReplicaSet, Kubernetes automatically sets the
`ownerReference` field of each Pod in the ReplicaSet. In 1.8, Kubernetes
automatically sets the value of `ownerReference` for objects created or adopted
by ReplicationController, ReplicaSet, StatefulSet, DaemonSet, Deployment, Job
and CronJob.

You can also specify relationships between owners and dependents by manually
setting the `ownerReference` field.

Here&#39;s a configuration file for a ReplicaSet that has three Pods:



 













&lt;table class=&#34;includecode&#34; id=&#34;controllersreplicasetyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/replicaset.yaml&#34; download=&#34;controllers/replicaset.yaml&#34;&gt;
                    &lt;code&gt;controllers/replicaset.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersreplicasetyaml&#39;)&#34; title=&#34;Copy controllers/replicaset.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicaSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-repset&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;pod-is-for&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;garbage-collection-example&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;pod-is-for&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;garbage-collection-example&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



If you create the ReplicaSet and then view the Pod metadata, you can see
OwnerReferences field:

```shell
kubectl apply -f https://k8s.io/examples/controllers/replicaset.yaml
kubectl get pods --output=yaml
```

The output shows that the Pod owner is a ReplicaSet named `my-repset`:

```yaml
apiVersion: v1
kind: Pod
metadata:
  ...
  ownerReferences:
  - apiVersion: apps/v1
    controller: true
    blockOwnerDeletion: true
    kind: ReplicaSet
    name: my-repset
    uid: d9607e19-f88f-11e6-a518-42010a800195
  ...
```

&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;Cross-namespace owner references are disallowed by design. This means:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Namespace-scoped dependents can only specify owners in the same namespace,
and owners that are cluster-scoped.&lt;/li&gt;
&lt;li&gt;Cluster-scoped dependents can only specify cluster-scoped owners, but not
namespace-scoped owners.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

 --&gt;
&lt;h2 id=&#34;所有者和从属者&#34;&gt;所有者和从属者&lt;/h2&gt;
&lt;p&gt;有些 k8s 对象是其它对象的所有者。 如，一个 ReplicaSet 就是一个 Pod 集合的所有者。
那些被拥有的对象就叫所有者的 &lt;em&gt;从属者(dependents)&lt;/em&gt;， 每个从属者都有一个 &lt;code&gt;metadata.ownerReferences&lt;/code&gt;
字段指向它的所有者。&lt;/p&gt;
&lt;p&gt;有时候， k8s 会自动添加 &lt;code&gt;ownerReference&lt;/code&gt; 的值。 如，当创建一个 ReplicaSet 时， k8s 自动
为这个 ReplicaSet 所属的 Pod 设置 &lt;code&gt;ownerReference&lt;/code&gt;。 在 &lt;code&gt;1.8&lt;/code&gt; 中， k8s 为创建或捕获对象
自动设置 &lt;code&gt;ownerReference&lt;/code&gt;值的对象有 ReplicationController, ReplicaSet, StatefulSet,
DaemonSet, Deployment, Job, CronJob.&lt;/p&gt;
&lt;p&gt;用户可以通过设置 &lt;code&gt;ownerReference&lt;/code&gt; 字段来手动指定所有者和从属者之间的关系
以下为一个包含 3 个 Pod 的 ReplicaSet 的配置文件:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;controllersreplicasetyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/replicaset.yaml&#34; download=&#34;controllers/replicaset.yaml&#34;&gt;
                    &lt;code&gt;controllers/replicaset.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersreplicasetyaml&#39;)&#34; title=&#34;Copy controllers/replicaset.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicaSet&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-repset&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;matchLabels&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;pod-is-for&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;garbage-collection-example&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;pod-is-for&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;garbage-collection-example&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;如果用户创建了 ReplicaSet 然后查看所属 Pod 的元数据(metadata), 就可以看到 &lt;code&gt;OwnerReferences&lt;/code&gt; 字段&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f replicaset.yaml
kubectl get pods --output&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从输出结果可以看到 Pod 的所有者是一个叫 &lt;code&gt;my-repset&lt;/code&gt; 的 ReplicaSet&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Pod&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ownerReferences&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;apps/v1&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;controller&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;blockOwnerDeletion&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicaSet&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;my-repset&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;uid&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;d9607e19-f88f-11e6-a518-42010a800195&lt;/span&gt;
  &lt;span style=&#34;color:#ae81ff&#34;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; &lt;p&gt;在设计上跨命名空间的所属关系是不允许的。 这就是说:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;命名空间作用域内从属者只能指定同一个命名空间的对象为其所有者和集群作用域的所有者&lt;/li&gt;
&lt;li&gt;集群作用哉的从属者只能指定集群作用域的所有者，不能指定命名空间作用域的所有者&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;!--
## Controlling how the garbage collector deletes dependents

When you delete an object, you can specify whether the object&#39;s dependents are
also deleted automatically. Deleting dependents automatically is called *cascading
deletion*.  There are two modes of *cascading deletion*: *background* and *foreground*.

If you delete an object without deleting its dependents
automatically, the dependents are said to be *orphaned*.
 --&gt;
&lt;h2 id=&#34;控制垃圾回收器怎么删除从属者&#34;&gt;控制垃圾回收器怎么删除从属者&lt;/h2&gt;
&lt;p&gt;当用户删除一个对象时，可能指定是否同时自动删除它的从属者。 自动删除从属都的行为叫做 &lt;em&gt;级联删除&lt;/em&gt;。
级联删除又有两种模式 &lt;em&gt;后台&lt;/em&gt; 和 &lt;em&gt;前台&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;如果用户在删除一个对象是没有自动删除它的所属者，这些从属者就认为是 &lt;em&gt;孤儿&lt;/em&gt;&lt;/p&gt;
&lt;!--
### Foreground cascading deletion

In *foreground cascading deletion*, the root object first
enters a &#34;deletion in progress&#34; state. In the &#34;deletion in progress&#34; state,
the following things are true:

 * The object is still visible via the REST API
 * The object&#39;s `deletionTimestamp` is set
 * The object&#39;s `metadata.finalizers` contains the value &#34;foregroundDeletion&#34;.

Once the &#34;deletion in progress&#34; state is set, the garbage
collector deletes the object&#39;s dependents. Once the garbage collector has deleted all
&#34;blocking&#34; dependents (objects with `ownerReference.blockOwnerDeletion=true`), it deletes
the owner object.

Note that in the &#34;foregroundDeletion&#34;, only dependents with
`ownerReference.blockOwnerDeletion=true` block the deletion of the owner object.
Kubernetes version 1.7 added an [admission controller](/docs/reference/access-authn-authz/admission-controllers/#ownerreferencespermissionenforcement) that controls user access to set
`blockOwnerDeletion` to true based on delete permissions on the owner object, so that
unauthorized dependents cannot delay deletion of an owner object.

If an object&#39;s `ownerReferences` field is set by a controller (such as Deployment or ReplicaSet),
blockOwnerDeletion is set automatically and you do not need to manually modify this field.
 --&gt;
&lt;h3 id=&#34;前台级联删除&#34;&gt;前台级联删除&lt;/h3&gt;
&lt;p&gt;In &lt;em&gt;foreground cascading deletion&lt;/em&gt;, the root object first
enters a &amp;ldquo;deletion in progress&amp;rdquo; state. In the &amp;ldquo;deletion in progress&amp;rdquo; state,
the following things are true:&lt;/p&gt;
&lt;p&gt;在使用 &lt;em&gt;前台级联删除&lt;/em&gt; 时， 根对象先进入 &amp;ldquo;删除中&amp;rdquo; 状态。 在 &amp;ldquo;删除中&amp;rdquo; 状态时, 以下状态为真:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;依然可以通过 REST API 访问该对象&lt;/li&gt;
&lt;li&gt;该对象上已经设置了 &lt;code&gt;deletionTimestamp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;该对象上的 &lt;code&gt;metadata.finalizers&lt;/code&gt; 包含值 &amp;ldquo;foregroundDeletion&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当 &amp;ldquo;删除中&amp;rdquo; 被设置后， 垃圾回收器就会删除它的从属对象。 当垃圾回收器删除所有 &amp;ldquo;阻塞&amp;rdquo; 的对象(包含 &lt;code&gt;ownerReference.blockOwnerDeletion=true&lt;/code&gt; 的对象)后，
就会删除对象本身。&lt;/p&gt;
&lt;p&gt;要注意到 &lt;em&gt;前台级联删除&lt;/em&gt; 只会被那些包含 &lt;code&gt;ownerReference.blockOwnerDeletion=true&lt;/code&gt; 对象的删除所阻塞。
在 k8s &lt;code&gt;v1.7&lt;/code&gt; 添加了 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/reference/access-authn-authz/admission-controllers/#ownerreferencespermissionenforcement&#34;&gt;admission controller&lt;/a&gt;
其上添加了基于所有者对象上的删除权限设置用户对 &lt;code&gt;blockOwnerDeletion&lt;/code&gt; 设置为 true 的权限。
因此对于未授权的从属对象不会延迟其所有者对象的删除。&lt;/p&gt;
&lt;p&gt;如果一个对象的 &lt;code&gt;ownerReferences&lt;/code&gt; 字段是由控制器(如 Deployment 或 ReplicaSet) 设置的，
&lt;code&gt;blockOwnerDeletion&lt;/code&gt; 也会自动被设置，用户不需要手动修改该字段。&lt;/p&gt;
&lt;!--
### Background cascading deletion

In *background cascading deletion*, Kubernetes deletes the owner object
immediately and the garbage collector then deletes the dependents in
the background.
 --&gt;
&lt;h3 id=&#34;后台级联删除&#34;&gt;后台级联删除&lt;/h3&gt;
&lt;p&gt;在使用 &lt;em&gt;后台级联删除&lt;/em&gt;, k8s 立马删除所有者对象，然后垃圾回器后台删除其从属对象&lt;/p&gt;
&lt;!--
### Setting the cascading deletion policy

To control the cascading deletion policy, set the `propagationPolicy`
field on the `deleteOptions` argument when deleting an Object. Possible values include &#34;Orphan&#34;,
&#34;Foreground&#34;, or &#34;Background&#34;.

Here&#39;s an example that deletes dependents in background:

```shell
kubectl proxy --port=8080
curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset \
  -d &#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Background&#34;}&#39; \
  -H &#34;Content-Type: application/json&#34;
```

Here&#39;s an example that deletes dependents in foreground:

```shell
kubectl proxy --port=8080
curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset \
  -d &#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Foreground&#34;}&#39; \
  -H &#34;Content-Type: application/json&#34;
```

Here&#39;s an example that orphans dependents:

```shell
kubectl proxy --port=8080
curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset \
  -d &#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Orphan&#34;}&#39; \
  -H &#34;Content-Type: application/json&#34;
```

kubectl also supports cascading deletion.
To delete dependents automatically using kubectl, set `--cascade` to true.  To
orphan dependents, set `--cascade` to false. The default value for `--cascade`
is true.

Here&#39;s an example that orphans the dependents of a ReplicaSet:

```shell
kubectl delete replicaset my-repset --cascade=false
```
 --&gt;
&lt;h3 id=&#34;设置级联删除策略&#34;&gt;设置级联删除策略&lt;/h3&gt;
&lt;p&gt;想要控制级联删除策略, 可以在删除对象时设置 &lt;code&gt;deleteOptions&lt;/code&gt; 参数的 &lt;code&gt;propagationPolicy&lt;/code&gt; 字段，
可选的值有 &amp;ldquo;Orphan&amp;rdquo;, &amp;ldquo;Foreground&amp;rdquo;, &amp;ldquo;Background&amp;rdquo;.
以下为一个后台删除从属对象的示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl proxy --port&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#34;kind&amp;#34;:&amp;#34;DeleteOptions&amp;#34;,&amp;#34;apiVersion&amp;#34;:&amp;#34;v1&amp;#34;,&amp;#34;propagationPolicy&amp;#34;:&amp;#34;Background&amp;#34;}&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -H &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Content-Type: application/json&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;以下为一个前台删除从属对象的示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl proxy --port&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#34;kind&amp;#34;:&amp;#34;DeleteOptions&amp;#34;,&amp;#34;apiVersion&amp;#34;:&amp;#34;v1&amp;#34;,&amp;#34;propagationPolicy&amp;#34;:&amp;#34;Foreground&amp;#34;}&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -H &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Content-Type: application/json&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;以下为一个将从属对象设置为 孤儿的示例:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl proxy --port&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;
curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -d &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{&amp;#34;kind&amp;#34;:&amp;#34;DeleteOptions&amp;#34;,&amp;#34;apiVersion&amp;#34;:&amp;#34;v1&amp;#34;,&amp;#34;propagationPolicy&amp;#34;:&amp;#34;Orphan&amp;#34;}&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;  -H &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Content-Type: application/json&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;kubectl 也支持级联删除。要使用 kubectl 自动删除从属对象， 设置 &lt;code&gt;--cascade&lt;/code&gt; 为 &lt;code&gt;true&lt;/code&gt;
将从属对象设置为 孤儿 设置 &lt;code&gt;--cascade&lt;/code&gt; 为 &lt;code&gt;false&lt;/code&gt;. &lt;code&gt;--cascade&lt;/code&gt; 默认值为 &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;以下为一个将 ReplicaSet 从属对象设置为 孤儿的示例&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl delete replicaset my-repset --cascade&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;!--
### Additional note on Deployments

Prior to 1.7, When using cascading deletes with Deployments you *must* use `propagationPolicy: Foreground`
to delete not only the ReplicaSets created, but also their Pods. If this type of _propagationPolicy_
is not used, only the ReplicaSets will be deleted, and the Pods will be orphaned.
See [kubeadm/#149](https://github.com/kubernetes/kubeadm/issues/149#issuecomment-284766613) for more information.
 --&gt;
&lt;h3 id=&#34;deployment-额外需要注意的地方&#34;&gt;Deployment 额外需要注意的地方&lt;/h3&gt;
&lt;p&gt;Prior to 1.7, When using cascading deletes with Deployments you &lt;em&gt;must&lt;/em&gt; use &lt;code&gt;propagationPolicy: Foreground&lt;/code&gt;
to delete not only the ReplicaSets created, but also their Pods. If this type of &lt;em&gt;propagationPolicy&lt;/em&gt;
is not used, only the ReplicaSets will be deleted, and the Pods will be orphaned.
See &lt;a href=&#34;https://github.com/kubernetes/kubeadm/issues/149#issuecomment-284766613&#34;&gt;kubeadm/#149&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;在 &lt;code&gt;1.7&lt;/code&gt; 版本之前， 当使用级联删除 Deployment 时，&lt;em&gt;必须&lt;/em&gt; 要使用 &lt;code&gt;propagationPolicy: Foreground&lt;/code&gt;
来确定不止删除创建的  ReplicaSet，还要删除对应的 Pod。如果没有使用该类型的 &lt;em&gt;propagationPolicy&lt;/em&gt;，
只有 ReplicaSet 会被删除，Pod 会被设置人孤儿。
更多信息见 &lt;a href=&#34;https://github.com/kubernetes/kubeadm/issues/149#issuecomment-284766613&#34;&gt;kubeadm/#149&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;已知问题&#34;&gt;已知问题&lt;/h2&gt;
&lt;p&gt;见问题单 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/26120&#34;&gt;#26120&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/api-machinery/garbage-collection.md&#34;&gt;设计文稿 1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://git.k8s.io/community/contributors/design-proposals/api-machinery/synchronous-garbage-collection.md&#34;&gt;设计文稿 2&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 用于已完成资源的 TTL 控制器</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/ttlafterfinished/</link>
      <pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/ttlafterfinished/</guid>
      <description>
        
        
        &lt;!--  
---
reviewers:
- janetkuo
title: TTL Controller for Finished Resources
content_type: concept
weight: 70
---
--&gt;
&lt;!-- overview --&gt;
&lt;!--





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.12 [alpha]&lt;/code&gt;
&lt;/div&gt;



The TTL controller provides a TTL (time to live) mechanism to limit the lifetime of resource
objects that have finished execution. TTL controller only handles
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/job/&#39; target=&#39;_blank&#39;&gt;Jobs&lt;span class=&#39;tooltip-text&#39;&gt;一个运行后即结束的有限或批量任务&lt;/span&gt;
&lt;/a&gt; for now,
and may be expanded to handle other resources that will finish execution,
such as Pods and custom resources.

Alpha Disclaimer: this feature is currently alpha, and can be enabled with both kube-apiserver and kube-controller-manager
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
`TTLAfterFinished`.
 --&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.12 [alpha]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;TTL 控制器提供了一个限制那些已经执行完成的资源对象生存期的 TTL (存活时间)机制。
目前 TTL 控制器只能控制 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/job/&#39; target=&#39;_blank&#39;&gt;Job&lt;span class=&#39;tooltip-text&#39;&gt;一个运行后即结束的有限或批量任务&lt;/span&gt;
&lt;/a&gt;， 可能会扩展到通解处理其它完成执行的资源
如 Pod 和 自定义资源。
Alpha Disclaimer: this feature is currently alpha, and can be enabled with both kube-apiserver and kube-controller-manager
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;feature gate&lt;/a&gt;
&lt;code&gt;TTLAfterFinished&lt;/code&gt;.
Alpha 版本的免责声明: 这个特性现在还是 alpha 版本， 可以在 kube-apiserver 中 kube-controller-manager
通过 &lt;code&gt;TTLAfterFinished&lt;/code&gt; &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/feature-gates/&#34;&gt;功能阀&lt;/a&gt; 开启&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## TTL Controller

The TTL controller only supports Jobs for now. A cluster operator can use this feature to clean
up finished Jobs (either `Complete` or `Failed`) automatically by specifying the
`.spec.ttlSecondsAfterFinished` field of a Job, as in this
[example](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically).
The TTL controller will assume that a resource is eligible to be cleaned up
TTL seconds after the resource has finished, in other words, when the TTL has expired. When the
TTL controller cleans up a resource, it will delete it cascadingly, that is to say it will delete
its dependent objects together with it. Note that when the resource is deleted,
its lifecycle guarantees, such as finalizers, will be honored.

The TTL seconds can be set at any time. Here are some examples for setting the
`.spec.ttlSecondsAfterFinished` field of a Job:

* Specify this field in the resource manifest, so that a Job can be cleaned up
  automatically some time after it finishes.
* Set this field of existing, already finished resources, to adopt this new
  feature.
* Use a
  [mutating admission webhook](/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks)
  to set this field dynamically at resource creation time. Cluster administrators can
  use this to enforce a TTL policy for finished resources.
* Use a
  [mutating admission webhook](/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks)
  to set this field dynamically after the resource has finished, and choose
  different TTL values based on resource status, labels, etc.
 --&gt;
&lt;h2 id=&#34;ttl-控制器&#34;&gt;TTL 控制器&lt;/h2&gt;
&lt;p&gt;目前 TTL 控制器只支持 Job。 集群管理员可以通过该特性来自动清理已完成的 Job(无论 &lt;code&gt;Complete&lt;/code&gt; 或 &lt;code&gt;Failed&lt;/code&gt;)，
只需要在 Job 对象上设置 &lt;code&gt;.spec.ttlSecondsAfterFinished&lt;/code&gt;， 见 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically&#34;&gt;示例&lt;/a&gt;.
TTL 控制器假定一个资源在完成后 TTL 秒之后就是能够被回收的，换句话来说，就是当 TTL 过期的时候。
当 TTL 控制器清理一个资源时，会级联地删除，也就是说会连同它的从属对象一起删除。 要注意当一个资源被删除时，
它的生命周期保证，如析构器，就会被触发。
TTL 时间可以在任意时刻设置。 以下为在 Job 上设置 &lt;code&gt;.spec.ttlSecondsAfterFinished&lt;/code&gt; 的一些示例:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在资源的配置清单中设置该字段，因而 Job 可以在完成后的某个时间点被自动清理。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在已经存在，已经完成的资源上设置该字段，然后享受该特性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks&#34;&gt;mutating admission webhook&lt;/a&gt;
来在资源创建时动态添加该字段。集群管理员可以使用它来给已经完成的资源加持一个 TTL 策略&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks&#34;&gt;mutating admission webhook&lt;/a&gt;
在资源完成时动态添加该字段。 通过不同的资源状态，标签，等来设置不同的 TTL 值&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
## Caveat

### Updating TTL Seconds

Note that the TTL period, e.g. `.spec.ttlSecondsAfterFinished` field of Jobs,
can be modified after the resource is created or has finished. However, once the
Job becomes eligible to be deleted (when the TTL has expired), the system won&#39;t
guarantee that the Jobs will be kept, even if an update to extend the TTL
returns a successful API response.

### Time Skew

Because TTL controller uses timestamps stored in the Kubernetes resources to
determine whether the TTL has expired or not, this feature is sensitive to time
skew in the cluster, which may cause TTL controller to clean up resource objects
at the wrong time.

In Kubernetes, it&#39;s required to run NTP on all nodes
(see [#6159](https://github.com/kubernetes/kubernetes/issues/6159#issuecomment-93844058))
to avoid time skew. Clocks aren&#39;t always correct, but the difference should be
very small. Please be aware of this risk when setting a non-zero TTL.
 --&gt;
&lt;h2 id=&#34;附加说明&#34;&gt;附加说明&lt;/h2&gt;
&lt;h3 id=&#34;更新-ttl-时间&#34;&gt;更新 TTL 时间&lt;/h3&gt;
&lt;p&gt;要注意 TTL 的时间， 例如 Job 的 &lt;code&gt;.spec.ttlSecondsAfterFinished&lt;/code&gt; 字段可以在资源创建或完成后进行修改。
但是，当 Job 变得可以被删除时(当 TTL 过期后)， 系统就不保证 Job 对象会继续保留，即便增加 TTL 的请求 API 请求响应成功的。&lt;/p&gt;
&lt;h3 id=&#34;时间偏差&#34;&gt;时间偏差&lt;/h3&gt;
&lt;p&gt;因为 TTL 控制器使用存放于 k8s 资源中的时间戳来决定 TTL 是否已经过期， 这个特性对集群中的时间偏差很敏感。
可能会导致 TTL 控制器在错误的时间清理资源对象。&lt;/p&gt;
&lt;p&gt;在 k8s 系统中，需要在所有的节点上运行 NTP (见问题单 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/6159#issuecomment-93844058&#34;&gt;#6159&lt;/a&gt;)
来避免引志时间偏差。 时钟不是始终正确的，但差异必须要特别小。 在设置非零 TTL 时一定要注意这个风险。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically&#34;&gt;自动清理 Job&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/0026-ttl-after-finish.md&#34;&gt;设置文稿&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: CronJob</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/cron-jobs/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/cron-jobs/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- erictune
- soltysh
- janetkuo
title: CronJob
content_type: concept
weight: 80
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.8 [beta]&lt;/code&gt;
&lt;/div&gt;



A _CronJob_ creates &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/job/&#39; target=&#39;_blank&#39;&gt;Jobs&lt;span class=&#39;tooltip-text&#39;&gt;一个运行后即结束的有限或批量任务&lt;/span&gt;
&lt;/a&gt; on a repeating schedule.

One CronJob object is like one line of a _crontab_ (cron table) file. It runs a job periodically
on a given schedule, written in [Cron](https://en.wikipedia.org/wiki/Cron) format.

&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; &lt;p&gt;All &lt;strong&gt;CronJob&lt;/strong&gt; &lt;code&gt;schedule:&lt;/code&gt; times are based on the timezone of the
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/kube-controller-manager/&#39; target=&#39;_blank&#39;&gt;kube-controller-manager&lt;span class=&#39;tooltip-text&#39;&gt;Control Plane component that runs controller processes.&lt;/span&gt;
&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If your control plane runs the kube-controller-manager in Pods or bare
containers, the timezone set for the kube-controller-manager container determines the timezone
that the cron job controller uses.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;


When creating the manifest for a CronJob resource, make sure the name you provide
is a valid [DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
The name must be no longer than 52 characters. This is because the CronJob controller will automatically
append 11 characters to the job name provided and there is a constraint that the
maximum length of a Job name is no more than 63 characters.
 --&gt;





&lt;div style=&#34;margin-top: 10px; margin-bottom: 10px;&#34;&gt;
&lt;b&gt;功能特性状态:&lt;/b&gt; &lt;code&gt;Kubernetes v1.8 [beta]&lt;/code&gt;
&lt;/div&gt;


&lt;p&gt;&lt;em&gt;CronJob&lt;/em&gt; 可以定时重复地创建 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/controllers/job/&#39; target=&#39;_blank&#39;&gt;Job&lt;span class=&#39;tooltip-text&#39;&gt;一个运行后即结束的有限或批量任务&lt;/span&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一个 CronJob 对象看起来像是 &lt;em&gt;crontab&lt;/em&gt; (cron table) 文件的一行。
它会按照以 &lt;a href=&#34;https://en.wikipedia.org/wiki/Cron&#34;&gt;Cron&lt;/a&gt; 编写的计划定期的运行 Job&lt;/p&gt;
&lt;blockquote class=&#34;caution&#34;&gt;
  &lt;div&gt;&lt;strong&gt;注意：&lt;/strong&gt; &lt;p&gt;所有 &lt;strong&gt;CronJob&lt;/strong&gt; &lt;code&gt;时间表:&lt;/code&gt;的时间都是基于
&lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/kube-controller-manager/&#39; target=&#39;_blank&#39;&gt;kube-controller-manager&lt;span class=&#39;tooltip-text&#39;&gt;Control Plane component that runs controller processes.&lt;/span&gt;
&lt;/a&gt;
的时间的。&lt;/p&gt;
&lt;p&gt;如果控制中心把 kube-controller-manager 运行在 Pod 中或裸容器中， kube-controller-manager 所在
容器的时区就决定了 CronJob 控制器所使用的时区&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;在创建 CronJob 资源的配置定义时，要确保其名称是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.
名称不能多于 52 个字符。 因为 CronJob 会自动在它创建的 Job 的名称是自身的名称再加 11 个字符，
这样 Job 的名称就不会超过 63 个字符的限制。&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## CronJob

CronJobs are useful for creating periodic and recurring tasks, like running backups or
sending emails. CronJobs can also schedule individual tasks for a specific time, such as
scheduling a Job for when your cluster is likely to be idle.
--&gt;
&lt;h2 id=&#34;cronjob&#34;&gt;CronJob&lt;/h2&gt;
&lt;p&gt;CronJob 对于创建定期重复的任务是很有用的，例如运行备份任务或发送邮件。
CronJob 也可以在指定时间调度单个应用，例如当集群变得空闲时调度 Job 任务&lt;/p&gt;
&lt;!--
### Example

This example CronJob manifest prints the current time and a hello message every minute:



 













&lt;table class=&#34;includecode&#34; id=&#34;applicationjobcronjobyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/application/job/cronjob.yaml&#34; download=&#34;application/job/cronjob.yaml&#34;&gt;
                    &lt;code&gt;application/job/cronjob.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;applicationjobcronjobyaml&#39;)&#34; title=&#34;Copy application/job/cronjob.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;batch/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;CronJob&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hello&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;schedule&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*/1 * * * *&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;jobTemplate&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hello&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;:
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;/bin/sh&lt;/span&gt;
            - -&lt;span style=&#34;color:#ae81ff&#34;&gt;c&lt;/span&gt;
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;date; echo Hello from the Kubernetes cluster&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;OnFailure&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



([Running Automated Tasks with a CronJob](/docs/tasks/job/automated-tasks-with-cron-jobs/)
takes you through this example in more detail).
 --&gt;
&lt;h3 id=&#34;示例&#34;&gt;示例&lt;/h3&gt;
&lt;p&gt;This example CronJob manifest prints the current time and a hello message every minute:
这个示例中 CronJob 定义的任务是每分钟打印当前时间和一个问候信息:&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;applicationjobcronjobyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/application/job/cronjob.yaml&#34; download=&#34;application/job/cronjob.yaml&#34;&gt;
                    &lt;code&gt;application/job/cronjob.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;applicationjobcronjobyaml&#39;)&#34; title=&#34;Copy application/job/cronjob.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;batch/v1beta1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;CronJob&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hello&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;schedule&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;*/1 * * * *&amp;#34;&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;jobTemplate&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
          - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hello&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;busybox&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;args&lt;/span&gt;:
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;/bin/sh&lt;/span&gt;
            - -&lt;span style=&#34;color:#ae81ff&#34;&gt;c&lt;/span&gt;
            - &lt;span style=&#34;color:#ae81ff&#34;&gt;date; echo Hello from the Kubernetes cluster&lt;/span&gt;
          &lt;span style=&#34;color:#f92672&#34;&gt;restartPolicy&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;OnFailure&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;(&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/job/automated-tasks-with-cron-jobs/&#34;&gt;使用 CronJob 运行自动化任务&lt;/a&gt;
中的示例有更多详细的说明).&lt;/p&gt;
&lt;!--
## CronJob limitations {#cron-job-limitations}

A cron job creates a job object _about_ once per execution time of its schedule. We say &#34;about&#34; because there
are certain circumstances where two jobs might be created, or no job might be created. We attempt to make these rare,
but do not completely prevent them. Therefore, jobs should be _idempotent_.

If `startingDeadlineSeconds` is set to a large value or left unset (the default)
and if `concurrencyPolicy` is set to `Allow`, the jobs will always run
at least once.

For every CronJob, the CronJob &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/2-concepts/01-architecture/02-controller/&#39; target=&#39;_blank&#39;&gt;控制器&lt;span class=&#39;tooltip-text&#39;&gt;一个控制回路，负责通过 apiserver 监控集群的共享状态，并尝试通过变更(某些对象)的方式实现集群从当前状态向期望状态迁移&lt;/span&gt;
&lt;/a&gt; checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the job and logs the error

````
Cannot determine if job needs to be started. Too many missed start time (&gt; 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.
````

It is important to note that if the `startingDeadlineSeconds` field is set (not `nil`), the controller counts how many missed jobs occurred from the value of `startingDeadlineSeconds` until now rather than from the last scheduled time until now. For example, if `startingDeadlineSeconds` is `200`, the controller counts how many missed jobs occurred in the last 200 seconds.

A CronJob is counted as missed if it has failed to be created at its scheduled time. For example, If `concurrencyPolicy` is set to `Forbid` and a CronJob was attempted to be scheduled when there was a previous schedule still running, then it would count as missed.

For example, suppose a CronJob is set to schedule a new Job every one minute beginning at `08:30:00`, and its
`startingDeadlineSeconds` field is not set. If the CronJob controller happens to
be down from `08:29:00` to `10:21:00`, the job will not start as the number of missed jobs which missed their schedule is greater than 100.

To illustrate this concept further, suppose a CronJob is set to schedule a new Job every one minute beginning at `08:30:00`, and its
`startingDeadlineSeconds` is set to 200 seconds. If the CronJob controller happens to
be down for the same period as the previous example (`08:29:00` to `10:21:00`,) the Job will still start at 10:22:00. This happens as the controller now checks how many missed schedules happened in the last 200 seconds (ie, 3 missed schedules), rather than from the last scheduled time until now.

The CronJob is only responsible for creating Jobs that match its schedule, and
the Job in turn is responsible for the management of the Pods it represents.
 --&gt;
&lt;h2 id=&#34;cron-job-limitations&#34;&gt;CronJob 局限&lt;/h2&gt;
&lt;p&gt;CronJob 按照计划在每个执行时刻创建 &lt;em&gt;大约&lt;/em&gt; 一个 Job 对象。 这里说 &amp;ldquo;大约&amp;rdquo; 是因为在某些特定的情况下可能会创建两个，或者一个都没有创建。
我们尽量避免这些情况的发生，但是不能实现完全不发生。 因此 Job 应该是 &lt;em&gt;幂等&lt;/em&gt; 的。&lt;/p&gt;
&lt;p&gt;如果 &lt;code&gt;startingDeadlineSeconds&lt;/code&gt; 设置得足够大或不设置(默认)并且 如果 &lt;code&gt;concurrencyPolicy&lt;/code&gt;  设置为 &lt;code&gt;Allow&lt;/code&gt;
这时候 Job 始终至少能运行一次&lt;/p&gt;
&lt;p&gt;如果错过的调度次数大于 100， 则不再启动这个 Job 并向日志输出如下错误信息。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Cannot determine if job needs to be started. Too many missed start time (&amp;gt; 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;一个重要的信息是如果 &lt;code&gt;startingDeadlineSeconds&lt;/code&gt; 字段被设置(不是 &lt;code&gt;nil&lt;/code&gt;)，控制器在计数错过的 Job 的时间是距离当前 &lt;code&gt;startingDeadlineSeconds&lt;/code&gt;和时间而不是上次调度到当前的时间。
例如， 如果 &lt;code&gt;startingDeadlineSeconds&lt;/code&gt; 的值为 &lt;code&gt;200&lt;/code&gt;， 控制器会计数过去 200 秒内错过的次数。&lt;/p&gt;
&lt;p&gt;CronJob 在调度时间创建失败被认为是一个错过计数。 例如， 如果 &lt;code&gt;concurrencyPolicy&lt;/code&gt; 设置为 &lt;code&gt;Forbid&lt;/code&gt;
并且 CronJob 在之前的调度还没有执行完的时间尝试新的调度，就会被认为是错过。&lt;/p&gt;
&lt;p&gt;例如， 假定，一个 CronJob 设置为 从 &lt;code&gt;08:30:00&lt;/code&gt; 开始，每分钟调度一个新的 Job，并且它的
&lt;code&gt;startingDeadlineSeconds&lt;/code&gt; 字段没有设置。 如果 CronJob 控制器恰好在 &lt;code&gt;08:29:00&lt;/code&gt; 到 &lt;code&gt;10:21:00&lt;/code&gt; 挂了，
Job 就会因为没有成功启动而引起没有成功调度的次数就会大于 100.
再假设， 如果一个 CronJob 设置为 从 &lt;code&gt;08:30:00&lt;/code&gt; 开始，每分钟调度一个新的 Job，
&lt;code&gt;startingDeadlineSeconds&lt;/code&gt; 设置为 200 秒。 CronJob 控制器挂掉的时间也是一样 (&lt;code&gt;08:29:00&lt;/code&gt; 到 &lt;code&gt;10:21:00&lt;/code&gt;)，
Job 仍然会在 &lt;code&gt;10:22:00&lt;/code&gt; 启动， 这是因为控制器计算的是过去 200 秒(也就是 3 个错过的调度)发生错过的调度数，而不是从最后一个调度时间到当前的时间。&lt;/p&gt;
&lt;p&gt;CronJob 只负责创建符合它时间表的 Job，然后 Job 负责管理它代表的 Pod。&lt;/p&gt;
&lt;h2 id=&#34;相关资料&#34;&gt;相关资料&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cron&#34;&gt;Cron 表达式格式&lt;/a&gt;
CronJob &lt;code&gt;schedule&lt;/code&gt; 字段的详细文档.&lt;/p&gt;
&lt;p&gt;更多 CronJob 创建和管理，以及示例，见  &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/job/automated-tasks-with-cron-jobs&#34;&gt;使用 CronJob 运行自动化任务&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: ReplicationController</title>
      <link>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/replicationcontroller/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/replicationcontroller/</guid>
      <description>
        
        
        &lt;!--
---
reviewers:
- bprashanth
- janetkuo
title: ReplicationController
feature:
  title: Self-healing
  anchor: How a ReplicationController Works
  description: &gt;
    Restarts containers that fail, replaces and reschedules containers when nodes die, kills containers that don&#39;t respond to your user-defined health check, and doesn&#39;t advertise them to clients until they are ready to serve.

content_type: concept
weight: 90
---
 --&gt;
&lt;!-- overview --&gt;
&lt;!--  
&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; A &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/deployment/&#34;&gt;&lt;code&gt;Deployment&lt;/code&gt;&lt;/a&gt; that configures a &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/replicaset/&#34;&gt;&lt;code&gt;ReplicaSet&lt;/code&gt;&lt;/a&gt; is now the recommended way to set up replication.&lt;/div&gt;
&lt;/blockquote&gt;


A _ReplicationController_ ensures that a specified number of pod replicas are running at any one
time. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is
always up and available.
--&gt;
&lt;p&gt;&lt;blockquote class=&#34;note&#34;&gt;
  &lt;div&gt;&lt;strong&gt;说明：&lt;/strong&gt; 使用 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/deployment/&#34;&gt;&lt;code&gt;Deployment&lt;/code&gt;&lt;/a&gt; 管理
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/replicaset/&#34;&gt;&lt;code&gt;ReplicaSet&lt;/code&gt;&lt;/a&gt;
是目前推荐的运行多副本的方式。&lt;/div&gt;
&lt;/blockquote&gt;

``
&lt;em&gt;ReplicationController&lt;/em&gt; 确保在任意时刻都有指定数量的 Pod 副本在运行， 换句话来说
就是 &lt;code&gt;ReplicationController&lt;/code&gt; 保证一个 Pod 或 一组同质 Pod 的集群始终运行并可用&lt;/p&gt;
&lt;!-- body --&gt;
&lt;!--
## How a ReplicationController Works

If there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the
ReplicationController starts more pods. Unlike manually created pods, the pods maintained by a
ReplicationController are automatically replaced if they fail, are deleted, or are terminated.
For example, your pods are re-created on a node after disruptive maintenance such as a kernel upgrade.
For this reason, you should use a ReplicationController even if your application requires
only a single pod. A ReplicationController is similar to a process supervisor,
but instead of supervising individual processes on a single node, the ReplicationController supervises multiple pods
across multiple nodes.

ReplicationController is often abbreviated to &#34;rc&#34; in discussion, and as a shortcut in
kubectl commands.

A simple case is to create one ReplicationController object to reliably run one instance of
a Pod indefinitely.  A more complex use case is to run several identical replicas of a replicated
service, such as web servers.
 --&gt;
&lt;h2 id=&#34;replicationcontroller-是怎么工作的&#34;&gt;ReplicationController 是怎么工作的&lt;/h2&gt;
&lt;p&gt;如果同时运行的 Pod 太多了， ReplicationController 终止多余的 Pod。
如果同时运行的 Pod 太少了， ReplicationController 启动更多的 Pod。
与手动创建 Pod 不同， 由 ReplicationController 维护的 Pod 在挂掉，被删除，或被终止都会自动被替换。
例如， Pod 因为所在的节点升级内核引起维护故障而被重新创建。因为这些原因，即便应用只需要一单个 Pod 也应该使用 ReplicationController。
ReplicationController 与进程监督类似，但是与只监督一个节点上的单个线程不同，
ReplicationController 监督多个节点上的多个 Pod。&lt;/p&gt;
&lt;p&gt;简单的一个场景为创建一个 ReplicationController 对象来运行一个不限期运行的单实例 Pod。
复杂的一个应用场景为运行一个多副本应用的多个副本，如 web 服务。&lt;/p&gt;
&lt;!--
## Running an example ReplicationController

This example ReplicationController config runs three copies of the nginx web server.



 













&lt;table class=&#34;includecode&#34; id=&#34;controllersreplicationyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/replication.yaml&#34; download=&#34;controllers/replication.yaml&#34;&gt;
                    &lt;code&gt;controllers/replication.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersreplicationyaml&#39;)&#34; title=&#34;Copy controllers/replication.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicationController&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;



Run the example job by downloading the example file and then running this command:

```shell
kubectl apply -f https://k8s.io/examples/controllers/replication.yaml
```
```
replicationcontroller/nginx created
```

Check on the status of the ReplicationController using this command:

```shell
kubectl describe replicationcontrollers/nginx
```
```
Name:        nginx
Namespace:   default
Selector:    app=nginx
Labels:      app=nginx
Annotations:    &lt;none&gt;
Replicas:    3 current / 3 desired
Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       app=nginx
  Containers:
   nginx:
    Image:              nginx
    Port:               80/TCP
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  FirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message
  ---------       --------     -----    ----                        -------------    ----      ------              -------
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v
```

Here, three pods are created, but none is running yet, perhaps because the image is being pulled.
A little later, the same command may show:

```shell
Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed
```

To list all the pods that belong to the ReplicationController in a machine readable form, you can use a command like this:

```shell
pods=$(kubectl get pods --selector=app=nginx --output=jsonpath={.items..metadata.name})
echo $pods
```
```
nginx-3ntk0 nginx-4ok8v nginx-qrm3m
```

Here, the selector is the same as the selector for the ReplicationController (seen in the
`kubectl describe` output), and in a different form in `replication.yaml`.  The `--output=jsonpath` option
specifies an expression that just gets the name from each pod in the returned list.
 --&gt;
&lt;h2 id=&#34;一个-replicationcontroller-的示例&#34;&gt;一个 ReplicationController 的示例&lt;/h2&gt;
&lt;p&gt;这是一个运行三个 nginx web 服务副本的 ReplicationController 配置：&lt;/p&gt;


 













&lt;table class=&#34;includecode&#34; id=&#34;controllersreplicationyaml&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th&gt;
                &lt;a href=&#34;https://%25!s%28%3cnil%3e%29/master/content/zh/examples/controllers/replication.yaml&#34; download=&#34;controllers/replication.yaml&#34;&gt;
                    &lt;code&gt;controllers/replication.yaml&lt;/code&gt;
                &lt;/a&gt;
                &lt;img src=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/images/copycode.svg&#34; style=&#34;max-height:24px; cursor: pointer&#34; onclick=&#34;copyCode(&#39;controllersreplicationyaml&#39;)&#34; title=&#34;Copy controllers/replication.yaml to clipboard&#34;&gt;
            &lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ReplicationController&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
        - &lt;span style=&#34;color:#f92672&#34;&gt;containerPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;下载这个示例配置文件，并运行以下命令:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f replication.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果为:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;replicationcontroller/nginx created
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;使用以下命令查看 ReplicationController 的状态:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl describe replicationcontrollers/nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:        nginx
Namespace:   default
Selector:    app=nginx
Labels:      app=nginx
Annotations:    &amp;lt;none&amp;gt;
Replicas:    3 current / 3 desired
Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       app=nginx
  Containers:
   nginx:
    Image:              nginx
    Port:               80/TCP
    Environment:        &amp;lt;none&amp;gt;
    Mounts:             &amp;lt;none&amp;gt;
  Volumes:              &amp;lt;none&amp;gt;
Events:
  FirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message
  ---------       --------     -----    ----                        -------------    ----      ------              -------
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这时，三个 Pod 都已经衩上创建， 但还没有一个在运行，可能是因为还在拉镜像。
再过一会，同一条命令的输出结果可能如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以机器可读的横笔列举所以属于 ReplicationController Pod，可运行以下命令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;pods&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;kubectl get pods --selector&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;app&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx --output&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;jsonpath&lt;span style=&#34;color:#f92672&#34;&gt;={&lt;/span&gt;.items..metadata.name&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;
echo $pods
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;输出结果类似如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nginx-3ntk0 nginx-4ok8v nginx-qrm3m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里使用的选择器与 ReplicationController 的选择器相同(可以通过 &lt;code&gt;kubectl describe&lt;/code&gt; 输出验证)，
与 &lt;code&gt;replication.yaml&lt;/code&gt; 的格式不一样。&lt;code&gt;--output=jsonpath&lt;/code&gt; 指定了一个表达式， 表示只返回每个 Pod 的名称为一个列表。&lt;/p&gt;
&lt;!--
## Writing a ReplicationController Spec

As with all other Kubernetes config, a ReplicationController needs `apiVersion`, `kind`, and `metadata` fields.
The name of a ReplicationController object must be a valid
[DNS subdomain name](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).
For general information about working with config files, see [object management ](/docs/concepts/overview/working-with-objects/object-management/).

A ReplicationController also needs a [`.spec` section](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).
 --&gt;
&lt;h2 id=&#34;编写-replicationcontroller-spec&#34;&gt;编写 ReplicationController Spec&lt;/h2&gt;
&lt;p&gt;与所以其它的 k8s 配置一样， ReplicationController 必要字段有 &lt;code&gt;apiVersion&lt;/code&gt;, &lt;code&gt;kind&lt;/code&gt;, &lt;code&gt;metadata&lt;/code&gt;。
ReplicationController 对象的名称必须是一个有效的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/names#dns-subdomain-names&#34;&gt;DNS 子域名&lt;/a&gt;.
更多编写配置文件所需要的信息，见
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/object-management/&#34;&gt;对象管理 &lt;/a&gt;.
ReplicationController 还需要一个 &lt;a href=&#34;https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&#34;&gt;&lt;code&gt;.spec&lt;/code&gt; 配置区&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### Pod Template

The `.spec.template` is the only required field of the `.spec`.

The `.spec.template` is a [pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;, except it is nested and does not have an `apiVersion` or `kind`.

In addition to required fields for a Pod, a pod template in a ReplicationController must specify appropriate
labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See [pod selector](#pod-selector).

Only a [`.spec.template.spec.restartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Always` is allowed, which is the default if not specified.

For local container restarts, ReplicationControllers delegate to an agent on the node,
for example the [Kubelet](/docs/reference/command-line-tools-reference/kubelet/) or Docker.
 --&gt;
&lt;h3 id=&#34;pod-模板&#34;&gt;Pod 模板&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.template&lt;/code&gt; 是 &lt;code&gt;.spec&lt;/code&gt; 的唯一必要字段。&lt;/p&gt;
&lt;p&gt;与 &lt;a class=&#39;glossary-tooltip&#39; href=&#39;https://lostsquirrel.github.io/k8sDocs/k8sDocs/concepts/workloads/pods/&#39; target=&#39;_blank&#39;&gt;Pod&lt;span class=&#39;tooltip-text&#39;&gt;Pod 表示集群中运行的一组容器的集合&lt;/span&gt;
&lt;/a&gt;， 除了因为嵌套没有 &lt;code&gt;apiVersion&lt;/code&gt; 或 &lt;code&gt;kind&lt;/code&gt;外完全相同的结构。&lt;/p&gt;
&lt;p&gt;写 Pod 相比额外的必要字段是 ReplicationController 的 Pod 模板必须要指定恰当的标签和一个恰当的重启策略。
对于标签，需要确保不能与其他控制器重叠。 见 &lt;a href=&#34;#pod-selector&#34;&gt;pod 选择器&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy&#34;&gt;&lt;code&gt;.spec.template.spec.restartPolicy&lt;/code&gt;&lt;/a&gt;
的值只能是 &lt;code&gt;Always&lt;/code&gt;， 也是不指定时的默认值。&lt;/p&gt;
&lt;p&gt;ReplicationControllers 委任节点上的代理，如，
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/reference/command-line-tools-reference/kubelet/&#34;&gt;Kubelet&lt;/a&gt;
或 Docker 来实现对于本地容器的重启。&lt;/p&gt;
&lt;!--
### Labels on the ReplicationController

The ReplicationController can itself have labels (`.metadata.labels`).  Typically, you
would set these the same as the `.spec.template.metadata.labels`; if `.metadata.labels` is not specified
then it defaults to  `.spec.template.metadata.labels`.  However, they are allowed to be
different, and the `.metadata.labels` do not affect the behavior of the ReplicationController.
 --&gt;
&lt;h3 id=&#34;replicationcontroller-的标签&#34;&gt;ReplicationController 的标签&lt;/h3&gt;
&lt;p&gt;ReplicationController 本身可以有标签(&lt;code&gt;.metadata.labels&lt;/code&gt;)。 通常与 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt; 设置相同的内容。
如果 &lt;code&gt;.metadata.labels&lt;/code&gt; 没有设置，则默认会设置与 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt; 相同。
但是，它们之间的内容可以不同， &lt;code&gt;.metadata.labels&lt;/code&gt; 不会影响 ReplicationController 的行为。&lt;/p&gt;
&lt;!--
### Pod Selector

The `.spec.selector` field is a [label selector](/docs/concepts/overview/working-with-objects/labels/#label-selectors). A ReplicationController
manages all the pods with labels that match the selector. It does not distinguish
between pods that it created or deleted and pods that another person or process created or
deleted. This allows the ReplicationController to be replaced without affecting the running pods.

If specified, the `.spec.template.metadata.labels` must be equal to the `.spec.selector`, or it will
be rejected by the API.  If `.spec.selector` is unspecified, it will be defaulted to
`.spec.template.metadata.labels`.

Also you should not normally create any pods whose labels match this selector, either directly, with
another ReplicationController, or with another controller such as Job. If you do so, the
ReplicationController thinks that it created the other pods.  Kubernetes does not stop you
from doing this.

If you do end up with multiple controllers that have overlapping selectors, you
will have to manage the deletion yourself (see [below](#working-with-replicationcontrollers)).
 --&gt;
&lt;h3 id=&#34;pod-selector&#34;&gt;Pod 选择器&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;.spec.selector&lt;/code&gt; 字段是一个
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/labels/#label-selectors&#34;&gt;标签选择器&lt;/a&gt;.
ReplicationController 会管理所有匹配它的选择器的 Pod。 不会区分这些 Pod 是由它自己创建或删除
还是由别的人或进行创建或删除的 Pod。 这让 ReplicationController 可以在不影响运行 Pod 的情况下被替换。&lt;/p&gt;
&lt;p&gt;如果设置 &lt;code&gt;.spec.selector&lt;/code&gt;，则它的内容必须要与 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt; 相同，
否则会被 API 拒绝。 如果没有设置 &lt;code&gt;.spec.selector&lt;/code&gt;， 则会被默认设置为 &lt;code&gt;.spec.template.metadata.labels&lt;/code&gt; 的内容。&lt;/p&gt;
&lt;p&gt;用户通常不应该再创建能被这个选择匹配标签的 Pod， 无论是直接创建还是通过另一个 ReplicationController，
或通过其它的诸如 Job 的控制器。 如果这样做， ReplicationController 也会把这些 Pod 认为是它的。
k8s 不会阻止用户这样做。&lt;/p&gt;
&lt;p&gt;如果最终有多个控制器拥有重叠的选择器，用户需要自己来管理删除(见 &lt;a href=&#34;#working-with-replicationcontrollers&#34;&gt;下面&lt;/a&gt;)&lt;/p&gt;
&lt;!--
### Multiple Replicas

You can specify how many pods should run concurrently by setting `.spec.replicas` to the number
of pods you would like to have running concurrently.  The number running at any time may be higher
or lower, such as if the replicas were just increased or decreased, or if a pod is gracefully
shutdown, and a replacement starts early.

If you do not specify `.spec.replicas`, then it defaults to 1.
 --&gt;
&lt;h3 id=&#34;多副本&#34;&gt;多副本&lt;/h3&gt;
&lt;p&gt;用户可以通过设置 &lt;code&gt;.spec.replicas&lt;/code&gt; 来指定同时运行的 Pod 的数量。 在任意时间内运行的 Pod 的数量
可能比这个值大也可能小， 例如，刚好的增加或减少副本数量， 或一个 Pod 正在被平滑地关闭，而它的替代者先启动起来了。&lt;/p&gt;
&lt;p&gt;如果没有指定 &lt;code&gt;.spec.replicas&lt;/code&gt;， 则默认为 1&lt;/p&gt;
&lt;!--
## Working with ReplicationControllers

### Deleting a ReplicationController and its Pods

To delete a ReplicationController and all its pods, use [`kubectl
delete`](/docs/reference/generated/kubectl/kubectl-commands#delete).  Kubectl will scale the ReplicationController to zero and wait
for it to delete each pod before deleting the ReplicationController itself.  If this kubectl
command is interrupted, it can be restarted.

When using the REST API or go client library, you need to do the steps explicitly (scale replicas to
0, wait for pod deletions, then delete the ReplicationController).

### Deleting just a ReplicationController

You can delete a ReplicationController without affecting any of its pods.

Using kubectl, specify the `--cascade=false` option to [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete).

When using the REST API or go client library, simply delete the ReplicationController object.

Once the original is deleted, you can create a new ReplicationController to replace it.  As long
as the old and new `.spec.selector` are the same, then the new one will adopt the old pods.
However, it will not make any effort to make existing pods match a new, different pod template.
To update pods to a new spec in a controlled way, use a [rolling update](#rolling-updates).

### Isolating pods from a ReplicationController

Pods may be removed from a ReplicationController&#39;s target set by changing their labels. This technique may be used to remove pods from service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (assuming that the number of replicas is not also changed).
 --&gt;
&lt;h2 id=&#34;working-with-replicationcontrollers&#34;&gt;ReplicationController 的使用&lt;/h2&gt;
&lt;h3 id=&#34;删除-replicationcontroller-及其-pod&#34;&gt;删除 ReplicationController 及其 Pod&lt;/h3&gt;
&lt;p&gt;要删除一个 ReplicationController 及其所有的 Pod 可以通过命令
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#delete&#34;&gt;&lt;code&gt;kubectl delete&lt;/code&gt;&lt;/a&gt;.
kubectl 会将 ReplicationController 的副本数缩减为 0， 然后等待删除掉它的每一个 Pod，最后删除
ReplicationController 本身。 如果这个 kubectl 命令被打断，可以被重新启动。&lt;/p&gt;
&lt;p&gt;当使用 REST API 或 go 客户端库时， 用户需要显示地进行这两个步骤(将副本数缩减为0，然后等待 Pod 被删除后，
再删除 ReplicationController)。&lt;/p&gt;
&lt;h3 id=&#34;仅删除-replicationcontroller-对象&#34;&gt;仅删除 ReplicationController 对象&lt;/h3&gt;
&lt;p&gt;用户可以只删除 ReplicationController 而不影响任何它的 Pod。&lt;/p&gt;
&lt;p&gt;在使用 kubectl 时，添加 &lt;code&gt;--cascade=false&lt;/code&gt; 选项到  &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#delete&#34;&gt;&lt;code&gt;kubectl delete&lt;/code&gt;&lt;/a&gt;.
在使用 REST API 或 go 客户端库， 直接删除 ReplicationController 对象。
当原来的对象被删除后，可以再创建一个新的 ReplicationController 来替代它。只要原来的对象和新创建的对象
拥有相同的 &lt;code&gt;.spec.selector&lt;/code&gt;， 新的对象就会接管旧的 Pod。 但是，它不会尝试对旧的 Pod 做任何修改以适应新的不同的 Pod 模板。
要受控地更新 Pod 的配置，需要使用 &lt;a href=&#34;#rolling-updates&#34;&gt;滚动更新&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;从-replicationcontroller-剥离-pod&#34;&gt;从 ReplicationController 剥离 Pod&lt;/h3&gt;
&lt;p&gt;可以通过修改 Pod 标签的方式将 Pod 从 ReplicationController 中移出来。 通过这种方式可以将
Pod 从服务从移出来用作测试，数据恢复等。 通过这种方式移除的 Pod 会自动地被替代(假设副本数没有同步修改)&lt;/p&gt;
&lt;!--
## Common usage patterns

### Rescheduling

As mentioned above, whether you have 1 pod you want to keep running, or 1000, a ReplicationController will ensure that the specified number of pods exists, even in the event of node failure or pod termination (for example, due to an action by another control agent).

### Scaling

The ReplicationController makes it easy to scale the number of replicas up or down, either manually or by an auto-scaling control agent, by simply updating the `replicas` field.
 --&gt;
&lt;h2 id=&#34;常见使用方式&#34;&gt;常见使用方式&lt;/h2&gt;
&lt;h3 id=&#34;重新调度&#34;&gt;重新调度&lt;/h3&gt;
&lt;p&gt;就是上面掉到的，无认是想运行 1 个 Pod 还是 1000 个， ReplicationController 就会确保指定数量
的 Pod 存在。 即便某个节点挂了或 Pod被终止(例如，由于其它控制代理的操作)&lt;/p&gt;
&lt;h3 id=&#34;容量变更&#34;&gt;容量变更&lt;/h3&gt;
&lt;p&gt;ReplicationController 使得对副本数量的增加或减少变得很容易， 只需要手动或自动容量控制程序来更新一个 &lt;code&gt;replicas&lt;/code&gt; 字段。&lt;/p&gt;
&lt;!--
### Rolling updates

The ReplicationController is designed to facilitate rolling updates to a service by replacing pods one-by-one.

As explained in [#1353](https://issue.k8s.io/1353), the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.

Ideally, the rolling update controller would take application readiness into account, and would ensure that a sufficient number of pods were productively serving at any given time.

The two ReplicationControllers would need to create pods with at least one differentiating label, such as the image tag of the primary container of the pod, since it is typically image updates that motivate rolling updates.
 --&gt;
&lt;h3 id=&#34;rolling-updates&#34;&gt;滚动更新&lt;/h3&gt;
&lt;p&gt;ReplicationController 设计的初衷便是通过一个一个替换 服务的 Pod 的方式实现滚动更新。&lt;/p&gt;
&lt;p&gt;就如 &lt;a href=&#34;https://issue.k8s.io/1353&#34;&gt;#1353&lt;/a&gt; 解释的那样。推荐的方是创建一个新的包含一个副本的 ReplicationController。
控制器一次一次地将 新的(+1) 然后 旧的 (-1)， 当旧的副本数降为 0 后再把它本身删除。 通过这种可预期的方式更新那认为是非预期的故障。&lt;/p&gt;
&lt;p&gt;理想情况下，滚动更新控制会考虑应用的就绪探针， 会确保有足够数量的 Pod 在任何时间都能有效的提供服务。&lt;/p&gt;
&lt;p&gt;这两个 ReplicationController 创建的 Pod 至少有一个标签是不同的， 例如 Pod 主要容器的镜像标签
因为通过滚动更新的动力就是更新镜像的版本。&lt;/p&gt;
&lt;!--
### Multiple release tracks

In addition to running multiple releases of an application while a rolling update is in progress, it&#39;s common to run multiple releases for an extended period of time, or even continuously, using multiple release tracks. The tracks would be differentiated by labels.

For instance, a service might target all pods with `tier in (frontend), environment in (prod)`.  Now say you have 10 replicated pods that make up this tier.  But you want to be able to &#39;canary&#39; a new version of this component.  You could set up a ReplicationController with `replicas` set to 9 for the bulk of the replicas, with labels `tier=frontend, environment=prod, track=stable`, and another ReplicationController with `replicas` set to 1 for the canary, with labels `tier=frontend, environment=prod, track=canary`.  Now the service is covering both the canary and non-canary pods.  But you can mess with the ReplicationControllers separately to test things out, monitor the results, etc.
 --&gt;
&lt;h3 id=&#34;多版本发布&#34;&gt;多版本发布&lt;/h3&gt;
&lt;p&gt;在滚动更新过程中运行多个版本的基础上，通常也可能运行多个版本一段时间，或长久的运行多个版本。 这些版本之间通过标签来区分。&lt;/p&gt;
&lt;p&gt;比如一个 Service 的标签选择器为 &lt;code&gt;tier in (frontend), environment in (prod)&lt;/code&gt;。 这时候假设这一层由 10 个副本的 Pod 组成。
此时想要在该组件发布一个&amp;quot;金丝雀&amp;quot;的新版本. 这时可以配置一个 9 副本的 ReplicationController 打标签为
&lt;code&gt;tier=frontend, environment=prod, track=stable&lt;/code&gt;， 再配置一个 1 副本的 ReplicationController
打标签为 &lt;code&gt;tier=frontend, environment=prod, track=canary&lt;/code&gt;。 此时 Service 能匹配所以
金丝雀版本和稳定版本的 Pod。 但却可以分不同的 ReplicationController 进行测试和查看监控结果等。&lt;/p&gt;
&lt;!--
### Using ReplicationControllers with Services

Multiple ReplicationControllers can sit behind a single service, so that, for example, some traffic
goes to the old version, and some goes to the new version.

A ReplicationController will never terminate on its own, but it isn&#39;t expected to be as long-lived as services. Services may be composed of pods controlled by multiple ReplicationControllers, and it is expected that many ReplicationControllers may be created and destroyed over the lifetime of a service (for instance, to perform an update of pods that run the service). Both services themselves and their clients should remain oblivious to the ReplicationControllers that maintain the pods of the services.
 --&gt;
&lt;h3 id=&#34;replicationcontroller-配合-service-使用&#34;&gt;ReplicationController 配合 Service 使用&lt;/h3&gt;
&lt;p&gt;Multiple ReplicationControllers can sit behind a single service, so that, for example, some traffic
goes to the old version, and some goes to the new version.
多个 ReplicationController 可以挂在一个 Service 下面， 这样可以让有些流量到新的版本，有的流量到旧的版本。&lt;/p&gt;
&lt;p&gt;ReplicationController 永远都不会把自己干掉， 但也没有期望它可以和 Service 活得一样久。
Service 可能由多个 ReplicationController 控制的 Pod 组成。
在 Service 的生存期内或心预料有许多 ReplicationController 被创建另一些被删除
(例如， 更新 Service 运行的 Pod)。 Service 和它的客户端都不会注意到这些维护 Service Pod 的
ReplicationController。&lt;/p&gt;
&lt;!--
## Writing programs for Replication

Pods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the [RabbitMQ work queues](https://www.rabbitmq.com/tutorials/tutorial-two-python.html), as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for example, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself.
 --&gt;
&lt;!--
## Writing programs for Replication

Pods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the [RabbitMQ work queues](https://www.rabbitmq.com/tutorials/tutorial-two-python.html), as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for example, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself.
  --&gt;
&lt;h2 id=&#34;编写多副本程序&#34;&gt;编写多副本程序&lt;/h2&gt;
&lt;p&gt;ReplicationController 创建的 Pod 在设计上是能够替换的和语义相同的， 尽管经过时间的推移它们的配置可能变得不同。
这很明显适合于无状态多副本服务， 但 ReplicationControllers 也可以用于维护 主选举，分片，
工作池应用的可用性。 这些应用应该有动态的工作分配机制， 例如
&lt;a href=&#34;https://www.rabbitmq.com/tutorials/tutorial-two-python.html&#34;&gt;RabbitMQ 工作队列&lt;/a&gt;,
这与静态的一次性对每个 Pod 的自定义配置相反， 被认为是反范式。
任意对 Pod 的自定义操作，如资源(如，CPU或RAM)资源的垂直自动容量调整， 应该通过另一个
在线控制器程序来执行， 而不是 ReplicationController 本身。
&lt;blockquote&gt;
  &lt;div&gt;&lt;strong&gt;TODO: &lt;/strong&gt;说得不太清楚，需要实践理解后再考虑怎么修改&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;!--
## Responsibilities of the ReplicationController

The ReplicationController simply ensures that the desired number of pods matches its label selector and are operational. Currently, only terminated pods are excluded from its count. In the future, [readiness](https://issue.k8s.io/620) and other information available from the system may be taken into account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external clients to implement arbitrarily sophisticated replacement and/or scale-down policies.

The ReplicationController is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in [#492](https://issue.k8s.io/492)), which would change its `replicas` field. We will not add scheduling policies (for example, [spreading](https://issue.k8s.io/367#issuecomment-48428019)) to the ReplicationController. Nor should it verify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated processes. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere. We even plan to factor out the mechanism for bulk pod creation ([#170](https://issue.k8s.io/170)).

The ReplicationController is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The &#34;macro&#34; operations currently supported by kubectl (run, scale) are proof-of-concept examples of this. For instance, we could imagine something like [Asgard](https://techblog.netflix.com/2012/06/asgard-web-based-cloud-management-and.html) managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.
 --&gt;
&lt;h2 id=&#34;replicationcontroller-的职责&#34;&gt;ReplicationController 的职责&lt;/h2&gt;
&lt;p&gt;ReplicationController 只是确保匹配其标签选择器的 Pod 的数量与期望值相同，并且可用。
目前，只有被终止的 Pod 才不会被计算。 在将来，&lt;a href=&#34;https://issue.k8s.io/620&#34;&gt;就绪探针&lt;/a&gt;
和其它来自系统的可用性信息也会被计入， 我们还可能添加由通过替代策略的控制，
我们还计划发出可以用于外部客户端实现任意广泛代替和缩减容量策略的事件。&lt;/p&gt;
&lt;p&gt;ReplicationController 永远被限制在这个小的责任范围内。 它本身也不会执行就绪探针或存活探针。
不执行自动容量控制， 在设计上就由外部自动容量控制器来控制(就如 &lt;a href=&#34;https://issue.k8s.io/492&#34;&gt;#492&lt;/a&gt; 中讨论的一样)
来修改它的 &lt;code&gt;replicas&lt;/code&gt; 字段。 我们不会添加调度策略(例如，&lt;a href=&#34;https://issue.k8s.io/367#issuecomment-48428019&#34;&gt;spreading&lt;/a&gt;)
到 ReplicationController， 也不会让它来验证它控制的 Pod 是否与当前的 Pod 模板一致，因为这会
妨碍自动容量这得和其它的自动化管理。 类似的 完成死线，顺序信赖，配置扩展和属于其它地方的其它鹅。
我们甚至计划把批量创建 Pod 的机制拆出来。 (&lt;a href=&#34;https://issue.k8s.io/170&#34;&gt;#170&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;ReplicationController 设计上就是一个可组合的构建基础模块。我个预期的是在未来使用更高级的 API 或
工具基于它和其它的互补基础来为用户提供便利。 目前由 kubectl (run, scale) 的宏操作就是证明。
目前我们能想到的一些时，如 &lt;a href=&#34;https://techblog.netflix.com/2012/06/asgard-web-based-cloud-management-and.html&#34;&gt;Asgard&lt;/a&gt;
管理 ReplicationControllers，自动容量管理器， Service, 调度策略，金丝雀等。&lt;/p&gt;
&lt;!--
## API Object

Replication controller is a top-level resource in the Kubernetes REST API. More details about the
API object can be found at:
[ReplicationController API object](/docs/reference/generated/kubernetes-api/v1.19/#replicationcontroller-v1-core).
 --&gt;
&lt;h2 id=&#34;api-对象&#34;&gt;API 对象&lt;/h2&gt;
&lt;p&gt;ReplicationController 是一个在 k8s REST API 中的顶级资源。更多关于 API 对象的信息见:
&lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#replicationcontroller-v1-core&#34;&gt;ReplicationController API 对象&lt;/a&gt;.&lt;/p&gt;
&lt;!--
## Alternatives to ReplicationController

### ReplicaSet

[`ReplicaSet`](/docs/concepts/workloads/controllers/replicaset/) is the next-generation ReplicationController that supports the new [set-based label selector](/docs/concepts/overview/working-with-objects/labels/#set-based-requirement).
It&#39;s mainly used by [Deployment](/docs/concepts/workloads/controllers/deployment/) as a mechanism to orchestrate pod creation, deletion and updates.
Note that we recommend using Deployments instead of directly using Replica Sets, unless you require custom update orchestration or don&#39;t require updates at all.


### Deployment (Recommended)

[`Deployment`](/docs/concepts/workloads/controllers/deployment/) is a higher-level API object that updates its underlying Replica Sets and their Pods. Deployments are recommended if you want this rolling update functionality because, they are declarative, server-side, and have additional features.

### Bare Pods

Unlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node.  A ReplicationController delegates local container restarts to some agent on the node (for example, Kubelet or Docker).

### Job

Use a [`Job`](/docs/concepts/workloads/controllers/job/) instead of a ReplicationController for pods that are expected to terminate on their own
(that is, batch jobs).

### DaemonSet

Use a [`DaemonSet`](/docs/concepts/workloads/controllers/daemonset/) instead of a ReplicationController for pods that provide a
machine-level function, such as machine monitoring or machine logging.  These pods have a lifetime that is tied
to a machine lifetime: the pod needs to be running on the machine before other pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.
 --&gt;
&lt;h2 id=&#34;replicationcontroller-替代方案&#34;&gt;ReplicationController 替代方案&lt;/h2&gt;
&lt;h3 id=&#34;replicaset&#34;&gt;ReplicaSet&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/replicaset/&#34;&gt;&lt;code&gt;ReplicaSet&lt;/code&gt;&lt;/a&gt;&lt;br&gt;
是下一代的 ReplicationController， 它支持新的
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/overview/working-with-objects/labels/#set-based-requirement&#34;&gt;基于集合的标签选择器&lt;/a&gt;.
它主要被 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/deployment/&#34;&gt;Deployment&lt;/a&gt; 用于编排 Pod 创建，删除，更新的一个机制。
还要注意的是我们推荐使用 Deployment 而不是直接使用 ReplicaSet，除非你需要自定义的更新编排或者完全不需要更新。&lt;/p&gt;
&lt;h3 id=&#34;deployment-推荐&#34;&gt;Deployment (推荐)&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/deployment/&#34;&gt;&lt;code&gt;Deployment&lt;/code&gt;&lt;/a&gt;
是一个更高级别的 API 对象，用于更新它下层的 ReplicaSet 和它们的 Pod。
如果想要使用滚动更新推荐使用 Deployment， 因为它们是声明式的，服务端的，还有其它额外的特性。&lt;/p&gt;
&lt;h3 id=&#34;裸-pod&#34;&gt;裸 Pod&lt;/h3&gt;
&lt;p&gt;与直接创建 Pod 的情况不同， ReplicationController 会替换那些因任意原为被删除或终止的 Pod，
例如节点掉挂，或因为升级内格而维护。 因此我们推荐即便应用只需要一个 Pod 也使用 ReplicationController。
可以认为它是一个进程监督者，只是它监督的是多个节点上的多个 Pod 而不是一个节点上的一个进程。
ReplicationController 委托节点上的一些代理(如 kubelet 或 Docker)来重启本地容器。&lt;/p&gt;
&lt;h3 id=&#34;job&#34;&gt;Job&lt;/h3&gt;
&lt;p&gt;如果 Pod 计划中会自己终止(如，批量任务)就应该使用&lt;br&gt;
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/job/&#34;&gt;&lt;code&gt;Job&lt;/code&gt;&lt;/a&gt;
而不是 ReplicationController&lt;/p&gt;
&lt;h3 id=&#34;daemonset&#34;&gt;DaemonSet&lt;/h3&gt;
&lt;p&gt;Use a &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/docs/concepts/workloads/controllers/daemonset/&#34;&gt;&lt;code&gt;DaemonSet&lt;/code&gt;&lt;/a&gt; instead of a ReplicationController for pods that provide a
machine-level function, such as machine monitoring or machine logging.  These pods have a lifetime that is tied
to a machine lifetime: the pod needs to be running on the machine before other pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.&lt;/p&gt;
&lt;p&gt;如果 Pod 提供的是机器级别的功能，如机器监控，机器日志。就应该使用
&lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/docs/concepts/workloads/controllers/daemonset/&#34;&gt;&lt;code&gt;DaemonSet&lt;/code&gt;&lt;/a&gt;
而不是 ReplicationController。 这些 Pod 的生命期与机器绑定: 这些 Pod 需要先于其它的 Pod
在机器上运行， 且它们只有在机器准备重启或关机时才是终止的时候。&lt;/p&gt;
&lt;h2 id=&#34;更多信息&#34;&gt;更多信息&lt;/h2&gt;
&lt;p&gt;实践 &lt;a href=&#34;https://lostsquirrel.github.io/k8sDocs/k8sDocs/tasks/run-application/run-stateless-application-deployment/&#34;&gt;使用 Deployment 运行无状态应用&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
